[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2509.26432v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26432v2",
                "updated": "2025-10-01T11:26:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    11,
                    26,
                    36,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-30T15:53:56Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    53,
                    56,
                    1,
                    273,
                    0
                ],
                "title": "AdaBlock-dLLM: Semantic-Aware Diffusion LLM Inference via Adaptive Block\n  Size",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaBlock-dLLM: Semantic-Aware Diffusion LLM Inference via Adaptive Block\n  Size"
                },
                "summary": "Diffusion-based large language models (dLLMs) are gaining attention for their\ninherent capacity for parallel decoding, offering a compelling alternative to\nautoregressive LLMs. Among various decoding strategies, blockwise\nsemi-autoregressive (semi-AR) approaches are widely adopted due to their\nnatural support for KV caching and their favorable accuracy-speed trade-off.\nHowever, this paper identifies two fundamental limitations in the conventional\nsemi-AR decoding approach that applies a fixed block size: i) late decoding\noverhead, where the unmasking of high-confidence tokens outside the current\nblock is unnecessarily delayed, and ii) premature decoding error, where\nlow-confidence tokens inside the current block are committed too early, leading\nto incorrect tokens. This paper presents the first systematic investigation\nchallenging the fixed block size assumption in semi-AR decoding. Through a\nstatistical analysis of confidence dynamics during the denoising process, we\nidentify a volatility band (VB) region during dLLM decoding, which encodes\nlocal semantic structure and can be used to guide adaptive block sizing.\nLeveraging these insights, we introduce AdaBlock-dLLM, a training-free,\nplug-and-play scheduler that adaptively aligns block boundaries with semantic\nsteps by adjusting block size during runtime. Extensive experiments across\ndiverse benchmarks show that AdaBlock-dLLM achieves up to 5.3% accuracy\nimprovement under the same throughput budget. Beyond inference-time\noptimization, we hope our semantics-aware adaptive scheduling approach and\nconfidence-based analysis will inspire future training strategies for dLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based large language models (dLLMs) are gaining attention for their\ninherent capacity for parallel decoding, offering a compelling alternative to\nautoregressive LLMs. Among various decoding strategies, blockwise\nsemi-autoregressive (semi-AR) approaches are widely adopted due to their\nnatural support for KV caching and their favorable accuracy-speed trade-off.\nHowever, this paper identifies two fundamental limitations in the conventional\nsemi-AR decoding approach that applies a fixed block size: i) late decoding\noverhead, where the unmasking of high-confidence tokens outside the current\nblock is unnecessarily delayed, and ii) premature decoding error, where\nlow-confidence tokens inside the current block are committed too early, leading\nto incorrect tokens. This paper presents the first systematic investigation\nchallenging the fixed block size assumption in semi-AR decoding. Through a\nstatistical analysis of confidence dynamics during the denoising process, we\nidentify a volatility band (VB) region during dLLM decoding, which encodes\nlocal semantic structure and can be used to guide adaptive block sizing.\nLeveraging these insights, we introduce AdaBlock-dLLM, a training-free,\nplug-and-play scheduler that adaptively aligns block boundaries with semantic\nsteps by adjusting block size during runtime. Extensive experiments across\ndiverse benchmarks show that AdaBlock-dLLM achieves up to 5.3% accuracy\nimprovement under the same throughput budget. Beyond inference-time\noptimization, we hope our semantics-aware adaptive scheduling approach and\nconfidence-based analysis will inspire future training strategies for dLLMs."
                },
                "authors": [
                    {
                        "name": "Guanxi Lu"
                    },
                    {
                        "name": "Hao Mark Chen"
                    },
                    {
                        "name": "Yuto Karashima"
                    },
                    {
                        "name": "Zhican Wang"
                    },
                    {
                        "name": "Daichi Fujiki"
                    },
                    {
                        "name": "Hongxiang Fan"
                    }
                ],
                "author_detail": {
                    "name": "Hongxiang Fan"
                },
                "author": "Hongxiang Fan",
                "arxiv_comment": "Preprint. Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26432v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26432v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25454v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25454v2",
                "updated": "2025-10-01T05:09:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    5,
                    9,
                    42,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-29T20:00:29Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    20,
                    0,
                    29,
                    0,
                    272,
                    0
                ],
                "title": "DeepSearch: Overcome the Bottleneck of Reinforcement Learning with\n  Verifiable Rewards via Monte Carlo Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepSearch: Overcome the Bottleneck of Reinforcement Learning with\n  Verifiable Rewards via Monte Carlo Tree Search"
                },
                "summary": "Although RLVR has become an essential component for developing advanced\nreasoning skills in LLMs, contemporary studies have documented training\nplateaus that emerge following thousands of optimization steps, demonstrating\nnotable decreases in performance gains despite increased computational\ninvestment. This limitation stems from the sparse exploration patterns inherent\nin current RLVR practices, where models rely on limited rollouts that often\nmiss critical reasoning paths and fail to provide systematic coverage of the\nsolution space. We present DeepSearch, a framework that integrates Monte Carlo\nTree Search directly into RLVR training. In contrast to existing methods that\nrely on tree search only at inference, DeepSearch embeds structured search into\nthe training loop, enabling systematic exploration and fine-grained credit\nassignment across reasoning steps. Through training-time exploration,\nDeepSearch addresses the fundamental bottleneck of insufficient exploration,\nwhich leads to diminishing performance improvements over prolonged training\nsteps. Our contributions include: (1) a global frontier selection strategy that\nprioritizes promising nodes across the search tree, (2) selection with\nentropy-based guidance that identifies confident paths for supervision, and (3)\nadaptive replay buffer training with solution caching for efficiency.\nExperiments on mathematical reasoning benchmarks show that DeepSearch achieves\n62.95% average accuracy and establishes a new state-of-the-art for 1.5B\nreasoning models - using 5.7x fewer GPU hours than extended training\napproaches. These results highlight the importance of strategic exploration\nover brute-force scaling and demonstrate the promise of algorithmic innovation\nfor advancing RLVR methodologies. DeepSearch establishes a new direction for\nscaling reasoning capabilities through systematic search rather than prolonged\ncomputation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although RLVR has become an essential component for developing advanced\nreasoning skills in LLMs, contemporary studies have documented training\nplateaus that emerge following thousands of optimization steps, demonstrating\nnotable decreases in performance gains despite increased computational\ninvestment. This limitation stems from the sparse exploration patterns inherent\nin current RLVR practices, where models rely on limited rollouts that often\nmiss critical reasoning paths and fail to provide systematic coverage of the\nsolution space. We present DeepSearch, a framework that integrates Monte Carlo\nTree Search directly into RLVR training. In contrast to existing methods that\nrely on tree search only at inference, DeepSearch embeds structured search into\nthe training loop, enabling systematic exploration and fine-grained credit\nassignment across reasoning steps. Through training-time exploration,\nDeepSearch addresses the fundamental bottleneck of insufficient exploration,\nwhich leads to diminishing performance improvements over prolonged training\nsteps. Our contributions include: (1) a global frontier selection strategy that\nprioritizes promising nodes across the search tree, (2) selection with\nentropy-based guidance that identifies confident paths for supervision, and (3)\nadaptive replay buffer training with solution caching for efficiency.\nExperiments on mathematical reasoning benchmarks show that DeepSearch achieves\n62.95% average accuracy and establishes a new state-of-the-art for 1.5B\nreasoning models - using 5.7x fewer GPU hours than extended training\napproaches. These results highlight the importance of strategic exploration\nover brute-force scaling and demonstrate the promise of algorithmic innovation\nfor advancing RLVR methodologies. DeepSearch establishes a new direction for\nscaling reasoning capabilities through systematic search rather than prolonged\ncomputation."
                },
                "authors": [
                    {
                        "name": "Fang Wu"
                    },
                    {
                        "name": "Weihao Xuan"
                    },
                    {
                        "name": "Heli Qi"
                    },
                    {
                        "name": "Ximing Lu"
                    },
                    {
                        "name": "Aaron Tu"
                    },
                    {
                        "name": "Li Erran Li"
                    },
                    {
                        "name": "Yejin Choi"
                    }
                ],
                "author_detail": {
                    "name": "Yejin Choi"
                },
                "author": "Yejin Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25454v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25454v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26541v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26541v1",
                "updated": "2025-09-30T17:15:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    15,
                    27,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T17:15:27Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    15,
                    27,
                    1,
                    273,
                    0
                ],
                "title": "TASP: Topology-aware Sequence Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TASP: Topology-aware Sequence Parallelism"
                },
                "summary": "Long-context large language models (LLMs) face constraints due to the\nquadratic complexity of the self-attention mechanism. The mainstream sequence\nparallelism (SP) method, Ring Attention, attempts to solve this by distributing\nthe query into multiple query chunks across accelerators and enable each Q\ntensor to access all KV tensors from other accelerators via the Ring AllGather\ncommunication primitive. However, it exhibits low communication efficiency,\nrestricting its practical applicability. This inefficiency stems from the\nmismatch between the Ring AllGather communication primitive it adopts and the\nAlltoAll topology of modern accelerators. A Ring AllGather primitive is\ncomposed of iterations of ring-styled data transfer, which can only utilize a\nvery limited fraction of an AlltoAll topology.\n  Inspired by the Hamiltonian decomposition of complete directed graphs, we\nidentify that modern accelerator topology can be decomposed into multiple\northogonal ring datapaths which can concurrently transfer data without\ninterference. Based on this, we further observe that the Ring AllGather\nprimitive can also be decomposed into the same number of concurrent ring-styled\ndata transfer at every iteration. Based on these insights, we propose TASP, a\ntopology-aware SP method for long-context LLMs that fully utilizes the\ncommunication capacity of modern accelerators via topology decomposition and\nprimitive decomposition. Experimental results on both single-node and\nmulti-node NVIDIA H100 systems and a single-node AMD MI300X system demonstrate\nthat TASP achieves higher communication efficiency than Ring Attention on these\nmodern accelerator topologies and achieves up to 3.58 speedup than Ring\nAttention and its variant Zigzag-Ring Attention. The code is available at\nhttps://github.com/infinigence/HamiltonAttention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context large language models (LLMs) face constraints due to the\nquadratic complexity of the self-attention mechanism. The mainstream sequence\nparallelism (SP) method, Ring Attention, attempts to solve this by distributing\nthe query into multiple query chunks across accelerators and enable each Q\ntensor to access all KV tensors from other accelerators via the Ring AllGather\ncommunication primitive. However, it exhibits low communication efficiency,\nrestricting its practical applicability. This inefficiency stems from the\nmismatch between the Ring AllGather communication primitive it adopts and the\nAlltoAll topology of modern accelerators. A Ring AllGather primitive is\ncomposed of iterations of ring-styled data transfer, which can only utilize a\nvery limited fraction of an AlltoAll topology.\n  Inspired by the Hamiltonian decomposition of complete directed graphs, we\nidentify that modern accelerator topology can be decomposed into multiple\northogonal ring datapaths which can concurrently transfer data without\ninterference. Based on this, we further observe that the Ring AllGather\nprimitive can also be decomposed into the same number of concurrent ring-styled\ndata transfer at every iteration. Based on these insights, we propose TASP, a\ntopology-aware SP method for long-context LLMs that fully utilizes the\ncommunication capacity of modern accelerators via topology decomposition and\nprimitive decomposition. Experimental results on both single-node and\nmulti-node NVIDIA H100 systems and a single-node AMD MI300X system demonstrate\nthat TASP achieves higher communication efficiency than Ring Attention on these\nmodern accelerator topologies and achieves up to 3.58 speedup than Ring\nAttention and its variant Zigzag-Ring Attention. The code is available at\nhttps://github.com/infinigence/HamiltonAttention."
                },
                "authors": [
                    {
                        "name": "Yida Wang"
                    },
                    {
                        "name": "Ke Hong"
                    },
                    {
                        "name": "Xiuhong Li"
                    },
                    {
                        "name": "Yuanchao Xu"
                    },
                    {
                        "name": "Wenxun Wang"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "arxiv_affiliation": "Tsinghua University",
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26541v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26541v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23666v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23666v2",
                "updated": "2025-09-30T16:42:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    42,
                    50,
                    1,
                    273,
                    0
                ],
                "published": "2025-05-29T17:12:42Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    12,
                    42,
                    3,
                    149,
                    0
                ],
                "title": "LoLA: Low-Rank Linear Attention With Sparse Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoLA: Low-Rank Linear Attention With Sparse Caching"
                },
                "summary": "The per-token cost of transformer inference scales with context length,\npreventing its application to lifelong in-context learning. Linear attention is\nan efficient alternative that maintains a constant memory footprint, even on\ninfinite context lengths. While this is a potential candidate for lifelong\nlearning, it falls short in memory capacity. In this paper, we propose LoLA, a\ntraining-free augmentation to linear attention that boosts associative recall.\nLoLA distributes past key-value pairs from context into three memory systems:\n(i) recent pairs in a local sliding window cache; (ii) difficult-to-memorize\npairs in a sparse, global cache; and (iii) generic pairs in the recurrent\nhidden state of linear attention. We show through ablations that our\nself-recall error metric is crucial to efficiently manage long-term associative\nmemories. On pass-key retrieval tasks, LoLA improves the base model's\nperformance from 0.6% to 97.4% accuracy. This is achieved with a 4.6x smaller\ncache than Llama-3.1 8B on 4K context length. LoLA also outperforms other 1B\nand 8B parameter subquadratic models on zero-shot commonsense reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The per-token cost of transformer inference scales with context length,\npreventing its application to lifelong in-context learning. Linear attention is\nan efficient alternative that maintains a constant memory footprint, even on\ninfinite context lengths. While this is a potential candidate for lifelong\nlearning, it falls short in memory capacity. In this paper, we propose LoLA, a\ntraining-free augmentation to linear attention that boosts associative recall.\nLoLA distributes past key-value pairs from context into three memory systems:\n(i) recent pairs in a local sliding window cache; (ii) difficult-to-memorize\npairs in a sparse, global cache; and (iii) generic pairs in the recurrent\nhidden state of linear attention. We show through ablations that our\nself-recall error metric is crucial to efficiently manage long-term associative\nmemories. On pass-key retrieval tasks, LoLA improves the base model's\nperformance from 0.6% to 97.4% accuracy. This is achieved with a 4.6x smaller\ncache than Llama-3.1 8B on 4K context length. LoLA also outperforms other 1B\nand 8B parameter subquadratic models on zero-shot commonsense reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Luke McDermott"
                    },
                    {
                        "name": "Robert W. Heath Jr."
                    },
                    {
                        "name": "Rahul Parhi"
                    }
                ],
                "author_detail": {
                    "name": "Rahul Parhi"
                },
                "author": "Rahul Parhi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23666v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23666v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18250v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18250v3",
                "updated": "2025-09-30T15:44:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    44,
                    29,
                    1,
                    273,
                    0
                ],
                "published": "2025-08-25T17:41:13Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    41,
                    13,
                    0,
                    237,
                    0
                ],
                "title": "SOT-MRAM Bitcell Scaling with BEOL Read Selectors: A DTCO Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SOT-MRAM Bitcell Scaling with BEOL Read Selectors: A DTCO Study"
                },
                "summary": "This work explores the cross-node scaling potential of SOT-MRAM for\nlast-level caches (LLCs) under heterogeneous system scaling paradigm. We\nperform extensive Design-Technology Co-Optimization (DTCO) exercises to\nevaluate the bitcell footprint for different cell configurations at a\nrepresentative 7 nm technology and to assess their implications on read and\nwrite power-performance. We crucially identify the MTJ routing struggle in\nconventional two-transistor one-resistor (2T1R) SOT-MRAMs as the primary\nbitcell area scaling challenge and propose to use BEOL read selectors (BEOL\nRSs) that enable (10 -- 40) % bitcell area reduction and eventually match\nsub-N3 SRAM. On writability, we affirm that BEOL RS-based bitcells could meet\nthe required SOT switching current, provided the magnetic free layer properties\nbe engineered in line with LLC-specific, (0.1 -- 100) s retention targets. This\nis particularly to attribute to their (i) more available Si fins for write\ntransistor and (ii) lower bitline resistance at reduced cell width. We\nnevertheless underscore the read tradeoff associated with BEOL RSs, with the\nlow-drive IGZO-FET selector sacrificing the latency up to (3 -- 5) ns and the\nimperfectly rectifying diode selectors suffering (2.5 -- 5)$\\times$ energy cost\nrelative to 2T1R. This article thus highlights the realistic prospects and\nhurdles of BEOL RSs towards holistic power-performance-area scaling of\nSOT-MRAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work explores the cross-node scaling potential of SOT-MRAM for\nlast-level caches (LLCs) under heterogeneous system scaling paradigm. We\nperform extensive Design-Technology Co-Optimization (DTCO) exercises to\nevaluate the bitcell footprint for different cell configurations at a\nrepresentative 7 nm technology and to assess their implications on read and\nwrite power-performance. We crucially identify the MTJ routing struggle in\nconventional two-transistor one-resistor (2T1R) SOT-MRAMs as the primary\nbitcell area scaling challenge and propose to use BEOL read selectors (BEOL\nRSs) that enable (10 -- 40) % bitcell area reduction and eventually match\nsub-N3 SRAM. On writability, we affirm that BEOL RS-based bitcells could meet\nthe required SOT switching current, provided the magnetic free layer properties\nbe engineered in line with LLC-specific, (0.1 -- 100) s retention targets. This\nis particularly to attribute to their (i) more available Si fins for write\ntransistor and (ii) lower bitline resistance at reduced cell width. We\nnevertheless underscore the read tradeoff associated with BEOL RSs, with the\nlow-drive IGZO-FET selector sacrificing the latency up to (3 -- 5) ns and the\nimperfectly rectifying diode selectors suffering (2.5 -- 5)$\\times$ energy cost\nrelative to 2T1R. This article thus highlights the realistic prospects and\nhurdles of BEOL RSs towards holistic power-performance-area scaling of\nSOT-MRAM."
                },
                "authors": [
                    {
                        "name": "Yang Xiang"
                    },
                    {
                        "name": "Fernando García-Redondo"
                    },
                    {
                        "name": "Arvind Sharma"
                    },
                    {
                        "name": "Van Dai Nguyen"
                    },
                    {
                        "name": "Andrea Fantini"
                    },
                    {
                        "name": "Philippe Matagne"
                    },
                    {
                        "name": "Siddharth Rao"
                    },
                    {
                        "name": "Subhali Subhechha"
                    },
                    {
                        "name": "Lynn Verschueren"
                    },
                    {
                        "name": "Mohammed Aftab Baig"
                    },
                    {
                        "name": "Marie Garcia Bardon"
                    },
                    {
                        "name": "Geert Hellings"
                    }
                ],
                "author_detail": {
                    "name": "Geert Hellings"
                },
                "author": "Geert Hellings",
                "arxiv_doi": "10.1109/TED.2025.3617043",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TED.2025.3617043",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.18250v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18250v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to IEEE Trans. Elec. Dev. Work enabled in part by NanoIC\n  pilot line; acquisition and operation jointly funded by Chips Joint\n  Undertaking, through EU's Digital Europe (101183266) and Horizon Europe\n  programs (101183277), as well as by the participating states\n  (Belgium-Flanders, France, Germany, Finland, Ireland, Romania)",
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26328v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26328v1",
                "updated": "2025-09-30T14:40:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    40,
                    18,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T14:40:18Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    40,
                    18,
                    1,
                    273,
                    0
                ],
                "title": "Fast-dLLM v2: Efficient Block-Diffusion LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast-dLLM v2: Efficient Block-Diffusion LLM"
                },
                "summary": "Autoregressive (AR) large language models (LLMs) have achieved remarkable\nperformance across a wide range of natural language tasks, yet their inherent\nsequential decoding limits inference efficiency. In this work, we propose\nFast-dLLM v2, a carefully designed block diffusion language model (dLLM) that\nefficiently adapts pretrained AR models into dLLMs for parallel text\ngeneration, requiring only approximately 1B tokens of fine-tuning. This\nrepresents a 500x reduction in training data compared to full-attention\ndiffusion LLMs such as Dream (580B tokens), while preserving the original\nmodel's performance. Our approach introduces a novel training recipe that\ncombines a block diffusion mechanism with a complementary attention mask,\nenabling blockwise bidirectional context modeling without sacrificing AR\ntraining objectives. To further accelerate decoding, we design a hierarchical\ncaching mechanism: a block-level cache that stores historical context\nrepresentations across blocks, and a sub-block cache that enables efficient\nparallel generation within partially decoded blocks. Coupled with our parallel\ndecoding pipeline, Fast-dLLM v2 achieves up to 2.5x speedup over standard AR\ndecoding without compromising generation quality. Extensive experiments across\ndiverse benchmarks demonstrate that Fast-dLLM v2 matches or surpasses AR\nbaselines in accuracy, while delivering state-of-the-art efficiency among dLLMs\n- marking a significant step toward the practical deployment of fast and\naccurate LLMs. Code and model will be publicly released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) large language models (LLMs) have achieved remarkable\nperformance across a wide range of natural language tasks, yet their inherent\nsequential decoding limits inference efficiency. In this work, we propose\nFast-dLLM v2, a carefully designed block diffusion language model (dLLM) that\nefficiently adapts pretrained AR models into dLLMs for parallel text\ngeneration, requiring only approximately 1B tokens of fine-tuning. This\nrepresents a 500x reduction in training data compared to full-attention\ndiffusion LLMs such as Dream (580B tokens), while preserving the original\nmodel's performance. Our approach introduces a novel training recipe that\ncombines a block diffusion mechanism with a complementary attention mask,\nenabling blockwise bidirectional context modeling without sacrificing AR\ntraining objectives. To further accelerate decoding, we design a hierarchical\ncaching mechanism: a block-level cache that stores historical context\nrepresentations across blocks, and a sub-block cache that enables efficient\nparallel generation within partially decoded blocks. Coupled with our parallel\ndecoding pipeline, Fast-dLLM v2 achieves up to 2.5x speedup over standard AR\ndecoding without compromising generation quality. Extensive experiments across\ndiverse benchmarks demonstrate that Fast-dLLM v2 matches or surpasses AR\nbaselines in accuracy, while delivering state-of-the-art efficiency among dLLMs\n- marking a significant step toward the practical deployment of fast and\naccurate LLMs. Code and model will be publicly released."
                },
                "authors": [
                    {
                        "name": "Chengyue Wu"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Shuchen Xue"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Yonggan Fu"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Enze Xie"
                    }
                ],
                "author_detail": {
                    "name": "Enze Xie"
                },
                "author": "Enze Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26328v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26328v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07966v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07966v4",
                "updated": "2025-09-30T14:13:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    13,
                    20,
                    1,
                    273,
                    0
                ],
                "published": "2025-07-10T17:47:40Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    47,
                    40,
                    3,
                    191,
                    0
                ],
                "title": "Scaling RL to Long Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling RL to Long Videos"
                },
                "summary": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 104K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves\nstrong performance on video benchmarks, reaching 65.1% and 71.1% accuracy on\nVideoMME without and with subtitles, respectively, and consistently\noutperforming LongVILA-7B across multiple benchmarks. Moreover, LongVILA-R1-7B\nsupports processing up to 8,192 video frames per video, and configurable FPS\nsettings. Notably, our MR-SP system achieves up to 2.1x speedup on long video\nRL training. In addition, we release our training system for public\navailability that supports RL training on various modalities (video, text, and\naudio), various models (VILA and Qwen series), and even image and video\ngeneration models. On a single A100 node (8 GPUs), it supports RL training on\nhour-long videos (e.g., 3,600 frames).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 104K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves\nstrong performance on video benchmarks, reaching 65.1% and 71.1% accuracy on\nVideoMME without and with subtitles, respectively, and consistently\noutperforming LongVILA-7B across multiple benchmarks. Moreover, LongVILA-R1-7B\nsupports processing up to 8,192 video frames per video, and configurable FPS\nsettings. Notably, our MR-SP system achieves up to 2.1x speedup on long video\nRL training. In addition, we release our training system for public\navailability that supports RL training on various modalities (video, text, and\naudio), various models (VILA and Qwen series), and even image and video\ngeneration models. On a single A100 node (8 GPUs), it supports RL training on\nhour-long videos (e.g., 3,600 frames)."
                },
                "authors": [
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Baifeng Shi"
                    },
                    {
                        "name": "Qinghao Hu"
                    },
                    {
                        "name": "Hanrong Ye"
                    },
                    {
                        "name": "Ligeng Zhu"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Xiaojuan Qi"
                    },
                    {
                        "name": "Sifei Liu"
                    },
                    {
                        "name": "Hongxu Yin"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Accepted by NeurIPS 2025. Code at https://github.com/NVlabs/Long-RL\n  and model at https://huggingface.co/Efficient-Large-Model/LongVILA-R1-7B",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07966v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07966v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17139v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17139v2",
                "updated": "2025-09-30T09:10:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    9,
                    10,
                    26,
                    1,
                    273,
                    0
                ],
                "published": "2025-02-24T13:30:30Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    30,
                    30,
                    0,
                    55,
                    0
                ],
                "title": "FastCoder: Accelerating Repository-level Code Generation via Efficient\n  Retrieval and Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastCoder: Accelerating Repository-level Code Generation via Efficient\n  Retrieval and Verification"
                },
                "summary": "Code generation is a latency-sensitive task that demands high timeliness.\nHowever, with the growing interest and inherent difficulty in repository-level\ncode generation, most existing code generation studies focus on improving the\ncorrectness of generated code while overlooking the inference efficiency, which\nis substantially affected by the overhead during LLM generation. Although there\nhas been work on accelerating LLM inference, these approaches are not tailored\nto the specific characteristics of code generation; instead, they treat code\nthe same as natural language sequences and ignore its unique syntax and\nsemantic characteristics, which are also crucial for improving efficiency.\nConsequently, these approaches exhibit limited effectiveness in code generation\ntasks, particularly for repository-level scenarios with considerable complexity\nand difficulty. To alleviate this issue, following draft-verification paradigm,\nwe propose FastCoder, a simple yet highly efficient inference acceleration\napproach specifically designed for code generation, without compromising the\nquality of the output. FastCoder constructs a multi-source datastore, providing\naccess to both general and project-specific knowledge, facilitating the\nretrieval of high-quality draft sequences. Moreover, FastCoder reduces the\nretrieval cost by controlling retrieval timing, and enhances efficiency through\nparallel retrieval and a context- and LLM preference-aware cache. Experimental\nresults show that FastCoder can reach up to 2.53x and 2.54x speedup compared to\nautoregressive decoding in repository-level and standalone code generation\ntasks, respectively, outperforming state-of-the-art inference acceleration\napproaches by up to 88%. FastCoder can also be integrated with existing\ncorrectness-focused code generation approaches to accelerate the LLM generation\nprocess, and reach a speedup exceeding 2.6x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code generation is a latency-sensitive task that demands high timeliness.\nHowever, with the growing interest and inherent difficulty in repository-level\ncode generation, most existing code generation studies focus on improving the\ncorrectness of generated code while overlooking the inference efficiency, which\nis substantially affected by the overhead during LLM generation. Although there\nhas been work on accelerating LLM inference, these approaches are not tailored\nto the specific characteristics of code generation; instead, they treat code\nthe same as natural language sequences and ignore its unique syntax and\nsemantic characteristics, which are also crucial for improving efficiency.\nConsequently, these approaches exhibit limited effectiveness in code generation\ntasks, particularly for repository-level scenarios with considerable complexity\nand difficulty. To alleviate this issue, following draft-verification paradigm,\nwe propose FastCoder, a simple yet highly efficient inference acceleration\napproach specifically designed for code generation, without compromising the\nquality of the output. FastCoder constructs a multi-source datastore, providing\naccess to both general and project-specific knowledge, facilitating the\nretrieval of high-quality draft sequences. Moreover, FastCoder reduces the\nretrieval cost by controlling retrieval timing, and enhances efficiency through\nparallel retrieval and a context- and LLM preference-aware cache. Experimental\nresults show that FastCoder can reach up to 2.53x and 2.54x speedup compared to\nautoregressive decoding in repository-level and standalone code generation\ntasks, respectively, outperforming state-of-the-art inference acceleration\napproaches by up to 88%. FastCoder can also be integrated with existing\ncorrectness-focused code generation approaches to accelerate the LLM generation\nprocess, and reach a speedup exceeding 2.6x."
                },
                "authors": [
                    {
                        "name": "Qianhui Zhao"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Fang Liu"
                    },
                    {
                        "name": "Xiaoli Lian"
                    },
                    {
                        "name": "Qiaoyuanhe Meng"
                    },
                    {
                        "name": "Ziqian Jiao"
                    },
                    {
                        "name": "Zetong Zhou"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Lin Shi"
                    }
                ],
                "author_detail": {
                    "name": "Lin Shi"
                },
                "author": "Lin Shi",
                "arxiv_comment": "Accepted by ASE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17139v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17139v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23416v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23416v2",
                "updated": "2025-09-30T02:51:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    2,
                    51,
                    5,
                    1,
                    273,
                    0
                ],
                "published": "2025-05-29T13:05:47Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    13,
                    5,
                    47,
                    3,
                    149,
                    0
                ],
                "title": "KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction"
                },
                "summary": "Transformer-based large language models (LLMs) cache context as key-value\n(KV) pairs during inference. As context length grows, KV cache sizes expand,\nleading to substantial memory overhead and increased attention latency. This\npaper introduces KVzip, a query-agnostic KV cache eviction method enabling\neffective reuse of compressed KV caches across diverse queries. KVzip\nquantifies the importance of a KV pair using the underlying LLM to reconstruct\noriginal contexts from cached KV pairs, subsequently evicting pairs with lower\nimportance. Extensive empirical evaluations demonstrate that KVzip reduces KV\ncache size by $3$-$4\\times$ and FlashAttention decoding latency by\napproximately $2\\times$, with negligible performance loss in\nquestion-answering, retrieval, reasoning, and code comprehension tasks.\nEvaluations include various models such as LLaMA3.1, Qwen2.5, and Gemma3, with\ncontext lengths reaching up to 170K tokens. KVzip significantly outperforms\nexisting query-aware KV eviction methods, which suffer from performance\ndegradation even at a 90% cache budget ratio under multi-query scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) cache context as key-value\n(KV) pairs during inference. As context length grows, KV cache sizes expand,\nleading to substantial memory overhead and increased attention latency. This\npaper introduces KVzip, a query-agnostic KV cache eviction method enabling\neffective reuse of compressed KV caches across diverse queries. KVzip\nquantifies the importance of a KV pair using the underlying LLM to reconstruct\noriginal contexts from cached KV pairs, subsequently evicting pairs with lower\nimportance. Extensive empirical evaluations demonstrate that KVzip reduces KV\ncache size by $3$-$4\\times$ and FlashAttention decoding latency by\napproximately $2\\times$, with negligible performance loss in\nquestion-answering, retrieval, reasoning, and code comprehension tasks.\nEvaluations include various models such as LLaMA3.1, Qwen2.5, and Gemma3, with\ncontext lengths reaching up to 170K tokens. KVzip significantly outperforms\nexisting query-aware KV eviction methods, which suffer from performance\ndegradation even at a 90% cache budget ratio under multi-query scenarios."
                },
                "authors": [
                    {
                        "name": "Jang-Hyun Kim"
                    },
                    {
                        "name": "Jinuk Kim"
                    },
                    {
                        "name": "Sangwoo Kwon"
                    },
                    {
                        "name": "Jae W. Lee"
                    },
                    {
                        "name": "Sangdoo Yun"
                    },
                    {
                        "name": "Hyun Oh Song"
                    }
                ],
                "author_detail": {
                    "name": "Hyun Oh Song"
                },
                "author": "Hyun Oh Song",
                "arxiv_comment": "NeurIPS 2025 Oral. Code: https://github.com/snu-mllab/KVzip",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23416v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23416v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25681v1",
                "updated": "2025-09-30T02:36:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    2,
                    36,
                    11,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T02:36:11Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    2,
                    36,
                    11,
                    1,
                    273,
                    0
                ],
                "title": "dVLA: Diffusion Vision-Language-Action Model with Multimodal\n  Chain-of-Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "dVLA: Diffusion Vision-Language-Action Model with Multimodal\n  Chain-of-Thought"
                },
                "summary": "Vision-Language-Action (VLA) models are emerging as a next-generation\nparadigm for robotics. We introduce dVLA, a diffusion-based VLA that leverages\na multimodal chain-of-thought to unify visual perception, language reasoning,\nand robotic control in a single system. dVLA jointly optimizes perception,\nlanguage understanding, and action under a single diffusion objective, enabling\nstronger cross-modal reasoning and better generalization to novel instructions\nand objects. For practical deployment, we mitigate inference latency by\nincorporating two acceleration strategies, a prefix attention mask and KV\ncaching, yielding up to around times speedup at test-time inference. We\nevaluate dVLA in both simulation and the real world: on the LIBERO benchmark,\nit achieves state-of-the-art performance with a 96.4% average success rate,\nconsistently surpassing both discrete and continuous action policies; on a real\nFranka robot, it succeeds across a diverse task suite, including a challenging\nbin-picking task that requires multi-step planning, demonstrating robust\nreal-world performance. Together, these results underscore the promise of\nunified diffusion frameworks for practical, high-performance VLA robotics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models are emerging as a next-generation\nparadigm for robotics. We introduce dVLA, a diffusion-based VLA that leverages\na multimodal chain-of-thought to unify visual perception, language reasoning,\nand robotic control in a single system. dVLA jointly optimizes perception,\nlanguage understanding, and action under a single diffusion objective, enabling\nstronger cross-modal reasoning and better generalization to novel instructions\nand objects. For practical deployment, we mitigate inference latency by\nincorporating two acceleration strategies, a prefix attention mask and KV\ncaching, yielding up to around times speedup at test-time inference. We\nevaluate dVLA in both simulation and the real world: on the LIBERO benchmark,\nit achieves state-of-the-art performance with a 96.4% average success rate,\nconsistently surpassing both discrete and continuous action policies; on a real\nFranka robot, it succeeds across a diverse task suite, including a challenging\nbin-picking task that requires multi-step planning, demonstrating robust\nreal-world performance. Together, these results underscore the promise of\nunified diffusion frameworks for practical, high-performance VLA robotics."
                },
                "authors": [
                    {
                        "name": "Junjie Wen"
                    },
                    {
                        "name": "Minjie Zhu"
                    },
                    {
                        "name": "Jiaming Liu"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Yicun Yang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Shanghang Zhang"
                    },
                    {
                        "name": "Yichen Zhu"
                    },
                    {
                        "name": "Yi Xu"
                    }
                ],
                "author_detail": {
                    "name": "Yi Xu"
                },
                "author": "Yi Xu",
                "arxiv_comment": "technique report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25401v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25401v1",
                "updated": "2025-09-29T18:57:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    18,
                    57,
                    14,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T18:57:14Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    18,
                    57,
                    14,
                    0,
                    272,
                    0
                ],
                "title": "FlashOmni: A Unified Sparse Attention Engine for Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashOmni: A Unified Sparse Attention Engine for Diffusion Transformers"
                },
                "summary": "Multi-Modal Diffusion Transformers (DiTs) demonstrate exceptional\ncapabilities in visual synthesis, yet their deployment remains constrained by\nsubstantial computational demands. To alleviate this bottleneck, many\nsparsity-based acceleration methods have been proposed. However, their diverse\nsparsity patterns often require customized kernels for high-performance\ninference, limiting universality. We propose FlashOmni, a unified sparse\nattention engine compatible with arbitrary DiT architectures. FlashOmni\nintroduces flexible sparse symbols to standardize the representation of a wide\nrange of sparsity strategies, such as feature caching and block-sparse\nskipping. This unified abstraction enables the execution of diverse sparse\ncomputations within a single attention kernel. In addition, FlashOmni designs\noptimized sparse GEMMs for attention blocks, leveraging sparse symbols to\neliminate redundant computations and further improve efficiency. Experiments\ndemonstrate that FlashOmni delivers near-linear, closely matching the sparsity\nratio speedup (1:1) in attention and GEMM-$Q$, and achieves\n2.5$\\times$-3.8$\\times$ acceleration in GEMM-$O$ (max peaking at about 87.5% of\nthe theoretical limit). Applied with a multi-granularity sparsity strategy, it\nenables the Hunyuan model (33K) to achieve about 1.5$\\times$ end-to-end\nacceleration without degrading visual quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Modal Diffusion Transformers (DiTs) demonstrate exceptional\ncapabilities in visual synthesis, yet their deployment remains constrained by\nsubstantial computational demands. To alleviate this bottleneck, many\nsparsity-based acceleration methods have been proposed. However, their diverse\nsparsity patterns often require customized kernels for high-performance\ninference, limiting universality. We propose FlashOmni, a unified sparse\nattention engine compatible with arbitrary DiT architectures. FlashOmni\nintroduces flexible sparse symbols to standardize the representation of a wide\nrange of sparsity strategies, such as feature caching and block-sparse\nskipping. This unified abstraction enables the execution of diverse sparse\ncomputations within a single attention kernel. In addition, FlashOmni designs\noptimized sparse GEMMs for attention blocks, leveraging sparse symbols to\neliminate redundant computations and further improve efficiency. Experiments\ndemonstrate that FlashOmni delivers near-linear, closely matching the sparsity\nratio speedup (1:1) in attention and GEMM-$Q$, and achieves\n2.5$\\times$-3.8$\\times$ acceleration in GEMM-$O$ (max peaking at about 87.5% of\nthe theoretical limit). Applied with a multi-granularity sparsity strategy, it\nenables the Hunyuan model (33K) to achieve about 1.5$\\times$ end-to-end\nacceleration without degrading visual quality."
                },
                "authors": [
                    {
                        "name": "Liang Qiao"
                    },
                    {
                        "name": "Yue Dai"
                    },
                    {
                        "name": "Yeqi Huang"
                    },
                    {
                        "name": "Hongyu Kan"
                    },
                    {
                        "name": "Jun Shi"
                    },
                    {
                        "name": "Hong An"
                    }
                ],
                "author_detail": {
                    "name": "Hong An"
                },
                "author": "Hong An",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25401v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25401v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25188v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25188v1",
                "updated": "2025-09-29T17:59:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    59,
                    54,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T17:59:54Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    59,
                    54,
                    0,
                    272,
                    0
                ],
                "title": "Learning to Parallel: Accelerating Diffusion Large Language Models via\n  Adaptive Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Parallel: Accelerating Diffusion Large Language Models via\n  Adaptive Parallel Decoding"
                },
                "summary": "Autoregressive decoding in large language models (LLMs) requires\n$\\mathcal{O}(n)$ sequential steps for $n$ tokens, fundamentally limiting\ninference throughput. Recent diffusion-based LLMs (dLLMs) enable parallel token\ngeneration through iterative denoising. However, current parallel decoding\nstrategies rely on fixed, input-agnostic heuristics (e.g., confidence\nthresholds), which fail to adapt to input-specific characteristics, resulting\nin suboptimal speed-quality trade-offs across diverse NLP tasks. In this work,\nwe explore a more flexible and dynamic approach to parallel decoding. We\npropose Learning to Parallel Decode (Learn2PD), a framework that trains a\nlightweight and adaptive filter model to predict, for each token position,\nwhether the current prediction matches the final output. This learned filter\napproximates an oracle parallel decoding strategy that unmasks tokens only when\ncorrectly predicted. Importantly, the filter model is learned in a\npost-training manner, requiring only a small amount of computation to optimize\nit (minute-level GPU time). Additionally, we introduce End-of-Text Prediction\n(EoTP) to detect decoding completion at the end of sequence, avoiding redundant\ndecoding of padding tokens. Experiments on the LLaDA benchmark demonstrate that\nour method achieves up to 22.58$\\times$ speedup without any performance drop,\nand up to 57.51$\\times$ when combined with KV-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive decoding in large language models (LLMs) requires\n$\\mathcal{O}(n)$ sequential steps for $n$ tokens, fundamentally limiting\ninference throughput. Recent diffusion-based LLMs (dLLMs) enable parallel token\ngeneration through iterative denoising. However, current parallel decoding\nstrategies rely on fixed, input-agnostic heuristics (e.g., confidence\nthresholds), which fail to adapt to input-specific characteristics, resulting\nin suboptimal speed-quality trade-offs across diverse NLP tasks. In this work,\nwe explore a more flexible and dynamic approach to parallel decoding. We\npropose Learning to Parallel Decode (Learn2PD), a framework that trains a\nlightweight and adaptive filter model to predict, for each token position,\nwhether the current prediction matches the final output. This learned filter\napproximates an oracle parallel decoding strategy that unmasks tokens only when\ncorrectly predicted. Importantly, the filter model is learned in a\npost-training manner, requiring only a small amount of computation to optimize\nit (minute-level GPU time). Additionally, we introduce End-of-Text Prediction\n(EoTP) to detect decoding completion at the end of sequence, avoiding redundant\ndecoding of padding tokens. Experiments on the LLaDA benchmark demonstrate that\nour method achieves up to 22.58$\\times$ speedup without any performance drop,\nand up to 57.51$\\times$ when combined with KV-Cache."
                },
                "authors": [
                    {
                        "name": "Wenrui Bao"
                    },
                    {
                        "name": "Zhiben Chen"
                    },
                    {
                        "name": "Dan Xu"
                    },
                    {
                        "name": "Yuzhang Shang"
                    }
                ],
                "author_detail": {
                    "name": "Yuzhang Shang"
                },
                "author": "Yuzhang Shang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25188v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25188v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25155v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25155v1",
                "updated": "2025-09-29T17:55:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    55,
                    43,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T17:55:43Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    55,
                    43,
                    0,
                    272,
                    0
                ],
                "title": "Context-Driven Performance Modeling for Causal Inference Operators on\n  Neural Processing Units",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-Driven Performance Modeling for Causal Inference Operators on\n  Neural Processing Units"
                },
                "summary": "The proliferation of large language models (LLMs) has driven demand for long\ncontext inference on resource constrained edge devices. However, deploying\nthese models on Neural Processing Units (NPUs) presents significant challenges\ndue to the architectural mismatch: quadratic complexity of standard attention\nmechanisms conflicts with memory and compute patterns of edge accelerators.\nThis paper presents a comprehensive performance analysis of various causal\ninference operators on a modern NPU. We benchmark standard quadratic attention\nagainst several sub-quadratic alternatives, including structured state-space\nand linear attention models. Our analysis reveals that while sub-quadratic\nmethods offer superior scalability, they introduce distinct computational\nbottlenecks on the NPU's specialized execution units. We identify that\nquadratic attention becomes severely memory-bound, suffering from cache\ninefficiency and pipeline stalls exceeding 95% at long contexts. In contrast,\nsub-quadratic models can become compute-bound on programmable vector cores.\nThese findings provide critical insights for the co-design of hardware-aware\nmodels and optimization strategies to enable on-device AI inference with\nlong-contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of large language models (LLMs) has driven demand for long\ncontext inference on resource constrained edge devices. However, deploying\nthese models on Neural Processing Units (NPUs) presents significant challenges\ndue to the architectural mismatch: quadratic complexity of standard attention\nmechanisms conflicts with memory and compute patterns of edge accelerators.\nThis paper presents a comprehensive performance analysis of various causal\ninference operators on a modern NPU. We benchmark standard quadratic attention\nagainst several sub-quadratic alternatives, including structured state-space\nand linear attention models. Our analysis reveals that while sub-quadratic\nmethods offer superior scalability, they introduce distinct computational\nbottlenecks on the NPU's specialized execution units. We identify that\nquadratic attention becomes severely memory-bound, suffering from cache\ninefficiency and pipeline stalls exceeding 95% at long contexts. In contrast,\nsub-quadratic models can become compute-bound on programmable vector cores.\nThese findings provide critical insights for the co-design of hardware-aware\nmodels and optimization strategies to enable on-device AI inference with\nlong-contexts."
                },
                "authors": [
                    {
                        "name": "Neelesh Gupta"
                    },
                    {
                        "name": "Rakshith Jayanth"
                    },
                    {
                        "name": "Dhruv Parikh"
                    },
                    {
                        "name": "Viktor Prasanna"
                    }
                ],
                "author_detail": {
                    "name": "Viktor Prasanna"
                },
                "author": "Viktor Prasanna",
                "arxiv_comment": "IEEE HiPC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25155v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25155v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02850v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02850v2",
                "updated": "2025-09-29T15:20:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    20,
                    29,
                    0,
                    272,
                    0
                ],
                "published": "2025-06-03T13:19:41Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    13,
                    19,
                    41,
                    1,
                    154,
                    0
                ],
                "title": "METok: Multi-Stage Event-based Token Compression for Efficient Long\n  Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "METok: Multi-Stage Event-based Token Compression for Efficient Long\n  Video Understanding"
                },
                "summary": "Recent advances in Video Large Language Models (VLLMs) have significantly\nenhanced their ability to understand video content. Nonetheless, processing\nlong videos remains challenging due to high computational demands and the\nredundancy present in the visual data. In this work, we propose METok, a\ntraining-free, Multi-stage Event-based Token compression framework designed to\naccelerate VLLMs' inference while preserving accuracy. METok progressively\neliminates redundant visual tokens across three critical stages: (1)\nevent-aware compression during vision encoding, (2) hierarchical token pruning\nin the prefilling stage based on semantic alignment and event importance, and\n(3) a decoding-stage KV Cache optimization that further reduces memory\nconsumption. Our experiments on diverse video benchmarks demonstrate that METok\nachieves an optimal trade-off between efficiency and accuracy by dynamically\nselecting informative visual tokens. For instance, equipping LongVA-7B with\nMETok realizes an 80.6% FLOPs reduction and 93.5% KV Cache memory savings, all\nwhile maintaining comparable or even superior accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Video Large Language Models (VLLMs) have significantly\nenhanced their ability to understand video content. Nonetheless, processing\nlong videos remains challenging due to high computational demands and the\nredundancy present in the visual data. In this work, we propose METok, a\ntraining-free, Multi-stage Event-based Token compression framework designed to\naccelerate VLLMs' inference while preserving accuracy. METok progressively\neliminates redundant visual tokens across three critical stages: (1)\nevent-aware compression during vision encoding, (2) hierarchical token pruning\nin the prefilling stage based on semantic alignment and event importance, and\n(3) a decoding-stage KV Cache optimization that further reduces memory\nconsumption. Our experiments on diverse video benchmarks demonstrate that METok\nachieves an optimal trade-off between efficiency and accuracy by dynamically\nselecting informative visual tokens. For instance, equipping LongVA-7B with\nMETok realizes an 80.6% FLOPs reduction and 93.5% KV Cache memory savings, all\nwhile maintaining comparable or even superior accuracy."
                },
                "authors": [
                    {
                        "name": "Mengyue Wang"
                    },
                    {
                        "name": "Shuo Chen"
                    },
                    {
                        "name": "Kristian Kersting"
                    },
                    {
                        "name": "Volker Tresp"
                    },
                    {
                        "name": "Yunpu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yunpu Ma"
                },
                "author": "Yunpu Ma",
                "arxiv_comment": "EMNLP 2025; 15 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02850v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02850v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16056v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16056v2",
                "updated": "2025-09-29T15:15:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    15,
                    49,
                    0,
                    272,
                    0
                ],
                "published": "2025-05-21T22:13:09Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    22,
                    13,
                    9,
                    2,
                    141,
                    0
                ],
                "title": "Not All Models Suit Expert Offloading: On Local Routing Consistency of\n  Mixture-of-Expert Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Models Suit Expert Offloading: On Local Routing Consistency of\n  Mixture-of-Expert Models"
                },
                "summary": "Mixture-of-Experts (MoE) enables efficient scaling of large language models\n(LLMs) with sparsely activated experts during inference. To effectively deploy\nlarge MoE models on memory-constrained devices, many systems introduce *expert\noffloading* that caches a subset of experts in fast memory, leaving others on\nslow memory to run on CPU or load on demand. While some research has exploited\nthe locality of expert activations, where consecutive tokens activate similar\nexperts, the degree of this **local routing consistency** varies across models\nand remains understudied. In this paper, we propose two metrics to measure\nlocal routing consistency of MoE models: (1) **Segment Routing Best Performance\n(SRP)**, which evaluates how well a fixed group of experts can cover the needs\nof a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which\nmeasures the optimal segment-level cache hit rate under a given cache size\nlimit. We analyzed 20 MoE LLMs with diverse sizes and architectures and found\nthat models that apply MoE on every layer and do not use shared experts exhibit\nthe highest local routing consistency. We further showed that\ndomain-specialized experts contribute more to routing consistency than\nvocabulary-specialized ones, and that most models can balance between cache\neffectiveness and efficiency with cache sizes approximately 2x the active\nexperts. These findings pave the way for memory-efficient MoE design and\ndeployment without compromising inference speed. We publish the code for\nreplicating experiments at https://github.com/ljcleo/moe-lrc .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) enables efficient scaling of large language models\n(LLMs) with sparsely activated experts during inference. To effectively deploy\nlarge MoE models on memory-constrained devices, many systems introduce *expert\noffloading* that caches a subset of experts in fast memory, leaving others on\nslow memory to run on CPU or load on demand. While some research has exploited\nthe locality of expert activations, where consecutive tokens activate similar\nexperts, the degree of this **local routing consistency** varies across models\nand remains understudied. In this paper, we propose two metrics to measure\nlocal routing consistency of MoE models: (1) **Segment Routing Best Performance\n(SRP)**, which evaluates how well a fixed group of experts can cover the needs\nof a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which\nmeasures the optimal segment-level cache hit rate under a given cache size\nlimit. We analyzed 20 MoE LLMs with diverse sizes and architectures and found\nthat models that apply MoE on every layer and do not use shared experts exhibit\nthe highest local routing consistency. We further showed that\ndomain-specialized experts contribute more to routing consistency than\nvocabulary-specialized ones, and that most models can balance between cache\neffectiveness and efficiency with cache sizes approximately 2x the active\nexperts. These findings pave the way for memory-efficient MoE design and\ndeployment without compromising inference speed. We publish the code for\nreplicating experiments at https://github.com/ljcleo/moe-lrc ."
                },
                "authors": [
                    {
                        "name": "Jingcong Liang"
                    },
                    {
                        "name": "Siyuan Wang"
                    },
                    {
                        "name": "Miren Tian"
                    },
                    {
                        "name": "Yitong Li"
                    },
                    {
                        "name": "Duyu Tang"
                    },
                    {
                        "name": "Zhongyu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Zhongyu Wei"
                },
                "author": "Zhongyu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16056v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16056v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24832v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24832v1",
                "updated": "2025-09-29T14:16:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    16,
                    13,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T14:16:13Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    16,
                    13,
                    0,
                    272,
                    0
                ],
                "title": "SemShareKV: Efficient KVCache Sharing for Semantically Similar Prompts\n  via Token-Level LSH Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SemShareKV: Efficient KVCache Sharing for Semantically Similar Prompts\n  via Token-Level LSH Matching"
                },
                "summary": "As large language models (LLMs) continue to scale, the memory footprint of\nkey-value (KV) caches during inference has become a significant bottleneck.\nExisting approaches primarily focus on compressing KV caches within a single\nprompt or reusing shared prefixes or frequently ocurred text segments across\nprompts. However, such strategies are limited in scenarios where prompts are\nsemantically similar but lexically different, which frequently occurs in tasks\nsuch as multi-document summarization and conversational agents. We propose\n\\textit{SemShareKV}, a KV cache sharing and compression framework that\naccelerates LLM inference by reusing KVCache in semantically similar prompts.\nInstead of relying on exact token matches, SemShareKV applies fuzzy token\nmatching using locality-sensitive hashing (LSH) on token embeddings and\nincorporates Rotary Position Embedding (RoPE) to better preserve positional\ninformation. By selectively reusing relevant key-value pairs from a reference\nprompt's cache, SemShareKV reduces redundant computation while maintaining\noutput quality. Experiments on diverse summarization datasets show up to\n6.25$\\times$ speedup and 42\\% lower GPU memory usage with 5k tokens input, with\nnegligible quality degradation. These results highlight the potential of\nsemantic-aware cache sharing for efficient LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to scale, the memory footprint of\nkey-value (KV) caches during inference has become a significant bottleneck.\nExisting approaches primarily focus on compressing KV caches within a single\nprompt or reusing shared prefixes or frequently ocurred text segments across\nprompts. However, such strategies are limited in scenarios where prompts are\nsemantically similar but lexically different, which frequently occurs in tasks\nsuch as multi-document summarization and conversational agents. We propose\n\\textit{SemShareKV}, a KV cache sharing and compression framework that\naccelerates LLM inference by reusing KVCache in semantically similar prompts.\nInstead of relying on exact token matches, SemShareKV applies fuzzy token\nmatching using locality-sensitive hashing (LSH) on token embeddings and\nincorporates Rotary Position Embedding (RoPE) to better preserve positional\ninformation. By selectively reusing relevant key-value pairs from a reference\nprompt's cache, SemShareKV reduces redundant computation while maintaining\noutput quality. Experiments on diverse summarization datasets show up to\n6.25$\\times$ speedup and 42\\% lower GPU memory usage with 5k tokens input, with\nnegligible quality degradation. These results highlight the potential of\nsemantic-aware cache sharing for efficient LLM inference."
                },
                "authors": [
                    {
                        "name": "Xinye Zhao"
                    },
                    {
                        "name": "Spyridon Mastorakis"
                    }
                ],
                "author_detail": {
                    "name": "Spyridon Mastorakis"
                },
                "author": "Spyridon Mastorakis",
                "arxiv_comment": "11 figures, 14pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24832v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24832v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24791v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24791v1",
                "updated": "2025-09-29T13:45:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    13,
                    45,
                    35,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T13:45:35Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    13,
                    45,
                    35,
                    0,
                    272,
                    0
                ],
                "title": "Vision Function Layer in Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Function Layer in Multimodal LLMs"
                },
                "summary": "This study identifies that visual-related functional decoding is distributed\nacross different decoder layers in Multimodal Large Language Models (MLLMs).\nTypically, each function, such as counting, grounding, or OCR recognition,\nnarrows down to two or three layers, which we define as Vision Function Layers\n(VFL). Additionally, the depth and its order of different VFLs exhibits a\nconsistent pattern across different MLLMs, which is well-aligned with human\nbehaviors (e.g., recognition occurs first, followed by counting, and then\ngrounding). These findings are derived from Visual Token Swapping, our novel\nanalytical framework that modifies targeted KV cache entries to precisely\nelucidate layer-specific functions during decoding. Furthermore, these insights\noffer substantial utility in tailoring MLLMs for real-world downstream\napplications. For instance, when LoRA training is selectively applied to VFLs\nwhose functions align with the training data, VFL-LoRA not only outperform\nfull-LoRA but also prevent out-of-domain function forgetting. Moreover, by\nanalyzing the performance differential on training data when particular VFLs\nare ablated, VFL-select automatically classifies data by function, enabling\nhighly efficient data selection to directly bolster corresponding capabilities.\nConsequently, VFL-select surpasses human experts in data selection, and\nachieves 98% of full-data performance with only 20% of the original dataset.\nThis study delivers deeper comprehension of MLLM visual processing, fostering\nthe creation of more efficient, interpretable, and robust models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study identifies that visual-related functional decoding is distributed\nacross different decoder layers in Multimodal Large Language Models (MLLMs).\nTypically, each function, such as counting, grounding, or OCR recognition,\nnarrows down to two or three layers, which we define as Vision Function Layers\n(VFL). Additionally, the depth and its order of different VFLs exhibits a\nconsistent pattern across different MLLMs, which is well-aligned with human\nbehaviors (e.g., recognition occurs first, followed by counting, and then\ngrounding). These findings are derived from Visual Token Swapping, our novel\nanalytical framework that modifies targeted KV cache entries to precisely\nelucidate layer-specific functions during decoding. Furthermore, these insights\noffer substantial utility in tailoring MLLMs for real-world downstream\napplications. For instance, when LoRA training is selectively applied to VFLs\nwhose functions align with the training data, VFL-LoRA not only outperform\nfull-LoRA but also prevent out-of-domain function forgetting. Moreover, by\nanalyzing the performance differential on training data when particular VFLs\nare ablated, VFL-select automatically classifies data by function, enabling\nhighly efficient data selection to directly bolster corresponding capabilities.\nConsequently, VFL-select surpasses human experts in data selection, and\nachieves 98% of full-data performance with only 20% of the original dataset.\nThis study delivers deeper comprehension of MLLM visual processing, fostering\nthe creation of more efficient, interpretable, and robust models."
                },
                "authors": [
                    {
                        "name": "Cheng Shi"
                    },
                    {
                        "name": "Yizhou Yu"
                    },
                    {
                        "name": "Sibei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Sibei Yang"
                },
                "author": "Sibei Yang",
                "arxiv_comment": "Accepted at NeurIPS 2025 (preview; camera-ready in preparation)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24791v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24791v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20776v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20776v3",
                "updated": "2025-09-29T12:34:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    12,
                    34,
                    50,
                    0,
                    272,
                    0
                ],
                "published": "2025-05-27T06:30:00Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    6,
                    30,
                    0,
                    1,
                    147,
                    0
                ],
                "title": "SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long\n  Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long\n  Sequences"
                },
                "summary": "Speculative decoding is a widely used technique for accelerating inference in\nlarge language models (LLMs), but its performance degrades as input length\ngrows, with significant drops even at moderate lengths. Yet, this early\ndegradation has remained largely underexplored. We introduce SpecExtend, a\ndrop-in enhancement that improves speculative decoding on long sequences\nwithout additional training. SpecExtend integrates efficient attention\nmechanisms such as FlashAttention and Hybrid Tree Attention to accelerate\nprefill and verification steps. To improve both draft accuracy and speed on\nlong inputs without retraining, we propose Cross-model Retrieval, a novel KV\ncache eviction strategy that leverages the target model's attention scores to\ndynamically select relevant context for the smaller draft model. Extensive\nevaluations show that SpecExtend accelerates speculative decoding by up to\n2.84x on 16K-token long summarization and up to 3.86x on long reasoning, while\npreserving the short-input performance of state-of-the-art frameworks. Our code\nis available at https://github.com/jycha98/SpecExtend .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is a widely used technique for accelerating inference in\nlarge language models (LLMs), but its performance degrades as input length\ngrows, with significant drops even at moderate lengths. Yet, this early\ndegradation has remained largely underexplored. We introduce SpecExtend, a\ndrop-in enhancement that improves speculative decoding on long sequences\nwithout additional training. SpecExtend integrates efficient attention\nmechanisms such as FlashAttention and Hybrid Tree Attention to accelerate\nprefill and verification steps. To improve both draft accuracy and speed on\nlong inputs without retraining, we propose Cross-model Retrieval, a novel KV\ncache eviction strategy that leverages the target model's attention scores to\ndynamically select relevant context for the smaller draft model. Extensive\nevaluations show that SpecExtend accelerates speculative decoding by up to\n2.84x on 16K-token long summarization and up to 3.86x on long reasoning, while\npreserving the short-input performance of state-of-the-art frameworks. Our code\nis available at https://github.com/jycha98/SpecExtend ."
                },
                "authors": [
                    {
                        "name": "Jungyoub Cha"
                    },
                    {
                        "name": "Hyunjong Kim"
                    },
                    {
                        "name": "Sungzoon Cho"
                    }
                ],
                "author_detail": {
                    "name": "Sungzoon Cho"
                },
                "author": "Sungzoon Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20776v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20776v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24695v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24695v1",
                "updated": "2025-09-29T12:28:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    12,
                    28,
                    9,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T12:28:09Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    12,
                    28,
                    9,
                    0,
                    272,
                    0
                ],
                "title": "SANA-Video: Efficient Video Generation with Block Linear Diffusion\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SANA-Video: Efficient Video Generation with Block Linear Diffusion\n  Transformer"
                },
                "summary": "We introduce SANA-Video, a small diffusion model that can efficiently\ngenerate videos up to 720x1280 resolution and minute-length duration.\nSANA-Video synthesizes high-resolution, high-quality and long videos with\nstrong text-video alignment at a remarkably fast speed, deployable on RTX 5090\nGPU. Two core designs ensure our efficient, effective and long video\ngeneration: (1) Linear DiT: We leverage linear attention as the core operation,\nwhich is more efficient than vanilla attention given the large number of tokens\nprocessed in video generation. (2) Constant-Memory KV cache for Block Linear\nAttention: we design block-wise autoregressive approach for long video\ngeneration by employing a constant-memory state, derived from the cumulative\nproperties of linear attention. This KV cache provides the Linear DiT with\nglobal context at a fixed memory cost, eliminating the need for a traditional\nKV cache and enabling efficient, minute-long video generation. In addition, we\nexplore effective data filters and model training strategies, narrowing the\ntraining cost to 12 days on 64 H100 GPUs, which is only 1% of the cost of\nMovieGen. Given its low cost, SANA-Video achieves competitive performance\ncompared to modern state-of-the-art small diffusion models (e.g., Wan 2.1-1.3B\nand SkyReel-V2-1.3B) while being 16x faster in measured latency. Moreover,\nSANA-Video can be deployed on RTX 5090 GPUs with NVFP4 precision, accelerating\nthe inference speed of generating a 5-second 720p video from 71s to 29s (2.4x\nspeedup). In summary, SANA-Video enables low-cost, high-quality video\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce SANA-Video, a small diffusion model that can efficiently\ngenerate videos up to 720x1280 resolution and minute-length duration.\nSANA-Video synthesizes high-resolution, high-quality and long videos with\nstrong text-video alignment at a remarkably fast speed, deployable on RTX 5090\nGPU. Two core designs ensure our efficient, effective and long video\ngeneration: (1) Linear DiT: We leverage linear attention as the core operation,\nwhich is more efficient than vanilla attention given the large number of tokens\nprocessed in video generation. (2) Constant-Memory KV cache for Block Linear\nAttention: we design block-wise autoregressive approach for long video\ngeneration by employing a constant-memory state, derived from the cumulative\nproperties of linear attention. This KV cache provides the Linear DiT with\nglobal context at a fixed memory cost, eliminating the need for a traditional\nKV cache and enabling efficient, minute-long video generation. In addition, we\nexplore effective data filters and model training strategies, narrowing the\ntraining cost to 12 days on 64 H100 GPUs, which is only 1% of the cost of\nMovieGen. Given its low cost, SANA-Video achieves competitive performance\ncompared to modern state-of-the-art small diffusion models (e.g., Wan 2.1-1.3B\nand SkyReel-V2-1.3B) while being 16x faster in measured latency. Moreover,\nSANA-Video can be deployed on RTX 5090 GPUs with NVFP4 precision, accelerating\nthe inference speed of generating a 5-second 720p video from 71s to 29s (2.4x\nspeedup). In summary, SANA-Video enables low-cost, high-quality video\ngeneration."
                },
                "authors": [
                    {
                        "name": "Junsong Chen"
                    },
                    {
                        "name": "Yuyang Zhao"
                    },
                    {
                        "name": "Jincheng Yu"
                    },
                    {
                        "name": "Ruihang Chu"
                    },
                    {
                        "name": "Junyu Chen"
                    },
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Xianbang Wang"
                    },
                    {
                        "name": "Yicheng Pan"
                    },
                    {
                        "name": "Daquan Zhou"
                    },
                    {
                        "name": "Huan Ling"
                    },
                    {
                        "name": "Haozhe Liu"
                    },
                    {
                        "name": "Hongwei Yi"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Muyang Li"
                    },
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Han Cai"
                    },
                    {
                        "name": "Sanja Fidler"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Enze Xie"
                    }
                ],
                "author_detail": {
                    "name": "Enze Xie"
                },
                "author": "Enze Xie",
                "arxiv_comment": "21 pages, 15 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24695v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24695v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24626v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24626v1",
                "updated": "2025-09-29T11:35:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    11,
                    35,
                    55,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T11:35:55Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    11,
                    35,
                    55,
                    0,
                    272,
                    0
                ],
                "title": "SparseServe: Unlocking Parallelism for Dynamic Sparse Attention in\n  Long-Context LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseServe: Unlocking Parallelism for Dynamic Sparse Attention in\n  Long-Context LLM Serving"
                },
                "summary": "Serving long-context LLMs is costly because attention computation grows\nlinearly with context length. Dynamic sparse attention algorithms (DSAs)\nmitigate this by attending only to the key-value (KV) cache of critical tokens.\nHowever, with DSAs, the main performance bottleneck shifts from HBM bandwidth\nto HBM capacity: KV caches for unselected tokens must remain in HBM for\nlow-latency decoding, constraining parallel batch size and stalling further\nthroughput gains. Offloading these underutilized KV caches to DRAM could free\nHBM capacity, allowing larger parallel batch sizes. Yet, achieving such\nhierarchical HBM-DRAM storage raises new challenges, including fragmented KV\ncache access, HBM cache contention, and high HBM demands of hybrid batching,\nthat remain unresolved in prior work.\n  This paper proposes SparseServe, an LLM serving system that unlocks the\nparallel potential of DSAs through efficient hierarchical HBM-DRAM management.\nSparseServe introduces three key innovations to address the challenges\nmentioned above: (1) fragmentation-aware KV cache transfer, which accelerates\nHBM-DRAM data movement through GPU-direct loading (FlashH2D) and CPU-assisted\nsaving (FlashD2H); (2) working-set-aware batch size control that adjusts batch\nsizes based on real-time working set estimation to minimize HBM cache\nthrashing; (3) layer-segmented prefill that bounds HBM use during prefill to a\nsingle layer, enabling efficient execution even for long prompts. Extensive\nexperimental results demonstrate that SparseServe achieves up to 9.26x lower\nmean time-to-first-token (TTFT) latency and up to 3.14x higher token generation\nthroughput compared to state-of-the-art LLM serving systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving long-context LLMs is costly because attention computation grows\nlinearly with context length. Dynamic sparse attention algorithms (DSAs)\nmitigate this by attending only to the key-value (KV) cache of critical tokens.\nHowever, with DSAs, the main performance bottleneck shifts from HBM bandwidth\nto HBM capacity: KV caches for unselected tokens must remain in HBM for\nlow-latency decoding, constraining parallel batch size and stalling further\nthroughput gains. Offloading these underutilized KV caches to DRAM could free\nHBM capacity, allowing larger parallel batch sizes. Yet, achieving such\nhierarchical HBM-DRAM storage raises new challenges, including fragmented KV\ncache access, HBM cache contention, and high HBM demands of hybrid batching,\nthat remain unresolved in prior work.\n  This paper proposes SparseServe, an LLM serving system that unlocks the\nparallel potential of DSAs through efficient hierarchical HBM-DRAM management.\nSparseServe introduces three key innovations to address the challenges\nmentioned above: (1) fragmentation-aware KV cache transfer, which accelerates\nHBM-DRAM data movement through GPU-direct loading (FlashH2D) and CPU-assisted\nsaving (FlashD2H); (2) working-set-aware batch size control that adjusts batch\nsizes based on real-time working set estimation to minimize HBM cache\nthrashing; (3) layer-segmented prefill that bounds HBM use during prefill to a\nsingle layer, enabling efficient execution even for long prompts. Extensive\nexperimental results demonstrate that SparseServe achieves up to 9.26x lower\nmean time-to-first-token (TTFT) latency and up to 3.14x higher token generation\nthroughput compared to state-of-the-art LLM serving systems."
                },
                "authors": [
                    {
                        "name": "Qihui Zhou"
                    },
                    {
                        "name": "Peiqi Yin"
                    },
                    {
                        "name": "Pengfei Zuo"
                    },
                    {
                        "name": "James Cheng"
                    }
                ],
                "author_detail": {
                    "name": "James Cheng"
                },
                "author": "James Cheng",
                "arxiv_comment": "14 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24626v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24626v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24407v1",
                "updated": "2025-09-29T07:54:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    7,
                    54,
                    44,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T07:54:44Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    7,
                    54,
                    44,
                    0,
                    272,
                    0
                ],
                "title": "Q-REACH: Quantum information Repetition, Error Analysis and Correction\n  using Caching Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Q-REACH: Quantum information Repetition, Error Analysis and Correction\n  using Caching Network"
                },
                "summary": "Quantum repeaters incorporating quantum memory play a pivotal role in\nmitigating loss in transmitted quantum information (photons) due to link\nattenuation over a long-distance quantum communication network. However,\nlimited availability of available storage in such quantum repeaters and the\nimpact on the time spent within the memory unit presents a trade-off between\nquantum information fidelity (a metric that quantifies the degree of similarity\nbetween a pair of quantum states) and qubit transmission rate. Thus, effective\nmanagement of storage time for qubits becomes a key consideration in multi-hop\nquantum networks. To address these challenges, we propose Q-REACH, which\nleverages queuing theory in caching networks to tune qubit transmission rate\nwhile considering fidelity as the cost metric. Our contributions in this work\ninclude (i) utilizing a method of repetition that encodes and broadcasts\nmultiple qubits through different quantum paths, (ii) analytically estimating\nthe time spent by these emitted qubits as a function of the number of paths and\nrepeaters, as well as memory units within a repeater, and (iii) formulating\noptimization problem that leverages this analysis to correct the transmitted\nlogic qubit and select the optimum repetition rate at the transmitter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum repeaters incorporating quantum memory play a pivotal role in\nmitigating loss in transmitted quantum information (photons) due to link\nattenuation over a long-distance quantum communication network. However,\nlimited availability of available storage in such quantum repeaters and the\nimpact on the time spent within the memory unit presents a trade-off between\nquantum information fidelity (a metric that quantifies the degree of similarity\nbetween a pair of quantum states) and qubit transmission rate. Thus, effective\nmanagement of storage time for qubits becomes a key consideration in multi-hop\nquantum networks. To address these challenges, we propose Q-REACH, which\nleverages queuing theory in caching networks to tune qubit transmission rate\nwhile considering fidelity as the cost metric. Our contributions in this work\ninclude (i) utilizing a method of repetition that encodes and broadcasts\nmultiple qubits through different quantum paths, (ii) analytically estimating\nthe time spent by these emitted qubits as a function of the number of paths and\nrepeaters, as well as memory units within a repeater, and (iii) formulating\noptimization problem that leverages this analysis to correct the transmitted\nlogic qubit and select the optimum repetition rate at the transmitter."
                },
                "authors": [
                    {
                        "name": "Karl C. Linne"
                    },
                    {
                        "name": "Yuanyuan Li"
                    },
                    {
                        "name": "Debashri Roy"
                    },
                    {
                        "name": "Kaushik Chowdhury"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Chowdhury"
                },
                "arxiv_affiliation": "Kai Li",
                "author": "Kaushik Chowdhury",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00970v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00970v2",
                "updated": "2025-09-29T05:12:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    5,
                    12,
                    51,
                    0,
                    272,
                    0
                ],
                "published": "2025-04-01T17:08:57Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    8,
                    57,
                    1,
                    91,
                    0
                ],
                "title": "SentenceKV: Efficient LLM Inference via Sentence-Level Semantic KV\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SentenceKV: Efficient LLM Inference via Sentence-Level Semantic KV\n  Caching"
                },
                "summary": "Large language models face significant computational and memory challenges\nwhen processing long contexts. During inference, efficient management of the\nkey-value (KV) cache, which stores intermediate activations for autoregressive\ngeneration, is critical to reducing memory overhead and improving computational\nefficiency. Traditional token-level efficient KV caching methods overlook\nsemantic information, treating tokens independently without considering their\nsemantic relationships. Meanwhile, existing semantic-preserving KV cache\nmanagement approaches often suffer from substantial memory usage and high\ntime-to-first-token. To address these limitations, we propose SentenceKV, a\nnovel sentence-level semantic KV caching approach designed to enhance inference\nefficiency while preserving semantic coherence. During prefilling, SentenceKV\ngroups tokens based on sentence-level semantic similarity, compressing sentence\nrepresentations into concise semantic vectors stored directly on the GPU, while\nindividual KV pairs are offloaded to CPU. During decoding, SentenceKV generates\ntokens by selectively retrieving semantically relevant sentence-level KV\nentries, leveraging the semantic similarity between the prefilling-stage\nsemantic vectors and decoding-stage queries. This ensures efficient and\ncontextually accurate predictions, minimizing the loading of redundant or\nirrelevant data into GPU memory and significantly reducing memory overhead\nwhile maintaining stable inference latency, even for extremely long contexts.\nExtensive evaluations on benchmarks including PG-19, LongBench, and\nNeedle-In-A-Haystack demonstrate that SentenceKV significantly outperforms\nstate-of-the-art methods in both efficiency and memory usage, without\ncompromising model accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models face significant computational and memory challenges\nwhen processing long contexts. During inference, efficient management of the\nkey-value (KV) cache, which stores intermediate activations for autoregressive\ngeneration, is critical to reducing memory overhead and improving computational\nefficiency. Traditional token-level efficient KV caching methods overlook\nsemantic information, treating tokens independently without considering their\nsemantic relationships. Meanwhile, existing semantic-preserving KV cache\nmanagement approaches often suffer from substantial memory usage and high\ntime-to-first-token. To address these limitations, we propose SentenceKV, a\nnovel sentence-level semantic KV caching approach designed to enhance inference\nefficiency while preserving semantic coherence. During prefilling, SentenceKV\ngroups tokens based on sentence-level semantic similarity, compressing sentence\nrepresentations into concise semantic vectors stored directly on the GPU, while\nindividual KV pairs are offloaded to CPU. During decoding, SentenceKV generates\ntokens by selectively retrieving semantically relevant sentence-level KV\nentries, leveraging the semantic similarity between the prefilling-stage\nsemantic vectors and decoding-stage queries. This ensures efficient and\ncontextually accurate predictions, minimizing the loading of redundant or\nirrelevant data into GPU memory and significantly reducing memory overhead\nwhile maintaining stable inference latency, even for extremely long contexts.\nExtensive evaluations on benchmarks including PG-19, LongBench, and\nNeedle-In-A-Haystack demonstrate that SentenceKV significantly outperforms\nstate-of-the-art methods in both efficiency and memory usage, without\ncompromising model accuracy."
                },
                "authors": [
                    {
                        "name": "Yuxuan Zhu"
                    },
                    {
                        "name": "Ali Falahati"
                    },
                    {
                        "name": "David H. Yang"
                    },
                    {
                        "name": "Mohammad Mohammadi Amiri"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Mohammadi Amiri"
                },
                "author": "Mohammad Mohammadi Amiri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00970v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00970v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16257v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16257v2",
                "updated": "2025-09-29T02:46:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    2,
                    46,
                    45,
                    0,
                    272,
                    0
                ],
                "published": "2025-03-20T15:52:43Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    52,
                    43,
                    3,
                    79,
                    0
                ],
                "title": "Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language\n  Models"
                },
                "summary": "Video large language models (VideoLLMs) have demonstrated the capability to\nprocess longer video inputs and enable complex reasoning and analysis. However,\ndue to the thousands of visual tokens from the video frames, the key-value (KV)\ncache can significantly increase memory requirements, becoming a bottleneck for\ninference speed and memory usage. KV cache quantization is a widely used\napproach to address this problem. In this paper, we find that 2-bit KV\nquantization of VideoLLMs can hardly hurt the model performance, while the\nlimit of KV cache quantization in even lower bits has not been investigated. To\nbridge this gap, we introduce VidKV, a plug-and-play KV cache quantization\nmethod to compress the KV cache to lower than 2 bits. Specifically, (1) for\nkey, we propose a mixed-precision quantization strategy in the channel\ndimension, where we perform 2-bit quantization for anomalous channels and 1-bit\nquantization combined with FFT for normal channels; (2) for value, we implement\n1.58-bit quantization while selectively filtering semantically salient visual\ntokens for targeted preservation, for a better trade-off between precision and\nmodel performance. Importantly, our findings suggest that the value cache of\nVideoLLMs should be quantized in a per-channel fashion instead of the per-token\nfashion proposed by prior KV cache quantization works for LLMs. Empirically,\nextensive results with LLaVA-OV-7B and Qwen2.5-VL-7B on six benchmarks show\nthat VidKV effectively compresses the KV cache to 1.5-bit and 1.58-bit\nprecision with almost no performance drop compared to the FP16 counterparts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VideoLLMs) have demonstrated the capability to\nprocess longer video inputs and enable complex reasoning and analysis. However,\ndue to the thousands of visual tokens from the video frames, the key-value (KV)\ncache can significantly increase memory requirements, becoming a bottleneck for\ninference speed and memory usage. KV cache quantization is a widely used\napproach to address this problem. In this paper, we find that 2-bit KV\nquantization of VideoLLMs can hardly hurt the model performance, while the\nlimit of KV cache quantization in even lower bits has not been investigated. To\nbridge this gap, we introduce VidKV, a plug-and-play KV cache quantization\nmethod to compress the KV cache to lower than 2 bits. Specifically, (1) for\nkey, we propose a mixed-precision quantization strategy in the channel\ndimension, where we perform 2-bit quantization for anomalous channels and 1-bit\nquantization combined with FFT for normal channels; (2) for value, we implement\n1.58-bit quantization while selectively filtering semantically salient visual\ntokens for targeted preservation, for a better trade-off between precision and\nmodel performance. Importantly, our findings suggest that the value cache of\nVideoLLMs should be quantized in a per-channel fashion instead of the per-token\nfashion proposed by prior KV cache quantization works for LLMs. Empirically,\nextensive results with LLaVA-OV-7B and Qwen2.5-VL-7B on six benchmarks show\nthat VidKV effectively compresses the KV cache to 1.5-bit and 1.58-bit\nprecision with almost no performance drop compared to the FP16 counterparts."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16257v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16257v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24178v1",
                "updated": "2025-09-29T01:52:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    1,
                    52,
                    10,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T01:52:10Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    1,
                    52,
                    10,
                    0,
                    272,
                    0
                ],
                "title": "BladderFormer: A Streaming Transformer for Real-Time Urological State\n  Monitoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BladderFormer: A Streaming Transformer for Real-Time Urological State\n  Monitoring"
                },
                "summary": "Bladder pressure monitoring systems are increasingly vital in diagnosing and\nmanaging urinary tract dysfunction. Existing solutions rely heavily on\nhand-crafted features and shallow classifiers, limiting their adaptability to\ncomplex signal dynamics. We propose a one-layer streaming transformer model for\nreal-time classification of bladder pressure states, operating on\nwavelet-transformed representations of raw time-series data. Our model\nincorporates temporal multi-head self-attention and state caching, enabling\nefficient online inference with high adaptability. Trained on a dataset of 91\npatients with 20,000-80,000 samples each, our method demonstrates improved\naccuracy, higher energy- and latency-efficiency. Implementation considerations\nfor edge deployment on low-power hardware, such as edge graphical processing\nunits (GPU) and micro-controllers, are also discussed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bladder pressure monitoring systems are increasingly vital in diagnosing and\nmanaging urinary tract dysfunction. Existing solutions rely heavily on\nhand-crafted features and shallow classifiers, limiting their adaptability to\ncomplex signal dynamics. We propose a one-layer streaming transformer model for\nreal-time classification of bladder pressure states, operating on\nwavelet-transformed representations of raw time-series data. Our model\nincorporates temporal multi-head self-attention and state caching, enabling\nefficient online inference with high adaptability. Trained on a dataset of 91\npatients with 20,000-80,000 samples each, our method demonstrates improved\naccuracy, higher energy- and latency-efficiency. Implementation considerations\nfor edge deployment on low-power hardware, such as edge graphical processing\nunits (GPU) and micro-controllers, are also discussed."
                },
                "authors": [
                    {
                        "name": "Chengwei Zhou"
                    },
                    {
                        "name": "Steve Majerus"
                    },
                    {
                        "name": "Gourav Datta"
                    }
                ],
                "author_detail": {
                    "name": "Gourav Datta"
                },
                "author": "Gourav Datta",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24088v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24088v1",
                "updated": "2025-09-28T21:47:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    28,
                    21,
                    47,
                    20,
                    6,
                    271,
                    0
                ],
                "published": "2025-09-28T21:47:20Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    21,
                    47,
                    20,
                    6,
                    271,
                    0
                ],
                "title": "CORRECT: COndensed eRror RECognition via knowledge Transfer in\n  multi-agent systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CORRECT: COndensed eRror RECognition via knowledge Transfer in\n  multi-agent systems"
                },
                "summary": "Multi-agent systems (MAS) are increasingly capable of tackling complex\nreal-world tasks, yet their reliance on inter-agent coordination, tool use, and\nlong-horizon reasoning makes error recognition particularly challenging. Minor\nerrors can propagate across agents, escalating into task failures while\nproducing long, intertwined execution trajectories that impose significant\ncosts for both human developers and automated systems to debug and analyze. Our\nkey insight is that, despite surface differences in failure trajectories (e.g.,\nlogs), MAS errors often recur with similar structural patterns. This paper\npresents CORRECT, the first lightweight, training-free framework that leverages\nan online cache of distilled error schemata to recognize and transfer knowledge\nof failure structures across new requests. This cache-based reuse allows LLMs\nto perform targeted error localization at inference time, avoiding the need for\nexpensive retraining while adapting to dynamic MAS deployments in subseconds.\nTo support rigorous study in this domain, we also introduce CORRECT-Error, a\nlarge-scale dataset of over 2,000 annotated trajectories collected through a\nnovel error-injection pipeline guided by real-world distributions, and further\nvalidated through human evaluation to ensure alignment with natural failure\npatterns. Experiments across seven diverse MAS applications show that CORRECT\nimproves step-level error localization up to 19.8% over existing advances while\nat near-zero overhead, substantially narrowing the gap between automated and\nhuman-level error recognition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent systems (MAS) are increasingly capable of tackling complex\nreal-world tasks, yet their reliance on inter-agent coordination, tool use, and\nlong-horizon reasoning makes error recognition particularly challenging. Minor\nerrors can propagate across agents, escalating into task failures while\nproducing long, intertwined execution trajectories that impose significant\ncosts for both human developers and automated systems to debug and analyze. Our\nkey insight is that, despite surface differences in failure trajectories (e.g.,\nlogs), MAS errors often recur with similar structural patterns. This paper\npresents CORRECT, the first lightweight, training-free framework that leverages\nan online cache of distilled error schemata to recognize and transfer knowledge\nof failure structures across new requests. This cache-based reuse allows LLMs\nto perform targeted error localization at inference time, avoiding the need for\nexpensive retraining while adapting to dynamic MAS deployments in subseconds.\nTo support rigorous study in this domain, we also introduce CORRECT-Error, a\nlarge-scale dataset of over 2,000 annotated trajectories collected through a\nnovel error-injection pipeline guided by real-world distributions, and further\nvalidated through human evaluation to ensure alignment with natural failure\npatterns. Experiments across seven diverse MAS applications show that CORRECT\nimproves step-level error localization up to 19.8% over existing advances while\nat near-zero overhead, substantially narrowing the gap between automated and\nhuman-level error recognition."
                },
                "authors": [
                    {
                        "name": "Yifan Yu"
                    },
                    {
                        "name": "Moyan Li"
                    },
                    {
                        "name": "Shaoyuan Xu"
                    },
                    {
                        "name": "Jinmiao Fu"
                    },
                    {
                        "name": "Xinhai Hou"
                    },
                    {
                        "name": "Fan Lai"
                    },
                    {
                        "name": "Bryan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Wang"
                },
                "author": "Bryan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24088v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24088v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24007v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24007v1",
                "updated": "2025-09-28T17:59:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    28,
                    17,
                    59,
                    15,
                    6,
                    271,
                    0
                ],
                "published": "2025-09-28T17:59:15Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    17,
                    59,
                    15,
                    6,
                    271,
                    0
                ],
                "title": "Sequential Diffusion Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential Diffusion Language Models"
                },
                "summary": "Diffusion language models (DLMs) have strong theoretical efficiency but are\nlimited by fixed-length decoding and incompatibility with key-value (KV)\ncaches. Block diffusion mitigates these issues, yet still enforces a fixed\nblock size and requires expensive training. We introduce Next Sequence\nPrediction (NSP), which unifies next-token and next-block prediction, enabling\nthe model to adaptively determine the generation length at each step. When the\nlength is fixed to 1, NSP reduces to standard next-token prediction. Building\non NSP, we propose Sequential Diffusion Language Model (SDLM), which can\nretrofit pre-trained autoregressive language models (ALMs) at minimal cost.\nSpecifically, SDLM performs diffusion inference within fixed-size mask blocks,\nbut dynamically decodes consecutive subsequences based on model confidence,\nthereby preserving KV-cache compatibility and improving robustness to varying\nuncertainty and semantics across the sequence. Experiments show that SDLM\nmatches or surpasses strong autoregressive baselines using only 3.5M training\nsamples, while achieving 2.1 higher throughput than Qwen-2.5. Notably, the\nSDLM-32B model delivers even more pronounced efficiency gains, demonstrating\nthe strong scalability potential of our modeling paradigm. Project page and\ncodes: https://github.com/OpenGVLab/SDLM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models (DLMs) have strong theoretical efficiency but are\nlimited by fixed-length decoding and incompatibility with key-value (KV)\ncaches. Block diffusion mitigates these issues, yet still enforces a fixed\nblock size and requires expensive training. We introduce Next Sequence\nPrediction (NSP), which unifies next-token and next-block prediction, enabling\nthe model to adaptively determine the generation length at each step. When the\nlength is fixed to 1, NSP reduces to standard next-token prediction. Building\non NSP, we propose Sequential Diffusion Language Model (SDLM), which can\nretrofit pre-trained autoregressive language models (ALMs) at minimal cost.\nSpecifically, SDLM performs diffusion inference within fixed-size mask blocks,\nbut dynamically decodes consecutive subsequences based on model confidence,\nthereby preserving KV-cache compatibility and improving robustness to varying\nuncertainty and semantics across the sequence. Experiments show that SDLM\nmatches or surpasses strong autoregressive baselines using only 3.5M training\nsamples, while achieving 2.1 higher throughput than Qwen-2.5. Notably, the\nSDLM-32B model delivers even more pronounced efficiency gains, demonstrating\nthe strong scalability potential of our modeling paradigm. Project page and\ncodes: https://github.com/OpenGVLab/SDLM"
                },
                "authors": [
                    {
                        "name": "Yangzhou Liu"
                    },
                    {
                        "name": "Yue Cao"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Gen Luo"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Weiyun Wang"
                    },
                    {
                        "name": "Xiaobo Liang"
                    },
                    {
                        "name": "Biqing Qi"
                    },
                    {
                        "name": "Lijun Wu"
                    },
                    {
                        "name": "Changyao Tian"
                    },
                    {
                        "name": "Yanting Zhang"
                    },
                    {
                        "name": "Yuqiang Li"
                    },
                    {
                        "name": "Tong Lu"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Jifeng Dai"
                    },
                    {
                        "name": "Wenhai Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wenhai Wang"
                },
                "author": "Wenhai Wang",
                "arxiv_comment": "14 pages, 5 figures, technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24007v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24007v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23928v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23928v1",
                "updated": "2025-09-28T15:05:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    28,
                    15,
                    5,
                    21,
                    6,
                    271,
                    0
                ],
                "published": "2025-09-28T15:05:21Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    15,
                    5,
                    21,
                    6,
                    271,
                    0
                ],
                "title": "HiViS: Hiding Visual Tokens from the Drafter for Speculative Decoding in\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HiViS: Hiding Visual Tokens from the Drafter for Speculative Decoding in\n  Vision-Language Models"
                },
                "summary": "Speculative decoding is an effective approach for accelerating inference in\nLarge Language models (LLMs), but its adaptation to Vision-Language models\n(VLMs) remains challenging for additional visual tokens in multimodal inputs.\nFirst, owing to the fact that the drafter and the target VLM may derived from\ndifferent families, the semantic representations of visual tokens in the target\nVLM are misaligned with those in the drafter, introducing bias into the\nKV-cache during the prefill stage. Second, the large number of visual tokens\nsubstantially slows down the drafter's self-attention during the decoding\nstage. We propose Hiding Visual Tokens from the Drafter for Speculative\nDecoding in Vision-Language Models (HiViS), an explicit-implicit input\ndecomposition framework that alleviates the above inefficiency. All visual\ntokens are removed from the drafter's input, retaining only textual tokens as\nexplicit inputs, while directly reusing the target VLM's corresponding\nlast-layer hidden states as implicit visual information without additional\nprocessing. To train the drafter efficiently, we introduces multi-step\nself-feedback training strategy with dynamic data selection and sequential\nembedding supervision to simulate reasoning during training. Our approach\ncompresses the prefill sequence length of the drafter to only 0.7%-1.3% of the\ntarget VLM's input, while maintaining lossless generation quality. Extensive\nexperiments across diverse models and tasks demonstrate up to 2.65x speedup,\nconfirming the effectiveness of HiViS in accelerating VLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is an effective approach for accelerating inference in\nLarge Language models (LLMs), but its adaptation to Vision-Language models\n(VLMs) remains challenging for additional visual tokens in multimodal inputs.\nFirst, owing to the fact that the drafter and the target VLM may derived from\ndifferent families, the semantic representations of visual tokens in the target\nVLM are misaligned with those in the drafter, introducing bias into the\nKV-cache during the prefill stage. Second, the large number of visual tokens\nsubstantially slows down the drafter's self-attention during the decoding\nstage. We propose Hiding Visual Tokens from the Drafter for Speculative\nDecoding in Vision-Language Models (HiViS), an explicit-implicit input\ndecomposition framework that alleviates the above inefficiency. All visual\ntokens are removed from the drafter's input, retaining only textual tokens as\nexplicit inputs, while directly reusing the target VLM's corresponding\nlast-layer hidden states as implicit visual information without additional\nprocessing. To train the drafter efficiently, we introduces multi-step\nself-feedback training strategy with dynamic data selection and sequential\nembedding supervision to simulate reasoning during training. Our approach\ncompresses the prefill sequence length of the drafter to only 0.7%-1.3% of the\ntarget VLM's input, while maintaining lossless generation quality. Extensive\nexperiments across diverse models and tasks demonstrate up to 2.65x speedup,\nconfirming the effectiveness of HiViS in accelerating VLM inference."
                },
                "authors": [
                    {
                        "name": "Zhinan Xie"
                    },
                    {
                        "name": "Peisong Wang"
                    },
                    {
                        "name": "Jian Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Jian Cheng"
                },
                "author": "Jian Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23928v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23928v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09081v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09081v2",
                "updated": "2025-09-28T08:32:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    28,
                    8,
                    32,
                    26,
                    6,
                    271,
                    0
                ],
                "published": "2025-05-14T02:29:46Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    2,
                    29,
                    46,
                    2,
                    134,
                    0
                ],
                "title": "SALM: A Multi-Agent Framework for Language Model-Driven Social Network\n  Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SALM: A Multi-Agent Framework for Language Model-Driven Social Network\n  Simulation"
                },
                "summary": "Contemporary approaches to agent-based modeling (ABM) of social systems have\ntraditionally emphasized rule-based behaviors, limiting their ability to\ncapture nuanced dynamics by moving beyond predefined rules and leveraging\ncontextual understanding from LMs of human social interaction. This paper\npresents SALM (Social Agent LM Framework), a novel approach for integrating\nlanguage models (LMs) into social network simulation that achieves\nunprecedented temporal stability in multi-agent scenarios. Our primary\ncontributions include: (1) a hierarchical prompting architecture enabling\nstable simulation beyond 4,000 timesteps while reducing token usage by 73%, (2)\nan attention-based memory system achieving 80% cache hit rates (95% CI [78%,\n82%]) with sub-linear memory growth of 9.5%, and (3) formal bounds on\npersonality stability. Through extensive validation against SNAP ego networks,\nwe demonstrate the first LLM-based framework capable of modeling long-term\nsocial phenomena while maintaining empirically validated behavioral fidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary approaches to agent-based modeling (ABM) of social systems have\ntraditionally emphasized rule-based behaviors, limiting their ability to\ncapture nuanced dynamics by moving beyond predefined rules and leveraging\ncontextual understanding from LMs of human social interaction. This paper\npresents SALM (Social Agent LM Framework), a novel approach for integrating\nlanguage models (LMs) into social network simulation that achieves\nunprecedented temporal stability in multi-agent scenarios. Our primary\ncontributions include: (1) a hierarchical prompting architecture enabling\nstable simulation beyond 4,000 timesteps while reducing token usage by 73%, (2)\nan attention-based memory system achieving 80% cache hit rates (95% CI [78%,\n82%]) with sub-linear memory growth of 9.5%, and (3) formal bounds on\npersonality stability. Through extensive validation against SNAP ego networks,\nwe demonstrate the first LLM-based framework capable of modeling long-term\nsocial phenomena while maintaining empirically validated behavioral fidelity."
                },
                "authors": [
                    {
                        "name": "Gaurav Koley"
                    }
                ],
                "author_detail": {
                    "name": "Gaurav Koley"
                },
                "author": "Gaurav Koley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09081v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09081v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23601v1",
                "updated": "2025-09-28T03:12:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    28,
                    3,
                    12,
                    43,
                    6,
                    271,
                    0
                ],
                "published": "2025-09-28T03:12:43Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    3,
                    12,
                    43,
                    6,
                    271,
                    0
                ],
                "title": "VAMamba: An Efficient Visual Adaptive Mamba for Image Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VAMamba: An Efficient Visual Adaptive Mamba for Image Restoration"
                },
                "summary": "Recent Mamba-based image restoration methods have achieved promising results\nbut remain\n  limited by fixed scanning patterns and inefficient feature utilization.\nConventional Mamba\n  architectures rely on predetermined paths that cannot adapt to diverse\ndegradations, constraining\n  both restoration performance and computational efficiency. To overcome these\nlimitations, we\n  propose VAMamba, a Visual Adaptive Mamba framework with two key innovations.\nFirst,\n  QCLAM(Queue-basedCacheLow-rankAdaptiveMemory)enhancesfeaturelearningthrougha\n  FIFO cache that stores historical representations. Similarity between current\nLoRA-adapted and\n  cached features guides intelligent fusion, enabling dynamic reuse while\neffectively controlling\n  memorygrowth.Second, GPS-SS2D(GreedyPathScanSS2D)introducesadaptive scanning.\nA\n  Vision Transformer generates score maps to estimate pixel importance, and a\ngreedy strategy de termines optimal forward and backward scanning paths. These\nlearned trajectories replace rigid\n  patterns, enabling SS2D to perform targeted feature extraction. The\nintegration of QCLAM and\n  GPS-SS2D allows VAMamba to adaptively focus on degraded regions while\nmaintaining high\n  computational efficiency. Extensive experiments across diverse restoration\ntasks demonstrate\n  that VAMamba consistently outperforms existing approaches in both restoration\nquality and\n  efficiency, establishing new benchmarks for adaptive image restoration. Our\ncode is available\n  at https://github.com/WaterHQH/VAMamba.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Mamba-based image restoration methods have achieved promising results\nbut remain\n  limited by fixed scanning patterns and inefficient feature utilization.\nConventional Mamba\n  architectures rely on predetermined paths that cannot adapt to diverse\ndegradations, constraining\n  both restoration performance and computational efficiency. To overcome these\nlimitations, we\n  propose VAMamba, a Visual Adaptive Mamba framework with two key innovations.\nFirst,\n  QCLAM(Queue-basedCacheLow-rankAdaptiveMemory)enhancesfeaturelearningthrougha\n  FIFO cache that stores historical representations. Similarity between current\nLoRA-adapted and\n  cached features guides intelligent fusion, enabling dynamic reuse while\neffectively controlling\n  memorygrowth.Second, GPS-SS2D(GreedyPathScanSS2D)introducesadaptive scanning.\nA\n  Vision Transformer generates score maps to estimate pixel importance, and a\ngreedy strategy de termines optimal forward and backward scanning paths. These\nlearned trajectories replace rigid\n  patterns, enabling SS2D to perform targeted feature extraction. The\nintegration of QCLAM and\n  GPS-SS2D allows VAMamba to adaptively focus on degraded regions while\nmaintaining high\n  computational efficiency. Extensive experiments across diverse restoration\ntasks demonstrate\n  that VAMamba consistently outperforms existing approaches in both restoration\nquality and\n  efficiency, establishing new benchmarks for adaptive image restoration. Our\ncode is available\n  at https://github.com/WaterHQH/VAMamba."
                },
                "authors": [
                    {
                        "name": "Han Hu"
                    },
                    {
                        "name": "Zhuoran Zheng"
                    },
                    {
                        "name": "Liang Li"
                    },
                    {
                        "name": "Chen Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Chen Lyu"
                },
                "author": "Chen Lyu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09072v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09072v2",
                "updated": "2025-09-27T20:13:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    27,
                    20,
                    13,
                    25,
                    5,
                    270,
                    0
                ],
                "published": "2025-08-12T16:47:48Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    16,
                    47,
                    48,
                    1,
                    224,
                    0
                ],
                "title": "READER: Retrieval-Assisted Drafter for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "READER: Retrieval-Assisted Drafter for Efficient LLM Inference"
                },
                "summary": "Autoregressive Language Models instantiate a factorized likelihood over token\nsequences, yet their strictly sequential decoding process imposes an intrinsic\nlower bound on inference latency. This bottleneck has emerged as a central\nobstacle to the scalable deployment of large-scale generative models. Existing\nacceleration techniques partially mitigate token-level latency by relying on\nauxiliary draft models or introducing an additional training phase, but fail to\naddress the dominant memory and communication costs. We present READER, a\nprovably lossless speculative decoding framework that bypasses the training of\nthe auxiliary draft model. READER formalizes speculative decoding as a\nstochastic tree construction problem and exploits the empirical redundancy\nstructure of natural language to generate high-probability candidate\ncontinuations. Our method revisits the problem of constructing draft trees,\nestablishing substantial statistical improvements over stochastic draft-tree\nmethods and providing a complexity-theoretic analysis that characterizes the\noptimality frontier of speculative decoding under bounded computation and\nmemory resources. Beyond the single-sequence regime traditionally considered in\nprior work, we introduce a memory-optimal key-value cache-serving strategy that\nguarantees amortized sublinear overhead in the batch dimension, allowing READER\nto scale to realistic inference workloads. Comprehensive experiments\ndemonstrate up to 6.13x wall-clock speedup on single-prompt inference and up to\n5.92x on batched inference, consistently surpassing prior speculative decoding\nbaselines, while preserving exact output equivalence, with even more pronounced\ngains in retrieval-augmented generation pipelines. Our results close a key gap\nbetween theoretical parallelism limits and practical LLM inference, suggesting\na new standard for efficient deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Language Models instantiate a factorized likelihood over token\nsequences, yet their strictly sequential decoding process imposes an intrinsic\nlower bound on inference latency. This bottleneck has emerged as a central\nobstacle to the scalable deployment of large-scale generative models. Existing\nacceleration techniques partially mitigate token-level latency by relying on\nauxiliary draft models or introducing an additional training phase, but fail to\naddress the dominant memory and communication costs. We present READER, a\nprovably lossless speculative decoding framework that bypasses the training of\nthe auxiliary draft model. READER formalizes speculative decoding as a\nstochastic tree construction problem and exploits the empirical redundancy\nstructure of natural language to generate high-probability candidate\ncontinuations. Our method revisits the problem of constructing draft trees,\nestablishing substantial statistical improvements over stochastic draft-tree\nmethods and providing a complexity-theoretic analysis that characterizes the\noptimality frontier of speculative decoding under bounded computation and\nmemory resources. Beyond the single-sequence regime traditionally considered in\nprior work, we introduce a memory-optimal key-value cache-serving strategy that\nguarantees amortized sublinear overhead in the batch dimension, allowing READER\nto scale to realistic inference workloads. Comprehensive experiments\ndemonstrate up to 6.13x wall-clock speedup on single-prompt inference and up to\n5.92x on batched inference, consistently surpassing prior speculative decoding\nbaselines, while preserving exact output equivalence, with even more pronounced\ngains in retrieval-augmented generation pipelines. Our results close a key gap\nbetween theoretical parallelism limits and practical LLM inference, suggesting\na new standard for efficient deployment."
                },
                "authors": [
                    {
                        "name": "Maxim Divilkovskiy"
                    },
                    {
                        "name": "Vitaly Malygin"
                    },
                    {
                        "name": "Sergey Zlobin"
                    },
                    {
                        "name": "Stanislav Ilyushin"
                    },
                    {
                        "name": "Sultan Isali"
                    },
                    {
                        "name": "Vasily Kalugin"
                    },
                    {
                        "name": "Nuriza Aitassova"
                    },
                    {
                        "name": "Fei Yi"
                    },
                    {
                        "name": "Weidi Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Weidi Zeng"
                },
                "author": "Weidi Zeng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09072v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09072v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23179v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23179v1",
                "updated": "2025-09-27T08:15:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    27,
                    8,
                    15,
                    17,
                    5,
                    270,
                    0
                ],
                "published": "2025-09-27T08:15:17Z",
                "published_parsed": [
                    2025,
                    9,
                    27,
                    8,
                    15,
                    17,
                    5,
                    270,
                    0
                ],
                "title": "A Near-Cache Architectural Framework for Cryptographic Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Near-Cache Architectural Framework for Cryptographic Computing"
                },
                "summary": "Recent advancements in post-quantum cryptographic algorithms have led to\ntheir standardization by the National Institute of Standards and Technology\n(NIST) to safeguard information security in the post-quantum era. These\nalgorithms, however, employ public keys and signatures that are 3 to 9$\\times$\nlonger than those used in pre-quantum cryptography, resulting in significant\nperformance and energy efficiency overheads. A critical bottleneck identified\nin our analysis is the cache bandwidth. This limitation motivates the adoption\nof on-chip in-/near-cache computing, a computing paradigm that offers\nhigh-performance, exceptional energy efficiency, and flexibility to accelerate\npost-quantum cryptographic algorithms. Our analysis of existing works reveals\nchallenges in integrating in-/near-cache computing into modern computer systems\nand performance limitations due to external bandwidth limitation, highlighting\nthe need for innovative solutions that can seamlessly integrate into existing\nsystems without performance and energy efficiency issues. In this paper, we\nintroduce a near-cache-slice computing paradigm with support of customization\nand virtual address, named Crypto-Near-Cache (CNC), designed to accelerate\npost-quantum cryptographic algorithms and other applications. By placing SRAM\narrays with bitline computing capability near cache slices, high internal\nbandwidth and short data movement are achieved with native support of virtual\naddressing. An ISA extension to facilitate CNC is also proposed, with detailed\ndiscussion on the implementation aspects of the core/cache datapath.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in post-quantum cryptographic algorithms have led to\ntheir standardization by the National Institute of Standards and Technology\n(NIST) to safeguard information security in the post-quantum era. These\nalgorithms, however, employ public keys and signatures that are 3 to 9$\\times$\nlonger than those used in pre-quantum cryptography, resulting in significant\nperformance and energy efficiency overheads. A critical bottleneck identified\nin our analysis is the cache bandwidth. This limitation motivates the adoption\nof on-chip in-/near-cache computing, a computing paradigm that offers\nhigh-performance, exceptional energy efficiency, and flexibility to accelerate\npost-quantum cryptographic algorithms. Our analysis of existing works reveals\nchallenges in integrating in-/near-cache computing into modern computer systems\nand performance limitations due to external bandwidth limitation, highlighting\nthe need for innovative solutions that can seamlessly integrate into existing\nsystems without performance and energy efficiency issues. In this paper, we\nintroduce a near-cache-slice computing paradigm with support of customization\nand virtual address, named Crypto-Near-Cache (CNC), designed to accelerate\npost-quantum cryptographic algorithms and other applications. By placing SRAM\narrays with bitline computing capability near cache slices, high internal\nbandwidth and short data movement are achieved with native support of virtual\naddressing. An ISA extension to facilitate CNC is also proposed, with detailed\ndiscussion on the implementation aspects of the core/cache datapath."
                },
                "authors": [
                    {
                        "name": "Jingyao Zhang"
                    },
                    {
                        "name": "Elaheh Sadredini"
                    }
                ],
                "author_detail": {
                    "name": "Elaheh Sadredini"
                },
                "author": "Elaheh Sadredini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23179v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23179v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17138v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17138v4",
                "updated": "2025-09-27T07:41:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    27,
                    7,
                    41,
                    38,
                    5,
                    270,
                    0
                ],
                "published": "2025-05-22T06:12:42Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    6,
                    12,
                    42,
                    3,
                    142,
                    0
                ],
                "title": "Runtime Adaptive Pruning for LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Runtime Adaptive Pruning for LLM Inference"
                },
                "summary": "Large language models (LLMs) excel at language understanding and generation,\nbut their enormous computational and memory requirements hinder deployment.\nCompression offers a potential solution to mitigate these constraints. However,\nmost existing methods rely on fixed heuristics and thus fail to adapt to\nruntime memory variations or heterogeneous KV-cache demands arising from\ndiverse user requests. To address these limitations, we propose RAP, an elastic\npruning framework driven by reinforcement learning (RL) that dynamically\nadjusts compression strategies in a runtime-aware manner. Specifically, RAP\ndynamically tracks the evolving ratio between model parameters and KV-cache\nacross practical execution. Recognizing that FFNs house most parameters,\nwhereas parameter -light attention layers dominate KV-cache formation, the RL\nagent retains only those components that maximize utility within the current\nmemory budget, conditioned on instantaneous workload and device state.\nExtensive experiments results demonstrate that RAP outperforms state-of-the-art\nbaselines, marking the first time to jointly consider model weights and\nKV-cache on the fly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at language understanding and generation,\nbut their enormous computational and memory requirements hinder deployment.\nCompression offers a potential solution to mitigate these constraints. However,\nmost existing methods rely on fixed heuristics and thus fail to adapt to\nruntime memory variations or heterogeneous KV-cache demands arising from\ndiverse user requests. To address these limitations, we propose RAP, an elastic\npruning framework driven by reinforcement learning (RL) that dynamically\nadjusts compression strategies in a runtime-aware manner. Specifically, RAP\ndynamically tracks the evolving ratio between model parameters and KV-cache\nacross practical execution. Recognizing that FFNs house most parameters,\nwhereas parameter -light attention layers dominate KV-cache formation, the RL\nagent retains only those components that maximize utility within the current\nmemory budget, conditioned on instantaneous workload and device state.\nExtensive experiments results demonstrate that RAP outperforms state-of-the-art\nbaselines, marking the first time to jointly consider model weights and\nKV-cache on the fly."
                },
                "authors": [
                    {
                        "name": "Huanrong Liu"
                    },
                    {
                        "name": "Chunlin Tian"
                    },
                    {
                        "name": "Xuyang Wei"
                    },
                    {
                        "name": "Qingbiao Li"
                    },
                    {
                        "name": "Li Li"
                    }
                ],
                "author_detail": {
                    "name": "Li Li"
                },
                "author": "Li Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17138v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17138v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23094v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23094v1",
                "updated": "2025-09-27T04:07:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    27,
                    4,
                    7,
                    23,
                    5,
                    270,
                    0
                ],
                "published": "2025-09-27T04:07:23Z",
                "published_parsed": [
                    2025,
                    9,
                    27,
                    4,
                    7,
                    23,
                    5,
                    270,
                    0
                ],
                "title": "d$^2$Cache: Accelerating Diffusion-Based LLMs via Dual Adaptive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "d$^2$Cache: Accelerating Diffusion-Based LLMs via Dual Adaptive Caching"
                },
                "summary": "Diffusion-based large language models (dLLMs), despite their promising\nperformance, still suffer from inferior inference efficiency. This is because\ndLLMs rely on bidirectional attention and cannot directly benefit from the\nstandard key-value (KV) cache as autoregressive models (ARMs) do. To tackle\nthis issue, we introduce \\textit{Dual aDaptive Cache} (d$^2$Cache), which is a\ntraining-free approximate KV cache framework for accelerating dLLM inference.\nd$^2$Cache features a two-stage fine-grained selection strategy to identify\ntokens and adaptively update their KV states at each decoding step, while\ncaching the KV states of the remaining tokens for reuse. Furthermore,\nd$^2$Cache naturally offers a more reliable decoding alternative, which can\nenable quasi left-to-right generation and mitigate premature overconfidence in\ntokens at the end of the sequence. Extensive experimental results on two\nrepresentative dLLMs (\\ie, LLaDA and Dream) demonstrate that d$^2$Cache not\nonly achieves substantial inference speedups, but also yields consistent\nimprovements in generation quality. The code is available at\nhttps://github.com/Kamichanw/d2Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based large language models (dLLMs), despite their promising\nperformance, still suffer from inferior inference efficiency. This is because\ndLLMs rely on bidirectional attention and cannot directly benefit from the\nstandard key-value (KV) cache as autoregressive models (ARMs) do. To tackle\nthis issue, we introduce \\textit{Dual aDaptive Cache} (d$^2$Cache), which is a\ntraining-free approximate KV cache framework for accelerating dLLM inference.\nd$^2$Cache features a two-stage fine-grained selection strategy to identify\ntokens and adaptively update their KV states at each decoding step, while\ncaching the KV states of the remaining tokens for reuse. Furthermore,\nd$^2$Cache naturally offers a more reliable decoding alternative, which can\nenable quasi left-to-right generation and mitigate premature overconfidence in\ntokens at the end of the sequence. Extensive experimental results on two\nrepresentative dLLMs (\\ie, LLaDA and Dream) demonstrate that d$^2$Cache not\nonly achieves substantial inference speedups, but also yields consistent\nimprovements in generation quality. The code is available at\nhttps://github.com/Kamichanw/d2Cache."
                },
                "authors": [
                    {
                        "name": "Yuchu Jiang"
                    },
                    {
                        "name": "Yue Cai"
                    },
                    {
                        "name": "Xiangzhong Luo"
                    },
                    {
                        "name": "Jiale Fu"
                    },
                    {
                        "name": "Jiarui Wang"
                    },
                    {
                        "name": "Chonghan Liu"
                    },
                    {
                        "name": "Xu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xu Yang"
                },
                "author": "Xu Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23094v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23094v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24357v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24357v3",
                "updated": "2025-09-27T03:37:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    27,
                    3,
                    37,
                    40,
                    5,
                    270,
                    0
                ],
                "published": "2025-05-30T08:49:27Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    8,
                    49,
                    27,
                    4,
                    150,
                    0
                ],
                "title": "ReCalKV: Low-Rank KV Cache Compression via Head Reordering and Offline\n  Calibration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReCalKV: Low-Rank KV Cache Compression via Head Reordering and Offline\n  Calibration"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable performance, but\ntheir long-context reasoning remains constrained by the excessive memory\nrequired for the Key-Value (KV) cache. This makes KV cache compression a\ncritical step toward efficient long-context inference. Recent methods have\nexplored low-rank techniques to reduce the hidden size of the KV cache.\nHowever, they neglect the distinct roles and varying importance of Keys and\nValues, leading to significant performance drops under high compression. To\naddress this, we propose ReCalKV, a post-training low-rank KV cache compression\napproach with tailored strategies for Keys and Values. For Keys, we propose\nHead-wise Similarity aware Reordering (HSR), which clusters structurally\nsimilar heads into groups, enabling more accurate low-rank approximation via\ngrouped SVD. For Values, we propose Offline Value Calibration (OVC), which\nefficiently calibrates the value projection matrix using calibration data\nwithout training, ensuring an accurate representation of contextual\ninformation. Extensive experiments show that ReCalKV consistently outperforms\nexisting low-rank compression methods, achieving high compression ratios with\nminimal performance loss. The code and models will be available\nat:https://github.com/XIANGLONGYAN/ReCalKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable performance, but\ntheir long-context reasoning remains constrained by the excessive memory\nrequired for the Key-Value (KV) cache. This makes KV cache compression a\ncritical step toward efficient long-context inference. Recent methods have\nexplored low-rank techniques to reduce the hidden size of the KV cache.\nHowever, they neglect the distinct roles and varying importance of Keys and\nValues, leading to significant performance drops under high compression. To\naddress this, we propose ReCalKV, a post-training low-rank KV cache compression\napproach with tailored strategies for Keys and Values. For Keys, we propose\nHead-wise Similarity aware Reordering (HSR), which clusters structurally\nsimilar heads into groups, enabling more accurate low-rank approximation via\ngrouped SVD. For Values, we propose Offline Value Calibration (OVC), which\nefficiently calibrates the value projection matrix using calibration data\nwithout training, ensuring an accurate representation of contextual\ninformation. Extensive experiments show that ReCalKV consistently outperforms\nexisting low-rank compression methods, achieving high compression ratios with\nminimal performance loss. The code and models will be available\nat:https://github.com/XIANGLONGYAN/ReCalKV."
                },
                "authors": [
                    {
                        "name": "Xianglong Yan"
                    },
                    {
                        "name": "Zhiteng Li"
                    },
                    {
                        "name": "Tianao Zhang"
                    },
                    {
                        "name": "Haotong Qin"
                    },
                    {
                        "name": "Linghe Kong"
                    },
                    {
                        "name": "Yulun Zhang"
                    },
                    {
                        "name": "Xiaokang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaokang Yang"
                },
                "author": "Xiaokang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24357v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24357v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03771v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03771v4",
                "updated": "2025-09-26T21:40:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    21,
                    40,
                    58,
                    4,
                    269,
                    0
                ],
                "published": "2025-02-06T04:16:20Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    4,
                    16,
                    20,
                    3,
                    37,
                    0
                ],
                "title": "vCache: Verified Semantic Prompt Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vCache: Verified Semantic Prompt Caching"
                },
                "summary": "Semantic caches return cached responses for semantically similar prompts to\nreduce LLM inference latency and cost. They embed cached prompts and store them\nalongside their response in a vector database. Embedding similarity metrics\nassign a numerical score to quantify the similarity between a request and its\nnearest neighbor prompt from the cache. Existing systems use the same static\nsimilarity threshold across all requests to determine whether two prompts can\nshare similar responses. However, we observe that static thresholds do not give\nformal correctness guarantees, can result in unexpected error rates, and lead\nto suboptimal cache hit rates. This paper proposes vCache, the first verified\nsemantic cache with user-defined error rate guarantees. It employs an online\nlearning algorithm to estimate an optimal threshold for each cached prompt,\nenabling reliable cache responses without additional training. Our experiments\nshow that vCache consistently meets the specified error bounds while\noutperforming state-of-the-art static-threshold and fine-tuned embedding\nbaselines. We release the vCache implementation and three benchmarks to support\nfuture research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic caches return cached responses for semantically similar prompts to\nreduce LLM inference latency and cost. They embed cached prompts and store them\nalongside their response in a vector database. Embedding similarity metrics\nassign a numerical score to quantify the similarity between a request and its\nnearest neighbor prompt from the cache. Existing systems use the same static\nsimilarity threshold across all requests to determine whether two prompts can\nshare similar responses. However, we observe that static thresholds do not give\nformal correctness guarantees, can result in unexpected error rates, and lead\nto suboptimal cache hit rates. This paper proposes vCache, the first verified\nsemantic cache with user-defined error rate guarantees. It employs an online\nlearning algorithm to estimate an optimal threshold for each cached prompt,\nenabling reliable cache responses without additional training. Our experiments\nshow that vCache consistently meets the specified error bounds while\noutperforming state-of-the-art static-threshold and fine-tuned embedding\nbaselines. We release the vCache implementation and three benchmarks to support\nfuture research."
                },
                "authors": [
                    {
                        "name": "Luis Gaspar Schroeder"
                    },
                    {
                        "name": "Aditya Desai"
                    },
                    {
                        "name": "Alejandro Cuadron"
                    },
                    {
                        "name": "Kyle Chu"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Stephan Krusche"
                    },
                    {
                        "name": "Alfons Kemper"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Matei Zaharia"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    }
                ],
                "author_detail": {
                    "name": "Joseph E. Gonzalez"
                },
                "author": "Joseph E. Gonzalez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03771v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03771v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22875v1",
                "updated": "2025-09-26T19:40:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    19,
                    40,
                    33,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T19:40:33Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    19,
                    40,
                    33,
                    4,
                    269,
                    0
                ],
                "title": "On KV-Poisson Structure and related invariants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On KV-Poisson Structure and related invariants"
                },
                "summary": "We propose an deepened analysis of KV-Poisson structures of on IR^2. We\npresent their classification their properties an their possible applications in\ndifferent domains. We prove that these structure give rise to a new\nCohomological invariant. We explicitly compute the Cohomological groups of some\nof these structures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose an deepened analysis of KV-Poisson structures of on IR^2. We\npresent their classification their properties an their possible applications in\ndifferent domains. We prove that these structure give rise to a new\nCohomological invariant. We explicitly compute the Cohomological groups of some\nof these structures."
                },
                "authors": [
                    {
                        "name": "Prosper Rosaire Mama Assandje"
                    },
                    {
                        "name": "Herguey Mopeng"
                    },
                    {
                        "name": "Joseph Dongho"
                    }
                ],
                "author_detail": {
                    "name": "Joseph Dongho"
                },
                "author": "Joseph Dongho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.DG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.DG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08799v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08799v2",
                "updated": "2025-09-26T17:59:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    59,
                    54,
                    4,
                    269,
                    0
                ],
                "published": "2025-07-11T17:59:36Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    17,
                    59,
                    36,
                    4,
                    192,
                    0
                ],
                "title": "KV Cache Steering for Controlling Frozen LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Cache Steering for Controlling Frozen LLMs"
                },
                "summary": "We propose cache steering, a lightweight method for implicit steering of\nlanguage models via a one-shot intervention applied directly to the key-value\ncache. To validate its effectiveness, we apply cache steering to induce\nchain-of-thought reasoning in small language models. Our approach constructs\nsteering vectors from reasoning traces, obtained either from teacher models\n(e.g., GPT-4o) or existing human annotations, that shift model behavior toward\nmore explicit, multi-step reasoning without fine-tuning or prompt\nmodifications. Experimental evaluations on diverse reasoning benchmarks\ndemonstrate that cache steering improves both the qualitative structure of\nmodel reasoning and quantitative task performance. Additional experiments show\nthat the method also scales to larger models and yields further gains on\nchallenging datasets such as GPQA and MATH. Compared to prior activation\nsteering techniques that require continuous interventions, our one-shot cache\nsteering offers substantial advantages in terms of inference latency,\nhyperparameter stability, and ease of integration with existing inference APIs.\nBeyond mere reasoning induction, we show that cache steering enables\ncontrollable transfer of reasoning styles (e.g., stepwise, causal, analogical),\nmaking it a practical tool for behavior-level guidance of language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose cache steering, a lightweight method for implicit steering of\nlanguage models via a one-shot intervention applied directly to the key-value\ncache. To validate its effectiveness, we apply cache steering to induce\nchain-of-thought reasoning in small language models. Our approach constructs\nsteering vectors from reasoning traces, obtained either from teacher models\n(e.g., GPT-4o) or existing human annotations, that shift model behavior toward\nmore explicit, multi-step reasoning without fine-tuning or prompt\nmodifications. Experimental evaluations on diverse reasoning benchmarks\ndemonstrate that cache steering improves both the qualitative structure of\nmodel reasoning and quantitative task performance. Additional experiments show\nthat the method also scales to larger models and yields further gains on\nchallenging datasets such as GPQA and MATH. Compared to prior activation\nsteering techniques that require continuous interventions, our one-shot cache\nsteering offers substantial advantages in terms of inference latency,\nhyperparameter stability, and ease of integration with existing inference APIs.\nBeyond mere reasoning induction, we show that cache steering enables\ncontrollable transfer of reasoning styles (e.g., stepwise, causal, analogical),\nmaking it a practical tool for behavior-level guidance of language models."
                },
                "authors": [
                    {
                        "name": "Max Belitsky"
                    },
                    {
                        "name": "Dawid J. Kopiczko"
                    },
                    {
                        "name": "Michael Dorkenwald"
                    },
                    {
                        "name": "M. Jehanzeb Mirza"
                    },
                    {
                        "name": "James R. Glass"
                    },
                    {
                        "name": "Cees G. M. Snoek"
                    },
                    {
                        "name": "Yuki M. Asano"
                    }
                ],
                "author_detail": {
                    "name": "Yuki M. Asano"
                },
                "author": "Yuki M. Asano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08799v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08799v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22622v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22622v1",
                "updated": "2025-09-26T17:48:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    48,
                    24,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T17:48:24Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    48,
                    24,
                    4,
                    269,
                    0
                ],
                "title": "LongLive: Real-time Interactive Long Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongLive: Real-time Interactive Long Video Generation"
                },
                "summary": "We present LongLive, a frame-level autoregressive (AR) framework for\nreal-time and interactive long video generation. Long video generation presents\nchallenges in both efficiency and quality. Diffusion and Diffusion-Forcing\nmodels can produce high-quality videos but suffer from low efficiency due to\nbidirectional attention. Causal attention AR models support KV caching for\nfaster inference, but often degrade in quality on long videos due to memory\nchallenges during long-video training. In addition, beyond static prompt-based\ngeneration, interactive capabilities, such as streaming prompt inputs, are\ncritical for dynamic content creation, enabling users to guide narratives in\nreal time. This interactive requirement significantly increases complexity,\nespecially in ensuring visual consistency and semantic coherence during prompt\ntransitions. To address these challenges, LongLive adopts a causal, frame-level\nAR design that integrates a KV-recache mechanism that refreshes cached states\nwith new prompts for smooth, adherent switches; streaming long tuning to enable\nlong video training and to align training and inference (train-long-test-long);\nand short window attention paired with a frame-level attention sink, shorten as\nframe sink, preserving long-range consistency while enabling faster generation.\nWith these key designs, LongLive fine-tunes a 1.3B-parameter short-clip model\nto minute-long generation in just 32 GPU-days. At inference, LongLive sustains\n20.7 FPS on a single NVIDIA H100, achieves strong performance on VBench in both\nshort and long videos. LongLive supports up to 240-second videos on a single\nH100 GPU. LongLive further supports INT8-quantized inference with only marginal\nquality loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present LongLive, a frame-level autoregressive (AR) framework for\nreal-time and interactive long video generation. Long video generation presents\nchallenges in both efficiency and quality. Diffusion and Diffusion-Forcing\nmodels can produce high-quality videos but suffer from low efficiency due to\nbidirectional attention. Causal attention AR models support KV caching for\nfaster inference, but often degrade in quality on long videos due to memory\nchallenges during long-video training. In addition, beyond static prompt-based\ngeneration, interactive capabilities, such as streaming prompt inputs, are\ncritical for dynamic content creation, enabling users to guide narratives in\nreal time. This interactive requirement significantly increases complexity,\nespecially in ensuring visual consistency and semantic coherence during prompt\ntransitions. To address these challenges, LongLive adopts a causal, frame-level\nAR design that integrates a KV-recache mechanism that refreshes cached states\nwith new prompts for smooth, adherent switches; streaming long tuning to enable\nlong video training and to align training and inference (train-long-test-long);\nand short window attention paired with a frame-level attention sink, shorten as\nframe sink, preserving long-range consistency while enabling faster generation.\nWith these key designs, LongLive fine-tunes a 1.3B-parameter short-clip model\nto minute-long generation in just 32 GPU-days. At inference, LongLive sustains\n20.7 FPS on a single NVIDIA H100, achieves strong performance on VBench in both\nshort and long videos. LongLive supports up to 240-second videos on a single\nH100 GPU. LongLive further supports INT8-quantized inference with only marginal\nquality loss."
                },
                "authors": [
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Ruihang Chu"
                    },
                    {
                        "name": "Yicheng Xiao"
                    },
                    {
                        "name": "Yuyang Zhao"
                    },
                    {
                        "name": "Xianbang Wang"
                    },
                    {
                        "name": "Muyang Li"
                    },
                    {
                        "name": "Enze Xie"
                    },
                    {
                        "name": "Yingcong Chen"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Yukang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yukang Chen"
                },
                "author": "Yukang Chen",
                "arxiv_comment": "Code, model, and demos are available at\n  https://github.com/NVlabs/LongLive",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22622v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22622v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22548v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22548v1",
                "updated": "2025-09-26T16:29:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    29,
                    37,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T16:29:37Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    29,
                    37,
                    4,
                    269,
                    0
                ],
                "title": "JanusVLN: Decoupling Semantics and Spatiality with Dual Implicit Memory\n  for Vision-Language Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JanusVLN: Decoupling Semantics and Spatiality with Dual Implicit Memory\n  for Vision-Language Navigation"
                },
                "summary": "Vision-and-Language Navigation requires an embodied agent to navigate through\nunseen environments, guided by natural language instructions and a continuous\nvideo stream. Recent advances in VLN have been driven by the powerful semantic\nunderstanding of Multimodal Large Language Models. However, these methods\ntypically rely on explicit semantic memory, such as building textual cognitive\nmaps or storing historical visual frames. This type of method suffers from\nspatial information loss, computational redundancy, and memory bloat, which\nimpede efficient navigation. Inspired by the implicit scene representation in\nhuman navigation, analogous to the left brain's semantic understanding and the\nright brain's spatial cognition, we propose JanusVLN, a novel VLN framework\nfeaturing a dual implicit neural memory that models spatial-geometric and\nvisual-semantic memory as separate, compact, and fixed-size neural\nrepresentations. This framework first extends the MLLM to incorporate 3D prior\nknowledge from the spatial-geometric encoder, thereby enhancing the spatial\nreasoning capabilities of models based solely on RGB input. Then, the\nhistorical key-value caches from the spatial-geometric and visual-semantic\nencoders are constructed into a dual implicit memory. By retaining only the KVs\nof tokens in the initial and sliding window, redundant computation is avoided,\nenabling efficient incremental updates. Extensive experiments demonstrate that\nJanusVLN outperforms over 20 recent methods to achieve SOTA performance. For\nexample, the success rate improves by 10.5-35.5 compared to methods using\nmultiple data types as input and by 3.6-10.8 compared to methods using more RGB\ntraining data. This indicates that the proposed dual implicit neural memory, as\na novel paradigm, explores promising new directions for future VLN research.\nOurs project page: https://miv-xjtu.github.io/JanusVLN.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-and-Language Navigation requires an embodied agent to navigate through\nunseen environments, guided by natural language instructions and a continuous\nvideo stream. Recent advances in VLN have been driven by the powerful semantic\nunderstanding of Multimodal Large Language Models. However, these methods\ntypically rely on explicit semantic memory, such as building textual cognitive\nmaps or storing historical visual frames. This type of method suffers from\nspatial information loss, computational redundancy, and memory bloat, which\nimpede efficient navigation. Inspired by the implicit scene representation in\nhuman navigation, analogous to the left brain's semantic understanding and the\nright brain's spatial cognition, we propose JanusVLN, a novel VLN framework\nfeaturing a dual implicit neural memory that models spatial-geometric and\nvisual-semantic memory as separate, compact, and fixed-size neural\nrepresentations. This framework first extends the MLLM to incorporate 3D prior\nknowledge from the spatial-geometric encoder, thereby enhancing the spatial\nreasoning capabilities of models based solely on RGB input. Then, the\nhistorical key-value caches from the spatial-geometric and visual-semantic\nencoders are constructed into a dual implicit memory. By retaining only the KVs\nof tokens in the initial and sliding window, redundant computation is avoided,\nenabling efficient incremental updates. Extensive experiments demonstrate that\nJanusVLN outperforms over 20 recent methods to achieve SOTA performance. For\nexample, the success rate improves by 10.5-35.5 compared to methods using\nmultiple data types as input and by 3.6-10.8 compared to methods using more RGB\ntraining data. This indicates that the proposed dual implicit neural memory, as\na novel paradigm, explores promising new directions for future VLN research.\nOurs project page: https://miv-xjtu.github.io/JanusVLN.github.io/."
                },
                "authors": [
                    {
                        "name": "Shuang Zeng"
                    },
                    {
                        "name": "Dekang Qi"
                    },
                    {
                        "name": "Xinyuan Chang"
                    },
                    {
                        "name": "Feng Xiong"
                    },
                    {
                        "name": "Shichao Xie"
                    },
                    {
                        "name": "Xiaolong Wu"
                    },
                    {
                        "name": "Shiyi Liang"
                    },
                    {
                        "name": "Mu Xu"
                    },
                    {
                        "name": "Xing Wei"
                    }
                ],
                "author_detail": {
                    "name": "Xing Wei"
                },
                "author": "Xing Wei",
                "arxiv_comment": "Project page: https://miv-xjtu.github.io/JanusVLN.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22548v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22548v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22516v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22516v1",
                "updated": "2025-09-26T16:00:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    0,
                    36,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T16:00:36Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    0,
                    36,
                    4,
                    269,
                    0
                ],
                "title": "TrueGradeAI: Retrieval-Augmented and Bias-Resistant AI for Transparent\n  and Explainable Digital Assessments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrueGradeAI: Retrieval-Augmented and Bias-Resistant AI for Transparent\n  and Explainable Digital Assessments"
                },
                "summary": "This paper introduces TrueGradeAI, an AI-driven digital examination framework\ndesigned to overcome the shortcomings of traditional paper-based assessments,\nincluding excessive paper usage, logistical complexity, grading delays, and\nevaluator bias. The system preserves natural handwriting by capturing stylus\ninput on secure tablets and applying transformer-based optical character\nrecognition for transcription. Evaluation is conducted through a\nretrieval-augmented pipeline that integrates faculty solutions, cache layers,\nand external references, enabling a large language model to assign scores with\nexplicit, evidence-linked reasoning. Unlike prior tablet-based exam systems\nthat primarily digitize responses, TrueGradeAI advances the field by\nincorporating explainable automation, bias mitigation, and auditable grading\ntrails. By uniting handwriting preservation with scalable and transparent\nevaluation, the framework reduces environmental costs, accelerates feedback\ncycles, and progressively builds a reusable knowledge base, while actively\nworking to mitigate grading bias and ensure fairness in assessment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces TrueGradeAI, an AI-driven digital examination framework\ndesigned to overcome the shortcomings of traditional paper-based assessments,\nincluding excessive paper usage, logistical complexity, grading delays, and\nevaluator bias. The system preserves natural handwriting by capturing stylus\ninput on secure tablets and applying transformer-based optical character\nrecognition for transcription. Evaluation is conducted through a\nretrieval-augmented pipeline that integrates faculty solutions, cache layers,\nand external references, enabling a large language model to assign scores with\nexplicit, evidence-linked reasoning. Unlike prior tablet-based exam systems\nthat primarily digitize responses, TrueGradeAI advances the field by\nincorporating explainable automation, bias mitigation, and auditable grading\ntrails. By uniting handwriting preservation with scalable and transparent\nevaluation, the framework reduces environmental costs, accelerates feedback\ncycles, and progressively builds a reusable knowledge base, while actively\nworking to mitigate grading bias and ensure fairness in assessment."
                },
                "authors": [
                    {
                        "name": "Rakesh Thakur"
                    },
                    {
                        "name": "Shivaansh Kaushik"
                    },
                    {
                        "name": "Gauri Chopra"
                    },
                    {
                        "name": "Harsh Rohilla"
                    }
                ],
                "author_detail": {
                    "name": "Harsh Rohilla"
                },
                "author": "Harsh Rohilla",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22516v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22516v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22512v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22512v1",
                "updated": "2025-09-26T15:54:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    54,
                    50,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T15:54:50Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    54,
                    50,
                    4,
                    269,
                    0
                ],
                "title": "AxLLM: accelerator architecture for large language models with\n  computation reuse capability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AxLLM: accelerator architecture for large language models with\n  computation reuse capability"
                },
                "summary": "Large language models demand massive computational power and memory\nresources, posing significant challenges for efficient deployment. While\nquantization has been widely explored to reduce model size and computation,\nthis paper demonstrates an additional benefit: quantization increases parameter\nlocality, creating opportunities for computation reuse. Building on this\ninsight, we propose AxLLM, a hardware accelerator architecture designed for\nquantized models. Axllm introduces a novel redundancy elimination technique\nthat caches and reuses multiplication results for repeated weight values,\nsubstantially reducing redundant operations. The architecture features dual\nmultiply and reuse pipelines, efficiently supporting both base models and LoRA\nfine-tuned models without altering parameters, retraining, or requiring offline\npreprocessing. Experimental results show that AxLLM achieves up to 90%\nreduction in computations, delivering 28% lower energy consumption and a 1.7x\nspeedup over baseline execution. These results highlight Axllm as a scalable\nand efficient solution for accelerating LLMs on specialized hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models demand massive computational power and memory\nresources, posing significant challenges for efficient deployment. While\nquantization has been widely explored to reduce model size and computation,\nthis paper demonstrates an additional benefit: quantization increases parameter\nlocality, creating opportunities for computation reuse. Building on this\ninsight, we propose AxLLM, a hardware accelerator architecture designed for\nquantized models. Axllm introduces a novel redundancy elimination technique\nthat caches and reuses multiplication results for repeated weight values,\nsubstantially reducing redundant operations. The architecture features dual\nmultiply and reuse pipelines, efficiently supporting both base models and LoRA\nfine-tuned models without altering parameters, retraining, or requiring offline\npreprocessing. Experimental results show that AxLLM achieves up to 90%\nreduction in computations, delivering 28% lower energy consumption and a 1.7x\nspeedup over baseline execution. These results highlight Axllm as a scalable\nand efficient solution for accelerating LLMs on specialized hardware."
                },
                "authors": [
                    {
                        "name": "Soroush Ahadi"
                    },
                    {
                        "name": "Mehdi Modarressi"
                    },
                    {
                        "name": "Masoud Daneshtalab"
                    }
                ],
                "author_detail": {
                    "name": "Masoud Daneshtalab"
                },
                "author": "Masoud Daneshtalab",
                "arxiv_comment": "7 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22512v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22512v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "n/a",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22488v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22488v1",
                "updated": "2025-09-26T15:35:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    35,
                    5,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T15:35:05Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    35,
                    5,
                    4,
                    269,
                    0
                ],
                "title": "Organ dose optimization for a point-of-care forearm X-ray\n  photon-counting CT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Organ dose optimization for a point-of-care forearm X-ray\n  photon-counting CT"
                },
                "summary": "Background: Spectral shaping is a computed tomography (CT) dose optimization\ntechnique that adjusts source voltage and filtration to reduce patient\nradiation exposure without compromising image quality. Traditionally, radiation\ndose has been assessed using the computed tomography dose index (CTDI).\nHowever, emerging dosimetric approaches aim to enable patient-specific\nevaluations by estimating organ absorbed doses, providing a more accurate\nrepresentation of the biological impact. This study investigates spectral\nshaping for an extremity photon-counting detector (PCD) CT, through organ\nabsorbed dose estimation and image quality evaluation. Method: Monte Carlo\nsimulations were conducted to evaluate various combinations of source voltage\nand filtration. Tube voltage ranged from 80 to 140 kV, combined with three\ndistinct filtration material and thicknesses. Simulations included three\nstages: a standardized phantom for CTDI assessment, an adult forearm phantom\nfor organ dose measurement, and an image quality phantom for evaluation of an\nadvanced image quality metric: the detectability index. Results: In a wrist\nPCD-CT imaging protocol, operating the source at 80 kV can reduce the radiation\ndose by up to 50%. This reduction is achieved while maintaining the same\ndetectability index value as the standard 120 kV protocol. However, the optimal\nfiltration depends on the organ targeted for dose reduction, as bone and skin\nbenefit from opposing filtration approaches. While CTDI provides a useful\ninitial estimate, it may lead to suboptimal optimization compared to\norgan-specific dose evaluation. Conclusions: Patient-specific dosimetry based\non organ absorbed dose estimation offers a more accurate framework for\noptimizing CT protocols through spectral shaping than conventional CTDI-based\napproaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Spectral shaping is a computed tomography (CT) dose optimization\ntechnique that adjusts source voltage and filtration to reduce patient\nradiation exposure without compromising image quality. Traditionally, radiation\ndose has been assessed using the computed tomography dose index (CTDI).\nHowever, emerging dosimetric approaches aim to enable patient-specific\nevaluations by estimating organ absorbed doses, providing a more accurate\nrepresentation of the biological impact. This study investigates spectral\nshaping for an extremity photon-counting detector (PCD) CT, through organ\nabsorbed dose estimation and image quality evaluation. Method: Monte Carlo\nsimulations were conducted to evaluate various combinations of source voltage\nand filtration. Tube voltage ranged from 80 to 140 kV, combined with three\ndistinct filtration material and thicknesses. Simulations included three\nstages: a standardized phantom for CTDI assessment, an adult forearm phantom\nfor organ dose measurement, and an image quality phantom for evaluation of an\nadvanced image quality metric: the detectability index. Results: In a wrist\nPCD-CT imaging protocol, operating the source at 80 kV can reduce the radiation\ndose by up to 50%. This reduction is achieved while maintaining the same\ndetectability index value as the standard 120 kV protocol. However, the optimal\nfiltration depends on the organ targeted for dose reduction, as bone and skin\nbenefit from opposing filtration approaches. While CTDI provides a useful\ninitial estimate, it may lead to suboptimal optimization compared to\norgan-specific dose evaluation. Conclusions: Patient-specific dosimetry based\non organ absorbed dose estimation offers a more accurate framework for\noptimizing CT protocols through spectral shaping than conventional CTDI-based\napproaches."
                },
                "authors": [
                    {
                        "name": "Pierre-Antoine Rodesch"
                    },
                    {
                        "name": "Anaïs Viry"
                    },
                    {
                        "name": "Mouad Khorsi"
                    },
                    {
                        "name": "Fabio Becce"
                    },
                    {
                        "name": "Jérôme Damet"
                    },
                    {
                        "name": "Lucía Gallego Manzano"
                    }
                ],
                "author_detail": {
                    "name": "Lucía Gallego Manzano"
                },
                "author": "Lucía Gallego Manzano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22488v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16950v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16950v3",
                "updated": "2025-09-26T14:35:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    35,
                    4,
                    4,
                    269,
                    0
                ],
                "published": "2025-05-22T17:33:49Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    33,
                    49,
                    3,
                    142,
                    0
                ],
                "title": "Bottlenecked Transformers: Periodic KV Cache Consolidation for\n  Generalised Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bottlenecked Transformers: Periodic KV Cache Consolidation for\n  Generalised Reasoning"
                },
                "summary": "Transformer LLMs have been shown to exhibit strong reasoning ability that\nscales with inference-time compute, most prominently through token-space\n\"thinking\" chains of thought. A growing line of work pushes extra computation\ninto the model's latent space, which we term Auxiliary Latent-Space Computation\n(ALSC). Existing ALSC methods largely fall into three buckets: (i)\ntoken-mediated latent rollouts, (ii) residual/activation steering, and (iii)\nmemory (KV) compression. An underexplored alternative is memory\nconsolidation/reconsolidation, two processes in the brain that are responsible\nfor stabilising newly formed memory traces, and, upon recall, transiently\nrendering established traces plastic such they can integrate new contextual\ninformation before restabilising. In Transformer LLMs, this can be seen as\nanalogous to performing in-place rewrites of new KV segments, and rewrites of\nrecalled past segments. In this work, we give a theoretical justification as to\nwhy memory (re)consolidation via KV cache rewrites is beneficial for improved\nreasoning. We do this through the lens of Information Bottleneck (IB) theory,\nwhich posits that model generalisation emerges from an optimal balance between\ninput information compression and retention of predictive information in latent\nrepresentations. We then introduce the Bottlenecked Transformer, which augments\na backbone LLM with a Cache Processor, an auxiliary Transformer that performs\nperiodic, non-causal, in-place KV rewrites at newline-delimited reasoning step\nboundaries. The Processor consolidates recently written KV entries and\nreconsolidates a small, top-k attention-selected set of prior entries. We\nevaluate our Bottlenecked Transformer architecture on math reasoning\nbenchmarks. Our model sees consistent performance gains over vanilla\nTransformers and pause-token augmented baselines, with gains of up to +6.6pp\nfor selected tasks/backbones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer LLMs have been shown to exhibit strong reasoning ability that\nscales with inference-time compute, most prominently through token-space\n\"thinking\" chains of thought. A growing line of work pushes extra computation\ninto the model's latent space, which we term Auxiliary Latent-Space Computation\n(ALSC). Existing ALSC methods largely fall into three buckets: (i)\ntoken-mediated latent rollouts, (ii) residual/activation steering, and (iii)\nmemory (KV) compression. An underexplored alternative is memory\nconsolidation/reconsolidation, two processes in the brain that are responsible\nfor stabilising newly formed memory traces, and, upon recall, transiently\nrendering established traces plastic such they can integrate new contextual\ninformation before restabilising. In Transformer LLMs, this can be seen as\nanalogous to performing in-place rewrites of new KV segments, and rewrites of\nrecalled past segments. In this work, we give a theoretical justification as to\nwhy memory (re)consolidation via KV cache rewrites is beneficial for improved\nreasoning. We do this through the lens of Information Bottleneck (IB) theory,\nwhich posits that model generalisation emerges from an optimal balance between\ninput information compression and retention of predictive information in latent\nrepresentations. We then introduce the Bottlenecked Transformer, which augments\na backbone LLM with a Cache Processor, an auxiliary Transformer that performs\nperiodic, non-causal, in-place KV rewrites at newline-delimited reasoning step\nboundaries. The Processor consolidates recently written KV entries and\nreconsolidates a small, top-k attention-selected set of prior entries. We\nevaluate our Bottlenecked Transformer architecture on math reasoning\nbenchmarks. Our model sees consistent performance gains over vanilla\nTransformers and pause-token augmented baselines, with gains of up to +6.6pp\nfor selected tasks/backbones."
                },
                "authors": [
                    {
                        "name": "Adnan Oomerjee"
                    },
                    {
                        "name": "Zafeirios Fountas"
                    },
                    {
                        "name": "Haitham Bou-Ammar"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16950v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16950v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22323v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22323v1",
                "updated": "2025-09-26T13:20:52Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    13,
                    20,
                    52,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T13:20:52Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    13,
                    20,
                    52,
                    4,
                    269,
                    0
                ],
                "title": "RAPID^3: Tri-Level Reinforced Acceleration Policies for Diffusion\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAPID^3: Tri-Level Reinforced Acceleration Policies for Diffusion\n  Transformer"
                },
                "summary": "Diffusion Transformers (DiTs) excel at visual generation yet remain hampered\nby slow sampling. Existing training-free accelerators - step reduction, feature\ncaching, and sparse attention - enhance inference speed but typically rely on a\nuniform heuristic or a manually designed adaptive strategy for all images,\nleaving quality on the table. Alternatively, dynamic neural networks offer\nper-image adaptive acceleration, but their high fine-tuning costs limit broader\napplicability. To address these limitations, we introduce RAPID3: Tri-Level\nReinforced Acceleration Policies for Diffusion Transformers, a framework that\ndelivers image-wise acceleration with zero updates to the base generator.\nSpecifically, three lightweight policy heads - Step-Skip, Cache-Reuse, and\nSparse-Attention - observe the current denoising state and independently decide\ntheir corresponding speed-up at each timestep. All policy parameters are\ntrained online via Group Relative Policy Optimization (GRPO) while the\ngenerator remains frozen. Meanwhile, an adversarially learned discriminator\naugments the reward signal, discouraging reward hacking by boosting returns\nonly when generated samples stay close to the original model's distribution.\nAcross state-of-the-art DiT backbones, including Stable Diffusion 3 and FLUX,\nRAPID3 achieves nearly 3x faster sampling with competitive generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) excel at visual generation yet remain hampered\nby slow sampling. Existing training-free accelerators - step reduction, feature\ncaching, and sparse attention - enhance inference speed but typically rely on a\nuniform heuristic or a manually designed adaptive strategy for all images,\nleaving quality on the table. Alternatively, dynamic neural networks offer\nper-image adaptive acceleration, but their high fine-tuning costs limit broader\napplicability. To address these limitations, we introduce RAPID3: Tri-Level\nReinforced Acceleration Policies for Diffusion Transformers, a framework that\ndelivers image-wise acceleration with zero updates to the base generator.\nSpecifically, three lightweight policy heads - Step-Skip, Cache-Reuse, and\nSparse-Attention - observe the current denoising state and independently decide\ntheir corresponding speed-up at each timestep. All policy parameters are\ntrained online via Group Relative Policy Optimization (GRPO) while the\ngenerator remains frozen. Meanwhile, an adversarially learned discriminator\naugments the reward signal, discouraging reward hacking by boosting returns\nonly when generated samples stay close to the original model's distribution.\nAcross state-of-the-art DiT backbones, including Stable Diffusion 3 and FLUX,\nRAPID3 achieves nearly 3x faster sampling with competitive generation quality."
                },
                "authors": [
                    {
                        "name": "Wangbo Zhao"
                    },
                    {
                        "name": "Yizeng Han"
                    },
                    {
                        "name": "Zhiwei Tang"
                    },
                    {
                        "name": "Jiasheng Tang"
                    },
                    {
                        "name": "Pengfei Zhou"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Bohan Zhuang"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Fan Wang"
                    },
                    {
                        "name": "Yang You"
                    }
                ],
                "author_detail": {
                    "name": "Yang You"
                },
                "author": "Yang You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22323v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22323v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v8",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v8",
                "updated": "2025-09-26T10:00:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    10,
                    0,
                    54,
                    4,
                    269,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty, and only those\nelements are processed to draw the next frame, dramatically reducing latency.\nHowever, the standard incremental layout algorithm must search the page for\ndirty elements, accessing auxiliary elements in the process. These auxiliary\nelements add cache misses and stalled cycles, and are responsible for a sizable\nfraction of all layout latency. We introduce a new, faster incremental layout\nalgorithm called Spineless Traversal. Spineless Traversal uses a\ncache-friendlier priority queue algorithm that avoids accessing auxiliary nodes\nand thus reduces cache traffic and stalls. This leads to dramatic speedups on\nthe most latency-critical interactions such as hovering, typing, and animation.\nMoreover, thanks to numerous low-level optimizations, Spineless Traversal is\ncompetitive across the whole spectrum of incremental layout workloads.\nSpineless Traversal is faster than the standard approach on 83.0% of 2216\nbenchmarks, with a mean speedup of 1.80x concentrated in the most\nlatency-critical interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty, and only those\nelements are processed to draw the next frame, dramatically reducing latency.\nHowever, the standard incremental layout algorithm must search the page for\ndirty elements, accessing auxiliary elements in the process. These auxiliary\nelements add cache misses and stalled cycles, and are responsible for a sizable\nfraction of all layout latency. We introduce a new, faster incremental layout\nalgorithm called Spineless Traversal. Spineless Traversal uses a\ncache-friendlier priority queue algorithm that avoids accessing auxiliary nodes\nand thus reduces cache traffic and stalls. This leads to dramatic speedups on\nthe most latency-critical interactions such as hovering, typing, and animation.\nMoreover, thanks to numerous low-level optimizations, Spineless Traversal is\ncompetitive across the whole spectrum of incremental layout workloads.\nSpineless Traversal is faster than the standard approach on 83.0% of 2216\nbenchmarks, with a mean speedup of 1.80x concentrated in the most\nlatency-critical interactions."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v8",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v8",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22756v1",
                "updated": "2025-09-26T09:33:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    9,
                    33,
                    36,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T09:33:36Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    9,
                    33,
                    36,
                    4,
                    269,
                    0
                ],
                "title": "Persistent Autoregressive Mapping with Traffic Rules for Autonomous\n  Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persistent Autoregressive Mapping with Traffic Rules for Autonomous\n  Driving"
                },
                "summary": "Safe autonomous driving requires both accurate HD map construction and\npersistent awareness of traffic rules, even when their associated signs are no\nlonger visible. However, existing methods either focus solely on geometric\nelements or treat rules as temporary classifications, failing to capture their\npersistent effectiveness across extended driving sequences. In this paper, we\npresent PAMR (Persistent Autoregressive Mapping with Traffic Rules), a novel\nframework that performs autoregressive co-construction of lane vectors and\ntraffic rules from visual observations. Our approach introduces two key\nmechanisms: Map-Rule Co-Construction for processing driving scenes in temporal\nsegments, and Map-Rule Cache for maintaining rule consistency across these\nsegments. To properly evaluate continuous and consistent map generation, we\ndevelop MapDRv2, featuring improved lane geometry annotations. Extensive\nexperiments demonstrate that PAMR achieves superior performance in joint\nvector-rule mapping tasks, while maintaining persistent rule effectiveness\nthroughout extended driving sequences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safe autonomous driving requires both accurate HD map construction and\npersistent awareness of traffic rules, even when their associated signs are no\nlonger visible. However, existing methods either focus solely on geometric\nelements or treat rules as temporary classifications, failing to capture their\npersistent effectiveness across extended driving sequences. In this paper, we\npresent PAMR (Persistent Autoregressive Mapping with Traffic Rules), a novel\nframework that performs autoregressive co-construction of lane vectors and\ntraffic rules from visual observations. Our approach introduces two key\nmechanisms: Map-Rule Co-Construction for processing driving scenes in temporal\nsegments, and Map-Rule Cache for maintaining rule consistency across these\nsegments. To properly evaluate continuous and consistent map generation, we\ndevelop MapDRv2, featuring improved lane geometry annotations. Extensive\nexperiments demonstrate that PAMR achieves superior performance in joint\nvector-rule mapping tasks, while maintaining persistent rule effectiveness\nthroughout extended driving sequences."
                },
                "authors": [
                    {
                        "name": "Shiyi Liang"
                    },
                    {
                        "name": "Xinyuan Chang"
                    },
                    {
                        "name": "Changjie Wu"
                    },
                    {
                        "name": "Huiyuan Yan"
                    },
                    {
                        "name": "Yifan Bai"
                    },
                    {
                        "name": "Xinran Liu"
                    },
                    {
                        "name": "Hang Zhang"
                    },
                    {
                        "name": "Yujian Yuan"
                    },
                    {
                        "name": "Shuang Zeng"
                    },
                    {
                        "name": "Mu Xu"
                    },
                    {
                        "name": "Xing Wei"
                    }
                ],
                "author_detail": {
                    "name": "Xing Wei"
                },
                "author": "Xing Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13681v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13681v2",
                "updated": "2025-09-26T07:14:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    7,
                    14,
                    44,
                    4,
                    269,
                    0
                ],
                "published": "2025-07-18T06:12:08Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    6,
                    12,
                    8,
                    4,
                    199,
                    0
                ],
                "title": "LoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for\n  Multi-Turn Dialogues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for\n  Multi-Turn Dialogues"
                },
                "summary": "Multi-turn dialogues are essential in many real-world applications of large\nlanguage models, such as chatbots and virtual assistants. As conversation\nhistories become longer, existing large language models face increasing\ncomputational and memory challenges, which hinder their ability to provide\nefficient and responsive interactions. Most current acceleration methods either\ncompress the context or optimize key value caching, but they often rely on\nfixed or position-based heuristics that do not adapt well to the dynamic and\nunpredictable patterns found in actual multi-turn conversations. As a result,\nthese models cannot accurately identify and prioritize the most relevant\ncontext, leading to degraded response quality. In this paper, we present\nLoopServe, an adaptive dual-phase inference acceleration framework for large\nlanguage models in multi-turn dialogues. LoopServe introduces two main\ninnovations. First, it performs online sparsification during the prefilling\nphase by dynamically selecting the most important parts of the attention matrix\nfor each new input. Second, it uses progressive key value compression during\ndecoding by adaptively maintaining a relevant and efficient cache based on the\nmost recently generated output tokens. We also propose a new benchmark with\neleven multi-turn datasets that reflect realistic query positions and\nconversational dependencies. Extensive experiments demonstrate that LoopServe\nconsistently achieves superior effectiveness compared to existing baselines and\nsignificantly accelerates LLM inference across a wide range of long-context\ndialogue tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-turn dialogues are essential in many real-world applications of large\nlanguage models, such as chatbots and virtual assistants. As conversation\nhistories become longer, existing large language models face increasing\ncomputational and memory challenges, which hinder their ability to provide\nefficient and responsive interactions. Most current acceleration methods either\ncompress the context or optimize key value caching, but they often rely on\nfixed or position-based heuristics that do not adapt well to the dynamic and\nunpredictable patterns found in actual multi-turn conversations. As a result,\nthese models cannot accurately identify and prioritize the most relevant\ncontext, leading to degraded response quality. In this paper, we present\nLoopServe, an adaptive dual-phase inference acceleration framework for large\nlanguage models in multi-turn dialogues. LoopServe introduces two main\ninnovations. First, it performs online sparsification during the prefilling\nphase by dynamically selecting the most important parts of the attention matrix\nfor each new input. Second, it uses progressive key value compression during\ndecoding by adaptively maintaining a relevant and efficient cache based on the\nmost recently generated output tokens. We also propose a new benchmark with\neleven multi-turn datasets that reflect realistic query positions and\nconversational dependencies. Extensive experiments demonstrate that LoopServe\nconsistently achieves superior effectiveness compared to existing baselines and\nsignificantly accelerates LLM inference across a wide range of long-context\ndialogue tasks."
                },
                "authors": [
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Zhanchao Xu"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Xuejia Chen"
                    },
                    {
                        "name": "Darian Li"
                    },
                    {
                        "name": "Anxin Tian"
                    },
                    {
                        "name": "Qingfa Xiao"
                    },
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Mingxuan Yuan"
                },
                "author": "Mingxuan Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13681v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13681v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21917v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21917v1",
                "updated": "2025-09-26T05:57:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    5,
                    57,
                    4,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T05:57:04Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    5,
                    57,
                    4,
                    4,
                    269,
                    0
                ],
                "title": "Taming Flow-based I2V Models for Creative Video Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Taming Flow-based I2V Models for Creative Video Editing"
                },
                "summary": "Although image editing techniques have advanced significantly, video editing,\nwhich aims to manipulate videos according to user intent, remains an emerging\nchallenge. Most existing image-conditioned video editing methods either require\ninversion with model-specific design or need extensive optimization, limiting\ntheir capability of leveraging up-to-date image-to-video (I2V) models to\ntransfer the editing capability of image editing models to the video domain. To\nthis end, we propose IF-V2V, an Inversion-Free method that can adapt\noff-the-shelf flow-matching-based I2V models for video editing without\nsignificant computational overhead. To circumvent inversion, we devise Vector\nField Rectification with Sample Deviation to incorporate information from the\nsource video into the denoising process by introducing a deviation term into\nthe denoising vector field. To further ensure consistency with the source video\nin a model-agnostic way, we introduce Structure-and-Motion-Preserving\nInitialization to generate motion-aware temporally correlated noise with\nstructural information embedded. We also present a Deviation Caching mechanism\nto minimize the additional computational cost for denoising vector\nrectification without significantly impacting editing quality. Evaluations\ndemonstrate that our method achieves superior editing quality and consistency\nover existing approaches, offering a lightweight plug-and-play solution to\nrealize visual creativity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although image editing techniques have advanced significantly, video editing,\nwhich aims to manipulate videos according to user intent, remains an emerging\nchallenge. Most existing image-conditioned video editing methods either require\ninversion with model-specific design or need extensive optimization, limiting\ntheir capability of leveraging up-to-date image-to-video (I2V) models to\ntransfer the editing capability of image editing models to the video domain. To\nthis end, we propose IF-V2V, an Inversion-Free method that can adapt\noff-the-shelf flow-matching-based I2V models for video editing without\nsignificant computational overhead. To circumvent inversion, we devise Vector\nField Rectification with Sample Deviation to incorporate information from the\nsource video into the denoising process by introducing a deviation term into\nthe denoising vector field. To further ensure consistency with the source video\nin a model-agnostic way, we introduce Structure-and-Motion-Preserving\nInitialization to generate motion-aware temporally correlated noise with\nstructural information embedded. We also present a Deviation Caching mechanism\nto minimize the additional computational cost for denoising vector\nrectification without significantly impacting editing quality. Evaluations\ndemonstrate that our method achieves superior editing quality and consistency\nover existing approaches, offering a lightweight plug-and-play solution to\nrealize visual creativity."
                },
                "authors": [
                    {
                        "name": "Xianghao Kong"
                    },
                    {
                        "name": "Hansheng Chen"
                    },
                    {
                        "name": "Yuwei Guo"
                    },
                    {
                        "name": "Lvmin Zhang"
                    },
                    {
                        "name": "Gordon Wetzstein"
                    },
                    {
                        "name": "Maneesh Agrawala"
                    },
                    {
                        "name": "Anyi Rao"
                    }
                ],
                "author_detail": {
                    "name": "Anyi Rao"
                },
                "author": "Anyi Rao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21917v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21917v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21857v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21857v1",
                "updated": "2025-09-26T04:32:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    4,
                    32,
                    56,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T04:32:56Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    4,
                    32,
                    56,
                    4,
                    269,
                    0
                ],
                "title": "2.34 kV \\b{eta}-Ga2O3 Vertical Trench RESURF Schottky Barrier Diode with\n  sub-micron fin width",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "2.34 kV \\b{eta}-Ga2O3 Vertical Trench RESURF Schottky Barrier Diode with\n  sub-micron fin width"
                },
                "summary": "In this letter, we present a kilovolt-class \\b{eta}-Ga2O3 vertical trench\nSchottky barrier diode with a field plate incorporating narrow fin width (Wfin)\nstructures of sub-micron dimensions. We used a nanolaminate dielectric\ncomprising a stack of multiple thin TiO2 and Al2O3 layers as RESURF dielectric\nand for field plate edge termination. Both Wfin of 200 nm and 500 nm\ndemonstrate excellent on-state performance with specific on-resistance (Ron,sp)\nof 9.8-12 mohmcm2, and 10^10 rectification ratio. A self-aligned photoresist\nplanarization and etch-back process was employed to expose the top of the fins\nfor Schottky contact formation, eliminating critical lithographic alignment\nchallenges in sub-micron scale processing. We achieved a breakdown of 2.34 kV\nwith very low leakage currents before catastrophic breakdown. The measured\nbreakdown voltage is limited by dielectric breakdown at the trench bottom\ncorner as verified by metal-oxide-semiconductor (MOS) test structure. TCAD\nsimulation shows a reduced electric field at the surface of the\nmetal-semiconductor junction due to the RESURF effect, resulting in very low\nreverse leakage before breakdown. The parallel plane electric field in the\n\\b{eta} -Ga2O3 is extracted to be 3.8 MV/cm from TCAD simulations using\naccurately extracted drift layer doping profile from high voltage CV\nmeasurements. A power figure of merit of 0.867 GW/cm2(0.56 GW/cm2 with current\nspreading) was calculated. Enhanced RESURF by integration of high-k dielectrics\nwith self-aligned photoresist planarization, offers a promising pathway towards\nhigh figure of merit, low leakage high-performance vertical devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this letter, we present a kilovolt-class \\b{eta}-Ga2O3 vertical trench\nSchottky barrier diode with a field plate incorporating narrow fin width (Wfin)\nstructures of sub-micron dimensions. We used a nanolaminate dielectric\ncomprising a stack of multiple thin TiO2 and Al2O3 layers as RESURF dielectric\nand for field plate edge termination. Both Wfin of 200 nm and 500 nm\ndemonstrate excellent on-state performance with specific on-resistance (Ron,sp)\nof 9.8-12 mohmcm2, and 10^10 rectification ratio. A self-aligned photoresist\nplanarization and etch-back process was employed to expose the top of the fins\nfor Schottky contact formation, eliminating critical lithographic alignment\nchallenges in sub-micron scale processing. We achieved a breakdown of 2.34 kV\nwith very low leakage currents before catastrophic breakdown. The measured\nbreakdown voltage is limited by dielectric breakdown at the trench bottom\ncorner as verified by metal-oxide-semiconductor (MOS) test structure. TCAD\nsimulation shows a reduced electric field at the surface of the\nmetal-semiconductor junction due to the RESURF effect, resulting in very low\nreverse leakage before breakdown. The parallel plane electric field in the\n\\b{eta} -Ga2O3 is extracted to be 3.8 MV/cm from TCAD simulations using\naccurately extracted drift layer doping profile from high voltage CV\nmeasurements. A power figure of merit of 0.867 GW/cm2(0.56 GW/cm2 with current\nspreading) was calculated. Enhanced RESURF by integration of high-k dielectrics\nwith self-aligned photoresist planarization, offers a promising pathway towards\nhigh figure of merit, low leakage high-performance vertical devices."
                },
                "authors": [
                    {
                        "name": "Chinmoy Nath Saha"
                    },
                    {
                        "name": "Saurav Roy"
                    },
                    {
                        "name": "Yizheng Liu"
                    },
                    {
                        "name": "Carl Peterson"
                    },
                    {
                        "name": "Sriram Krishnamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Sriram Krishnamoorthy"
                },
                "author": "Sriram Krishnamoorthy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21857v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21842v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21842v1",
                "updated": "2025-09-26T04:03:52Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    4,
                    3,
                    52,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T04:03:52Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    4,
                    3,
                    52,
                    4,
                    269,
                    0
                ],
                "title": "DeepTravel: An End-to-End Agentic Reinforcement Learning Framework for\n  Autonomous Travel Planning Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepTravel: An End-to-End Agentic Reinforcement Learning Framework for\n  Autonomous Travel Planning Agents"
                },
                "summary": "Travel planning (TP) agent has recently worked as an emerging building block\nto interact with external tools and resources for travel itinerary generation,\nensuring enjoyable user experience. Despite its benefits, existing studies rely\non hand craft prompt and fixed agent workflow, hindering more flexible and\nautonomous TP agent. This paper proposes DeepTravel, an end to end agentic\nreinforcement learning framework for building autonomous travel planning agent,\ncapable of autonomously planning, executing tools, and reflecting on tool\nresponses to explore, verify, and refine intermediate actions in multi step\nreasoning. To achieve this, we first construct a robust sandbox environment by\ncaching transportation, accommodation and POI data, facilitating TP agent\ntraining without being constrained by real world APIs limitations (e.g.,\ninconsistent outputs). Moreover, we develop a hierarchical reward modeling\nsystem, where a trajectory level verifier first checks spatiotemporal\nfeasibility and filters unsatisfied travel itinerary, and then the turn level\nverifier further validate itinerary detail consistency with tool responses,\nenabling efficient and precise reward service. Finally, we propose the reply\naugmented reinforcement learning method that enables TP agent to periodically\nreplay from a failures experience buffer, emerging notable agentic capacity. We\ndeploy trained TP agent on DiDi Enterprise Solutions App and conduct\ncomprehensive online and offline evaluations, demonstrating that DeepTravel\nenables small size LLMs (e.g., Qwen3 32B) to significantly outperform existing\nfrontier LLMs such as OpenAI o1, o3 and DeepSeek R1 in travel planning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Travel planning (TP) agent has recently worked as an emerging building block\nto interact with external tools and resources for travel itinerary generation,\nensuring enjoyable user experience. Despite its benefits, existing studies rely\non hand craft prompt and fixed agent workflow, hindering more flexible and\nautonomous TP agent. This paper proposes DeepTravel, an end to end agentic\nreinforcement learning framework for building autonomous travel planning agent,\ncapable of autonomously planning, executing tools, and reflecting on tool\nresponses to explore, verify, and refine intermediate actions in multi step\nreasoning. To achieve this, we first construct a robust sandbox environment by\ncaching transportation, accommodation and POI data, facilitating TP agent\ntraining without being constrained by real world APIs limitations (e.g.,\ninconsistent outputs). Moreover, we develop a hierarchical reward modeling\nsystem, where a trajectory level verifier first checks spatiotemporal\nfeasibility and filters unsatisfied travel itinerary, and then the turn level\nverifier further validate itinerary detail consistency with tool responses,\nenabling efficient and precise reward service. Finally, we propose the reply\naugmented reinforcement learning method that enables TP agent to periodically\nreplay from a failures experience buffer, emerging notable agentic capacity. We\ndeploy trained TP agent on DiDi Enterprise Solutions App and conduct\ncomprehensive online and offline evaluations, demonstrating that DeepTravel\nenables small size LLMs (e.g., Qwen3 32B) to significantly outperform existing\nfrontier LLMs such as OpenAI o1, o3 and DeepSeek R1 in travel planning tasks."
                },
                "authors": [
                    {
                        "name": "Yansong Ning"
                    },
                    {
                        "name": "Rui Liu"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Jun Fang"
                    },
                    {
                        "name": "Kan Zheng"
                    },
                    {
                        "name": "Naiqiang Tan"
                    },
                    {
                        "name": "Hao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hao Liu"
                },
                "author": "Hao Liu",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21842v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21842v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01199v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01199v3",
                "updated": "2025-09-26T03:24:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    3,
                    24,
                    20,
                    4,
                    269,
                    0
                ],
                "published": "2025-03-03T05:52:02Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    5,
                    52,
                    2,
                    0,
                    62,
                    0
                ],
                "title": "LiteGS: A High-performance Framework to Train 3DGS in Subminutes via\n  System and Algorithm Codesign",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiteGS: A High-performance Framework to Train 3DGS in Subminutes via\n  System and Algorithm Codesign"
                },
                "summary": "3D Gaussian Splatting (3DGS) has emerged as promising alternative in 3D\nrepresentation. However, it still suffers from high training cost. This paper\nintroduces LiteGS, a high performance framework that systematically optimizes\nthe 3DGS training pipeline from multiple aspects. At the low-level computation\nlayer, we design a ``warp-based raster'' associated with two hardware-aware\noptimizations to significantly reduce gradient reduction overhead. At the\nmid-level data management layer, we introduce dynamic spatial sorting based on\nMorton coding to enable a performant ``Cluster-Cull-Compact'' pipeline and\nimprove data locality, therefore reducing cache misses. At the top-level\nalgorithm layer, we establish a new robust densification criterion based on the\nvariance of the opacity gradient, paired with a more stable opacity control\nmechanism, to achieve more precise parameter growth. Experimental results\ndemonstrate that LiteGS accelerates the original 3DGS training by up to 13.4x\nwith comparable or superior quality and surpasses the current SOTA in\nlightweight models by up to 1.4x speedup. For high-quality reconstruction\ntasks, LiteGS sets a new accuracy record and decreases the training time by an\norder of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Gaussian Splatting (3DGS) has emerged as promising alternative in 3D\nrepresentation. However, it still suffers from high training cost. This paper\nintroduces LiteGS, a high performance framework that systematically optimizes\nthe 3DGS training pipeline from multiple aspects. At the low-level computation\nlayer, we design a ``warp-based raster'' associated with two hardware-aware\noptimizations to significantly reduce gradient reduction overhead. At the\nmid-level data management layer, we introduce dynamic spatial sorting based on\nMorton coding to enable a performant ``Cluster-Cull-Compact'' pipeline and\nimprove data locality, therefore reducing cache misses. At the top-level\nalgorithm layer, we establish a new robust densification criterion based on the\nvariance of the opacity gradient, paired with a more stable opacity control\nmechanism, to achieve more precise parameter growth. Experimental results\ndemonstrate that LiteGS accelerates the original 3DGS training by up to 13.4x\nwith comparable or superior quality and surpasses the current SOTA in\nlightweight models by up to 1.4x speedup. For high-quality reconstruction\ntasks, LiteGS sets a new accuracy record and decreases the training time by an\norder of magnitude."
                },
                "authors": [
                    {
                        "name": "Kaimin Liao"
                    },
                    {
                        "name": "Hua Wang"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Luchao Wang"
                    },
                    {
                        "name": "Yaohua Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yaohua Tang"
                },
                "author": "Yaohua Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01199v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01199v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19375v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19375v3",
                "updated": "2025-09-26T03:17:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    3,
                    17,
                    15,
                    4,
                    269,
                    0
                ],
                "published": "2024-09-28T15:03:28Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    15,
                    3,
                    28,
                    5,
                    272,
                    0
                ],
                "title": "DOTA: Distributional Test-Time Adaptation of Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DOTA: Distributional Test-Time Adaptation of Vision-Language Models"
                },
                "summary": "Vision-language foundation models (VLMs), such as CLIP, exhibit remarkable\nperformance across a wide range of tasks. However, deploying these models can\nbe unreliable when significant distribution gaps exist between training and\ntest data, while fine-tuning for diverse scenarios is often costly. Cache-based\ntest-time adapters offer an efficient alternative by storing representative\ntest samples to guide subsequent classifications. Yet, these methods typically\nemploy naive cache management with limited capacity, leading to severe\ncatastrophic forgetting when samples are inevitably dropped during updates. In\nthis paper, we propose DOTA (DistributiOnal Test-time Adaptation), a simple yet\neffective method addressing this limitation. Crucially, instead of merely\nmemorizing individual test samples, DOTA continuously estimates the underlying\ndistribution of the test data stream. Test-time posterior probabilities are\nthen computed using these dynamically estimated distributions via Bayes'\ntheorem for adaptation. This distribution-centric approach enables the model to\ncontinually learn and adapt to the deployment environment. Extensive\nexperiments validate that DOTA significantly mitigates forgetting and achieves\nstate-of-the-art performance compared to existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language foundation models (VLMs), such as CLIP, exhibit remarkable\nperformance across a wide range of tasks. However, deploying these models can\nbe unreliable when significant distribution gaps exist between training and\ntest data, while fine-tuning for diverse scenarios is often costly. Cache-based\ntest-time adapters offer an efficient alternative by storing representative\ntest samples to guide subsequent classifications. Yet, these methods typically\nemploy naive cache management with limited capacity, leading to severe\ncatastrophic forgetting when samples are inevitably dropped during updates. In\nthis paper, we propose DOTA (DistributiOnal Test-time Adaptation), a simple yet\neffective method addressing this limitation. Crucially, instead of merely\nmemorizing individual test samples, DOTA continuously estimates the underlying\ndistribution of the test data stream. Test-time posterior probabilities are\nthen computed using these dynamically estimated distributions via Bayes'\ntheorem for adaptation. This distribution-centric approach enables the model to\ncontinually learn and adapt to the deployment environment. Extensive\nexperiments validate that DOTA significantly mitigates forgetting and achieves\nstate-of-the-art performance compared to existing methods."
                },
                "authors": [
                    {
                        "name": "Zongbo Han"
                    },
                    {
                        "name": "Jialong Yang"
                    },
                    {
                        "name": "Guangyu Wang"
                    },
                    {
                        "name": "Junfan Li"
                    },
                    {
                        "name": "Qianli Xu"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    },
                    {
                        "name": "Changqing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Changqing Zhang"
                },
                "author": "Changqing Zhang",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19375v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19375v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21623v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21623v1",
                "updated": "2025-09-25T21:42:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    21,
                    42,
                    27,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T21:42:27Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    21,
                    42,
                    27,
                    3,
                    268,
                    0
                ],
                "title": "OjaKV: Context-Aware Online Low-Rank KV Cache Compression with Oja's\n  Rule",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OjaKV: Context-Aware Online Low-Rank KV Cache Compression with Oja's\n  Rule"
                },
                "summary": "The expanding long-context capabilities of large language models are\nconstrained by a significant memory bottleneck: the key-value (KV) cache\nrequired for autoregressive generation. This bottleneck is substantial; for\ninstance, a Llama-3.1-8B model processing a 32K-token prompt at a batch size of\n4 requires approximately 16GB for its KV cache, a size exceeding the model's\nweights. While KV-cache compression via low-rank projection is a promising\ndirection, existing methods rely on a static, offline-learned subspace that\nperforms poorly under data distribution shifts. To overcome these limitations,\nwe introduce OjaKV, a novel framework that integrates a strategic hybrid\nstorage policy with online subspace adaptation. First, OjaKV recognizes that\nnot all tokens are equally important for compression; it preserves the crucial\nfirst and most recent tokens in full-rank, maintaining high-fidelity anchors\nfor attention. Second, for the vast majority of intermediate tokens, it applies\nlow-rank compression by incrementally adapting the projection basis using Oja's\nalgorithm for online principal component analysis. This adaptation involves a\ncomprehensive update during prompt prefilling and lightweight periodic updates\nduring decoding, ensuring the subspace remains aligned with the evolving\ncontext. Crucially, our framework is fully compatible with modern attention\nmodules like FlashAttention. Experiments demonstrate that OjaKV maintains or\neven improves zero-shot accuracy at high compression ratios. In particular,\nOjaKV achieves its strongest gains on very long-context benchmarks that require\ncomplex reasoning, highlighting the importance of online subspace adaptation in\ndynamically tracking context shifts. These results establish our hybrid\nframework as a practical, plug-and-play solution for memory-efficient\nlong-context inference without requiring model fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expanding long-context capabilities of large language models are\nconstrained by a significant memory bottleneck: the key-value (KV) cache\nrequired for autoregressive generation. This bottleneck is substantial; for\ninstance, a Llama-3.1-8B model processing a 32K-token prompt at a batch size of\n4 requires approximately 16GB for its KV cache, a size exceeding the model's\nweights. While KV-cache compression via low-rank projection is a promising\ndirection, existing methods rely on a static, offline-learned subspace that\nperforms poorly under data distribution shifts. To overcome these limitations,\nwe introduce OjaKV, a novel framework that integrates a strategic hybrid\nstorage policy with online subspace adaptation. First, OjaKV recognizes that\nnot all tokens are equally important for compression; it preserves the crucial\nfirst and most recent tokens in full-rank, maintaining high-fidelity anchors\nfor attention. Second, for the vast majority of intermediate tokens, it applies\nlow-rank compression by incrementally adapting the projection basis using Oja's\nalgorithm for online principal component analysis. This adaptation involves a\ncomprehensive update during prompt prefilling and lightweight periodic updates\nduring decoding, ensuring the subspace remains aligned with the evolving\ncontext. Crucially, our framework is fully compatible with modern attention\nmodules like FlashAttention. Experiments demonstrate that OjaKV maintains or\neven improves zero-shot accuracy at high compression ratios. In particular,\nOjaKV achieves its strongest gains on very long-context benchmarks that require\ncomplex reasoning, highlighting the importance of online subspace adaptation in\ndynamically tracking context shifts. These results establish our hybrid\nframework as a practical, plug-and-play solution for memory-efficient\nlong-context inference without requiring model fine-tuning."
                },
                "authors": [
                    {
                        "name": "Yuxuan Zhu"
                    },
                    {
                        "name": "David H. Yang"
                    },
                    {
                        "name": "Mohammad Mohammadi Amiri"
                    },
                    {
                        "name": "Keerthiram Murugesan"
                    },
                    {
                        "name": "Tejaswini Pedapati"
                    },
                    {
                        "name": "Pin-Yu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Pin-Yu Chen"
                },
                "author": "Pin-Yu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21623v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21623v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21463v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21463v1",
                "updated": "2025-09-25T19:29:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    19,
                    29,
                    25,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T19:29:25Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    19,
                    29,
                    25,
                    3,
                    268,
                    0
                ],
                "title": "Enhanced Generative Machine Listener",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhanced Generative Machine Listener"
                },
                "summary": "We present GMLv2, a reference-based model designed for the prediction of\nsubjective audio quality as measured by MUSHRA scores. GMLv2 introduces a Beta\ndistribution-based loss to model the listener ratings and incorporates\nadditional neural audio coding (NAC) subjective datasets to extend its\ngeneralization and applicability. Extensive evaluations on diverse testset\ndemonstrate that proposed GMLv2 consistently outperforms widely used metrics,\nsuch as PEAQ and ViSQOL, both in terms of correlation with subjective scores\nand in reliably predicting these scores across diverse content types and codec\nconfigurations. Consequently, GMLv2 offers a scalable and automated framework\nfor perceptual audio quality evaluation, poised to accelerate research and\ndevelopment in modern audio coding technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present GMLv2, a reference-based model designed for the prediction of\nsubjective audio quality as measured by MUSHRA scores. GMLv2 introduces a Beta\ndistribution-based loss to model the listener ratings and incorporates\nadditional neural audio coding (NAC) subjective datasets to extend its\ngeneralization and applicability. Extensive evaluations on diverse testset\ndemonstrate that proposed GMLv2 consistently outperforms widely used metrics,\nsuch as PEAQ and ViSQOL, both in terms of correlation with subjective scores\nand in reliably predicting these scores across diverse content types and codec\nconfigurations. Consequently, GMLv2 offers a scalable and automated framework\nfor perceptual audio quality evaluation, poised to accelerate research and\ndevelopment in modern audio coding technologies."
                },
                "authors": [
                    {
                        "name": "Vishnu Raj"
                    },
                    {
                        "name": "Gouthaman KV"
                    },
                    {
                        "name": "Shiv Gehlot"
                    },
                    {
                        "name": "Lars Villemoes"
                    },
                    {
                        "name": "Arijit Biswas"
                    }
                ],
                "author_detail": {
                    "name": "Arijit Biswas"
                },
                "author": "Arijit Biswas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21463v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21463v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10568v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10568v2",
                "updated": "2025-09-25T13:55:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    55,
                    44,
                    3,
                    268,
                    0
                ],
                "published": "2025-03-13T17:19:51Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    19,
                    51,
                    3,
                    72,
                    0
                ],
                "title": "Autoregressive Image Generation with Randomized Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Image Generation with Randomized Parallel Decoding"
                },
                "summary": "We introduce ARPG, a novel visual autoregressive model that enables\nrandomized parallel generation, addressing the inherent limitations of\nconventional raster-order approaches, which hinder inference efficiency and\nzero-shot generalization due to their sequential, predefined token generation\norder. Our key insight is that effective random-order modeling necessitates\nexplicit guidance for determining the position of the next predicted token. To\nthis end, we propose a novel decoupled decoding framework that decouples\npositional guidance from content representation, encoding them separately as\nqueries and key-value pairs. By directly incorporating this guidance into the\ncausal attention mechanism, our approach enables fully random-order training\nand generation, eliminating the need for bidirectional attention. Consequently,\nARPG readily generalizes to zero-shot inference tasks such as image inpainting,\noutpainting, and resolution expansion. Furthermore, it supports parallel\ninference by concurrently processing multiple queries using a shared KV cache.\nOn the ImageNet-1K 256 benchmark, our approach attains an FID of 1.83 with only\n32 sampling steps, achieving over a 30 times speedup in inference and a 75\npercent reduction in memory consumption compared to representative recent\nautoregressive models at a similar scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce ARPG, a novel visual autoregressive model that enables\nrandomized parallel generation, addressing the inherent limitations of\nconventional raster-order approaches, which hinder inference efficiency and\nzero-shot generalization due to their sequential, predefined token generation\norder. Our key insight is that effective random-order modeling necessitates\nexplicit guidance for determining the position of the next predicted token. To\nthis end, we propose a novel decoupled decoding framework that decouples\npositional guidance from content representation, encoding them separately as\nqueries and key-value pairs. By directly incorporating this guidance into the\ncausal attention mechanism, our approach enables fully random-order training\nand generation, eliminating the need for bidirectional attention. Consequently,\nARPG readily generalizes to zero-shot inference tasks such as image inpainting,\noutpainting, and resolution expansion. Furthermore, it supports parallel\ninference by concurrently processing multiple queries using a shared KV cache.\nOn the ImageNet-1K 256 benchmark, our approach attains an FID of 1.83 with only\n32 sampling steps, achieving over a 30 times speedup in inference and a 75\npercent reduction in memory consumption compared to representative recent\nautoregressive models at a similar scale."
                },
                "authors": [
                    {
                        "name": "Haopeng Li"
                    },
                    {
                        "name": "Jinyue Yang"
                    },
                    {
                        "name": "Guoqi Li"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10568v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10568v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21153v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21153v1",
                "updated": "2025-09-25T13:39:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    39,
                    16,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T13:39:16Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    39,
                    16,
                    3,
                    268,
                    0
                ],
                "title": "WAVECLIP: Wavelet Tokenization for Adaptive-Resolution CLIP",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WAVECLIP: Wavelet Tokenization for Adaptive-Resolution CLIP"
                },
                "summary": "We introduce WAVECLIP, a single unified model for adaptive resolution\ninference in CLIP, enabled by wavelet-based tokenization. WAVECLIP replaces\nstandard patch embeddings with a multi-level wavelet decomposition, enabling\nthe model to process images coarse to fine while naturally supporting multiple\nresolutions within the same model. At inference time, the model begins with low\nresolution tokens and refines only when needed, using key-value caching and\ncausal cross-level attention to reuse computation, effectively introducing to\nthe model only new information when needed. We evaluate WAVECLIP in zero-shot\nclassification, demonstrating that a simple confidence-based gating mechanism\nenables adaptive early exits. This allows users to dynamically choose a\ncompute-accuracy trade-off using a single deployed model. Our approach requires\nonly lightweight distillation from a frozen CLIP teacher and achieves\ncompetitive accuracy with significant computational savings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce WAVECLIP, a single unified model for adaptive resolution\ninference in CLIP, enabled by wavelet-based tokenization. WAVECLIP replaces\nstandard patch embeddings with a multi-level wavelet decomposition, enabling\nthe model to process images coarse to fine while naturally supporting multiple\nresolutions within the same model. At inference time, the model begins with low\nresolution tokens and refines only when needed, using key-value caching and\ncausal cross-level attention to reuse computation, effectively introducing to\nthe model only new information when needed. We evaluate WAVECLIP in zero-shot\nclassification, demonstrating that a simple confidence-based gating mechanism\nenables adaptive early exits. This allows users to dynamically choose a\ncompute-accuracy trade-off using a single deployed model. Our approach requires\nonly lightweight distillation from a frozen CLIP teacher and achieves\ncompetitive accuracy with significant computational savings."
                },
                "authors": [
                    {
                        "name": "Moshe Kimhi"
                    },
                    {
                        "name": "Erez Koifman"
                    },
                    {
                        "name": "Ehud Rivlin"
                    },
                    {
                        "name": "Eli Schwartz"
                    },
                    {
                        "name": "Chaim Baskin"
                    }
                ],
                "author_detail": {
                    "name": "Chaim Baskin"
                },
                "author": "Chaim Baskin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21153v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22156v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22156v2",
                "updated": "2025-09-25T13:15:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    15,
                    45,
                    3,
                    268,
                    0
                ],
                "published": "2025-05-28T09:20:18Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    9,
                    20,
                    18,
                    2,
                    148,
                    0
                ],
                "title": "InComeS: Integrating Compression and Selection Mechanisms into LLMs for\n  Efficient Model Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InComeS: Integrating Compression and Selection Mechanisms into LLMs for\n  Efficient Model Editing"
                },
                "summary": "Although existing model editing methods perform well in recalling exact edit\nfacts, they often struggle in complex scenarios that require deeper semantic\nunderstanding rather than mere knowledge regurgitation. Leveraging the strong\ncontextual reasoning abilities of large language models (LLMs), in-context\nlearning (ICL) becomes a promising editing method by comprehending edit\ninformation through context encoding. However, this method is constrained by\nthe limited context window of LLMs, leading to degraded performance and\nefficiency as the number of edits increases. To overcome this limitation, we\npropose InComeS, a flexible framework that enhances LLMs' ability to process\nediting contexts through explicit compression and selection mechanisms.\nSpecifically, InComeS compresses each editing context into the key-value (KV)\ncache of a special gist token, enabling efficient handling of multiple edits\nwithout being restricted by the model's context window. Furthermore,\nspecialized cross-attention modules are added to dynamically select the most\nrelevant information from the gist pools, enabling adaptive and effective\nutilization of edit information. We conduct experiments on diverse model\nediting benchmarks with various editing formats, and the results demonstrate\nthe effectiveness and efficiency of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although existing model editing methods perform well in recalling exact edit\nfacts, they often struggle in complex scenarios that require deeper semantic\nunderstanding rather than mere knowledge regurgitation. Leveraging the strong\ncontextual reasoning abilities of large language models (LLMs), in-context\nlearning (ICL) becomes a promising editing method by comprehending edit\ninformation through context encoding. However, this method is constrained by\nthe limited context window of LLMs, leading to degraded performance and\nefficiency as the number of edits increases. To overcome this limitation, we\npropose InComeS, a flexible framework that enhances LLMs' ability to process\nediting contexts through explicit compression and selection mechanisms.\nSpecifically, InComeS compresses each editing context into the key-value (KV)\ncache of a special gist token, enabling efficient handling of multiple edits\nwithout being restricted by the model's context window. Furthermore,\nspecialized cross-attention modules are added to dynamically select the most\nrelevant information from the gist pools, enabling adaptive and effective\nutilization of edit information. We conduct experiments on diverse model\nediting benchmarks with various editing formats, and the results demonstrate\nthe effectiveness and efficiency of our method."
                },
                "authors": [
                    {
                        "name": "Shuaiyi Li"
                    },
                    {
                        "name": "Zhisong Zhang"
                    },
                    {
                        "name": "Yang Deng"
                    },
                    {
                        "name": "Chenlong Deng"
                    },
                    {
                        "name": "Tianqing Fang"
                    },
                    {
                        "name": "Hongming Zhang"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Dong Yu"
                    },
                    {
                        "name": "Wai Lam"
                    }
                ],
                "author_detail": {
                    "name": "Wai Lam"
                },
                "author": "Wai Lam",
                "arxiv_comment": "18 pages,5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22156v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22156v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17396v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17396v2",
                "updated": "2025-09-25T10:24:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    10,
                    24,
                    14,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-22T06:56:35Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    6,
                    56,
                    35,
                    0,
                    265,
                    0
                ],
                "title": "EpiCache: Episodic KV Cache Management for Long Conversational Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EpiCache: Episodic KV Cache Management for Long Conversational Question\n  Answering"
                },
                "summary": "Modern large language models (LLMs) extend context lengths to up to millions\nof tokens, enabling AI assistants to generate coherent and personalized\nresponses grounded in long conversational histories. This ability, however,\nhinges on Key-Value (KV) caching, whose memory grows linearly with dialogue\nlength and quickly becomes the bottleneck in resource-constrained environments.\nAn active line of research for reducing memory bottleneck is KV cache\ncompression, which seeks to limit cache size while preserving accuracy. Yet\nexisting methods face two major limitations: (i) evicting the KV cache after\nfull-context prefill causes unbounded peak memory, and (ii) query-dependent\neviction narrows the cache to a single query, leading to failure cases in\nmulti-turn conversations. We introduce EpiCache, a training-free KV cache\nmanagement framework for long conversational question answering (LongConvQA)\nunder fixed memory budgets. EpiCache bounds cache growth through block-wise\nprefill and preserves topic-relevant context via episodic KV compression, which\nclusters conversation history into coherent episodes and applies\nepisode-specific KV cache eviction. We further design an adaptive layer-wise\nbudget allocation strategy that measures each layer's sensitivity to eviction\nand distributes the memory budget across layers accordingly. Across three\nLongConvQA benchmarks, EpiCache improves accuracy by up to 40% over recent\nbaselines, sustains near-full KV accuracy under 4-6x compression, and reduces\nlatency and memory by up to 2.4x and 3.5x, thereby enabling efficient\nmulti-turn interaction under strict resource constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language models (LLMs) extend context lengths to up to millions\nof tokens, enabling AI assistants to generate coherent and personalized\nresponses grounded in long conversational histories. This ability, however,\nhinges on Key-Value (KV) caching, whose memory grows linearly with dialogue\nlength and quickly becomes the bottleneck in resource-constrained environments.\nAn active line of research for reducing memory bottleneck is KV cache\ncompression, which seeks to limit cache size while preserving accuracy. Yet\nexisting methods face two major limitations: (i) evicting the KV cache after\nfull-context prefill causes unbounded peak memory, and (ii) query-dependent\neviction narrows the cache to a single query, leading to failure cases in\nmulti-turn conversations. We introduce EpiCache, a training-free KV cache\nmanagement framework for long conversational question answering (LongConvQA)\nunder fixed memory budgets. EpiCache bounds cache growth through block-wise\nprefill and preserves topic-relevant context via episodic KV compression, which\nclusters conversation history into coherent episodes and applies\nepisode-specific KV cache eviction. We further design an adaptive layer-wise\nbudget allocation strategy that measures each layer's sensitivity to eviction\nand distributes the memory budget across layers accordingly. Across three\nLongConvQA benchmarks, EpiCache improves accuracy by up to 40% over recent\nbaselines, sustains near-full KV accuracy under 4-6x compression, and reduces\nlatency and memory by up to 2.4x and 3.5x, thereby enabling efficient\nmulti-turn interaction under strict resource constraints."
                },
                "authors": [
                    {
                        "name": "Minsoo Kim"
                    },
                    {
                        "name": "Arnav Kundu"
                    },
                    {
                        "name": "Han-Byul Kim"
                    },
                    {
                        "name": "Richa Dixit"
                    },
                    {
                        "name": "Minsik Cho"
                    }
                ],
                "author_detail": {
                    "name": "Minsik Cho"
                },
                "author": "Minsik Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17396v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17396v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20979v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20979v1",
                "updated": "2025-09-25T10:23:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    10,
                    23,
                    50,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T10:23:50Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    10,
                    23,
                    50,
                    3,
                    268,
                    0
                ],
                "title": "Toward Robust and Efficient ML-Based GPU Caching for Modern Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Robust and Efficient ML-Based GPU Caching for Modern Inference"
                },
                "summary": "In modern GPU inference, cache efficiency remains a major bottleneck. In\nrecommendation models, embedding hit rates largely determine throughput, while\nin large language models, KV-cache misses substantially increase\ntime-to-first-token (TTFT). Heuristic policies such as \\textsc{LRU} often\nstruggle under structured access patterns. Learning-based approaches are\npromising, but in practice face two major limitations: they degrade sharply\nwhen predictions are inaccurate, or they gain little even with accurate\npredictions due to conservative designs. Some also incur high overhead, further\nlimiting practicality.\n  We present \\textsc{LCR}, a practical framework for learning-based GPU caching\nthat delivers performance gains while ensuring robustness and efficiency. Its\ncore algorithm, \\textsc{LARU}, enhances \\textsc{LRU} with machine-learned\npredictions and dynamically adapts to prediction accuracy through online error\nestimation. When predictions are accurate, \\textsc{LARU} achieves near-optimal\nperformance. With inaccurate predictions, it degrades gracefully to\nnear-\\textsc{LRU} performance. With \\textsc{LCR}, we bridge the gap between\nempirical progress and theoretical advances in learning-based caching.\n  Experiments show that \\textsc{LCR} delivers consistent gains under realistic\nconditions. In DLRM and LLM scenarios, it improves throughput by up to 24.2\\%\nand reduces P99 TTFT by up to 28.3\\%, outperforming widely used inference\nsystems. Even under poor predictions, its performance remains stable,\ndemonstrating practical robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern GPU inference, cache efficiency remains a major bottleneck. In\nrecommendation models, embedding hit rates largely determine throughput, while\nin large language models, KV-cache misses substantially increase\ntime-to-first-token (TTFT). Heuristic policies such as \\textsc{LRU} often\nstruggle under structured access patterns. Learning-based approaches are\npromising, but in practice face two major limitations: they degrade sharply\nwhen predictions are inaccurate, or they gain little even with accurate\npredictions due to conservative designs. Some also incur high overhead, further\nlimiting practicality.\n  We present \\textsc{LCR}, a practical framework for learning-based GPU caching\nthat delivers performance gains while ensuring robustness and efficiency. Its\ncore algorithm, \\textsc{LARU}, enhances \\textsc{LRU} with machine-learned\npredictions and dynamically adapts to prediction accuracy through online error\nestimation. When predictions are accurate, \\textsc{LARU} achieves near-optimal\nperformance. With inaccurate predictions, it degrades gracefully to\nnear-\\textsc{LRU} performance. With \\textsc{LCR}, we bridge the gap between\nempirical progress and theoretical advances in learning-based caching.\n  Experiments show that \\textsc{LCR} delivers consistent gains under realistic\nconditions. In DLRM and LLM scenarios, it improves throughput by up to 24.2\\%\nand reduces P99 TTFT by up to 28.3\\%, outperforming widely used inference\nsystems. Even under poor predictions, its performance remains stable,\ndemonstrating practical robustness."
                },
                "authors": [
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Jiaji Zhang"
                    },
                    {
                        "name": "Hailiang Zhao"
                    },
                    {
                        "name": "Yirong Zhang"
                    },
                    {
                        "name": "Jiahong Yu"
                    },
                    {
                        "name": "Xueyan Tang"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Jianping Zou"
                    },
                    {
                        "name": "Gang Xiong"
                    },
                    {
                        "name": "Kingsum Chow"
                    },
                    {
                        "name": "Shuibing He"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20979v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20979v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16242v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16242v5",
                "updated": "2025-09-25T09:49:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    9,
                    49,
                    59,
                    3,
                    268,
                    0
                ],
                "published": "2025-07-22T05:26:28Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    5,
                    26,
                    28,
                    1,
                    203,
                    0
                ],
                "title": "Robustifying Learning-Augmented Caching Efficiently without Compromising\n  1-Consistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robustifying Learning-Augmented Caching Efficiently without Compromising\n  1-Consistency"
                },
                "summary": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce significant computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_k + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only $O(1)$ additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce significant computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_k + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only $O(1)$ additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice."
                },
                "authors": [
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Hailiang Zhao"
                    },
                    {
                        "name": "Jiaji Zhang"
                    },
                    {
                        "name": "Xueyan Tang"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng",
                "arxiv_comment": "Accepted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16242v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16242v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17892v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17892v2",
                "updated": "2025-09-25T03:30:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    3,
                    30,
                    6,
                    3,
                    268,
                    0
                ],
                "published": "2025-08-25T10:59:02Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    10,
                    59,
                    2,
                    0,
                    237,
                    0
                ],
                "title": "ILRe: Intermediate Layer Retrieval for Context Compression in Causal\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ILRe: Intermediate Layer Retrieval for Context Compression in Causal\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated success across many\nbenchmarks. However, they still exhibit limitations in long-context scenarios,\nprimarily due to their short effective context length, quadratic computational\ncomplexity, and high memory overhead when processing lengthy inputs. To\nmitigate these issues, we introduce a novel context compression pipeline,\ncalled Intermediate Layer Retrieval (ILRe), which determines one intermediate\ndecoder layer offline, encodes context by streaming chunked prefill only up to\nthat layer, and recalls tokens by the attention scores between the input query\nand full key cache in that specified layer. In particular, we propose a\nmulti-pooling kernels allocating strategy in the token recalling process to\nmaintain the completeness of semantics. Our approach not only reduces the\nprefilling complexity from $O(L^2)$ to $O(L)$ and trims the memory footprint to\na few tenths of that required for the full context, but also delivers\nperformance comparable to or superior to the full-context setup in long-context\nscenarios. Without additional post training or operator development, ILRe can\nprocess a single $1M$ tokens request in less than half a minute (speedup\n$\\approx 180\\times$) and scores RULER-$1M$ benchmark of $\\approx 79.8$ with\nmodel Llama-3.1-UltraLong-8B-1M-Instruct on a Huawei Ascend 910B NPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated success across many\nbenchmarks. However, they still exhibit limitations in long-context scenarios,\nprimarily due to their short effective context length, quadratic computational\ncomplexity, and high memory overhead when processing lengthy inputs. To\nmitigate these issues, we introduce a novel context compression pipeline,\ncalled Intermediate Layer Retrieval (ILRe), which determines one intermediate\ndecoder layer offline, encodes context by streaming chunked prefill only up to\nthat layer, and recalls tokens by the attention scores between the input query\nand full key cache in that specified layer. In particular, we propose a\nmulti-pooling kernels allocating strategy in the token recalling process to\nmaintain the completeness of semantics. Our approach not only reduces the\nprefilling complexity from $O(L^2)$ to $O(L)$ and trims the memory footprint to\na few tenths of that required for the full context, but also delivers\nperformance comparable to or superior to the full-context setup in long-context\nscenarios. Without additional post training or operator development, ILRe can\nprocess a single $1M$ tokens request in less than half a minute (speedup\n$\\approx 180\\times$) and scores RULER-$1M$ benchmark of $\\approx 79.8$ with\nmodel Llama-3.1-UltraLong-8B-1M-Instruct on a Huawei Ascend 910B NPU."
                },
                "authors": [
                    {
                        "name": "Manlai Liang"
                    },
                    {
                        "name": "Mandi Liu"
                    },
                    {
                        "name": "Jiangzhou Ji"
                    },
                    {
                        "name": "Huaijun Li"
                    },
                    {
                        "name": "Haobo Yang"
                    },
                    {
                        "name": "Yaohan He"
                    },
                    {
                        "name": "Jinlong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinlong Li"
                },
                "author": "Jinlong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17892v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17892v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15919v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15919v2",
                "updated": "2025-09-25T03:00:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    3,
                    0,
                    22,
                    3,
                    268,
                    0
                ],
                "published": "2025-08-21T18:40:20Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    18,
                    40,
                    20,
                    3,
                    233,
                    0
                ],
                "title": "HyperFlexis: Joint Design of Algorithms and Systems for Multi-SLO\n  Serving and Fast Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyperFlexis: Joint Design of Algorithms and Systems for Multi-SLO\n  Serving and Fast Scaling"
                },
                "summary": "Modern large language model (LLM) serving systems face challenges from highly\nvariable requests with diverse lengths, priorities, and stage-specific\nservice-level objectives (SLOs). Meeting these requires real-time scheduling,\nrapid and cost-effective scaling, and support for both collocated and\ndisaggregated Prefill/Decode (P/D) architectures. We present HyperFlexis, a\nunified LLM serving system that integrates algorithmic and system-level\ninnovations to jointly optimize scheduling and scaling under multiple SLOs. It\nfeatures a multi-SLO-aware scheduler that leverages budget estimation and\nrequest prioritization to ensure proactive SLO compliance for both new and\nongoing requests. The system supports prefill- and decode-stage multi-SLO\nscheduling for P/D-disaggregated architectures and KV cache transfers. It also\nenables cost-effective scaling decisions, prefill-decode instance linking\nduring scaling, and rapid P/D role transitions. To accelerate scaling and\nreduce cold-start latency, a device-to-device (D2D) weight transfer mechanism\nis proposed that lowers weight loading overhead by up to 19.39$\\times$. These\noptimizations allow the system to achieve up to 4.44$\\times$ higher SLO\nattainment, 65.82% lower request latency, and cost parity with state-of-the-art\nbaselines. The code will be released soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language model (LLM) serving systems face challenges from highly\nvariable requests with diverse lengths, priorities, and stage-specific\nservice-level objectives (SLOs). Meeting these requires real-time scheduling,\nrapid and cost-effective scaling, and support for both collocated and\ndisaggregated Prefill/Decode (P/D) architectures. We present HyperFlexis, a\nunified LLM serving system that integrates algorithmic and system-level\ninnovations to jointly optimize scheduling and scaling under multiple SLOs. It\nfeatures a multi-SLO-aware scheduler that leverages budget estimation and\nrequest prioritization to ensure proactive SLO compliance for both new and\nongoing requests. The system supports prefill- and decode-stage multi-SLO\nscheduling for P/D-disaggregated architectures and KV cache transfers. It also\nenables cost-effective scaling decisions, prefill-decode instance linking\nduring scaling, and rapid P/D role transitions. To accelerate scaling and\nreduce cold-start latency, a device-to-device (D2D) weight transfer mechanism\nis proposed that lowers weight loading overhead by up to 19.39$\\times$. These\noptimizations allow the system to achieve up to 4.44$\\times$ higher SLO\nattainment, 65.82% lower request latency, and cost parity with state-of-the-art\nbaselines. The code will be released soon."
                },
                "authors": [
                    {
                        "name": "Zahra Yousefijamarani"
                    },
                    {
                        "name": "Xinglu Wang"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Morgan Lindsay Heisler"
                    },
                    {
                        "name": "Taha Shabani"
                    },
                    {
                        "name": "Niloofar Gholipour"
                    },
                    {
                        "name": "Parham Yassini"
                    },
                    {
                        "name": "Hong Chang"
                    },
                    {
                        "name": "Kan Chen"
                    },
                    {
                        "name": "Qiantao Zhang"
                    },
                    {
                        "name": "Xiaolong Bai"
                    },
                    {
                        "name": "Jiannan Wang"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Zhenan Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Fan"
                },
                "author": "Zhenan Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15919v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15919v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20617v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20617v1",
                "updated": "2025-09-24T23:47:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    23,
                    47,
                    55,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T23:47:55Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    23,
                    47,
                    55,
                    2,
                    267,
                    0
                ],
                "title": "DELM: a Python toolkit for Data Extraction with Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DELM: a Python toolkit for Data Extraction with Language Models"
                },
                "summary": "Large Language Models (LLMs) have become powerful tools for annotating\nunstructured data. However, most existing workflows rely on ad hoc scripts,\nmaking reproducibility, robustness, and systematic evaluation difficult. To\naddress these challenges, we introduce DELM (Data Extraction with Language\nModels), an open-source Python toolkit designed for rapid experimental\niteration of LLM-based data extraction pipelines and for quantifying the\ntrade-offs between them. DELM minimizes boilerplate code and offers a modular\nframework with structured outputs, built-in validation, flexible data-loading\nand scoring strategies, and efficient batch processing. It also includes robust\nsupport for working with LLM APIs, featuring retry logic, result caching,\ndetailed cost tracking, and comprehensive configuration management. We showcase\nDELM's capabilities through two case studies: one featuring a novel prompt\noptimization algorithm, and another illustrating how DELM quantifies trade-offs\nbetween cost and coverage when selecting keywords to decide which paragraphs to\npass to an LLM. DELM is available at\n\\href{https://github.com/Center-for-Applied-AI/delm}{\\texttt{github.com/Center-for-Applied-AI/delm}}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become powerful tools for annotating\nunstructured data. However, most existing workflows rely on ad hoc scripts,\nmaking reproducibility, robustness, and systematic evaluation difficult. To\naddress these challenges, we introduce DELM (Data Extraction with Language\nModels), an open-source Python toolkit designed for rapid experimental\niteration of LLM-based data extraction pipelines and for quantifying the\ntrade-offs between them. DELM minimizes boilerplate code and offers a modular\nframework with structured outputs, built-in validation, flexible data-loading\nand scoring strategies, and efficient batch processing. It also includes robust\nsupport for working with LLM APIs, featuring retry logic, result caching,\ndetailed cost tracking, and comprehensive configuration management. We showcase\nDELM's capabilities through two case studies: one featuring a novel prompt\noptimization algorithm, and another illustrating how DELM quantifies trade-offs\nbetween cost and coverage when selecting keywords to decide which paragraphs to\npass to an LLM. DELM is available at\n\\href{https://github.com/Center-for-Applied-AI/delm}{\\texttt{github.com/Center-for-Applied-AI/delm}}."
                },
                "authors": [
                    {
                        "name": "Eric Fithian"
                    },
                    {
                        "name": "Kirill Skobelev"
                    }
                ],
                "author_detail": {
                    "name": "Kirill Skobelev"
                },
                "author": "Kirill Skobelev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20617v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20617v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03090v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03090v2",
                "updated": "2025-09-24T16:56:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    16,
                    56,
                    17,
                    2,
                    267,
                    0
                ],
                "published": "2024-10-04T02:32:36Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    2,
                    32,
                    36,
                    4,
                    278,
                    0
                ],
                "title": "UNComp: Can Matrix Entropy Uncover Sparsity? -- A Compressor Design from\n  an Uncertainty-Aware Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UNComp: Can Matrix Entropy Uncover Sparsity? -- A Compressor Design from\n  an Uncertainty-Aware Perspective"
                },
                "summary": "Deploying large language models (LLMs) for long-context inference remains\nchallenging due to their substantial memory and computational demands. While\ntechniques such as Key-Value (KV) cache compression are designed to reduce\nmemory usage, they often neglect the structured sparsity inherent in the\nrelationship between hidden states and their corresponding KV cache. In this\nwork, we explore the role of uncertainty as a potential indicator of sparsity\nwithin LLMs. We propose UNComp, an uncertainty-aware framework that leverages\ntruncated matrix entropy to identify areas of low information content, thereby\nrevealing sparsity patterns that can be used for adaptive compression. Unlike\ntraditional methods that apply uniform compression, UNComp dynamically adjusts\nits approach to compression, guided by uncertainty measures that reflect the\nimportance of various model components. Our analysis shows that sparsity\npatterns, when derived from uncertainty estimates, can be exploited to reveal\nspecial long-range dependencies, such as retrieval heads and retrieval layers.\nThis perspective not only enhances our understanding of how compression can be\noptimized but also provides new insights into the inherent sparsity of LLMs\nduring long-context inference. By focusing on uncertainty to analyze the\nsparsity pattern in detail, UNComp reduces the KV cache size to 4.74% of the\noriginal, achieves a 6% prefill speedup, and improves throughput by 6.4x - not\nonly delivering strong lossless compression performance, but also validating\nthe effectiveness of the underlying theoretical tool. We release the code at\nhttps://github.com/menik1126/UNComp.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying large language models (LLMs) for long-context inference remains\nchallenging due to their substantial memory and computational demands. While\ntechniques such as Key-Value (KV) cache compression are designed to reduce\nmemory usage, they often neglect the structured sparsity inherent in the\nrelationship between hidden states and their corresponding KV cache. In this\nwork, we explore the role of uncertainty as a potential indicator of sparsity\nwithin LLMs. We propose UNComp, an uncertainty-aware framework that leverages\ntruncated matrix entropy to identify areas of low information content, thereby\nrevealing sparsity patterns that can be used for adaptive compression. Unlike\ntraditional methods that apply uniform compression, UNComp dynamically adjusts\nits approach to compression, guided by uncertainty measures that reflect the\nimportance of various model components. Our analysis shows that sparsity\npatterns, when derived from uncertainty estimates, can be exploited to reveal\nspecial long-range dependencies, such as retrieval heads and retrieval layers.\nThis perspective not only enhances our understanding of how compression can be\noptimized but also provides new insights into the inherent sparsity of LLMs\nduring long-context inference. By focusing on uncertainty to analyze the\nsparsity pattern in detail, UNComp reduces the KV cache size to 4.74% of the\noriginal, achieves a 6% prefill speedup, and improves throughput by 6.4x - not\nonly delivering strong lossless compression performance, but also validating\nthe effectiveness of the underlying theoretical tool. We release the code at\nhttps://github.com/menik1126/UNComp."
                },
                "authors": [
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Jianghan Shen"
                    },
                    {
                        "name": "Fanghua Ye"
                    },
                    {
                        "name": "Chaofan Tao"
                    },
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Jianqiao Lu"
                    },
                    {
                        "name": "Xun Wu"
                    },
                    {
                        "name": "Chuanyang Zheng"
                    },
                    {
                        "name": "Zhijiang Guo"
                    },
                    {
                        "name": "Min Yang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Ngai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Ngai Wong"
                },
                "author": "Ngai Wong",
                "arxiv_comment": "Accepted at EMNLP 2025 (Main Conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03090v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03090v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19729v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19729v1",
                "updated": "2025-09-24T03:15:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    3,
                    15,
                    37,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T03:15:37Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    3,
                    15,
                    37,
                    2,
                    267,
                    0
                ],
                "title": "Gyges: Dynamic Cross-Instance Parallelism Transformation for Efficient\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gyges: Dynamic Cross-Instance Parallelism Transformation for Efficient\n  LLM Inference"
                },
                "summary": "Efficiently processing the dynamics of requests, especially the context\nlength variance, is important in Large Language Model (LLM) serving scenarios.\nHowever, there is an intrinsic trade-off: while leveraging parallelism\nstrategies, such as Tensor Parallelism (TP), can coordinate multiple GPUs to\naccommodate larger context lengths, it inevitably results in degraded overall\nthroughput. In this paper, we propose Cross-Instance Parallelism Transformation\n(Gyges), which adaptively adjusts the parallelism strategies of running\ninstances to align with the dynamics of incoming requests. We design (1) a\npage-friendly, header-centric layout to accelerate KV cache transformations;\n(2) dedicated weight padding to accelerate model weight transformations; and\n(3) a transformation-aware scheduler to cooperatively schedule requests and\nparallelism transformations, optimizing the overall performance. Evaluations\nusing real-world traces show that Gyges improves throughput by 1.75x-6.57x\ncompared to state-of-the-art solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently processing the dynamics of requests, especially the context\nlength variance, is important in Large Language Model (LLM) serving scenarios.\nHowever, there is an intrinsic trade-off: while leveraging parallelism\nstrategies, such as Tensor Parallelism (TP), can coordinate multiple GPUs to\naccommodate larger context lengths, it inevitably results in degraded overall\nthroughput. In this paper, we propose Cross-Instance Parallelism Transformation\n(Gyges), which adaptively adjusts the parallelism strategies of running\ninstances to align with the dynamics of incoming requests. We design (1) a\npage-friendly, header-centric layout to accelerate KV cache transformations;\n(2) dedicated weight padding to accelerate model weight transformations; and\n(3) a transformation-aware scheduler to cooperatively schedule requests and\nparallelism transformations, optimizing the overall performance. Evaluations\nusing real-world traces show that Gyges improves throughput by 1.75x-6.57x\ncompared to state-of-the-art solutions."
                },
                "authors": [
                    {
                        "name": "Haoyu Chen"
                    },
                    {
                        "name": "Xue Li"
                    },
                    {
                        "name": "Kun Qian"
                    },
                    {
                        "name": "Yu Guan"
                    },
                    {
                        "name": "Jin Zhao"
                    },
                    {
                        "name": "Xin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Wang"
                },
                "author": "Xin Wang",
                "arxiv_comment": "12 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19729v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19729v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13523v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13523v2",
                "updated": "2025-09-24T01:32:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    1,
                    32,
                    55,
                    2,
                    267,
                    0
                ],
                "published": "2025-08-19T05:27:53Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    5,
                    27,
                    53,
                    1,
                    231,
                    0
                ],
                "title": "LAMMPS-KOKKOS: Performance Portable Molecular Dynamics Across Exascale\n  Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAMMPS-KOKKOS: Performance Portable Molecular Dynamics Across Exascale\n  Architectures"
                },
                "summary": "Since its inception in 1995, LAMMPS has grown to be a world-class molecular\ndynamics code, with thousands of users, over one million lines of code, and\nmulti-scale simulation capabilities. We discuss how LAMMPS has adapted to the\nmodern heterogeneous computing landscape by integrating the Kokkos performance\nportability library into the existing C++ code. We investigate performance\nportability of simple pairwise, many-body reactive, and machine-learned\nforce-field interatomic potentials. We present results on GPUs across different\nvendors and generations, and analyze performance trends, probing FLOPS\nthroughput, memory bandwidths, cache capabilities, and thread-atomic operation\nperformance. Finally, we demonstrate strong scaling on three exascale machines\n-- OLCF Frontier, ALCF Aurora, and NNSA El Capitan -- as well as on the CSCS\nAlps supercomputer, for the three potentials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since its inception in 1995, LAMMPS has grown to be a world-class molecular\ndynamics code, with thousands of users, over one million lines of code, and\nmulti-scale simulation capabilities. We discuss how LAMMPS has adapted to the\nmodern heterogeneous computing landscape by integrating the Kokkos performance\nportability library into the existing C++ code. We investigate performance\nportability of simple pairwise, many-body reactive, and machine-learned\nforce-field interatomic potentials. We present results on GPUs across different\nvendors and generations, and analyze performance trends, probing FLOPS\nthroughput, memory bandwidths, cache capabilities, and thread-atomic operation\nperformance. Finally, we demonstrate strong scaling on three exascale machines\n-- OLCF Frontier, ALCF Aurora, and NNSA El Capitan -- as well as on the CSCS\nAlps supercomputer, for the three potentials."
                },
                "authors": [
                    {
                        "name": "Anders Johansson"
                    },
                    {
                        "name": "Evan Weinberg"
                    },
                    {
                        "name": "Christian R. Trott"
                    },
                    {
                        "name": "Megan J. McCarthy"
                    },
                    {
                        "name": "Stan G. Moore"
                    }
                ],
                "author_detail": {
                    "name": "Stan G. Moore"
                },
                "author": "Stan G. Moore",
                "arxiv_doi": "10.1145/3731599.3767498",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3731599.3767498",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.13523v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13523v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "16 pages, 7 figures",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1.4; C.2.4; C.4; D.1.3; D.3.4; E.1; I.6; I.6.8; J.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19599v1",
                "updated": "2025-09-23T21:46:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    21,
                    46,
                    38,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T21:46:38Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    21,
                    46,
                    38,
                    1,
                    266,
                    0
                ],
                "title": "Knowledge Base-Aware Orchestration: A Dynamic, Privacy-Preserving Method\n  for Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Base-Aware Orchestration: A Dynamic, Privacy-Preserving Method\n  for Multi-Agent Systems"
                },
                "summary": "Multi-agent systems (MAS) are increasingly tasked with solving complex,\nknowledge-intensive problems where effective agent orchestration is critical.\nConventional orchestration methods rely on static agent descriptions, which\noften become outdated or incomplete. This limitation leads to inefficient task\nrouting, particularly in dynamic environments where agent capabilities\ncontinuously evolve. We introduce Knowledge Base-Aware (KBA) Orchestration, a\nnovel approach that augments static descriptions with dynamic,\nprivacy-preserving relevance signals derived from each agent's internal\nknowledge base (KB). In the proposed framework, when static descriptions are\ninsufficient for a clear routing decision, the orchestrator prompts the\nsubagents in parallel. Each agent then assesses the task's relevance against\nits private KB, returning a lightweight ACK signal without exposing the\nunderlying data. These collected signals populate a shared semantic cache,\nproviding dynamic indicators of agent suitability for future queries. By\ncombining this novel mechanism with static descriptions, our method achieves\nmore accurate and adaptive task routing preserving agent autonomy and data\nconfidentiality. Benchmarks show that our KBA Orchestration significantly\noutperforms static description-driven methods in routing precision and overall\nsystem efficiency, making it suitable for large-scale systems that require\nhigher accuracy than standard description-driven routing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent systems (MAS) are increasingly tasked with solving complex,\nknowledge-intensive problems where effective agent orchestration is critical.\nConventional orchestration methods rely on static agent descriptions, which\noften become outdated or incomplete. This limitation leads to inefficient task\nrouting, particularly in dynamic environments where agent capabilities\ncontinuously evolve. We introduce Knowledge Base-Aware (KBA) Orchestration, a\nnovel approach that augments static descriptions with dynamic,\nprivacy-preserving relevance signals derived from each agent's internal\nknowledge base (KB). In the proposed framework, when static descriptions are\ninsufficient for a clear routing decision, the orchestrator prompts the\nsubagents in parallel. Each agent then assesses the task's relevance against\nits private KB, returning a lightweight ACK signal without exposing the\nunderlying data. These collected signals populate a shared semantic cache,\nproviding dynamic indicators of agent suitability for future queries. By\ncombining this novel mechanism with static descriptions, our method achieves\nmore accurate and adaptive task routing preserving agent autonomy and data\nconfidentiality. Benchmarks show that our KBA Orchestration significantly\noutperforms static description-driven methods in routing precision and overall\nsystem efficiency, making it suitable for large-scale systems that require\nhigher accuracy than standard description-driven routing."
                },
                "authors": [
                    {
                        "name": "Danilo Trombino"
                    },
                    {
                        "name": "Vincenzo Pecorella"
                    },
                    {
                        "name": "Alessandro de Giulii"
                    },
                    {
                        "name": "Davide Tresoldi"
                    }
                ],
                "author_detail": {
                    "name": "Davide Tresoldi"
                },
                "author": "Davide Tresoldi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07772v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07772v4",
                "updated": "2025-09-23T21:08:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    21,
                    8,
                    3,
                    1,
                    266,
                    0
                ],
                "published": "2024-12-10T18:59:50Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    50,
                    1,
                    345,
                    0
                ],
                "title": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models"
                },
                "summary": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to an autoregressive\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nachieves a total score of 84.27 on the VBench-Long benchmark, surpassing all\nprevious video generation models. It enables fast streaming generation of\nhigh-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our\napproach also enables streaming video-to-video translation, image-to-video, and\ndynamic prompting in a zero-shot manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to an autoregressive\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nachieves a total score of 84.27 on the VBench-Long benchmark, surpassing all\nprevious video generation models. It enables fast streaming generation of\nhigh-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our\napproach also enables streaming video-to-video translation, image-to-video, and\ndynamic prompting in a zero-shot manner."
                },
                "authors": [
                    {
                        "name": "Tianwei Yin"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Richard Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Fredo Durand"
                    },
                    {
                        "name": "Eli Shechtman"
                    },
                    {
                        "name": "Xun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xun Huang"
                },
                "author": "Xun Huang",
                "arxiv_comment": "CVPR 2025. Project Page: https://causvid.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07772v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07772v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03227v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03227v3",
                "updated": "2025-09-23T20:25:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    20,
                    25,
                    15,
                    1,
                    266,
                    0
                ],
                "published": "2024-04-04T06:24:11Z",
                "published_parsed": [
                    2024,
                    4,
                    4,
                    6,
                    24,
                    11,
                    3,
                    95,
                    0
                ],
                "title": "Decentralized Learning Strategies for Estimation Error Minimization with\n  Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized Learning Strategies for Estimation Error Minimization with\n  Graph Neural Networks"
                },
                "summary": "We address the challenge of sampling and remote estimation for autoregressive\nMarkovian processes in a multi-hop wireless network with\nstatistically-identical agents. Agents cache the most recent samples from\nothers and communicate over wireless collision channels governed by an\nunderlying graph topology. Our goal is to minimize time-average estimation\nerror and/or age of information with decentralized scalable sampling and\ntransmission policies, considering both oblivious (where decision-making is\nindependent of the physical processes) and non-oblivious policies (where\ndecision-making depends on physical processes). We prove that in oblivious\npolicies, minimizing estimation error is equivalent to minimizing the age of\ninformation. The complexity of the problem, especially the multi-dimensional\naction spaces and arbitrary network topologies, makes theoretical methods for\nfinding optimal transmission policies intractable. We optimize the policies\nusing a graphical multi-agent reinforcement learning framework, where each\nagent employs a permutation-equivariant graph neural network architecture.\nTheoretically, we prove that our proposed framework exhibits desirable\ntransferability properties, allowing transmission policies trained on small- or\nmoderate-size networks to be executed effectively on large-scale topologies.\nNumerical experiments demonstrate that (i) Our proposed framework outperforms\nstate-of-the-art baselines; (ii) The trained policies are transferable to\nlarger networks, and their performance gains increase with the number of\nagents; (iii) The training procedure withstands non-stationarity even if we\nutilize independent learning techniques; and, (iv) Recurrence is pivotal in\nboth independent learning and centralized training and decentralized execution,\nand improves the resilience to non-stationarity in independent learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the challenge of sampling and remote estimation for autoregressive\nMarkovian processes in a multi-hop wireless network with\nstatistically-identical agents. Agents cache the most recent samples from\nothers and communicate over wireless collision channels governed by an\nunderlying graph topology. Our goal is to minimize time-average estimation\nerror and/or age of information with decentralized scalable sampling and\ntransmission policies, considering both oblivious (where decision-making is\nindependent of the physical processes) and non-oblivious policies (where\ndecision-making depends on physical processes). We prove that in oblivious\npolicies, minimizing estimation error is equivalent to minimizing the age of\ninformation. The complexity of the problem, especially the multi-dimensional\naction spaces and arbitrary network topologies, makes theoretical methods for\nfinding optimal transmission policies intractable. We optimize the policies\nusing a graphical multi-agent reinforcement learning framework, where each\nagent employs a permutation-equivariant graph neural network architecture.\nTheoretically, we prove that our proposed framework exhibits desirable\ntransferability properties, allowing transmission policies trained on small- or\nmoderate-size networks to be executed effectively on large-scale topologies.\nNumerical experiments demonstrate that (i) Our proposed framework outperforms\nstate-of-the-art baselines; (ii) The trained policies are transferable to\nlarger networks, and their performance gains increase with the number of\nagents; (iii) The training procedure withstands non-stationarity even if we\nutilize independent learning techniques; and, (iv) Recurrence is pivotal in\nboth independent learning and centralized training and decentralized execution,\nand improves the resilience to non-stationarity in independent learning."
                },
                "authors": [
                    {
                        "name": "Xingran Chen"
                    },
                    {
                        "name": "Navid NaderiAlizadeh"
                    },
                    {
                        "name": "Alejandro Ribeiro"
                    },
                    {
                        "name": "Shirin Saeedi Bidokhti"
                    }
                ],
                "author_detail": {
                    "name": "Shirin Saeedi Bidokhti"
                },
                "author": "Shirin Saeedi Bidokhti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03227v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03227v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19459v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19459v1",
                "updated": "2025-09-23T18:14:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    18,
                    14,
                    21,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T18:14:21Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    18,
                    14,
                    21,
                    1,
                    266,
                    0
                ],
                "title": "Automated Insertion of Flushes and Fences for Persistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Insertion of Flushes and Fences for Persistency"
                },
                "summary": "CXL shared memory and persistent memory allow the contents of memory to\npersist beyond crashes. Stores to persistent or CXL memory are typically not\nimmediately made persistent; developers must manually flush the corresponding\ncache lines to force the data to be written to the underlying storage.\nCorrectly using flush and fence operations is known to be challenging. While\nstate-of-the-art tools can find missing flush instructions, they often require\nbug-revealing test cases. No existing tools can ensure the absence of missing\nflush bugs.\n  In this paper, we present PMRobust, a compiler that automatically inserts\nflush and fence operations to ensure that code using persistent memory is free\nfrom missing flush and fence bugs. PMRobust employs a novel static analysis\nwith optimizations that target newly allocated objects. We have evaluated\nPMRobust on persistent memory libraries and several persistent memory data\nstructures and measured a geometric mean overhead of 0.26% relative to the\noriginal benchmarks with hand-placed flush and fence operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXL shared memory and persistent memory allow the contents of memory to\npersist beyond crashes. Stores to persistent or CXL memory are typically not\nimmediately made persistent; developers must manually flush the corresponding\ncache lines to force the data to be written to the underlying storage.\nCorrectly using flush and fence operations is known to be challenging. While\nstate-of-the-art tools can find missing flush instructions, they often require\nbug-revealing test cases. No existing tools can ensure the absence of missing\nflush bugs.\n  In this paper, we present PMRobust, a compiler that automatically inserts\nflush and fence operations to ensure that code using persistent memory is free\nfrom missing flush and fence bugs. PMRobust employs a novel static analysis\nwith optimizations that target newly allocated objects. We have evaluated\nPMRobust on persistent memory libraries and several persistent memory data\nstructures and measured a geometric mean overhead of 0.26% relative to the\noriginal benchmarks with hand-placed flush and fence operations."
                },
                "authors": [
                    {
                        "name": "Yutong Guo"
                    },
                    {
                        "name": "Weiyu Luo"
                    },
                    {
                        "name": "Brian Demsky"
                    }
                ],
                "author_detail": {
                    "name": "Brian Demsky"
                },
                "author": "Brian Demsky",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19459v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19459v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19228v1",
                "updated": "2025-09-23T16:49:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    16,
                    49,
                    43,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T16:49:43Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    16,
                    49,
                    43,
                    1,
                    266,
                    0
                ],
                "title": "CompLLM: Compression for Long Context Q&A",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CompLLM: Compression for Long Context Q&A"
                },
                "summary": "Large Language Models (LLMs) face significant computational challenges when\nprocessing long contexts due to the quadratic complexity of self-attention.\nWhile soft context compression methods, which map input text to smaller latent\nrepresentations, have shown promise, their real-world adoption is limited.\nExisting techniques typically compress the context as a single unit, which\nleads to quadratic compression complexity and an inability to reuse\ncomputations across queries with overlapping contexts. In this work, we\nintroduce CompLLM, a soft compression technique designed for practical\ndeployment. Instead of processing the context holistically, CompLLM divides it\ninto segments and compresses each one independently. This simple design choice\nyields three critical properties: efficiency, as the compression step scales\nlinearly with the context length; scalability, enabling models trained on short\nsequences (e.g., 1k tokens) to generalize to contexts of 100k tokens; and\nreusability, allowing compressed segments to be cached and reused across\ndifferent queries. Our experiments show that with a 2x compression rate, at\nhigh context lengths CompLLM speeds up Time To First Token (TTFT) by up to 4x\nand reduces the KV cache size by 50%. Furthermore, CompLLM achieves performance\ncomparable to that obtained with the uncompressed context, and even surpasses\nit on very long sequences, demonstrating its effectiveness and practical\nutility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) face significant computational challenges when\nprocessing long contexts due to the quadratic complexity of self-attention.\nWhile soft context compression methods, which map input text to smaller latent\nrepresentations, have shown promise, their real-world adoption is limited.\nExisting techniques typically compress the context as a single unit, which\nleads to quadratic compression complexity and an inability to reuse\ncomputations across queries with overlapping contexts. In this work, we\nintroduce CompLLM, a soft compression technique designed for practical\ndeployment. Instead of processing the context holistically, CompLLM divides it\ninto segments and compresses each one independently. This simple design choice\nyields three critical properties: efficiency, as the compression step scales\nlinearly with the context length; scalability, enabling models trained on short\nsequences (e.g., 1k tokens) to generalize to contexts of 100k tokens; and\nreusability, allowing compressed segments to be cached and reused across\ndifferent queries. Our experiments show that with a 2x compression rate, at\nhigh context lengths CompLLM speeds up Time To First Token (TTFT) by up to 4x\nand reduces the KV cache size by 50%. Furthermore, CompLLM achieves performance\ncomparable to that obtained with the uncompressed context, and even surpasses\nit on very long sequences, demonstrating its effectiveness and practical\nutility."
                },
                "authors": [
                    {
                        "name": "Gabriele Berton"
                    },
                    {
                        "name": "Jayakrishnan Unnikrishnan"
                    },
                    {
                        "name": "Son Tran"
                    },
                    {
                        "name": "Mubarak Shah"
                    }
                ],
                "author_detail": {
                    "name": "Mubarak Shah"
                },
                "author": "Mubarak Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19061v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19061v1",
                "updated": "2025-09-23T14:25:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    25,
                    13,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T14:25:13Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    25,
                    13,
                    1,
                    266,
                    0
                ],
                "title": "3D Blocking for Matrix-free Smoothers in 2D Variable-Viscosity Stokes\n  Equations with Applications to Geodynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Blocking for Matrix-free Smoothers in 2D Variable-Viscosity Stokes\n  Equations with Applications to Geodynamics"
                },
                "summary": "We present the design, implementation, and evaluation of optimized\nmatrix-free stencil kernels for multigrid smoothing in the incompressible\nStokes equations with variable viscosity, motivated by geophysical flow\nproblems. We investigate five smoother variants derived from different\noptimisation strategies: Red-Black Gauss-Seidel, Jacobi, fused Jacobi, blocked\nfused Jacobi, and a novel Jacobi smoother with RAS-type temporal blocking, a\nstrategy that applies local iterations on overlapping tiles to improve cache\nreuse. To ensure correctness, we introduce an energy-based residual norm that\nbalances velocity and pressure contributions, and validate all implementations\nusing a high-contrast sinker benchmark representative of realistic geodynamic\nnumerical models. Our performance study on NVIDIA GH200 Grace Hopper nodes of\nthe ALPS supercomputer demonstrates that all smoothers scale well within a\nsingle NUMA domain, but the RAS-Jacobi smoother consistently achieves the best\nperformance at higher core counts. It sustains over 90% weak-scaling efficiency\nup to 64 cores and delivers up to a threefold speedup compared to the C++\nJacobi baseline, owing to improved cache reuse and reduced memory traffic.\nThese results show that temporal blocking, already employed in\ndistributed-memory solvers to reduce communication, can also provide\nsubstantial benefits at the socket and NUMA level. This work highlights the\nimportance of cache-aware stencil design for harnessing modern heterogeneous\narchitectures and lays the groundwork for extending RAS-type temporal blocking\nstrategies to three-dimensional problems and GPU accelerators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the design, implementation, and evaluation of optimized\nmatrix-free stencil kernels for multigrid smoothing in the incompressible\nStokes equations with variable viscosity, motivated by geophysical flow\nproblems. We investigate five smoother variants derived from different\noptimisation strategies: Red-Black Gauss-Seidel, Jacobi, fused Jacobi, blocked\nfused Jacobi, and a novel Jacobi smoother with RAS-type temporal blocking, a\nstrategy that applies local iterations on overlapping tiles to improve cache\nreuse. To ensure correctness, we introduce an energy-based residual norm that\nbalances velocity and pressure contributions, and validate all implementations\nusing a high-contrast sinker benchmark representative of realistic geodynamic\nnumerical models. Our performance study on NVIDIA GH200 Grace Hopper nodes of\nthe ALPS supercomputer demonstrates that all smoothers scale well within a\nsingle NUMA domain, but the RAS-Jacobi smoother consistently achieves the best\nperformance at higher core counts. It sustains over 90% weak-scaling efficiency\nup to 64 cores and delivers up to a threefold speedup compared to the C++\nJacobi baseline, owing to improved cache reuse and reduced memory traffic.\nThese results show that temporal blocking, already employed in\ndistributed-memory solvers to reduce communication, can also provide\nsubstantial benefits at the socket and NUMA level. This work highlights the\nimportance of cache-aware stencil design for harnessing modern heterogeneous\narchitectures and lays the groundwork for extending RAS-type temporal blocking\nstrategies to three-dimensional problems and GPU accelerators."
                },
                "authors": [
                    {
                        "name": "Marcel Ferrari"
                    },
                    {
                        "name": "Cyrill Püntener"
                    },
                    {
                        "name": "Alexander Sotoudeh"
                    },
                    {
                        "name": "Niklas Viebig"
                    }
                ],
                "author_detail": {
                    "name": "Niklas Viebig"
                },
                "author": "Niklas Viebig",
                "arxiv_comment": "15 pages, 5 figures, appendix has 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19061v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19061v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.comp-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65F08, 65N55, 65N22, 76M20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.1.8; F.2.1; D.1.3; C.1.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18909v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18909v1",
                "updated": "2025-09-23T12:32:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    12,
                    32,
                    51,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T12:32:51Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    12,
                    32,
                    51,
                    1,
                    266,
                    0
                ],
                "title": "Obelix: Mitigating Side-Channels Through Dynamic Obfuscation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Obelix: Mitigating Side-Channels Through Dynamic Obfuscation"
                },
                "summary": "Trusted execution environments (TEEs) offer hardware-assisted means to\nprotect code and data. However, as shown in numerous results over the years,\nattackers can use side-channels to leak data access patterns and even\nsingle-step the code. While the vendors are slowly introducing hardware-based\ncountermeasures for some attacks, others will stay unaddressed. This makes a\nsoftware-level countermeasure desirable, but current available solutions only\naddress very specific attack vectors or have a narrow leakage model.\n  In this work, we take a holistic view at the vulnerabilities of TEEs and\ndesign a tool named Obelix, which is the first to protect both code and data\nagainst a wide range of TEE attacks, from cache attacks over single-stepping to\nciphertext side-channels. We analyze the practically achievable precision of\nstate-of-the-art single-stepping tools, and present an algorithm which uses\nthat knowledge to divide a program into uniform code blocks, that are\nindistinguishable for a strong attacker. By storing these blocks and the\nprogram data in oblivious RAM, the attacker cannot follow execution,\neffectively protecting both secret code and data. We describe how we automate\nour approach to make it available for developers who are unfamiliar with\nside-channels. As an obfuscation tool, Obelix comes with a considerable\nperformance overhead, but compensates this with strong security guarantees and\neasy applicability without requiring any expert knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trusted execution environments (TEEs) offer hardware-assisted means to\nprotect code and data. However, as shown in numerous results over the years,\nattackers can use side-channels to leak data access patterns and even\nsingle-step the code. While the vendors are slowly introducing hardware-based\ncountermeasures for some attacks, others will stay unaddressed. This makes a\nsoftware-level countermeasure desirable, but current available solutions only\naddress very specific attack vectors or have a narrow leakage model.\n  In this work, we take a holistic view at the vulnerabilities of TEEs and\ndesign a tool named Obelix, which is the first to protect both code and data\nagainst a wide range of TEE attacks, from cache attacks over single-stepping to\nciphertext side-channels. We analyze the practically achievable precision of\nstate-of-the-art single-stepping tools, and present an algorithm which uses\nthat knowledge to divide a program into uniform code blocks, that are\nindistinguishable for a strong attacker. By storing these blocks and the\nprogram data in oblivious RAM, the attacker cannot follow execution,\neffectively protecting both secret code and data. We describe how we automate\nour approach to make it available for developers who are unfamiliar with\nside-channels. As an obfuscation tool, Obelix comes with a considerable\nperformance overhead, but compensates this with strong security guarantees and\neasy applicability without requiring any expert knowledge."
                },
                "authors": [
                    {
                        "name": "Jan Wichelmann"
                    },
                    {
                        "name": "Anja Rabich"
                    },
                    {
                        "name": "Anna P\"atschke"
                    },
                    {
                        "name": "Thomas Eisenbarth"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Eisenbarth"
                },
                "author": "Thomas Eisenbarth",
                "arxiv_doi": "10.1109/SP54263.2024.00261",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/SP54263.2024.00261",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.18909v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18909v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "2024 IEEE Symposium on Security and Privacy (SP), San Francisco,\n  CA, USA, 2024, pp. 4182-4199",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04467v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04467v3",
                "updated": "2025-09-23T08:31:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    8,
                    31,
                    26,
                    1,
                    266,
                    0
                ],
                "published": "2025-08-29T02:29:52Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    2,
                    29,
                    52,
                    4,
                    241,
                    0
                ],
                "title": "PDTrim: Targeted Pruning for Prefill-Decode Disaggregation in Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PDTrim: Targeted Pruning for Prefill-Decode Disaggregation in Inference"
                },
                "summary": "Large Language Models (LLMs) demonstrate exceptional capabilities across\nvarious tasks, but their deployment is constrained by high computational and\nmemory costs. Model pruning provides an effective means to alleviate these\ndemands. However, existing methods often ignore the characteristics of\nprefill-decode (PD) disaggregation in practice. In this paper, we propose a\nnovel pruning method for PD disaggregation inference, enabling more precise and\nefficient block and KV Cache pruning. Our approach constructs pruning and\ndistillation sets to perform iterative block removal independently for the\nprefill and decode stages, obtaining better pruning solutions. Moreover, we\nintroduce a token-aware cache pruning mechanism that retains all KV Cache in\nthe prefill stage but selectively reuses entries for the first and last token\nsequences in selected layers during decode, reducing communication costs with\nminimal overhead. Extensive experiments demonstrate that our approach\nconsistently achieves strong performance in both PD disaggregation and PD\nunified settings without disaggregation. Under the same (default) settings, our\nmethod achieves improved performance and faster inference, along with a\n4.95$\\times$ reduction in data transmission bandwidth consumption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate exceptional capabilities across\nvarious tasks, but their deployment is constrained by high computational and\nmemory costs. Model pruning provides an effective means to alleviate these\ndemands. However, existing methods often ignore the characteristics of\nprefill-decode (PD) disaggregation in practice. In this paper, we propose a\nnovel pruning method for PD disaggregation inference, enabling more precise and\nefficient block and KV Cache pruning. Our approach constructs pruning and\ndistillation sets to perform iterative block removal independently for the\nprefill and decode stages, obtaining better pruning solutions. Moreover, we\nintroduce a token-aware cache pruning mechanism that retains all KV Cache in\nthe prefill stage but selectively reuses entries for the first and last token\nsequences in selected layers during decode, reducing communication costs with\nminimal overhead. Extensive experiments demonstrate that our approach\nconsistently achieves strong performance in both PD disaggregation and PD\nunified settings without disaggregation. Under the same (default) settings, our\nmethod achieves improved performance and faster inference, along with a\n4.95$\\times$ reduction in data transmission bandwidth consumption."
                },
                "authors": [
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Mengsi Lyu"
                    },
                    {
                        "name": "Zhuo Chen"
                    },
                    {
                        "name": "Xingrun Xing"
                    },
                    {
                        "name": "Yulong Ao"
                    },
                    {
                        "name": "Yonghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Yonghua Lin"
                },
                "author": "Yonghua Lin",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04467v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04467v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08378v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08378v2",
                "updated": "2025-09-23T08:24:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    8,
                    24,
                    7,
                    1,
                    266,
                    0
                ],
                "published": "2025-04-11T09:26:47Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    9,
                    26,
                    47,
                    4,
                    101,
                    0
                ],
                "title": "Scaling Up On-Device LLMs via Active-Weight Swapping Between DRAM and\n  Flash",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Up On-Device LLMs via Active-Weight Swapping Between DRAM and\n  Flash"
                },
                "summary": "Large language models (LLMs) are increasingly being deployed on mobile\ndevices, but the limited DRAM capacity constrains the deployable model size.\nThis paper introduces ActiveFlow, the first LLM inference framework that can\nachieve adaptive DRAM usage for modern LLMs (not ReLU-based), enabling the\nscaling up of deployable model sizes. The framework is based on the novel\nconcept of active weight DRAM-flash swapping and incorporates three novel\ntechniques: (1) Cross-layer active weights preloading. It uses the activations\nfrom the current layer to predict the active weights of several subsequent\nlayers, enabling computation and data loading to overlap, as well as\nfacilitating large I/O transfers. (2) Sparsity-aware self-distillation. It\nadjusts the active weights to align with the dense-model output distribution,\ncompensating for approximations introduced by contextual sparsity. (3) Active\nweight DRAM-flash swapping pipeline. It orchestrates the DRAM space allocation\namong the hot weight cache, preloaded active weights, and computation-involved\nweights based on available memory. Results show ActiveFlow achieves the\nperformance-cost Pareto frontier compared to existing efficiency optimization\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly being deployed on mobile\ndevices, but the limited DRAM capacity constrains the deployable model size.\nThis paper introduces ActiveFlow, the first LLM inference framework that can\nachieve adaptive DRAM usage for modern LLMs (not ReLU-based), enabling the\nscaling up of deployable model sizes. The framework is based on the novel\nconcept of active weight DRAM-flash swapping and incorporates three novel\ntechniques: (1) Cross-layer active weights preloading. It uses the activations\nfrom the current layer to predict the active weights of several subsequent\nlayers, enabling computation and data loading to overlap, as well as\nfacilitating large I/O transfers. (2) Sparsity-aware self-distillation. It\nadjusts the active weights to align with the dense-model output distribution,\ncompensating for approximations introduced by contextual sparsity. (3) Active\nweight DRAM-flash swapping pipeline. It orchestrates the DRAM space allocation\namong the hot weight cache, preloaded active weights, and computation-involved\nweights based on available memory. Results show ActiveFlow achieves the\nperformance-cost Pareto frontier compared to existing efficiency optimization\nmethods."
                },
                "authors": [
                    {
                        "name": "Fucheng Jia"
                    },
                    {
                        "name": "Zewen Wu"
                    },
                    {
                        "name": "Shiqi Jiang"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Yunxin Liu"
                    },
                    {
                        "name": "Ju Ren"
                    },
                    {
                        "name": "Deyu Zhang"
                    },
                    {
                        "name": "Ting Cao"
                    }
                ],
                "author_detail": {
                    "name": "Ting Cao"
                },
                "author": "Ting Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08378v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08378v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18684v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18684v1",
                "updated": "2025-09-23T06:10:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    6,
                    10,
                    20,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T06:10:20Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    6,
                    10,
                    20,
                    1,
                    266,
                    0
                ],
                "title": "Static Estimation of Reuse Profiles for Arrays in Nested Loops",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Static Estimation of Reuse Profiles for Arrays in Nested Loops"
                },
                "summary": "Efficient memory access patterns play a crucial role in determining the\noverall performance of applications by exploiting temporal and spatial\nlocality, thus maximizing cache locality. The Reuse Distance Histogram (RDH) is\na widely used metric to quantify temporal locality, measuring the distance\nbetween consecutive accesses to the same memory location. Traditionally,\ncalculating RDH requires program execution and memory trace collection to\nobtain dynamic memory access behavior. This trace collection is often\ntime-consuming, resource-intensive, and unsuitable for early-stage optimization\nor large-scale applications. Static prediction, on the other hand, offers a\nsignificant speedup in estimating RDH and cache hit rates. However, these\napproaches lack accuracy, since the predictions come without running the\nprogram and knowing the complete memory access pattern, more specifically when\narrays are used inside nested loops. This paper presents a novel static\nanalysis framework for predicting the reuse profiles of array references in\nprograms with nested loop structures, without requiring any runtime\ninformation. By analyzing loop bounds, access patterns in smaller problem\nsizes, and predictive equations, our method predicts access patterns of arrays\nand estimates reuse distances and cache hit rate at compile time. This paper\nextends our previous study by incorporating more analysis and improving\nprediction by addressing previously unhandled reuse patterns. We evaluate our\ntechnique against a widely accepted traditional trace-driven profiling tool,\nParallel Reuse Distance Analysis (PARDA). The results demonstrate that our\nstatic predictor achieves comparable accuracy while offering\norders-of-magnitude improvement in the analysis speed. This work offers a\npractical alternative to dynamic reuse profiling and paves the way for\nintegration into compilers and static performance modeling tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient memory access patterns play a crucial role in determining the\noverall performance of applications by exploiting temporal and spatial\nlocality, thus maximizing cache locality. The Reuse Distance Histogram (RDH) is\na widely used metric to quantify temporal locality, measuring the distance\nbetween consecutive accesses to the same memory location. Traditionally,\ncalculating RDH requires program execution and memory trace collection to\nobtain dynamic memory access behavior. This trace collection is often\ntime-consuming, resource-intensive, and unsuitable for early-stage optimization\nor large-scale applications. Static prediction, on the other hand, offers a\nsignificant speedup in estimating RDH and cache hit rates. However, these\napproaches lack accuracy, since the predictions come without running the\nprogram and knowing the complete memory access pattern, more specifically when\narrays are used inside nested loops. This paper presents a novel static\nanalysis framework for predicting the reuse profiles of array references in\nprograms with nested loop structures, without requiring any runtime\ninformation. By analyzing loop bounds, access patterns in smaller problem\nsizes, and predictive equations, our method predicts access patterns of arrays\nand estimates reuse distances and cache hit rate at compile time. This paper\nextends our previous study by incorporating more analysis and improving\nprediction by addressing previously unhandled reuse patterns. We evaluate our\ntechnique against a widely accepted traditional trace-driven profiling tool,\nParallel Reuse Distance Analysis (PARDA). The results demonstrate that our\nstatic predictor achieves comparable accuracy while offering\norders-of-magnitude improvement in the analysis speed. This work offers a\npractical alternative to dynamic reuse profiling and paves the way for\nintegration into compilers and static performance modeling tools."
                },
                "authors": [
                    {
                        "name": "Abdur Razzak"
                    },
                    {
                        "name": "Atanu Barai"
                    },
                    {
                        "name": "Nandakishore Santhi"
                    },
                    {
                        "name": "Abdel-Hameed A. Badawy"
                    }
                ],
                "author_detail": {
                    "name": "Abdel-Hameed A. Badawy"
                },
                "author": "Abdel-Hameed A. Badawy",
                "arxiv_comment": "This paper is accepted at the MEMSYS 2025 conference, 11th\n  International Symposium on Memory Systems, Washington D.C., October 7 -\n  October 8, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18684v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18684v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18670v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18670v1",
                "updated": "2025-09-23T05:39:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    5,
                    39,
                    47,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T05:39:47Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    5,
                    39,
                    47,
                    1,
                    266,
                    0
                ],
                "title": "CALL: Context-Aware Low-Latency Retrieval in Disk-Based Vector Databases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CALL: Context-Aware Low-Latency Retrieval in Disk-Based Vector Databases"
                },
                "summary": "Embedding models capture both semantic and syntactic structures of queries,\noften mapping different queries to similar regions in vector space. This\nresults in non-uniform cluster access patterns in modern disk-based vector\ndatabases. While existing approaches optimize individual queries, they overlook\nthe impact of cluster access patterns, failing to account for the locality\neffects of queries that access similar clusters. This oversight increases cache\nmiss penalty. To minimize the cache miss penalty, we propose CALL, a\ncontext-aware query grouping mechanism that organizes queries based on shared\ncluster access patterns. Additionally, CALL incorporates a group-aware\nprefetching method to minimize cache misses during transitions between query\ngroups and latency-aware cluster loading. Experimental results show that CALL\nreduces the 99th percentile tail latency by up to 33% while consistently\nmaintaining a higher cache hit ratio, substantially reducing search latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embedding models capture both semantic and syntactic structures of queries,\noften mapping different queries to similar regions in vector space. This\nresults in non-uniform cluster access patterns in modern disk-based vector\ndatabases. While existing approaches optimize individual queries, they overlook\nthe impact of cluster access patterns, failing to account for the locality\neffects of queries that access similar clusters. This oversight increases cache\nmiss penalty. To minimize the cache miss penalty, we propose CALL, a\ncontext-aware query grouping mechanism that organizes queries based on shared\ncluster access patterns. Additionally, CALL incorporates a group-aware\nprefetching method to minimize cache misses during transitions between query\ngroups and latency-aware cluster loading. Experimental results show that CALL\nreduces the 99th percentile tail latency by up to 33% while consistently\nmaintaining a higher cache hit ratio, substantially reducing search latency."
                },
                "authors": [
                    {
                        "name": "Yeonwoo Jeong"
                    },
                    {
                        "name": "Hyunji Cho"
                    },
                    {
                        "name": "Kyuri Park"
                    },
                    {
                        "name": "Youngjae Kim"
                    },
                    {
                        "name": "Sungyong Park"
                    }
                ],
                "author_detail": {
                    "name": "Sungyong Park"
                },
                "author": "Sungyong Park",
                "arxiv_comment": "11 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18670v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18670v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18592v1",
                "updated": "2025-09-23T03:23:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    3,
                    23,
                    3,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T03:23:03Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    3,
                    23,
                    3,
                    1,
                    266,
                    0
                ],
                "title": "VLN-Zero: Rapid Exploration and Cache-Enabled Neurosymbolic\n  Vision-Language Planning for Zero-Shot Transfer in Robot Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLN-Zero: Rapid Exploration and Cache-Enabled Neurosymbolic\n  Vision-Language Planning for Zero-Shot Transfer in Robot Navigation"
                },
                "summary": "Rapid adaptation in unseen environments is essential for scalable real-world\nautonomy, yet existing approaches rely on exhaustive exploration or rigid\nnavigation policies that fail to generalize. We present VLN-Zero, a two-phase\nvision-language navigation framework that leverages vision-language models to\nefficiently construct symbolic scene graphs and enable zero-shot neurosymbolic\nnavigation. In the exploration phase, structured prompts guide VLM-based search\ntoward informative and diverse trajectories, yielding compact scene graph\nrepresentations. In the deployment phase, a neurosymbolic planner reasons over\nthe scene graph and environmental observations to generate executable plans,\nwhile a cache-enabled execution module accelerates adaptation by reusing\npreviously computed task-location trajectories. By combining rapid exploration,\nsymbolic reasoning, and cache-enabled execution, the proposed framework\novercomes the computational inefficiency and poor generalization of prior\nvision-language navigation methods, enabling robust and scalable\ndecision-making in unseen environments. VLN-Zero achieves 2x higher success\nrate compared to state-of-the-art zero-shot models, outperforms most fine-tuned\nbaselines, and reaches goal locations in half the time with 55% fewer VLM calls\non average compared to state-of-the-art models across diverse environments.\nCodebase, datasets, and videos for VLN-Zero are available at:\nhttps://vln-zero.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid adaptation in unseen environments is essential for scalable real-world\nautonomy, yet existing approaches rely on exhaustive exploration or rigid\nnavigation policies that fail to generalize. We present VLN-Zero, a two-phase\nvision-language navigation framework that leverages vision-language models to\nefficiently construct symbolic scene graphs and enable zero-shot neurosymbolic\nnavigation. In the exploration phase, structured prompts guide VLM-based search\ntoward informative and diverse trajectories, yielding compact scene graph\nrepresentations. In the deployment phase, a neurosymbolic planner reasons over\nthe scene graph and environmental observations to generate executable plans,\nwhile a cache-enabled execution module accelerates adaptation by reusing\npreviously computed task-location trajectories. By combining rapid exploration,\nsymbolic reasoning, and cache-enabled execution, the proposed framework\novercomes the computational inefficiency and poor generalization of prior\nvision-language navigation methods, enabling robust and scalable\ndecision-making in unseen environments. VLN-Zero achieves 2x higher success\nrate compared to state-of-the-art zero-shot models, outperforms most fine-tuned\nbaselines, and reaches goal locations in half the time with 55% fewer VLM calls\non average compared to state-of-the-art models across diverse environments.\nCodebase, datasets, and videos for VLN-Zero are available at:\nhttps://vln-zero.github.io/."
                },
                "authors": [
                    {
                        "name": "Neel P. Bhatt"
                    },
                    {
                        "name": "Yunhao Yang"
                    },
                    {
                        "name": "Rohan Siva"
                    },
                    {
                        "name": "Pranay Samineni"
                    },
                    {
                        "name": "Daniel Milan"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Ufuk Topcu"
                    }
                ],
                "author_detail": {
                    "name": "Ufuk Topcu"
                },
                "author": "Ufuk Topcu",
                "arxiv_comment": "Codebase, datasets, and videos for VLN-Zero are available at:\n  https://vln-zero.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00329v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00329v2",
                "updated": "2025-09-22T19:20:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    19,
                    20,
                    33,
                    0,
                    265,
                    0
                ],
                "published": "2025-05-31T00:52:17Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    0,
                    52,
                    17,
                    5,
                    151,
                    0
                ],
                "title": "Foresight: Adaptive Layer Reuse for Accelerated and High-Quality\n  Text-to-Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foresight: Adaptive Layer Reuse for Accelerated and High-Quality\n  Text-to-Video Generation"
                },
                "summary": "Diffusion Transformers (DiTs) achieve state-of-the-art results in\ntext-to-image, text-to-video generation, and editing. However, their large\nmodel size and the quadratic cost of spatial-temporal attention over multiple\ndenoising steps make video generation computationally expensive. Static caching\nmitigates this by reusing features across fixed steps but fails to adapt to\ngeneration dynamics, leading to suboptimal trade-offs between speed and\nquality.\n  We propose Foresight, an adaptive layer-reuse technique that reduces\ncomputational redundancy across denoising steps while preserving baseline\nperformance. Foresight dynamically identifies and reuses DiT block outputs for\nall layers across steps, adapting to generation parameters such as resolution\nand denoising schedules to optimize efficiency. Applied to OpenSora, Latte, and\nCogVideoX, Foresight achieves up to \\latencyimprv end-to-end speedup, while\nmaintaining video quality. The source code of Foresight is available at\n\\href{https://github.com/STAR-Laboratory/foresight}{https://github.com/STAR-Laboratory/foresight}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) achieve state-of-the-art results in\ntext-to-image, text-to-video generation, and editing. However, their large\nmodel size and the quadratic cost of spatial-temporal attention over multiple\ndenoising steps make video generation computationally expensive. Static caching\nmitigates this by reusing features across fixed steps but fails to adapt to\ngeneration dynamics, leading to suboptimal trade-offs between speed and\nquality.\n  We propose Foresight, an adaptive layer-reuse technique that reduces\ncomputational redundancy across denoising steps while preserving baseline\nperformance. Foresight dynamically identifies and reuses DiT block outputs for\nall layers across steps, adapting to generation parameters such as resolution\nand denoising schedules to optimize efficiency. Applied to OpenSora, Latte, and\nCogVideoX, Foresight achieves up to \\latencyimprv end-to-end speedup, while\nmaintaining video quality. The source code of Foresight is available at\n\\href{https://github.com/STAR-Laboratory/foresight}{https://github.com/STAR-Laboratory/foresight}."
                },
                "authors": [
                    {
                        "name": "Muhammad Adnan"
                    },
                    {
                        "name": "Nithesh Kurella"
                    },
                    {
                        "name": "Akhil Arunkumar"
                    },
                    {
                        "name": "Prashant J. Nair"
                    }
                ],
                "author_detail": {
                    "name": "Prashant J. Nair"
                },
                "author": "Prashant J. Nair",
                "arxiv_comment": "Accepted at the 39th Conference on Neural Information Processing\n  Systems (NeurIPS), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00329v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00329v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18344v1",
                "updated": "2025-09-22T19:08:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    19,
                    8,
                    57,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T19:08:57Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    19,
                    8,
                    57,
                    0,
                    265,
                    0
                ],
                "title": "Speculate Deep and Accurate: Lossless and Training-Free Acceleration for\n  Offloaded LLMs via Substitute Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculate Deep and Accurate: Lossless and Training-Free Acceleration for\n  Offloaded LLMs via Substitute Speculative Decoding"
                },
                "summary": "The immense model sizes of large language models (LLMs) challenge deployment\non memory-limited consumer GPUs. Although model compression and parameter\noffloading are common strategies to address memory limitations, compression can\ndegrade quality, and offloading maintains quality but suffers from slow\ninference. Speculative decoding presents a promising avenue to accelerate\nparameter offloading, utilizing a fast draft model to propose multiple draft\ntokens, which are then verified by the target LLM in parallel with a single\nforward pass. This method reduces the time-consuming data transfers in forward\npasses that involve offloaded weight transfers. Existing methods often rely on\npretrained weights of the same family, but require additional training to align\nwith custom-trained models. Moreover, approaches that involve draft model\ntraining usually yield only modest speedups. This limitation arises from\ninsufficient alignment with the target model, preventing higher token\nacceptance lengths. To address these challenges and achieve greater speedups,\nwe propose SubSpec, a plug-and-play method to accelerate parameter offloading\nthat is lossless and training-free. SubSpec constructs a highly aligned draft\nmodel by generating low-bit quantized substitute layers from offloaded target\nLLM portions. Additionally, our method shares the remaining GPU-resident layers\nand the KV-Cache, further reducing memory overhead and enhance alignment.\nSubSpec achieves a high average acceptance length, delivering 9.1x speedup for\nQwen2.5 7B on MT-Bench (8GB VRAM limit) and an average of 12.5x speedup for\nQwen2.5 32B on popular generation benchmarks (24GB VRAM limit).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The immense model sizes of large language models (LLMs) challenge deployment\non memory-limited consumer GPUs. Although model compression and parameter\noffloading are common strategies to address memory limitations, compression can\ndegrade quality, and offloading maintains quality but suffers from slow\ninference. Speculative decoding presents a promising avenue to accelerate\nparameter offloading, utilizing a fast draft model to propose multiple draft\ntokens, which are then verified by the target LLM in parallel with a single\nforward pass. This method reduces the time-consuming data transfers in forward\npasses that involve offloaded weight transfers. Existing methods often rely on\npretrained weights of the same family, but require additional training to align\nwith custom-trained models. Moreover, approaches that involve draft model\ntraining usually yield only modest speedups. This limitation arises from\ninsufficient alignment with the target model, preventing higher token\nacceptance lengths. To address these challenges and achieve greater speedups,\nwe propose SubSpec, a plug-and-play method to accelerate parameter offloading\nthat is lossless and training-free. SubSpec constructs a highly aligned draft\nmodel by generating low-bit quantized substitute layers from offloaded target\nLLM portions. Additionally, our method shares the remaining GPU-resident layers\nand the KV-Cache, further reducing memory overhead and enhance alignment.\nSubSpec achieves a high average acceptance length, delivering 9.1x speedup for\nQwen2.5 7B on MT-Bench (8GB VRAM limit) and an average of 12.5x speedup for\nQwen2.5 32B on popular generation benchmarks (24GB VRAM limit)."
                },
                "authors": [
                    {
                        "name": "Pei-Shuo Wang"
                    },
                    {
                        "name": "Jian-Jia Chen"
                    },
                    {
                        "name": "Chun-Che Yang"
                    },
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Ning-Chi Huang"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    },
                    {
                        "name": "Kai-Chiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Chiang Wu"
                },
                "author": "Kai-Chiang Wu",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18307v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18307v1",
                "updated": "2025-09-22T18:32:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    18,
                    32,
                    32,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T18:32:32Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    18,
                    32,
                    32,
                    0,
                    265,
                    0
                ],
                "title": "Comparison of Adaptive plan doses using Velocity generated synthetic CT\n  with KV CBCT and re-planning CT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparison of Adaptive plan doses using Velocity generated synthetic CT\n  with KV CBCT and re-planning CT"
                },
                "summary": "Introduction: This study uses KV CBCT based Synthetic CT (sCT) generated\nthrough Velocity workstation and compare the target and normal tissue doses\nwith Adaptive plan CT doses.\n  Methods: Thirty head and neck cancer patients undergoing Adaptive Radiation\nTherapy (ART) were included in this retrospective study. Initially, patient\nunderwent treatment with the primary plan. After subsequent indications of\nmajor changes in patients' physicality and anatomy adaptive CT scans were\nacquired as per institutional protocol. Both the primary planning CT and the\nindicative cone-beam CT (CBCT) last acquired before the commencement of the\nadaptive treatment were imported into Velocity workstation. Rigid and\ndeformable image registration techniques were used for the generation of a\nSynthetic CT (sCT). Simultaneously replanning was done on re-planning CT (rCT)\nfor adaptive plan execution. The primary plan dose was subsequently mapped and\ndeformed onto the Synthetic CT in Velocity workstation, allowing for a\ncomparative dosimetric analysis between the sCT and rCT plan doses. This\ncomparison was conducted in both Velocity and Eclipse, focusing on dose\nvariations across different organs at risk (OARs) and the planning target\nvolume (PTV). Additionally, dosimetric indices were evaluated to assess and\nvalidate the accuracy and quality of the synthetic CT-based dose mapping\nrelative to adaptive planning.\n  Results: The dosimetric comparison between sCT and rCT stated that Mean dose\nfor OARs and PTVs were found to be similar in the two planning and the level of\nconfidence by using T-statistics. Collaborative research has the potential to\neliminate the need of rCT as a standard requirement.\n  Conclusion: The sCT shows comparable CT numbers and doses to the replanning\nCT, suggesting it's potential as a replacement pending clinical correlation and\ncontour adjustments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Introduction: This study uses KV CBCT based Synthetic CT (sCT) generated\nthrough Velocity workstation and compare the target and normal tissue doses\nwith Adaptive plan CT doses.\n  Methods: Thirty head and neck cancer patients undergoing Adaptive Radiation\nTherapy (ART) were included in this retrospective study. Initially, patient\nunderwent treatment with the primary plan. After subsequent indications of\nmajor changes in patients' physicality and anatomy adaptive CT scans were\nacquired as per institutional protocol. Both the primary planning CT and the\nindicative cone-beam CT (CBCT) last acquired before the commencement of the\nadaptive treatment were imported into Velocity workstation. Rigid and\ndeformable image registration techniques were used for the generation of a\nSynthetic CT (sCT). Simultaneously replanning was done on re-planning CT (rCT)\nfor adaptive plan execution. The primary plan dose was subsequently mapped and\ndeformed onto the Synthetic CT in Velocity workstation, allowing for a\ncomparative dosimetric analysis between the sCT and rCT plan doses. This\ncomparison was conducted in both Velocity and Eclipse, focusing on dose\nvariations across different organs at risk (OARs) and the planning target\nvolume (PTV). Additionally, dosimetric indices were evaluated to assess and\nvalidate the accuracy and quality of the synthetic CT-based dose mapping\nrelative to adaptive planning.\n  Results: The dosimetric comparison between sCT and rCT stated that Mean dose\nfor OARs and PTVs were found to be similar in the two planning and the level of\nconfidence by using T-statistics. Collaborative research has the potential to\neliminate the need of rCT as a standard requirement.\n  Conclusion: The sCT shows comparable CT numbers and doses to the replanning\nCT, suggesting it's potential as a replacement pending clinical correlation and\ncontour adjustments."
                },
                "authors": [
                    {
                        "name": "Sudam Masanta"
                    },
                    {
                        "name": "Gurvinder Singh"
                    },
                    {
                        "name": "Shefali Pahwa"
                    },
                    {
                        "name": "Shekhar Dwivedi"
                    },
                    {
                        "name": "Devaraju Sampathirao"
                    },
                    {
                        "name": "Ramandeep Singh"
                    }
                ],
                "author_detail": {
                    "name": "Ramandeep Singh"
                },
                "author": "Ramandeep Singh",
                "arxiv_comment": "8 pages; comments welcome!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18307v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18307v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18085v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18085v1",
                "updated": "2025-09-22T17:58:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    58,
                    21,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T17:58:21Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    58,
                    21,
                    0,
                    265,
                    0
                ],
                "title": "Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative\n  Decoding"
                },
                "summary": "Diffusion LLMs (dLLMs) have recently emerged as a powerful alternative to\nautoregressive LLMs (AR-LLMs) with the potential to operate at significantly\nhigher token generation rates. However, currently available open-source dLLMs\noften generate at much lower rates, typically decoding only a single token at\nevery denoising timestep in order to maximize output quality. We present\nSpiffy, a speculative decoding algorithm that accelerates dLLM inference by\n$\\mathbf{2.8{-}3.1\\times}$ while provably preserving the model's output\ndistribution. This work addresses the unique challenges involved in applying\nideas from speculative decoding of AR-LLMs to the dLLM setting. Spiffy proposes\ndraft states by leveraging the dLLM's distribution itself in an\nauto-speculative manner. This approach is efficient and effective, and\neliminates the overheads of training and running an independent draft model. To\nstructure the candidate draft states, we propose a novel directed draft graph\nwhich is uniquely designed to take advantage of the bidirectional, block-wise\nnature of dLLM generation and can be verified in parallel by the dLLM. To\nfurther optimize the structure of these draft graphs, we introduce an\nefficient, offline calibration algorithm that procedurally determines\nhigh-quality graph configurations. These optimized draft graphs, enabling\nincreased acceptance rates, lead to a significant boost in the overall speedup\nachieved by the system. Crucially, Spiffy is also complementary to other recent\ninnovations in improving dLLM generation speeds such as KV-caching and\nmulti-token unmasking. We demonstrate that when combined with such parallel\ndecoding algorithms, Spiffy is able to effectively multiply the benefits of\nthese methods leading to total speedups of up to $\\mathbf{7.9\\times}$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion LLMs (dLLMs) have recently emerged as a powerful alternative to\nautoregressive LLMs (AR-LLMs) with the potential to operate at significantly\nhigher token generation rates. However, currently available open-source dLLMs\noften generate at much lower rates, typically decoding only a single token at\nevery denoising timestep in order to maximize output quality. We present\nSpiffy, a speculative decoding algorithm that accelerates dLLM inference by\n$\\mathbf{2.8{-}3.1\\times}$ while provably preserving the model's output\ndistribution. This work addresses the unique challenges involved in applying\nideas from speculative decoding of AR-LLMs to the dLLM setting. Spiffy proposes\ndraft states by leveraging the dLLM's distribution itself in an\nauto-speculative manner. This approach is efficient and effective, and\neliminates the overheads of training and running an independent draft model. To\nstructure the candidate draft states, we propose a novel directed draft graph\nwhich is uniquely designed to take advantage of the bidirectional, block-wise\nnature of dLLM generation and can be verified in parallel by the dLLM. To\nfurther optimize the structure of these draft graphs, we introduce an\nefficient, offline calibration algorithm that procedurally determines\nhigh-quality graph configurations. These optimized draft graphs, enabling\nincreased acceptance rates, lead to a significant boost in the overall speedup\nachieved by the system. Crucially, Spiffy is also complementary to other recent\ninnovations in improving dLLM generation speeds such as KV-caching and\nmulti-token unmasking. We demonstrate that when combined with such parallel\ndecoding algorithms, Spiffy is able to effectively multiply the benefits of\nthese methods leading to total speedups of up to $\\mathbf{7.9\\times}$."
                },
                "authors": [
                    {
                        "name": "Sudhanshu Agrawal"
                    },
                    {
                        "name": "Risheek Garrepalli"
                    },
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Christopher Lott"
                    },
                    {
                        "name": "Fatih Porikli"
                    }
                ],
                "author_detail": {
                    "name": "Fatih Porikli"
                },
                "author": "Fatih Porikli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18085v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18085v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00919v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00919v2",
                "updated": "2025-09-22T16:16:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    16,
                    25,
                    0,
                    265,
                    0
                ],
                "published": "2025-02-02T21:15:07Z",
                "published_parsed": [
                    2025,
                    2,
                    2,
                    21,
                    15,
                    7,
                    6,
                    33,
                    0
                ],
                "title": "Attention Sinks: A 'Catch, Tag, Release' Mechanism for Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention Sinks: A 'Catch, Tag, Release' Mechanism for Embeddings"
                },
                "summary": "Large language models (LLMs) often concentrate their attention on a few\nspecific tokens referred to as attention sinks. Common examples include the\nfirst token, a prompt-independent sink, and punctuation tokens, which are\nprompt-dependent. While the tokens causing the sinks often lack direct semantic\nmeaning, the presence of the sinks is critical for model performance,\nparticularly under model compression and KV-caching. Despite their ubiquity,\nthe function, semantic role, and origin of attention sinks -- especially those\nbeyond the first token -- remain poorly understood. In this work, we conduct a\ncomprehensive investigation demonstrating that attention sinks: catch a\nsequence of tokens, tag them using a common direction in embedding space, and\nrelease them back into the residual stream, where tokens are later retrieved\nbased on the tags they have acquired. Probing experiments reveal these tags\ncarry semantically meaningful information, such as the truth of a statement.\nThese findings extend to reasoning models, where the mechanism spans more heads\nand explains greater variance in embeddings, or recent models with query-key\nnormalization, where sinks remain just as prevalent. To encourage future\ntheoretical analysis, we introduce a minimal problem which can be solved\nthrough the 'catch, tag, release' mechanism, and where it emerges through\ntraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often concentrate their attention on a few\nspecific tokens referred to as attention sinks. Common examples include the\nfirst token, a prompt-independent sink, and punctuation tokens, which are\nprompt-dependent. While the tokens causing the sinks often lack direct semantic\nmeaning, the presence of the sinks is critical for model performance,\nparticularly under model compression and KV-caching. Despite their ubiquity,\nthe function, semantic role, and origin of attention sinks -- especially those\nbeyond the first token -- remain poorly understood. In this work, we conduct a\ncomprehensive investigation demonstrating that attention sinks: catch a\nsequence of tokens, tag them using a common direction in embedding space, and\nrelease them back into the residual stream, where tokens are later retrieved\nbased on the tags they have acquired. Probing experiments reveal these tags\ncarry semantically meaningful information, such as the truth of a statement.\nThese findings extend to reasoning models, where the mechanism spans more heads\nand explains greater variance in embeddings, or recent models with query-key\nnormalization, where sinks remain just as prevalent. To encourage future\ntheoretical analysis, we introduce a minimal problem which can be solved\nthrough the 'catch, tag, release' mechanism, and where it emerges through\ntraining."
                },
                "authors": [
                    {
                        "name": "Stephen Zhang"
                    },
                    {
                        "name": "Mustafa Khan"
                    },
                    {
                        "name": "Vardan Papyan"
                    }
                ],
                "author_detail": {
                    "name": "Vardan Papyan"
                },
                "author": "Vardan Papyan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00919v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00919v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00085v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00085v2",
                "updated": "2025-09-22T12:28:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    12,
                    28,
                    41,
                    0,
                    265,
                    0
                ],
                "published": "2025-01-31T16:22:36Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    22,
                    36,
                    4,
                    31,
                    0
                ],
                "title": "Efficient Beam Search for Large Language Models Using Trie-Based\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Beam Search for Large Language Models Using Trie-Based\n  Decoding"
                },
                "summary": "This work presents a novel trie (prefix-tree)-based parallel decoding method\nthat addresses the memory inefficiency of batch-based beam search. By sharing a\nsingle KV cache across beams with common prefixes, our approach dramatically\nreduces memory usage and enables efficient decoding. We evaluated our method\nacross three attention architectures, Multi-Head Attention\n(Phi-3.5-mini-instruct), Grouped Query Attention (Llama-3.1-8B-Instruct), and\nSliding Window Attention (Mistral-Small-24B-Instruct-2501), using CNN/DailyMail\nfor abstractive summarization and HumanEval for code generation. Our\nexperiments demonstrate substantial memory savings (4--8$\\times$) and up to\n2.4$\\times$ faster decoding, without compromising generation quality. These\nresults highlight our method's suitability for memory-constrained environments\nand large-scale deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents a novel trie (prefix-tree)-based parallel decoding method\nthat addresses the memory inefficiency of batch-based beam search. By sharing a\nsingle KV cache across beams with common prefixes, our approach dramatically\nreduces memory usage and enables efficient decoding. We evaluated our method\nacross three attention architectures, Multi-Head Attention\n(Phi-3.5-mini-instruct), Grouped Query Attention (Llama-3.1-8B-Instruct), and\nSliding Window Attention (Mistral-Small-24B-Instruct-2501), using CNN/DailyMail\nfor abstractive summarization and HumanEval for code generation. Our\nexperiments demonstrate substantial memory savings (4--8$\\times$) and up to\n2.4$\\times$ faster decoding, without compromising generation quality. These\nresults highlight our method's suitability for memory-constrained environments\nand large-scale deployments."
                },
                "authors": [
                    {
                        "name": "Brian J Chan"
                    },
                    {
                        "name": "MaoXun Huang"
                    },
                    {
                        "name": "Jui-Hung Cheng"
                    },
                    {
                        "name": "Chao-Ting Chen"
                    },
                    {
                        "name": "Hen-Hsen Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hen-Hsen Huang"
                },
                "author": "Hen-Hsen Huang",
                "arxiv_comment": "13 pages, accepted as a main conference paper at EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00085v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00085v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13251v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13251v3",
                "updated": "2025-09-22T12:03:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    12,
                    3,
                    22,
                    0,
                    265,
                    0
                ],
                "published": "2025-02-18T19:22:44Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    19,
                    22,
                    44,
                    1,
                    49,
                    0
                ],
                "title": "Neural Attention Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Attention Search"
                },
                "summary": "We present Neural Attention Search (NAtS), a framework that automatically\nevaluates the importance of each token within a sequence and determines if the\ncorresponding token can be dropped after several steps. This approach can\nefficiently reduce the KV cache sizes required by transformer-based models\nduring inference and thus reduce inference costs. In this paper, we design a\nsearch space that contains three token types: (i) Global Tokens will be\npreserved and queried by all the following tokens. (ii) Local Tokens survive\nuntil the next global token appears. (iii) Sliding Window Tokens have an impact\non the inference of a fixed size of the next following tokens. Similar to the\nOne-Shot Neural Architecture Search approach, this token-type information can\nbe learned jointly with the architecture weights via a learnable attention\nmask. Experiments on both training a new transformer from scratch and\nfine-tuning existing large language models show that NAtS can efficiently\nreduce the KV cache size required for the models while maintaining the models'\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Neural Attention Search (NAtS), a framework that automatically\nevaluates the importance of each token within a sequence and determines if the\ncorresponding token can be dropped after several steps. This approach can\nefficiently reduce the KV cache sizes required by transformer-based models\nduring inference and thus reduce inference costs. In this paper, we design a\nsearch space that contains three token types: (i) Global Tokens will be\npreserved and queried by all the following tokens. (ii) Local Tokens survive\nuntil the next global token appears. (iii) Sliding Window Tokens have an impact\non the inference of a fixed size of the next following tokens. Similar to the\nOne-Shot Neural Architecture Search approach, this token-type information can\nbe learned jointly with the architecture weights via a learnable attention\nmask. Experiments on both training a new transformer from scratch and\nfine-tuning existing large language models show that NAtS can efficiently\nreduce the KV cache size required for the models while maintaining the models'\nperformance."
                },
                "authors": [
                    {
                        "name": "Difan Deng"
                    },
                    {
                        "name": "Marius Lindauer"
                    }
                ],
                "author_detail": {
                    "name": "Marius Lindauer"
                },
                "author": "Marius Lindauer",
                "arxiv_comment": "18 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13251v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13251v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17650v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17650v1",
                "updated": "2025-09-22T11:54:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    11,
                    54,
                    58,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T11:54:58Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    11,
                    54,
                    58,
                    0,
                    265,
                    0
                ],
                "title": "Evict3R: Training-Free Token Eviction for Memory-Bounded Streaming\n  Visual Geometry Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evict3R: Training-Free Token Eviction for Memory-Bounded Streaming\n  Visual Geometry Transformers"
                },
                "summary": "Streaming visual transformers like StreamVGGT achieve strong 3D perception\nbut suffer from unbounded growth of key value (KV) memory, which limits\nscalability. We propose a training-free, inference-time token eviction policy\nthat bounds memory by discarding redundant tokens while keeping the most\ninformative ones. Our method uses significantly less memory with little to no\ndrop in accuracy: on 7-Scenes with long sequences it reduces peak memory from\n18.63 GB to 9.39 GB while accuracy and completeness drop by only 0.003. Under\nstrict memory budgets, eviction enables denser frame sampling, which improves\nreconstruction accuracy compared to the baseline. Experiments across video\ndepth estimation (Sintel, KITTI), 3D reconstruction (7-Scenes, NRGBD), and\ncamera pose estimation (Sintel, TUM-dynamics) show that our approach closely\nmatches StreamVGGT at a fraction of the memory and makes long-horizon streaming\ninference more practical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming visual transformers like StreamVGGT achieve strong 3D perception\nbut suffer from unbounded growth of key value (KV) memory, which limits\nscalability. We propose a training-free, inference-time token eviction policy\nthat bounds memory by discarding redundant tokens while keeping the most\ninformative ones. Our method uses significantly less memory with little to no\ndrop in accuracy: on 7-Scenes with long sequences it reduces peak memory from\n18.63 GB to 9.39 GB while accuracy and completeness drop by only 0.003. Under\nstrict memory budgets, eviction enables denser frame sampling, which improves\nreconstruction accuracy compared to the baseline. Experiments across video\ndepth estimation (Sintel, KITTI), 3D reconstruction (7-Scenes, NRGBD), and\ncamera pose estimation (Sintel, TUM-dynamics) show that our approach closely\nmatches StreamVGGT at a fraction of the memory and makes long-horizon streaming\ninference more practical."
                },
                "authors": [
                    {
                        "name": "Soroush Mahdi"
                    },
                    {
                        "name": "Fardin Ayar"
                    },
                    {
                        "name": "Ehsan Javanmardi"
                    },
                    {
                        "name": "Manabu Tsukada"
                    },
                    {
                        "name": "Mahdi Javanmardi"
                    }
                ],
                "author_detail": {
                    "name": "Mahdi Javanmardi"
                },
                "author": "Mahdi Javanmardi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17650v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17650v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17388v1",
                "updated": "2025-09-22T06:52:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    6,
                    52,
                    35,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T06:52:35Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    6,
                    52,
                    35,
                    0,
                    265,
                    0
                ],
                "title": "Prefetching in Deep Memory Hierarchies with NVRAM as Main Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prefetching in Deep Memory Hierarchies with NVRAM as Main Memory"
                },
                "summary": "Emerging applications, such as big data analytics and machine learning,\nrequire increasingly large amounts of main memory, often exceeding the capacity\nof current commodity processors built on DRAM technology. To address this,\nrecent research has focused on off-chip memory controllers that facilitate\naccess to diverse memory media, each with unique density and latency\ncharacteristics. While these solutions improve memory system performance, they\nalso exacerbate the already significant memory latency. As a result,\nmulti-level prefetching techniques are essential to mitigate these extended\nlatencies.\n  This paper investigates the advantages of prefetching across both sides of\nthe memory system: the off-chip memory and the on-chip cache hierarchy. Our\nprimary objective is to assess the impact of a multi-level prefetching engine\non overall system performance. Additionally, we analyze the individual\ncontribution of each prefetching level to system efficiency. To achieve this,\nthe study evaluates two key prefetching approaches: HMC (Hybrid Memory\nController) and HMC+L1, both of which employ prefetching mechanisms commonly\nused by processor vendors. The HMC approach integrates a prefetcher within the\noff-chip hybrid memory controller, while the HMC+L1 approach combines this with\nadditional L1 on-chip prefetchers.\n  Experimental results on an out-of-order execution processor show that on-chip\ncache prefetchers are crucial for maximizing the benefits of off-chip\nprefetching, which in turn further enhances performance. Specifically, the\noff-chip HMC prefetcher achieves coverage and accuracy rates exceeding 60% and\nup to 80%, while the combined HMC+L1 approach boosts off-chip prefetcher\ncoverage to as much as 92%. Consequently, overall performance increases from 9%\nwith the HMC approach to 12% when L1 prefetching is also employed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging applications, such as big data analytics and machine learning,\nrequire increasingly large amounts of main memory, often exceeding the capacity\nof current commodity processors built on DRAM technology. To address this,\nrecent research has focused on off-chip memory controllers that facilitate\naccess to diverse memory media, each with unique density and latency\ncharacteristics. While these solutions improve memory system performance, they\nalso exacerbate the already significant memory latency. As a result,\nmulti-level prefetching techniques are essential to mitigate these extended\nlatencies.\n  This paper investigates the advantages of prefetching across both sides of\nthe memory system: the off-chip memory and the on-chip cache hierarchy. Our\nprimary objective is to assess the impact of a multi-level prefetching engine\non overall system performance. Additionally, we analyze the individual\ncontribution of each prefetching level to system efficiency. To achieve this,\nthe study evaluates two key prefetching approaches: HMC (Hybrid Memory\nController) and HMC+L1, both of which employ prefetching mechanisms commonly\nused by processor vendors. The HMC approach integrates a prefetcher within the\noff-chip hybrid memory controller, while the HMC+L1 approach combines this with\nadditional L1 on-chip prefetchers.\n  Experimental results on an out-of-order execution processor show that on-chip\ncache prefetchers are crucial for maximizing the benefits of off-chip\nprefetching, which in turn further enhances performance. Specifically, the\noff-chip HMC prefetcher achieves coverage and accuracy rates exceeding 60% and\nup to 80%, while the combined HMC+L1 approach boosts off-chip prefetcher\ncoverage to as much as 92%. Consequently, overall performance increases from 9%\nwith the HMC approach to 12% when L1 prefetching is also employed."
                },
                "authors": [
                    {
                        "name": "Manel Lurbe"
                    },
                    {
                        "name": "Miguel Avargues"
                    },
                    {
                        "name": "Salvador Petit"
                    },
                    {
                        "name": "Maria E. Gomez"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Guanhao Wang"
                    },
                    {
                        "name": "Julio Sahuquillo"
                    }
                ],
                "author_detail": {
                    "name": "Julio Sahuquillo"
                },
                "author": "Julio Sahuquillo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17360v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17360v1",
                "updated": "2025-09-22T05:24:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    5,
                    24,
                    22,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T05:24:22Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    5,
                    24,
                    22,
                    0,
                    265,
                    0
                ],
                "title": "Asteria: Semantic-Aware Cross-Region Caching for Agentic LLM Tool Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asteria: Semantic-Aware Cross-Region Caching for Agentic LLM Tool Access"
                },
                "summary": "Large Language Model (LLM) agents tackle data-intensive tasks such as deep\nresearch and code generation. However, their effectiveness depends on frequent\ninteractions with knowledge sources across remote clouds or regions. Such\ninteractions can create non-trivial latency and cost bottlenecks. Existing\ncaching solutions focus on exact-match queries, limiting their effectiveness\nfor semantic knowledge reuse.\n  To address this challenge, we introduce Asteria, a novel cross-region\nknowledge caching architecture for LLM agents. At its core are two\nabstractions: Semantic Element (SE) and Semantic Retrieval Index (Sine). A\nsemantic element captures the semantic embedding representation of an LLM query\ntogether with performance-aware metadata such as latency, cost, and staticity.\nSine then provides two-stage retrieval: a vector similar index with semantic\nembedding for fast candidate selection and a lightweight LLM-powered semantic\njudger for precise validation. Atop these primitives, Asteria builds a new\ncache interface that includes a new semantic-aware cache hit definition, a\ncost-efficient eviction policy, and proactive prefetching. To reduce overhead,\nAsteria co-locates the small LLM judger with the main LLM using adaptive\nscheduling and resource sharing. Our evaluation demonstrates that Asteria\ndelivers substantial performance improvements without compromising correctness.\nOn representative search workloads, Asteria achieves up to a 3.6$\\times$\nincrease in throughput by maintaining cache hit rates of over 85%, while\npreserving accuracy virtually identical to non-cached baselines. Asteria also\nimproves throughput for complex coding tasks by 20%, showcasing its versatility\nacross diverse agentic workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) agents tackle data-intensive tasks such as deep\nresearch and code generation. However, their effectiveness depends on frequent\ninteractions with knowledge sources across remote clouds or regions. Such\ninteractions can create non-trivial latency and cost bottlenecks. Existing\ncaching solutions focus on exact-match queries, limiting their effectiveness\nfor semantic knowledge reuse.\n  To address this challenge, we introduce Asteria, a novel cross-region\nknowledge caching architecture for LLM agents. At its core are two\nabstractions: Semantic Element (SE) and Semantic Retrieval Index (Sine). A\nsemantic element captures the semantic embedding representation of an LLM query\ntogether with performance-aware metadata such as latency, cost, and staticity.\nSine then provides two-stage retrieval: a vector similar index with semantic\nembedding for fast candidate selection and a lightweight LLM-powered semantic\njudger for precise validation. Atop these primitives, Asteria builds a new\ncache interface that includes a new semantic-aware cache hit definition, a\ncost-efficient eviction policy, and proactive prefetching. To reduce overhead,\nAsteria co-locates the small LLM judger with the main LLM using adaptive\nscheduling and resource sharing. Our evaluation demonstrates that Asteria\ndelivers substantial performance improvements without compromising correctness.\nOn representative search workloads, Asteria achieves up to a 3.6$\\times$\nincrease in throughput by maintaining cache hit rates of over 85%, while\npreserving accuracy virtually identical to non-cached baselines. Asteria also\nimproves throughput for complex coding tasks by 20%, showcasing its versatility\nacross diverse agentic workloads."
                },
                "authors": [
                    {
                        "name": "Chaoyi Ruan"
                    },
                    {
                        "name": "Chao Bi"
                    },
                    {
                        "name": "Kaiwen Zheng"
                    },
                    {
                        "name": "Ziji Shi"
                    },
                    {
                        "name": "Xinyi Wan"
                    },
                    {
                        "name": "Jialin Li"
                    }
                ],
                "author_detail": {
                    "name": "Jialin Li"
                },
                "author": "Jialin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17360v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17360v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17257v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17257v1",
                "updated": "2025-09-21T22:14:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    21,
                    22,
                    14,
                    56,
                    6,
                    264,
                    0
                ],
                "published": "2025-09-21T22:14:56Z",
                "published_parsed": [
                    2025,
                    9,
                    21,
                    22,
                    14,
                    56,
                    6,
                    264,
                    0
                ],
                "title": "On efficient block Krylov-solvers for $\\mathcal H^2$-matrices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On efficient block Krylov-solvers for $\\mathcal H^2$-matrices"
                },
                "summary": "Hierarchical matrices provide a highly memory-efficient way of storing dense\nlinear operators arising, for example, from boundary element methods,\nparticularly when stored in the H^2 format. In such data-sparse\nrepresentations, iterative solvers are preferred over direct ones due to the\ncost-efficient matrix-vector multiplications they enable. Solving multiple\nsystems of linear equations with the same hierarchical matrix naturally leads\nto block methods, which in turn make heavy use of BLAS level-3 functions such\nas GEMM. We present an efficient implementation of H^2-matrix-vector and\nH^2-matrix-matrix multiplication that fully exploits the potential of modern\nhardware in terms of memory and cache utilization. The latter is employed to\naccelerate block Krylov subspace methods, which we present later as the main\nresults of this paper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical matrices provide a highly memory-efficient way of storing dense\nlinear operators arising, for example, from boundary element methods,\nparticularly when stored in the H^2 format. In such data-sparse\nrepresentations, iterative solvers are preferred over direct ones due to the\ncost-efficient matrix-vector multiplications they enable. Solving multiple\nsystems of linear equations with the same hierarchical matrix naturally leads\nto block methods, which in turn make heavy use of BLAS level-3 functions such\nas GEMM. We present an efficient implementation of H^2-matrix-vector and\nH^2-matrix-matrix multiplication that fully exploits the potential of modern\nhardware in terms of memory and cache utilization. The latter is employed to\naccelerate block Krylov subspace methods, which we present later as the main\nresults of this paper."
                },
                "authors": [
                    {
                        "name": "Sven Christophersen"
                    }
                ],
                "author_detail": {
                    "name": "Sven Christophersen"
                },
                "author": "Sven Christophersen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17257v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17257v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65F55, 65F08, 65F10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17238v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17238v1",
                "updated": "2025-09-21T21:05:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    21,
                    21,
                    5,
                    29,
                    6,
                    264,
                    0
                ],
                "published": "2025-09-21T21:05:29Z",
                "published_parsed": [
                    2025,
                    9,
                    21,
                    21,
                    5,
                    29,
                    6,
                    264,
                    0
                ],
                "title": "MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with\n  RoE",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with\n  RoE"
                },
                "summary": "The generation quality of large language models (LLMs) is often improved by\nutilizing inference-time sequence-level scaling methods (e.g.,\nChain-of-Thought). We introduce hyper-parallel scaling, a complementary\nframework that improves prediction quality at the token level. Hyper-parallel\nscaling computes and aggregates multiple output proposals for a single token\nfrom the model. We implement this concept in Mixture-of-Experts (MoE) models,\nwhich we refer to as Roster of Experts (RoE). RoE is a training-free inference\nalgorithm that turns a single MoE into a dynamic ensemble of MoEs. RoE injects\ncontrolled stochasticity into the expert routing mechanism, enabling it to\nsample multiple diverse experts for each token and aggregate their outputs for\na more accurate final prediction.To overcome the computational cost, we\nintroduce an efficient batching strategy and a specialized KV-caching mechanism\nthat minimizes compute and memory overhead. For example, RoE enables a 7B MoE\nmodel to match the performance of a 10.5B MoE model while using 30% less\ncompute for inference. These gains are achieved without any fine-tuning of\nmodel parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The generation quality of large language models (LLMs) is often improved by\nutilizing inference-time sequence-level scaling methods (e.g.,\nChain-of-Thought). We introduce hyper-parallel scaling, a complementary\nframework that improves prediction quality at the token level. Hyper-parallel\nscaling computes and aggregates multiple output proposals for a single token\nfrom the model. We implement this concept in Mixture-of-Experts (MoE) models,\nwhich we refer to as Roster of Experts (RoE). RoE is a training-free inference\nalgorithm that turns a single MoE into a dynamic ensemble of MoEs. RoE injects\ncontrolled stochasticity into the expert routing mechanism, enabling it to\nsample multiple diverse experts for each token and aggregate their outputs for\na more accurate final prediction.To overcome the computational cost, we\nintroduce an efficient batching strategy and a specialized KV-caching mechanism\nthat minimizes compute and memory overhead. For example, RoE enables a 7B MoE\nmodel to match the performance of a 10.5B MoE model while using 30% less\ncompute for inference. These gains are achieved without any fine-tuning of\nmodel parameters."
                },
                "authors": [
                    {
                        "name": "Soheil Zibakhsh"
                    },
                    {
                        "name": "Mohammad Samragh"
                    },
                    {
                        "name": "Kumari Nishu"
                    },
                    {
                        "name": "Lauren Hannah"
                    },
                    {
                        "name": "Arnav Kundu"
                    },
                    {
                        "name": "Minsik Cho"
                    }
                ],
                "author_detail": {
                    "name": "Minsik Cho"
                },
                "author": "Minsik Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17238v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17238v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07639v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07639v2",
                "updated": "2025-09-21T11:48:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    21,
                    11,
                    48,
                    15,
                    6,
                    264,
                    0
                ],
                "published": "2025-06-09T11:04:13Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    11,
                    4,
                    13,
                    0,
                    160,
                    0
                ],
                "title": "Fast ECoT: Efficient Embodied Chain-of-Thought via Thoughts Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast ECoT: Efficient Embodied Chain-of-Thought via Thoughts Reuse"
                },
                "summary": "Embodied Chain-of-Thought (ECoT) reasoning enhances vision-language-action\n(VLA) models by improving performance and interpretability through intermediate\nreasoning steps. However, its sequential autoregressive token generation\nintroduces significant inference latency, limiting real-time deployment. We\npropose Fast ECoT, an inference-time acceleration method that exploits the\nstructured and repetitive nature of ECoT to (1) cache and reuse high-level\nreasoning across timesteps and (2) parallelise the generation of modular\nreasoning steps. Additionally, we introduce an asynchronous scheduler that\ndecouples reasoning from action decoding, further boosting responsiveness. Fast\nECoT requires no model changes or additional training and integrates easily\ninto existing VLA pipelines. Experiments in both simulation (LIBERO) and\nreal-world robot tasks show up to a 7.5% reduction in latency with comparable\nor improved task success rate and reasoning faithfulness, bringing ECoT\npolicies closer to practical real-time deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied Chain-of-Thought (ECoT) reasoning enhances vision-language-action\n(VLA) models by improving performance and interpretability through intermediate\nreasoning steps. However, its sequential autoregressive token generation\nintroduces significant inference latency, limiting real-time deployment. We\npropose Fast ECoT, an inference-time acceleration method that exploits the\nstructured and repetitive nature of ECoT to (1) cache and reuse high-level\nreasoning across timesteps and (2) parallelise the generation of modular\nreasoning steps. Additionally, we introduce an asynchronous scheduler that\ndecouples reasoning from action decoding, further boosting responsiveness. Fast\nECoT requires no model changes or additional training and integrates easily\ninto existing VLA pipelines. Experiments in both simulation (LIBERO) and\nreal-world robot tasks show up to a 7.5% reduction in latency with comparable\nor improved task success rate and reasoning faithfulness, bringing ECoT\npolicies closer to practical real-time deployment."
                },
                "authors": [
                    {
                        "name": "Zhekai Duan"
                    },
                    {
                        "name": "Yuan Zhang"
                    },
                    {
                        "name": "Shikai Geng"
                    },
                    {
                        "name": "Gaowen Liu"
                    },
                    {
                        "name": "Joschka Boedecker"
                    },
                    {
                        "name": "Chris Xiaoxuan Lu"
                    }
                ],
                "author_detail": {
                    "name": "Chris Xiaoxuan Lu"
                },
                "author": "Chris Xiaoxuan Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07639v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07639v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10367v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10367v2",
                "updated": "2025-09-21T07:03:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    21,
                    7,
                    3,
                    46,
                    6,
                    264,
                    0
                ],
                "published": "2025-07-14T15:09:01Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    9,
                    1,
                    0,
                    195,
                    0
                ],
                "title": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline"
                },
                "summary": "Client-side metadata caching has long been considered an effective method for\naccelerating metadata operations in distributed file systems (DFSs). However,\nwe have found that client-side state (e.g., caching) is not only ineffective\nbut also consumes valuable memory resources in the deep learning pipelines. We\nthus propose FalconFS, a DFS optimized for deep learning pipelines with the\nstateless-client architecture. Specifically, instead of performing client-side\npath resolution and caching, FalconFS efficiently resolves paths on the server\nside using hybrid metadata indexing and lazy namespace replication. FalconFS\nalso boosts server concurrency with concurrent request merging and provides\neasy deployment with VFS shortcut. Evaluations against CephFS and Lustre show\nthat FalconFS achieves up to 5.72$\\times$ throughput for small file read/write\nand up to 12.81$\\times$ throughput for deep learning model training. FalconFS\nhas been running in Huawei autonomous driving system's production environment\nwith 10,000 NPUs for one year.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Client-side metadata caching has long been considered an effective method for\naccelerating metadata operations in distributed file systems (DFSs). However,\nwe have found that client-side state (e.g., caching) is not only ineffective\nbut also consumes valuable memory resources in the deep learning pipelines. We\nthus propose FalconFS, a DFS optimized for deep learning pipelines with the\nstateless-client architecture. Specifically, instead of performing client-side\npath resolution and caching, FalconFS efficiently resolves paths on the server\nside using hybrid metadata indexing and lazy namespace replication. FalconFS\nalso boosts server concurrency with concurrent request merging and provides\neasy deployment with VFS shortcut. Evaluations against CephFS and Lustre show\nthat FalconFS achieves up to 5.72$\\times$ throughput for small file read/write\nand up to 12.81$\\times$ throughput for deep learning model training. FalconFS\nhas been running in Huawei autonomous driving system's production environment\nwith 10,000 NPUs for one year."
                },
                "authors": [
                    {
                        "name": "Jingwei Xu"
                    },
                    {
                        "name": "Junbin Kang"
                    },
                    {
                        "name": "Mingkai Dong"
                    },
                    {
                        "name": "Mingyu Liu"
                    },
                    {
                        "name": "Lu Zhang"
                    },
                    {
                        "name": "Shaohong Guo"
                    },
                    {
                        "name": "Ziyan Qiu"
                    },
                    {
                        "name": "Mingzhen You"
                    },
                    {
                        "name": "Ziyi Tian"
                    },
                    {
                        "name": "Anqi Yu"
                    },
                    {
                        "name": "Tianhong Ding"
                    },
                    {
                        "name": "Xinwei Hu"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "Accepted by NSDI'26",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10367v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10367v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11815v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11815v2",
                "updated": "2025-09-21T03:35:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    21,
                    3,
                    35,
                    36,
                    6,
                    264,
                    0
                ],
                "published": "2025-09-15T11:53:56Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    11,
                    53,
                    56,
                    0,
                    258,
                    0
                ],
                "title": "SpecVLM: Fast Speculative Decoding in Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecVLM: Fast Speculative Decoding in Vision-Language Models"
                },
                "summary": "Speculative decoding is a powerful way to accelerate autoregressive large\nlanguage models (LLMs), but directly porting it to vision-language models\n(VLMs) faces unique systems constraints: the prefill stage is dominated by\nvisual tokens whose count scales with image resolution and video length,\ninflating both compute and memory, especially the key-value (KV) cache. We\nstudy speculative decoding for VLMs and introduce SpecVLM, a practical system\nthat (1) establishes a strong EAGLE-2-style baseline, EagleVLM, delivering\n1.5--2.3x end-to-end speedups over full autoregressive inference, and (2)\nfurther accelerates VLM inference with an elastic visual compressor that\nadaptively selects among pruning, pooling, convolution, and resampler\nprimitives to balance FLOPs/parameters and accuracy per input. To avoid costly\noffline distillation corpora, we propose an online-logit distillation protocol\nthat trains the draft model with on-the-fly teacher logits and penultimate\nfeatures using a combined cross-entropy and Smooth L1 objective, eliminating\nstorage and preprocessing while remaining compute-efficient. This protocol\nreveals a training-time scaling effect: longer online training monotonically\nincreases the draft model's average accepted length, improving speculative\nefficiency. Empirically, SpecVLM achieves additional acceleration, culminating\nin 2.5--2.9x end-to-end speedups within 5 epochs across LLaVA and MMMU,\nconsistently over resolutions and task difficulties, while preserving the\ntarget model's output distribution (lossless decoding). Our code is available\nat https://github.com/haiduo/SpecVLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is a powerful way to accelerate autoregressive large\nlanguage models (LLMs), but directly porting it to vision-language models\n(VLMs) faces unique systems constraints: the prefill stage is dominated by\nvisual tokens whose count scales with image resolution and video length,\ninflating both compute and memory, especially the key-value (KV) cache. We\nstudy speculative decoding for VLMs and introduce SpecVLM, a practical system\nthat (1) establishes a strong EAGLE-2-style baseline, EagleVLM, delivering\n1.5--2.3x end-to-end speedups over full autoregressive inference, and (2)\nfurther accelerates VLM inference with an elastic visual compressor that\nadaptively selects among pruning, pooling, convolution, and resampler\nprimitives to balance FLOPs/parameters and accuracy per input. To avoid costly\noffline distillation corpora, we propose an online-logit distillation protocol\nthat trains the draft model with on-the-fly teacher logits and penultimate\nfeatures using a combined cross-entropy and Smooth L1 objective, eliminating\nstorage and preprocessing while remaining compute-efficient. This protocol\nreveals a training-time scaling effect: longer online training monotonically\nincreases the draft model's average accepted length, improving speculative\nefficiency. Empirically, SpecVLM achieves additional acceleration, culminating\nin 2.5--2.9x end-to-end speedups within 5 epochs across LLaVA and MMMU,\nconsistently over resolutions and task difficulties, while preserving the\ntarget model's output distribution (lossless decoding). Our code is available\nat https://github.com/haiduo/SpecVLM."
                },
                "authors": [
                    {
                        "name": "Haiduo Huang"
                    },
                    {
                        "name": "Fuwei Yang"
                    },
                    {
                        "name": "Zhenhua Liu"
                    },
                    {
                        "name": "Xuanwu Yin"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Pengju Ren"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11815v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11815v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16857v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16857v1",
                "updated": "2025-09-21T00:59:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    21,
                    0,
                    59,
                    45,
                    6,
                    264,
                    0
                ],
                "published": "2025-09-21T00:59:45Z",
                "published_parsed": [
                    2025,
                    9,
                    21,
                    0,
                    59,
                    45,
                    6,
                    264,
                    0
                ],
                "title": "ShadowServe: Interference-Free KV Cache Fetching for Distributed Prefix\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShadowServe: Interference-Free KV Cache Fetching for Distributed Prefix\n  Caching"
                },
                "summary": "Distributed prefix caching accelerates long-context LLM serving by reusing KV\ncache entries for common context prefixes. However, KV cache fetches can become\na bottleneck when network bandwidth is limited. Compression mitigates the\nbandwidth issue, but can degrade overall performance when decompression\ninterferes with model computation.\n  We present ShadowServe, the first SmartNIC-accelerated, interference-free\nprefix caching system for LLM serving. ShadowServe separates a control plane on\nthe host and a data plane fully offloaded to the SmartNIC, which eliminates\ninterference to both host GPU and CPU. To overcome the SmartNIC's limited\ncompute and memory resources, we design a chunked pipeline that parallelizes\ndata plane operations across the SmartNIC's compute resources, and a\nminimal-copy memory management scheme that reduces memory pressure on the\nSmartNIC. Compared to state-of-the-art solutions, ShadowServe achieves up to\n2.2x lower loaded time-per-output-token (TPOT), and reduces time-to-first-token\n(TTFT) by up to 1.38x in low-bandwidth scenarios (<= 20 Gbps), translating to\nup to 1.35x higher throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed prefix caching accelerates long-context LLM serving by reusing KV\ncache entries for common context prefixes. However, KV cache fetches can become\na bottleneck when network bandwidth is limited. Compression mitigates the\nbandwidth issue, but can degrade overall performance when decompression\ninterferes with model computation.\n  We present ShadowServe, the first SmartNIC-accelerated, interference-free\nprefix caching system for LLM serving. ShadowServe separates a control plane on\nthe host and a data plane fully offloaded to the SmartNIC, which eliminates\ninterference to both host GPU and CPU. To overcome the SmartNIC's limited\ncompute and memory resources, we design a chunked pipeline that parallelizes\ndata plane operations across the SmartNIC's compute resources, and a\nminimal-copy memory management scheme that reduces memory pressure on the\nSmartNIC. Compared to state-of-the-art solutions, ShadowServe achieves up to\n2.2x lower loaded time-per-output-token (TPOT), and reduces time-to-first-token\n(TTFT) by up to 1.38x in low-bandwidth scenarios (<= 20 Gbps), translating to\nup to 1.35x higher throughput."
                },
                "authors": [
                    {
                        "name": "Xingyu Xiang"
                    },
                    {
                        "name": "Raj Joshi"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Chenxingyu Zhao"
                    },
                    {
                        "name": "Junchen Jiang"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Eddie Kohler"
                    },
                    {
                        "name": "Minlan Yu"
                    }
                ],
                "author_detail": {
                    "name": "Minlan Yu"
                },
                "author": "Minlan Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16857v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01960v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01960v2",
                "updated": "2025-09-20T13:54:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    20,
                    13,
                    54,
                    37,
                    5,
                    263,
                    0
                ],
                "published": "2025-02-04T03:13:09Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    3,
                    13,
                    9,
                    1,
                    35,
                    0
                ],
                "title": "MPIC: Position-Independent Multimodal Context Caching System for\n  Efficient MLLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MPIC: Position-Independent Multimodal Context Caching System for\n  Efficient MLLM Serving"
                },
                "summary": "The context caching technique is employed to accelerate the Multimodal Large\nLanguage Model (MLLM) inference by prevailing serving platforms currently.\nHowever, this approach merely reuses the Key-Value (KV) cache of the initial\nsequence of prompt, resulting in full KV cache recomputation even if the prefix\ndiffers slightly. This becomes particularly inefficient in the context of\ninterleaved text and images, as well as multimodal retrieval-augmented\ngeneration. This paper proposes position-independent caching as a more\neffective approach for multimodal information management. We have designed and\nimplemented a caching system, named MPIC, to address both system-level and\nalgorithm-level challenges. MPIC stores the KV cache on local disks when\nreceiving multimodal data, and calculates and loads the KV cache in parallel\nduring inference. To mitigate accuracy degradation, we have incorporated the\nintegrated reuse and recompute mechanism within the system. The experimental\nresults demonstrate that MPIC can achieve up to 54\\% reduction in response time\nand 2$\\times$ improvement in throughput compared to existing context caching\nsystems, while maintaining negligible or no accuracy loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The context caching technique is employed to accelerate the Multimodal Large\nLanguage Model (MLLM) inference by prevailing serving platforms currently.\nHowever, this approach merely reuses the Key-Value (KV) cache of the initial\nsequence of prompt, resulting in full KV cache recomputation even if the prefix\ndiffers slightly. This becomes particularly inefficient in the context of\ninterleaved text and images, as well as multimodal retrieval-augmented\ngeneration. This paper proposes position-independent caching as a more\neffective approach for multimodal information management. We have designed and\nimplemented a caching system, named MPIC, to address both system-level and\nalgorithm-level challenges. MPIC stores the KV cache on local disks when\nreceiving multimodal data, and calculates and loads the KV cache in parallel\nduring inference. To mitigate accuracy degradation, we have incorporated the\nintegrated reuse and recompute mechanism within the system. The experimental\nresults demonstrate that MPIC can achieve up to 54\\% reduction in response time\nand 2$\\times$ improvement in throughput compared to existing context caching\nsystems, while maintaining negligible or no accuracy loss."
                },
                "authors": [
                    {
                        "name": "Shiju Zhao"
                    },
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Rongxiao Huang"
                    },
                    {
                        "name": "Jiaqi Zheng"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "arxiv_comment": "17 pages, 13 figures, the second version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01960v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01960v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16686v1",
                "updated": "2025-09-20T13:27:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    20,
                    13,
                    27,
                    13,
                    5,
                    263,
                    0
                ],
                "published": "2025-09-20T13:27:13Z",
                "published_parsed": [
                    2025,
                    9,
                    20,
                    13,
                    27,
                    13,
                    5,
                    263,
                    0
                ],
                "title": "EG-MLA: Embedding-Gated Multi-head Latent Attention for Scalable and\n  Efficient LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EG-MLA: Embedding-Gated Multi-head Latent Attention for Scalable and\n  Efficient LLMs"
                },
                "summary": "Reducing the key-value (KV) cache size is a crucial step toward enabling\nefficient inference in large language models (LLMs), especially under latency\nand memory constraints. While Multi-Head Attention (MHA) offers strong\nrepresentational power, it incurs significant memory overhead. Recent work on\nMulti-head Latent Attention (MLA) mitigates this by compressing KV\nrepresentations into a shared latent space, achieving a better trade-off\nbetween performance and cache efficiency. While MLA already achieves\nsignificant KV cache reduction, the scope for further compression remains\nlimited without performance loss. In this paper, we propose\n\\textbf{Embedding-Gated Multi-head Latent Attention (EG-MLA)}, a novel\nextension of MLA that further reduces KV cache size while enhancing\nrepresentational expressiveness. EG-MLA introduces a token-specific embedding\ngating mechanism applied in the latent space, enabling fine-grained modulation\nof compressed KV vectors with minimal additional computation. Compared to MHA,\nEG-MLA achieves over 91.6\\% reduction in KV cache size with negligible\nperformance degradation. Relative to MLA, EG-MLA consistently improves task\naccuracy across diverse reasoning benchmarks while achieving up to 59.9\\%\nadditional memory savings. Our theoretical analysis highlights how embedding\ngating induces implicit high-order interactions, and empirical evaluations\ndemonstrate robust generalization across model scales and compression regimes.\nNotably, we successfully scale EG-MLA to over 1 billion parameters,\ndemonstrating its practical viability for large-scale LLM deployment. These\nresults establish EG-MLA as a memory- and compute-efficient attention mechanism\nthat enables scalable, high-performance inference in modern LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing the key-value (KV) cache size is a crucial step toward enabling\nefficient inference in large language models (LLMs), especially under latency\nand memory constraints. While Multi-Head Attention (MHA) offers strong\nrepresentational power, it incurs significant memory overhead. Recent work on\nMulti-head Latent Attention (MLA) mitigates this by compressing KV\nrepresentations into a shared latent space, achieving a better trade-off\nbetween performance and cache efficiency. While MLA already achieves\nsignificant KV cache reduction, the scope for further compression remains\nlimited without performance loss. In this paper, we propose\n\\textbf{Embedding-Gated Multi-head Latent Attention (EG-MLA)}, a novel\nextension of MLA that further reduces KV cache size while enhancing\nrepresentational expressiveness. EG-MLA introduces a token-specific embedding\ngating mechanism applied in the latent space, enabling fine-grained modulation\nof compressed KV vectors with minimal additional computation. Compared to MHA,\nEG-MLA achieves over 91.6\\% reduction in KV cache size with negligible\nperformance degradation. Relative to MLA, EG-MLA consistently improves task\naccuracy across diverse reasoning benchmarks while achieving up to 59.9\\%\nadditional memory savings. Our theoretical analysis highlights how embedding\ngating induces implicit high-order interactions, and empirical evaluations\ndemonstrate robust generalization across model scales and compression regimes.\nNotably, we successfully scale EG-MLA to over 1 billion parameters,\ndemonstrating its practical viability for large-scale LLM deployment. These\nresults establish EG-MLA as a memory- and compute-efficient attention mechanism\nthat enables scalable, high-performance inference in modern LLMs."
                },
                "authors": [
                    {
                        "name": "Zhengge Cai"
                    },
                    {
                        "name": "Haowen Hou"
                    }
                ],
                "author_detail": {
                    "name": "Haowen Hou"
                },
                "author": "Haowen Hou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16630v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16630v1",
                "updated": "2025-09-20T11:09:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    20,
                    11,
                    9,
                    1,
                    5,
                    263,
                    0
                ],
                "published": "2025-09-20T11:09:01Z",
                "published_parsed": [
                    2025,
                    9,
                    20,
                    11,
                    9,
                    1,
                    5,
                    263,
                    0
                ],
                "title": "Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and\n  Expressive Freestyle Portrait Animation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and\n  Expressive Freestyle Portrait Animation"
                },
                "summary": "We present Follow-Your-Emoji-Faster, an efficient diffusion-based framework\nfor freestyle portrait animation driven by facial landmarks. The main\nchallenges in this task are preserving the identity of the reference portrait,\naccurately transferring target expressions, and maintaining long-term temporal\nconsistency while ensuring generation efficiency. To address identity\npreservation and accurate expression retargeting, we enhance Stable Diffusion\nwith two key components: a expression-aware landmarks as explicit motion\nsignals, which improve motion alignment, support exaggerated expressions, and\nreduce identity leakage; and a fine-grained facial loss that leverages both\nexpression and facial masks to better capture subtle expressions and faithfully\npreserve the reference appearance. With these components, our model supports\ncontrollable and expressive animation across diverse portrait types, including\nreal faces, cartoons, sculptures, and animals. However, diffusion-based\nframeworks typically struggle to efficiently generate long-term stable\nanimation results, which remains a core challenge in this task. To address\nthis, we propose a progressive generation strategy for stable long-term\nanimation, and introduce a Taylor-interpolated cache, achieving a 2.6X lossless\nacceleration. These two strategies ensure that our method produces high-quality\nresults efficiently, making it user-friendly and accessible. Finally, we\nintroduce EmojiBench++, a more comprehensive benchmark comprising diverse\nportraits, driving videos, and landmark sequences. Extensive evaluations on\nEmojiBench++ demonstrate that Follow-Your-Emoji-Faster achieves superior\nperformance in both animation quality and controllability. The code, training\ndataset and benchmark will be found in https://follow-your-emoji.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Follow-Your-Emoji-Faster, an efficient diffusion-based framework\nfor freestyle portrait animation driven by facial landmarks. The main\nchallenges in this task are preserving the identity of the reference portrait,\naccurately transferring target expressions, and maintaining long-term temporal\nconsistency while ensuring generation efficiency. To address identity\npreservation and accurate expression retargeting, we enhance Stable Diffusion\nwith two key components: a expression-aware landmarks as explicit motion\nsignals, which improve motion alignment, support exaggerated expressions, and\nreduce identity leakage; and a fine-grained facial loss that leverages both\nexpression and facial masks to better capture subtle expressions and faithfully\npreserve the reference appearance. With these components, our model supports\ncontrollable and expressive animation across diverse portrait types, including\nreal faces, cartoons, sculptures, and animals. However, diffusion-based\nframeworks typically struggle to efficiently generate long-term stable\nanimation results, which remains a core challenge in this task. To address\nthis, we propose a progressive generation strategy for stable long-term\nanimation, and introduce a Taylor-interpolated cache, achieving a 2.6X lossless\nacceleration. These two strategies ensure that our method produces high-quality\nresults efficiently, making it user-friendly and accessible. Finally, we\nintroduce EmojiBench++, a more comprehensive benchmark comprising diverse\nportraits, driving videos, and landmark sequences. Extensive evaluations on\nEmojiBench++ demonstrate that Follow-Your-Emoji-Faster achieves superior\nperformance in both animation quality and controllability. The code, training\ndataset and benchmark will be found in https://follow-your-emoji.github.io/."
                },
                "authors": [
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Zexuan Yan"
                    },
                    {
                        "name": "Hongyu Liu"
                    },
                    {
                        "name": "Hongfa Wang"
                    },
                    {
                        "name": "Heng Pan"
                    },
                    {
                        "name": "Yingqing He"
                    },
                    {
                        "name": "Junkun Yuan"
                    },
                    {
                        "name": "Ailing Zeng"
                    },
                    {
                        "name": "Chengfei Cai"
                    },
                    {
                        "name": "Heung-Yeung Shum"
                    },
                    {
                        "name": "Zhifeng Li"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Qifeng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Qifeng Chen"
                },
                "author": "Qifeng Chen",
                "arxiv_comment": "accepted by IJCV2025. project\n  page:https://follow-your-emoji.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16630v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16630v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21354v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21354v1",
                "updated": "2025-09-20T02:04:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    20,
                    2,
                    4,
                    24,
                    5,
                    263,
                    0
                ],
                "published": "2025-09-20T02:04:24Z",
                "published_parsed": [
                    2025,
                    9,
                    20,
                    2,
                    4,
                    24,
                    5,
                    263,
                    0
                ],
                "title": "KV-Efficient VLA: A Method of Speed up Vision Language Model with\n  RNN-Gated Chunked KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Efficient VLA: A Method of Speed up Vision Language Model with\n  RNN-Gated Chunked KV Cache"
                },
                "summary": "Vision-Language-Action (VLA) models promise unified robotic perception and\ncontrol, yet their scalability is constrained by the quadratic cost of\nattention and the unbounded growth of key-value (KV) memory during long-horizon\ninference. While recent methods improve generalization through scaling backbone\narchitectures, they often neglect the inference inefficiencies critical to\nreal-time deployment. In this work, we present KV-Efficient VLA, a\nmodel-agnostic memory compression framework that addresses these limitations by\nintroducing a lightweight, training-friendly mechanism to selectively retain\nhigh-utility context. Our method partitions the KV cache into fixed size chunks\nand employs a recurrent gating module to summarize and filter historical\ncontext according to learned utility scores. This design preserves recent\nfine-grained detail while aggressively pruning stale, low-relevance memory, all\nwhile maintaining causality. Theoretically, KV-Efficient VLA yields up to 1.21x\ninference speedup and 36% KV memory reduction, with minimal impact on task\nsuccess. Our method integrates seamlessly into existing autoregressive and\nhybrid VLA stacks, enabling scalable inference without modifying training\npipelines or downstream control logic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models promise unified robotic perception and\ncontrol, yet their scalability is constrained by the quadratic cost of\nattention and the unbounded growth of key-value (KV) memory during long-horizon\ninference. While recent methods improve generalization through scaling backbone\narchitectures, they often neglect the inference inefficiencies critical to\nreal-time deployment. In this work, we present KV-Efficient VLA, a\nmodel-agnostic memory compression framework that addresses these limitations by\nintroducing a lightweight, training-friendly mechanism to selectively retain\nhigh-utility context. Our method partitions the KV cache into fixed size chunks\nand employs a recurrent gating module to summarize and filter historical\ncontext according to learned utility scores. This design preserves recent\nfine-grained detail while aggressively pruning stale, low-relevance memory, all\nwhile maintaining causality. Theoretically, KV-Efficient VLA yields up to 1.21x\ninference speedup and 36% KV memory reduction, with minimal impact on task\nsuccess. Our method integrates seamlessly into existing autoregressive and\nhybrid VLA stacks, enabling scalable inference without modifying training\npipelines or downstream control logic."
                },
                "authors": [
                    {
                        "name": "Wanshun Xu"
                    },
                    {
                        "name": "Long Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Long Zhuang"
                },
                "author": "Long Zhuang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21354v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21354v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16495v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16495v1",
                "updated": "2025-09-20T01:56:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    20,
                    1,
                    56,
                    25,
                    5,
                    263,
                    0
                ],
                "published": "2025-09-20T01:56:25Z",
                "published_parsed": [
                    2025,
                    9,
                    20,
                    1,
                    56,
                    25,
                    5,
                    263,
                    0
                ],
                "title": "Shift Parallelism: Low-Latency, High-Throughput LLM Inference for\n  Dynamic Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shift Parallelism: Low-Latency, High-Throughput LLM Inference for\n  Dynamic Workloads"
                },
                "summary": "Efficient parallelism is necessary for achieving low-latency, high-throughput\ninference with large language models (LLMs). Tensor parallelism (TP) is the\nstate-of-the-art method for reducing LLM response latency, however GPU\ncommunications reduces combined token throughput. On the other hand, data\nparallelism (DP) obtains a higher throughput yet is slow in response latency.\nBest of both worlds does not exist, and it is not possible to combine TP and DP\nbecause of the KV cache variance across the parallelisms.\n  We notice Sequence Parallelism (SP - Ulysses in training) has similar\nproperties as DP but with KV cache invariance. We adapt SP to inference, and\ncombine it with TP to get the best of both worlds. Our solution: Shift\nParallelism.\n  Shift Parallelism dynamically switches across TP and SP, and minimizes\nlatency in low traffic without losing throughput in high traffic. The efficient\nGPU communications of Shift Parallelism yields up to i) 1.51x faster response\nin interactive workloads and ii) 50% higher throughput in batch workloads,\ncompared to a TP-only solution.\n  We evaluate Shift Parallelism with real-world production traces with dynamic\ntraffic patterns as well as synthetic benchmarking patterns across models,\ncontext sizes, and arrival rates. All results affirm the same: Shift\nParallelism has a better the latency vs. throughput tradeoff than TP or DP, and\nhence obtains low latency without degrading throughput in dynamic workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient parallelism is necessary for achieving low-latency, high-throughput\ninference with large language models (LLMs). Tensor parallelism (TP) is the\nstate-of-the-art method for reducing LLM response latency, however GPU\ncommunications reduces combined token throughput. On the other hand, data\nparallelism (DP) obtains a higher throughput yet is slow in response latency.\nBest of both worlds does not exist, and it is not possible to combine TP and DP\nbecause of the KV cache variance across the parallelisms.\n  We notice Sequence Parallelism (SP - Ulysses in training) has similar\nproperties as DP but with KV cache invariance. We adapt SP to inference, and\ncombine it with TP to get the best of both worlds. Our solution: Shift\nParallelism.\n  Shift Parallelism dynamically switches across TP and SP, and minimizes\nlatency in low traffic without losing throughput in high traffic. The efficient\nGPU communications of Shift Parallelism yields up to i) 1.51x faster response\nin interactive workloads and ii) 50% higher throughput in batch workloads,\ncompared to a TP-only solution.\n  We evaluate Shift Parallelism with real-world production traces with dynamic\ntraffic patterns as well as synthetic benchmarking patterns across models,\ncontext sizes, and arrival rates. All results affirm the same: Shift\nParallelism has a better the latency vs. throughput tradeoff than TP or DP, and\nhence obtains low latency without degrading throughput in dynamic workloads."
                },
                "authors": [
                    {
                        "name": "Mert Hidayetoglu"
                    },
                    {
                        "name": "Aurick Qiao"
                    },
                    {
                        "name": "Michael Wyatt"
                    },
                    {
                        "name": "Jeff Rasley"
                    },
                    {
                        "name": "Yuxiong He"
                    },
                    {
                        "name": "Samyam Rajbhandari"
                    }
                ],
                "author_detail": {
                    "name": "Samyam Rajbhandari"
                },
                "author": "Samyam Rajbhandari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16495v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16495v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16471v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16471v1",
                "updated": "2025-09-19T23:46:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    23,
                    46,
                    8,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T23:46:08Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    23,
                    46,
                    8,
                    4,
                    262,
                    0
                ],
                "title": "From Coated to Uncoated: Scanning Electron Microscopy Corrections to\n  Estimate True Surface Pore Size in Nanoporous Membranes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Coated to Uncoated: Scanning Electron Microscopy Corrections to\n  Estimate True Surface Pore Size in Nanoporous Membranes"
                },
                "summary": "Scanning electron microscopy (SEM) is the premier method for characterizing\nthe nanoscale surface pores in ultrafiltration (UF) membranes and the support\nlayers of reverse osmosis (RO) membranes. Based on SEM, the conventional\nunderstanding is that membranes typically have low surface porosities of <10%.\nWe hypothesized that high acceleration voltage during SEM imaging and sputter\nmetal coatings required for SEM have led to systematic underestimations of\nporosity and pore size. We showed that imaging a commercial UF membrane at 1,\n5, and 10 kV reduced measured porosity from 10.3% (1 kV) to 6.3% (10 kV), while\nincreasing Pt coating thickness from 1.5 to 5 nm lowered porosity by 54% for\nthe UF membrane (12.9% to 5.8%) and 46% for an RO support (13.1% to 7.0%). To\naccount for coating thickness, we developed a digital correction method that\nsimulates pore dilation, enabling the pore structure to be estimated for\nuncoated membranes. Dilation yielded uncoated porosity values of 23% for the UF\nmembrane and 20% for the RO support, about 3-fold greater than values observed\nwith a 4 nm coating. Mean pore diameters were 2-fold greater for the UF\nmembrane and 1.5-fold greater for the RO support. Critically, dilation-derived\npore-size distributions agreed with low-flux dextran-retention data fitted with\nthe Bungay-Brenner model. Our results suggest that surface porosities and pore\nsizes of nanoporous membranes are much larger than previously understood, with\nmajor implications for structure/transport relationships. For future nanoscale\npore analysis of membranes (and other nanoporous materials), we recommend low\nacceleration voltage (1 kV), minimal coatings (1-2 nm), and digital dilation to\naccount for coating artifacts",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scanning electron microscopy (SEM) is the premier method for characterizing\nthe nanoscale surface pores in ultrafiltration (UF) membranes and the support\nlayers of reverse osmosis (RO) membranes. Based on SEM, the conventional\nunderstanding is that membranes typically have low surface porosities of <10%.\nWe hypothesized that high acceleration voltage during SEM imaging and sputter\nmetal coatings required for SEM have led to systematic underestimations of\nporosity and pore size. We showed that imaging a commercial UF membrane at 1,\n5, and 10 kV reduced measured porosity from 10.3% (1 kV) to 6.3% (10 kV), while\nincreasing Pt coating thickness from 1.5 to 5 nm lowered porosity by 54% for\nthe UF membrane (12.9% to 5.8%) and 46% for an RO support (13.1% to 7.0%). To\naccount for coating thickness, we developed a digital correction method that\nsimulates pore dilation, enabling the pore structure to be estimated for\nuncoated membranes. Dilation yielded uncoated porosity values of 23% for the UF\nmembrane and 20% for the RO support, about 3-fold greater than values observed\nwith a 4 nm coating. Mean pore diameters were 2-fold greater for the UF\nmembrane and 1.5-fold greater for the RO support. Critically, dilation-derived\npore-size distributions agreed with low-flux dextran-retention data fitted with\nthe Bungay-Brenner model. Our results suggest that surface porosities and pore\nsizes of nanoporous membranes are much larger than previously understood, with\nmajor implications for structure/transport relationships. For future nanoscale\npore analysis of membranes (and other nanoporous materials), we recommend low\nacceleration voltage (1 kV), minimal coatings (1-2 nm), and digital dilation to\naccount for coating artifacts"
                },
                "authors": [
                    {
                        "name": "Sima Zeinali Danalou"
                    },
                    {
                        "name": "Dian Yu"
                    },
                    {
                        "name": "Niher R. Sarker"
                    },
                    {
                        "name": "Hooman Chamani"
                    },
                    {
                        "name": "Jane Y. Howe"
                    },
                    {
                        "name": "Patrick C. Lee"
                    },
                    {
                        "name": "Jay R. Werber"
                    }
                ],
                "author_detail": {
                    "name": "Jay R. Werber"
                },
                "author": "Jay R. Werber",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16471v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16471v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2507.12142v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12142v2",
                "updated": "2025-10-01T17:59:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    17,
                    59,
                    10,
                    2,
                    274,
                    0
                ],
                "published": "2025-07-16T11:17:12Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    11,
                    17,
                    12,
                    2,
                    197,
                    0
                ],
                "title": "LoRA meets Riemannion: Muon Optimizer for Parametrization-independent\n  Low-Rank Adapters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRA meets Riemannion: Muon Optimizer for Parametrization-independent\n  Low-Rank Adapters"
                },
                "summary": "This work presents a novel, fully Riemannian framework for Low-Rank\nAdaptation (LoRA) that geometrically treats low-rank adapters by optimizing\nthem directly on the fixed-rank manifold. This formulation eliminates the\nparametrization ambiguity present in standard Euclidean optimizers. Our\nframework integrates three key components to achieve this: (1) we derive\nRiemannion, a new Riemannian optimizer on the fixed-rank matrix manifold that\ngeneralizes the recently proposed Muon optimizer; (2) we develop a Riemannian\ngradient-informed LoRA initialization, and (3) we provide an efficient\nimplementation without prominent overhead that uses automatic differentiation\nto compute arising geometric operations while adhering to best practices in\nnumerical linear algebra. Comprehensive experimental results on both LLM and\ndiffusion model architectures demonstrate that our approach yields consistent\nand noticeable improvements in convergence speed and final task performance\nover both standard LoRA and its state-of-the-art modifications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents a novel, fully Riemannian framework for Low-Rank\nAdaptation (LoRA) that geometrically treats low-rank adapters by optimizing\nthem directly on the fixed-rank manifold. This formulation eliminates the\nparametrization ambiguity present in standard Euclidean optimizers. Our\nframework integrates three key components to achieve this: (1) we derive\nRiemannion, a new Riemannian optimizer on the fixed-rank matrix manifold that\ngeneralizes the recently proposed Muon optimizer; (2) we develop a Riemannian\ngradient-informed LoRA initialization, and (3) we provide an efficient\nimplementation without prominent overhead that uses automatic differentiation\nto compute arising geometric operations while adhering to best practices in\nnumerical linear algebra. Comprehensive experimental results on both LLM and\ndiffusion model architectures demonstrate that our approach yields consistent\nand noticeable improvements in convergence speed and final task performance\nover both standard LoRA and its state-of-the-art modifications."
                },
                "authors": [
                    {
                        "name": "Vladimir Bogachev"
                    },
                    {
                        "name": "Vladimir Aletov"
                    },
                    {
                        "name": "Alexander Molozhavenko"
                    },
                    {
                        "name": "Denis Bobkov"
                    },
                    {
                        "name": "Vera Soboleva"
                    },
                    {
                        "name": "Aibek Alanov"
                    },
                    {
                        "name": "Maxim Rakhuba"
                    }
                ],
                "author_detail": {
                    "name": "Maxim Rakhuba"
                },
                "author": "Maxim Rakhuba",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12142v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12142v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.DG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07, 65F55, 53Z50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00907v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00907v4",
                "updated": "2025-10-01T17:58:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    17,
                    58,
                    0,
                    2,
                    274,
                    0
                ],
                "published": "2025-04-01T15:41:50Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    15,
                    41,
                    50,
                    1,
                    91,
                    0
                ],
                "title": "Grounding Multimodal LLMs to Embodied Agents that Ask for Help with\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grounding Multimodal LLMs to Embodied Agents that Ask for Help with\n  Reinforcement Learning"
                },
                "summary": "Embodied agents operating in household environments must interpret ambiguous\nand under-specified human instructions. A capable household robot should\nrecognize ambiguity and ask relevant clarification questions to infer the user\nintent accurately, leading to more effective task execution. To study this\nproblem, we introduce the Ask-to-Act task, where an embodied agent is tasked\nwith a single or multi-object rearrangement task using an under-specified\ninstruction in a home environment. The agent must strategically ask minimal,\nyet relevant, clarification questions to resolve ambiguity while navigating\nunder partial observability. To address this challenge, we propose a novel\napproach that fine-tunes multi-modal large language models (MLLMs) as\nvision-language-action (VLA) policies using online reinforcement learning (RL)\nwith LLM-generated rewards. Our method eliminates the need for large-scale\nhuman demonstrations or manually engineered rewards for training such agents.\nWe benchmark against strong zero-shot baselines including GPT-4o as well as\nsupervised fine-tuned MLLMs on our task. Our results show that our RL-finetuned\nMLLM outperforms all baselines by a significant margin (10.4-16.5%),\ngeneralizing well to novel scenes and tasks. To the best of our knowledge, this\nis the first demonstration of adapting MLLMs as VLA agents that can act and ask\nfor help using LLM-generated rewards with online RL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied agents operating in household environments must interpret ambiguous\nand under-specified human instructions. A capable household robot should\nrecognize ambiguity and ask relevant clarification questions to infer the user\nintent accurately, leading to more effective task execution. To study this\nproblem, we introduce the Ask-to-Act task, where an embodied agent is tasked\nwith a single or multi-object rearrangement task using an under-specified\ninstruction in a home environment. The agent must strategically ask minimal,\nyet relevant, clarification questions to resolve ambiguity while navigating\nunder partial observability. To address this challenge, we propose a novel\napproach that fine-tunes multi-modal large language models (MLLMs) as\nvision-language-action (VLA) policies using online reinforcement learning (RL)\nwith LLM-generated rewards. Our method eliminates the need for large-scale\nhuman demonstrations or manually engineered rewards for training such agents.\nWe benchmark against strong zero-shot baselines including GPT-4o as well as\nsupervised fine-tuned MLLMs on our task. Our results show that our RL-finetuned\nMLLM outperforms all baselines by a significant margin (10.4-16.5%),\ngeneralizing well to novel scenes and tasks. To the best of our knowledge, this\nis the first demonstration of adapting MLLMs as VLA agents that can act and ask\nfor help using LLM-generated rewards with online RL."
                },
                "authors": [
                    {
                        "name": "Ram Ramrakhya"
                    },
                    {
                        "name": "Matthew Chang"
                    },
                    {
                        "name": "Xavier Puig"
                    },
                    {
                        "name": "Ruta Desai"
                    },
                    {
                        "name": "Zsolt Kira"
                    },
                    {
                        "name": "Roozbeh Mottaghi"
                    }
                ],
                "author_detail": {
                    "name": "Roozbeh Mottaghi"
                },
                "author": "Roozbeh Mottaghi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00907v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00907v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04979v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04979v3",
                "updated": "2025-10-01T17:56:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    17,
                    56,
                    28,
                    2,
                    274,
                    0
                ],
                "published": "2025-02-07T14:57:17Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    14,
                    57,
                    17,
                    4,
                    38,
                    0
                ],
                "title": "Prompt Tuning Decision Transformers with Structured and Scalable Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt Tuning Decision Transformers with Structured and Scalable Bandits"
                },
                "summary": "Prompt tuning has emerged as a key technique for adapting large pre-trained\nDecision Transformers (DTs) in offline Reinforcement Learning (RL),\nparticularly in multi-task and few-shot settings. The Prompting Decision\nTransformer (PDT) enables task generalization via trajectory prompts sampled\nuniformly from expert demonstrations -- without accounting for prompt\ninformativeness. In this work, we propose a bandit-based prompt-tuning method\nthat learns to construct optimal trajectory prompts from demonstration data at\ninference time. We devise a structured bandit architecture operating in the\ntrajectory prompt space, achieving linear rather than combinatorial scaling\nwith prompt size. Additionally, we show that the pre-trained PDT itself can\nserve as a powerful feature extractor for the bandit, enabling efficient reward\nmodeling across various environments. We theoretically establish regret bounds\nand demonstrate empirically that our method consistently enhances performance\nacross a wide range of tasks, high-dimensional environments, and\nout-of-distribution scenarios, outperforming existing baselines in prompt\ntuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt tuning has emerged as a key technique for adapting large pre-trained\nDecision Transformers (DTs) in offline Reinforcement Learning (RL),\nparticularly in multi-task and few-shot settings. The Prompting Decision\nTransformer (PDT) enables task generalization via trajectory prompts sampled\nuniformly from expert demonstrations -- without accounting for prompt\ninformativeness. In this work, we propose a bandit-based prompt-tuning method\nthat learns to construct optimal trajectory prompts from demonstration data at\ninference time. We devise a structured bandit architecture operating in the\ntrajectory prompt space, achieving linear rather than combinatorial scaling\nwith prompt size. Additionally, we show that the pre-trained PDT itself can\nserve as a powerful feature extractor for the bandit, enabling efficient reward\nmodeling across various environments. We theoretically establish regret bounds\nand demonstrate empirically that our method consistently enhances performance\nacross a wide range of tasks, high-dimensional environments, and\nout-of-distribution scenarios, outperforming existing baselines in prompt\ntuning."
                },
                "authors": [
                    {
                        "name": "Finn Rietz"
                    },
                    {
                        "name": "Oleg Smirnov"
                    },
                    {
                        "name": "Sara Karimi"
                    },
                    {
                        "name": "Lele Cao"
                    }
                ],
                "author_detail": {
                    "name": "Lele Cao"
                },
                "author": "Lele Cao",
                "arxiv_comment": "Accepted at NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04979v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04979v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16928v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16928v2",
                "updated": "2025-10-01T17:51:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    17,
                    51,
                    44,
                    2,
                    274,
                    0
                ],
                "published": "2025-05-22T17:20:38Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    20,
                    38,
                    3,
                    142,
                    0
                ],
                "title": "Beyond Needle(s) in the Embodied Haystack: Environment, Architecture,\n  and Training Considerations for Long Context Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Needle(s) in the Embodied Haystack: Environment, Architecture,\n  and Training Considerations for Long Context Reasoning"
                },
                "summary": "We introduce $\\infty$-THOR, a new framework for long-horizon embodied tasks\nthat advances long-context understanding in embodied AI. $\\infty$-THOR\nprovides: (1) a generation framework for synthesizing scalable, reproducible,\nand unlimited long-horizon trajectories; (2) a novel embodied QA task,\nNeedle(s) in the Embodied Haystack, where multiple scattered clues across\nextended trajectories test agents' long-context reasoning ability; and (3) a\nlong-horizon dataset and benchmark suite featuring complex tasks that span\nhundreds of environment steps, each paired with ground-truth action sequences.\nTo enable this capability, we explore architectural adaptations, including\ninterleaved Goal-State-Action modeling, context extension techniques, and\nContext Parallelism, to equip LLM-based agents for extreme long-context\nreasoning and interaction. Experimental results and analyses highlight the\nchallenges posed by our benchmark and provide insights into training strategies\nand model behaviors under long-horizon conditions. Our work provides a\nfoundation for the next generation of embodied AI systems capable of robust,\nlong-term reasoning and planning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce $\\infty$-THOR, a new framework for long-horizon embodied tasks\nthat advances long-context understanding in embodied AI. $\\infty$-THOR\nprovides: (1) a generation framework for synthesizing scalable, reproducible,\nand unlimited long-horizon trajectories; (2) a novel embodied QA task,\nNeedle(s) in the Embodied Haystack, where multiple scattered clues across\nextended trajectories test agents' long-context reasoning ability; and (3) a\nlong-horizon dataset and benchmark suite featuring complex tasks that span\nhundreds of environment steps, each paired with ground-truth action sequences.\nTo enable this capability, we explore architectural adaptations, including\ninterleaved Goal-State-Action modeling, context extension techniques, and\nContext Parallelism, to equip LLM-based agents for extreme long-context\nreasoning and interaction. Experimental results and analyses highlight the\nchallenges posed by our benchmark and provide insights into training strategies\nand model behaviors under long-horizon conditions. Our work provides a\nfoundation for the next generation of embodied AI systems capable of robust,\nlong-term reasoning and planning."
                },
                "authors": [
                    {
                        "name": "Bosung Kim"
                    },
                    {
                        "name": "Prithviraj Ammanabrolu"
                    }
                ],
                "author_detail": {
                    "name": "Prithviraj Ammanabrolu"
                },
                "author": "Prithviraj Ammanabrolu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16928v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16928v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25035v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25035v2",
                "updated": "2025-10-01T17:45:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    17,
                    45,
                    9,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-29T16:55:44Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    55,
                    44,
                    0,
                    272,
                    0
                ],
                "title": "Ultra-Fast Language Generation via Discrete Diffusion Divergence\n  Instruct",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultra-Fast Language Generation via Discrete Diffusion Divergence\n  Instruct"
                },
                "summary": "Fast and high-quality language generation is the holy grail that people\npursue in the age of AI. In this work, we introduce Discrete Diffusion\nDivergence Instruct (DiDi-Instruct), a training-based method that initializes\nfrom a pre-trained (masked) discrete diffusion language model (dLLM) and\ndistills a few-step student for fast generation. The resulting DiDi-Instruct\nmodel achieves comparable or superior performance to its dLLM teacher and the\nGPT-2 baseline while enabling up to 64$\\times$ acceleration. The theoretical\nfoundation of DiDi-Instruct is a novel framework based on integral\nKL-divergence minimization, which yields a practical training algorithm. We\nfurther introduce grouped reward normalization, intermediate-state matching,\nand the reward-guided ancestral sampler that significantly improve training\nstability, model coverage, and inference quality. On OpenWebText, DiDi-Instruct\nachieves perplexity from 62.2 (8 NFEs) to 18.4 (128 NFEs), which outperforms\nprior accelerated dLLMs and GPT-2 baseline. These gains come with a negligible\nentropy loss (around $1\\%$) and reduce additional training wall-clock time by\nmore than $20\\times$ compared to competing dLLM distillation methods. We\nfurther validate the robustness and effectiveness of DiDi-Instruct through\nextensive ablation studies, model scaling, and the generation of discrete\nprotein sequences. In conclusion, DiDi-Instruct is an efficient yet effective\ndistillation method, enabling language generation in the blink of an eye. We\nwill release both code and models at github.com/haoyangzheng-ai/didi-instruct.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and high-quality language generation is the holy grail that people\npursue in the age of AI. In this work, we introduce Discrete Diffusion\nDivergence Instruct (DiDi-Instruct), a training-based method that initializes\nfrom a pre-trained (masked) discrete diffusion language model (dLLM) and\ndistills a few-step student for fast generation. The resulting DiDi-Instruct\nmodel achieves comparable or superior performance to its dLLM teacher and the\nGPT-2 baseline while enabling up to 64$\\times$ acceleration. The theoretical\nfoundation of DiDi-Instruct is a novel framework based on integral\nKL-divergence minimization, which yields a practical training algorithm. We\nfurther introduce grouped reward normalization, intermediate-state matching,\nand the reward-guided ancestral sampler that significantly improve training\nstability, model coverage, and inference quality. On OpenWebText, DiDi-Instruct\nachieves perplexity from 62.2 (8 NFEs) to 18.4 (128 NFEs), which outperforms\nprior accelerated dLLMs and GPT-2 baseline. These gains come with a negligible\nentropy loss (around $1\\%$) and reduce additional training wall-clock time by\nmore than $20\\times$ compared to competing dLLM distillation methods. We\nfurther validate the robustness and effectiveness of DiDi-Instruct through\nextensive ablation studies, model scaling, and the generation of discrete\nprotein sequences. In conclusion, DiDi-Instruct is an efficient yet effective\ndistillation method, enabling language generation in the blink of an eye. We\nwill release both code and models at github.com/haoyangzheng-ai/didi-instruct."
                },
                "authors": [
                    {
                        "name": "Haoyang Zheng"
                    },
                    {
                        "name": "Xinyang Liu"
                    },
                    {
                        "name": "Cindy Xiangrui Kong"
                    },
                    {
                        "name": "Nan Jiang"
                    },
                    {
                        "name": "Zheyuan Hu"
                    },
                    {
                        "name": "Weijian Luo"
                    },
                    {
                        "name": "Wei Deng"
                    },
                    {
                        "name": "Guang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Guang Lin"
                },
                "author": "Guang Lin",
                "arxiv_comment": "56 pages, 7 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25035v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25035v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25297v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25297v2",
                "updated": "2025-10-01T17:32:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    17,
                    32,
                    51,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-29T16:18:19Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    18,
                    19,
                    0,
                    272,
                    0
                ],
                "title": "Automatically Generating Web Applications from Requirements Via\n  Multi-Agent Test-Driven Development",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatically Generating Web Applications from Requirements Via\n  Multi-Agent Test-Driven Development"
                },
                "summary": "Developing full-stack web applications is complex and time-intensive,\ndemanding proficiency across diverse technologies and frameworks. Although\nrecent advances in multimodal large language models (MLLMs) enable automated\nwebpage generation from visual inputs, current solutions remain limited to\nfront-end tasks and fail to deliver fully functional applications. In this\nwork, we introduce TDDev, the first test-driven development (TDD)-enabled\nLLM-agent framework for end-to-end full-stack web application generation. Given\na natural language description or design image, TDDev automatically derives\nexecutable test cases, generates front-end and back-end code, simulates user\ninteractions, and iteratively refines the implementation until all requirements\nare satisfied. Our framework addresses key challenges in full-stack automation,\nincluding underspecified user requirements, complex interdependencies among\nmultiple files, and the need for both functional correctness and visual\nfidelity. Through extensive experiments on diverse application scenarios, TDDev\nachieves a 14.4% improvement on overall accuracy compared to state-of-the-art\nbaselines, demonstrating its effectiveness in producing reliable, high-quality\nweb applications without requiring manual intervention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing full-stack web applications is complex and time-intensive,\ndemanding proficiency across diverse technologies and frameworks. Although\nrecent advances in multimodal large language models (MLLMs) enable automated\nwebpage generation from visual inputs, current solutions remain limited to\nfront-end tasks and fail to deliver fully functional applications. In this\nwork, we introduce TDDev, the first test-driven development (TDD)-enabled\nLLM-agent framework for end-to-end full-stack web application generation. Given\na natural language description or design image, TDDev automatically derives\nexecutable test cases, generates front-end and back-end code, simulates user\ninteractions, and iteratively refines the implementation until all requirements\nare satisfied. Our framework addresses key challenges in full-stack automation,\nincluding underspecified user requirements, complex interdependencies among\nmultiple files, and the need for both functional correctness and visual\nfidelity. Through extensive experiments on diverse application scenarios, TDDev\nachieves a 14.4% improvement on overall accuracy compared to state-of-the-art\nbaselines, demonstrating its effectiveness in producing reliable, high-quality\nweb applications without requiring manual intervention."
                },
                "authors": [
                    {
                        "name": "Yuxuan Wan"
                    },
                    {
                        "name": "Tingshuo Liang"
                    },
                    {
                        "name": "Jiakai Xu"
                    },
                    {
                        "name": "Jingyu Xiao"
                    },
                    {
                        "name": "Yintong Huo"
                    },
                    {
                        "name": "Michael R. Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Michael R. Lyu"
                },
                "author": "Michael R. Lyu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25297v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25297v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25477v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25477v3",
                "updated": "2025-10-02T03:08:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    3,
                    8,
                    37,
                    3,
                    275,
                    0
                ],
                "published": "2025-09-29T20:31:12Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    20,
                    31,
                    12,
                    0,
                    272,
                    0
                ],
                "title": "The Rise of AfricaNLP: Contributions, Contributors, and Community Impact\n  (2005-2025)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Rise of AfricaNLP: Contributions, Contributors, and Community Impact\n  (2005-2025)"
                },
                "summary": "Natural Language Processing (NLP) is undergoing constant transformation, as\nLarge Language Models (LLMs) are driving daily breakthroughs in research and\npractice. In this regard, tracking the progress of NLP research and\nautomatically analyzing the contributions of research papers provides key\ninsights into the nature of the field and the researchers. This study explores\nthe progress of African NLP (AfricaNLP) by asking (and answering) basic\nresearch questions such as: i) How has the nature of NLP evolved over the last\ntwo decades?, ii) What are the contributions of AfricaNLP papers?, and iii)\nWhich individuals and organizations (authors, affiliated institutions, and\nfunding bodies) have been involved in the development of AfricaNLP? We\nquantitatively examine the contributions of AfricaNLP research using 1.9K NLP\npaper abstracts, 4.9K author contributors, and 7.8K human-annotated\ncontribution sentences (AfricaNLPContributions) along with benchmark results.\nOur dataset and continuously existing NLP progress tracking website provide a\npowerful lens for tracing AfricaNLP research trends and hold potential for\ngenerating data-driven literature surveys.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language Processing (NLP) is undergoing constant transformation, as\nLarge Language Models (LLMs) are driving daily breakthroughs in research and\npractice. In this regard, tracking the progress of NLP research and\nautomatically analyzing the contributions of research papers provides key\ninsights into the nature of the field and the researchers. This study explores\nthe progress of African NLP (AfricaNLP) by asking (and answering) basic\nresearch questions such as: i) How has the nature of NLP evolved over the last\ntwo decades?, ii) What are the contributions of AfricaNLP papers?, and iii)\nWhich individuals and organizations (authors, affiliated institutions, and\nfunding bodies) have been involved in the development of AfricaNLP? We\nquantitatively examine the contributions of AfricaNLP research using 1.9K NLP\npaper abstracts, 4.9K author contributors, and 7.8K human-annotated\ncontribution sentences (AfricaNLPContributions) along with benchmark results.\nOur dataset and continuously existing NLP progress tracking website provide a\npowerful lens for tracing AfricaNLP research trends and hold potential for\ngenerating data-driven literature surveys."
                },
                "authors": [
                    {
                        "name": "Tadesse Destaw Belay"
                    },
                    {
                        "name": "Kedir Yassin Hussen"
                    },
                    {
                        "name": "Sukairaj Hafiz Imam"
                    },
                    {
                        "name": "Ibrahim Said Ahmad"
                    },
                    {
                        "name": "Isa Inuwa-Dutse"
                    },
                    {
                        "name": "Abrham Belete Haile"
                    },
                    {
                        "name": "Grigori Sidorov"
                    },
                    {
                        "name": "Iqra Ameer"
                    },
                    {
                        "name": "Idris Abdulmumin"
                    },
                    {
                        "name": "Tajuddeen Gwadabe"
                    },
                    {
                        "name": "Vukosi Marivate"
                    },
                    {
                        "name": "Seid Muhie Yimam"
                    },
                    {
                        "name": "Shamsuddeen Hassan Muhammad"
                    }
                ],
                "author_detail": {
                    "name": "Shamsuddeen Hassan Muhammad"
                },
                "author": "Shamsuddeen Hassan Muhammad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25477v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25477v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08584v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08584v2",
                "updated": "2025-10-01T17:10:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    17,
                    10,
                    2,
                    2,
                    274,
                    0
                ],
                "published": "2025-06-10T08:53:06Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    8,
                    53,
                    6,
                    1,
                    161,
                    0
                ],
                "title": "CounselBench: A Large-Scale Expert Evaluation and Adversarial\n  Benchmarking of Large Language Models in Mental Health Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CounselBench: A Large-Scale Expert Evaluation and Adversarial\n  Benchmarking of Large Language Models in Mental Health Question Answering"
                },
                "summary": "Medical question answering (QA) benchmarks often focus on multiple-choice or\nfact-based tasks, leaving open-ended answers to real patient questions\nunderexplored. This gap is particularly critical in mental health, where\npatient questions often mix symptoms, treatment concerns, and emotional needs,\nrequiring answers that balance clinical caution with contextual sensitivity. We\npresent CounselBench, a large-scale benchmark developed with 100 mental health\nprofessionals to evaluate and stress-test large language models (LLMs) in\nrealistic help-seeking scenarios. The first component, CounselBench-EVAL,\ncontains 2,000 expert evaluations of answers from GPT-4, LLaMA 3, Gemini, and\nhuman therapists on patient questions from the public forum CounselChat. Each\nanswer is rated across six clinically grounded dimensions, with span-level\nannotations and written rationales. Expert evaluations show that while LLMs\nachieve high scores on several dimensions, they also exhibit recurring issues,\nincluding unconstructive feedback, overgeneralization, and limited\npersonalization or relevance. Responses were frequently flagged for safety\nrisks, most notably unauthorized medical advice. Follow-up experiments show\nthat LLM judges systematically overrate model responses and overlook safety\nconcerns identified by human experts. To probe failure modes more directly, we\nconstruct CounselBench-Adv, an adversarial dataset of 120 expert-authored\nmental health questions designed to trigger specific model issues. Evaluation\nof 3,240 responses from nine LLMs reveals consistent, model-specific failure\npatterns. Together, CounselBench establishes a clinically grounded framework\nfor benchmarking LLMs in mental health QA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical question answering (QA) benchmarks often focus on multiple-choice or\nfact-based tasks, leaving open-ended answers to real patient questions\nunderexplored. This gap is particularly critical in mental health, where\npatient questions often mix symptoms, treatment concerns, and emotional needs,\nrequiring answers that balance clinical caution with contextual sensitivity. We\npresent CounselBench, a large-scale benchmark developed with 100 mental health\nprofessionals to evaluate and stress-test large language models (LLMs) in\nrealistic help-seeking scenarios. The first component, CounselBench-EVAL,\ncontains 2,000 expert evaluations of answers from GPT-4, LLaMA 3, Gemini, and\nhuman therapists on patient questions from the public forum CounselChat. Each\nanswer is rated across six clinically grounded dimensions, with span-level\nannotations and written rationales. Expert evaluations show that while LLMs\nachieve high scores on several dimensions, they also exhibit recurring issues,\nincluding unconstructive feedback, overgeneralization, and limited\npersonalization or relevance. Responses were frequently flagged for safety\nrisks, most notably unauthorized medical advice. Follow-up experiments show\nthat LLM judges systematically overrate model responses and overlook safety\nconcerns identified by human experts. To probe failure modes more directly, we\nconstruct CounselBench-Adv, an adversarial dataset of 120 expert-authored\nmental health questions designed to trigger specific model issues. Evaluation\nof 3,240 responses from nine LLMs reveals consistent, model-specific failure\npatterns. Together, CounselBench establishes a clinically grounded framework\nfor benchmarking LLMs in mental health QA."
                },
                "authors": [
                    {
                        "name": "Yahan Li"
                    },
                    {
                        "name": "Jifan Yao"
                    },
                    {
                        "name": "John Bosco S. Bunyi"
                    },
                    {
                        "name": "Adam C. Frank"
                    },
                    {
                        "name": "Angel Hwang"
                    },
                    {
                        "name": "Ruishan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ruishan Liu"
                },
                "author": "Ruishan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08584v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08584v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08359v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08359v2",
                "updated": "2025-10-01T16:56:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    16,
                    56,
                    5,
                    2,
                    274,
                    0
                ],
                "published": "2025-06-10T02:16:50Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    2,
                    16,
                    50,
                    1,
                    161,
                    0
                ],
                "title": "REAL: Reading Out Transformer Activations for Precise Localization in\n  Language Model Steering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REAL: Reading Out Transformer Activations for Precise Localization in\n  Language Model Steering"
                },
                "summary": "Inference-time steering aims to alter a large language model's (LLM's)\nresponses without changing its parameters, but a central challenge is\nidentifying the internal modules that most strongly govern the target behavior.\nExisting approaches often rely on simplistic cues or ad hoc heuristics, leading\nto suboptimal or unintended effects. We introduce REAL, a framework for\nidentifying behavior-relevant modules (attention heads or layers) in\nTransformer models. For each module, REAL trains a vector-quantized autoencoder\n(VQ-AE) on its hidden activations and uses a shared, learnable codebook to\npartition the latent space into behavior-relevant and behavior-irrelevant\nsubspaces. REAL quantifies a module's behavioral relevance by how well its\nVQ-AE encodings discriminate behavior-aligned from behavior-violating responses\nvia a binary classification metric; this score guides both module selection and\nsteering strength. We evaluate REAL across eight LLMs from the Llama and Qwen\nfamilies and nine datasets spanning truthfulness enhancement, open-domain QA\nunder knowledge conflicts, and general alignment tasks. REAL enables more\neffective inference-time interventions, achieving an average relative\nimprovement of 20% (up to 81.5%) over the ITI method on truthfulness steering.\nIn addition, the modules selected by REAL exhibit strong zero-shot\ngeneralization in cross-domain truthfulness-steering scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-time steering aims to alter a large language model's (LLM's)\nresponses without changing its parameters, but a central challenge is\nidentifying the internal modules that most strongly govern the target behavior.\nExisting approaches often rely on simplistic cues or ad hoc heuristics, leading\nto suboptimal or unintended effects. We introduce REAL, a framework for\nidentifying behavior-relevant modules (attention heads or layers) in\nTransformer models. For each module, REAL trains a vector-quantized autoencoder\n(VQ-AE) on its hidden activations and uses a shared, learnable codebook to\npartition the latent space into behavior-relevant and behavior-irrelevant\nsubspaces. REAL quantifies a module's behavioral relevance by how well its\nVQ-AE encodings discriminate behavior-aligned from behavior-violating responses\nvia a binary classification metric; this score guides both module selection and\nsteering strength. We evaluate REAL across eight LLMs from the Llama and Qwen\nfamilies and nine datasets spanning truthfulness enhancement, open-domain QA\nunder knowledge conflicts, and general alignment tasks. REAL enables more\neffective inference-time interventions, achieving an average relative\nimprovement of 20% (up to 81.5%) over the ITI method on truthfulness steering.\nIn addition, the modules selected by REAL exhibit strong zero-shot\ngeneralization in cross-domain truthfulness-steering scenarios."
                },
                "authors": [
                    {
                        "name": "Li-Ming Zhan"
                    },
                    {
                        "name": "Bo Liu"
                    },
                    {
                        "name": "Chengqiang Xie"
                    },
                    {
                        "name": "Jiannong Cao"
                    },
                    {
                        "name": "Xiao-Ming Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xiao-Ming Wu"
                },
                "author": "Xiao-Ming Wu",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08359v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08359v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22415v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22415v2",
                "updated": "2025-10-01T16:48:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    16,
                    48,
                    1,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-26T14:39:13Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    39,
                    13,
                    4,
                    269,
                    0
                ],
                "title": "Explaining multimodal LLMs via intra-modal token interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explaining multimodal LLMs via intra-modal token interactions"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success\nacross diverse vision-language tasks, yet their internal decision-making\nmechanisms remain insufficiently understood. Existing interpretability research\nhas primarily focused on cross-modal attribution, identifying which image\nregions the model attends to during output generation. However, these\napproaches often overlook intra-modal dependencies. In the visual modality,\nattributing importance to isolated image patches ignores spatial context due to\nlimited receptive fields, resulting in fragmented and noisy explanations. In\nthe textual modality, reliance on preceding tokens introduces spurious\nactivations. Failing to effectively mitigate these interference compromises\nattribution fidelity. To address these limitations, we propose enhancing\ninterpretability by leveraging intra-modal interaction. For the visual branch,\nwe introduce \\textit{Multi-Scale Explanation Aggregation} (MSEA), which\naggregates attributions over multi-scale inputs to dynamically adjust receptive\nfields, producing more holistic and spatially coherent visual explanations. For\nthe textual branch, we propose \\textit{Activation Ranking Correlation} (ARC),\nwhich measures the relevance of contextual tokens to the current token via\nalignment of their top-$k$ prediction rankings. ARC leverages this relevance to\nsuppress spurious activations from irrelevant contexts while preserving\nsemantically coherent ones. Extensive experiments across state-of-the-art MLLMs\nand benchmark datasets demonstrate that our approach consistently outperforms\nexisting interpretability methods, yielding more faithful and fine-grained\nexplanations of model behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have achieved remarkable success\nacross diverse vision-language tasks, yet their internal decision-making\nmechanisms remain insufficiently understood. Existing interpretability research\nhas primarily focused on cross-modal attribution, identifying which image\nregions the model attends to during output generation. However, these\napproaches often overlook intra-modal dependencies. In the visual modality,\nattributing importance to isolated image patches ignores spatial context due to\nlimited receptive fields, resulting in fragmented and noisy explanations. In\nthe textual modality, reliance on preceding tokens introduces spurious\nactivations. Failing to effectively mitigate these interference compromises\nattribution fidelity. To address these limitations, we propose enhancing\ninterpretability by leveraging intra-modal interaction. For the visual branch,\nwe introduce \\textit{Multi-Scale Explanation Aggregation} (MSEA), which\naggregates attributions over multi-scale inputs to dynamically adjust receptive\nfields, producing more holistic and spatially coherent visual explanations. For\nthe textual branch, we propose \\textit{Activation Ranking Correlation} (ARC),\nwhich measures the relevance of contextual tokens to the current token via\nalignment of their top-$k$ prediction rankings. ARC leverages this relevance to\nsuppress spurious activations from irrelevant contexts while preserving\nsemantically coherent ones. Extensive experiments across state-of-the-art MLLMs\nand benchmark datasets demonstrate that our approach consistently outperforms\nexisting interpretability methods, yielding more faithful and fine-grained\nexplanations of model behavior."
                },
                "authors": [
                    {
                        "name": "Jiawei Liang"
                    },
                    {
                        "name": "Ruoyu Chen"
                    },
                    {
                        "name": "Xianghao Jiao"
                    },
                    {
                        "name": "Siyuan Liang"
                    },
                    {
                        "name": "Shiming Liu"
                    },
                    {
                        "name": "Qunli Zhang"
                    },
                    {
                        "name": "Zheng Hu"
                    },
                    {
                        "name": "Xiaochun Cao"
                    }
                ],
                "author_detail": {
                    "name": "Xiaochun Cao"
                },
                "author": "Xiaochun Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22415v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22415v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.04671v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.04671v4",
                "updated": "2025-10-01T16:40:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    16,
                    40,
                    22,
                    2,
                    274,
                    0
                ],
                "published": "2024-04-06T16:16:30Z",
                "published_parsed": [
                    2024,
                    4,
                    6,
                    16,
                    16,
                    30,
                    5,
                    97,
                    0
                ],
                "title": "PhyloLM : Inferring the Phylogeny of Large Language Models and\n  Predicting their Performances in Benchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhyloLM : Inferring the Phylogeny of Large Language Models and\n  Predicting their Performances in Benchmarks"
                },
                "summary": "This paper introduces PhyloLM, a method adapting phylogenetic algorithms to\nLarge Language Models (LLMs) to explore whether and how they relate to each\nother and to predict their performance characteristics. Our method calculates a\nphylogenetic distance metric based on the similarity of LLMs' output. The\nresulting metric is then used to construct dendrograms, which satisfactorily\ncapture known relationships across a set of 111 open-source and 45 closed\nmodels. Furthermore, our phylogenetic distance predicts performance in standard\nbenchmarks, thus demonstrating its functional validity and paving the way for a\ntime and cost-effective estimation of LLM capabilities. To sum up, by\ntranslating population genetic concepts to machine learning, we propose and\nvalidate a tool to evaluate LLM development, relationships and capabilities,\neven in the absence of transparent training information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces PhyloLM, a method adapting phylogenetic algorithms to\nLarge Language Models (LLMs) to explore whether and how they relate to each\nother and to predict their performance characteristics. Our method calculates a\nphylogenetic distance metric based on the similarity of LLMs' output. The\nresulting metric is then used to construct dendrograms, which satisfactorily\ncapture known relationships across a set of 111 open-source and 45 closed\nmodels. Furthermore, our phylogenetic distance predicts performance in standard\nbenchmarks, thus demonstrating its functional validity and paving the way for a\ntime and cost-effective estimation of LLM capabilities. To sum up, by\ntranslating population genetic concepts to machine learning, we propose and\nvalidate a tool to evaluate LLM development, relationships and capabilities,\neven in the absence of transparent training information."
                },
                "authors": [
                    {
                        "name": "Nicolas Yax"
                    },
                    {
                        "name": "Pierre-Yves Oudeyer"
                    },
                    {
                        "name": "Stefano Palminteri"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Palminteri"
                },
                "author": "Stefano Palminteri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.04671v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.04671v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14285v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14285v2",
                "updated": "2025-10-01T16:39:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    16,
                    39,
                    48,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-16T19:11:28Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    19,
                    11,
                    28,
                    1,
                    259,
                    0
                ],
                "title": "A Multi-Agent LLM Defense Pipeline Against Prompt Injection Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multi-Agent LLM Defense Pipeline Against Prompt Injection Attacks"
                },
                "summary": "Prompt injection attacks represent a major vulnerability in Large Language\nModel (LLM) deployments, where malicious instructions embedded in user inputs\ncan override system prompts and induce unintended behaviors. This paper\npresents a novel multi-agent defense framework that employs specialized LLM\nagents in coordinated pipelines to detect and neutralize prompt injection\nattacks in real-time. We evaluate our approach using two distinct\narchitectures: a sequential chain-of-agents pipeline and a hierarchical\ncoordinator-based system. Our comprehensive evaluation on 55 unique prompt\ninjection attacks, grouped into 8 categories and totaling 400 attack instances\nacross two LLM platforms (ChatGLM and Llama2), demonstrates significant\nsecurity improvements. Without defense mechanisms, baseline Attack Success\nRates (ASR) reached 30% for ChatGLM and 20% for Llama2. Our multi-agent\npipeline achieved 100% mitigation, reducing ASR to 0% across all tested\nscenarios. The framework demonstrates robustness across multiple attack\ncategories including direct overrides, code execution attempts, data\nexfiltration, and obfuscation techniques, while maintaining system\nfunctionality for legitimate queries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt injection attacks represent a major vulnerability in Large Language\nModel (LLM) deployments, where malicious instructions embedded in user inputs\ncan override system prompts and induce unintended behaviors. This paper\npresents a novel multi-agent defense framework that employs specialized LLM\nagents in coordinated pipelines to detect and neutralize prompt injection\nattacks in real-time. We evaluate our approach using two distinct\narchitectures: a sequential chain-of-agents pipeline and a hierarchical\ncoordinator-based system. Our comprehensive evaluation on 55 unique prompt\ninjection attacks, grouped into 8 categories and totaling 400 attack instances\nacross two LLM platforms (ChatGLM and Llama2), demonstrates significant\nsecurity improvements. Without defense mechanisms, baseline Attack Success\nRates (ASR) reached 30% for ChatGLM and 20% for Llama2. Our multi-agent\npipeline achieved 100% mitigation, reducing ASR to 0% across all tested\nscenarios. The framework demonstrates robustness across multiple attack\ncategories including direct overrides, code execution attempts, data\nexfiltration, and obfuscation techniques, while maintaining system\nfunctionality for legitimate queries."
                },
                "authors": [
                    {
                        "name": "S M Asif Hossain"
                    },
                    {
                        "name": "Ruksat Khan Shayoni"
                    },
                    {
                        "name": "Mohd Ruhul Ameen"
                    },
                    {
                        "name": "Akif Islam"
                    },
                    {
                        "name": "M. F. Mridha"
                    },
                    {
                        "name": "Jungpil Shin"
                    }
                ],
                "author_detail": {
                    "name": "Jungpil Shin"
                },
                "author": "Jungpil Shin",
                "arxiv_comment": "IEEE Conference standard paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14285v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14285v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12950v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12950v2",
                "updated": "2025-10-01T16:25:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    16,
                    25,
                    22,
                    2,
                    274,
                    0
                ],
                "published": "2025-05-19T10:42:36Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    10,
                    42,
                    36,
                    0,
                    139,
                    0
                ],
                "title": "GuRE:Generative Query REwriter for Legal Passage Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GuRE:Generative Query REwriter for Legal Passage Retrieval"
                },
                "summary": "Legal Passage Retrieval (LPR) systems are crucial as they help practitioners\nsave time when drafting legal arguments. However, it remains an underexplored\navenue. One primary reason is the significant vocabulary mismatch between the\nquery and the target passage. To address this, we propose a simple yet\neffective method, the Generative query REwriter (GuRE). We leverage the\ngenerative capabilities of Large Language Models (LLMs) by training the LLM for\nquery rewriting. \"Rewritten queries\" help retrievers to retrieve target\npassages by mitigating vocabulary mismatch. Experimental results show that GuRE\nsignificantly improves performance in a retriever-agnostic manner,\noutperforming all baseline methods. Further analysis reveals that different\ntraining objectives lead to distinct retrieval behaviors, making GuRE more\nsuitable than direct retriever fine-tuning for real-world applications. Codes\nare avaiable at github.com/daehuikim/GuRE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Legal Passage Retrieval (LPR) systems are crucial as they help practitioners\nsave time when drafting legal arguments. However, it remains an underexplored\navenue. One primary reason is the significant vocabulary mismatch between the\nquery and the target passage. To address this, we propose a simple yet\neffective method, the Generative query REwriter (GuRE). We leverage the\ngenerative capabilities of Large Language Models (LLMs) by training the LLM for\nquery rewriting. \"Rewritten queries\" help retrievers to retrieve target\npassages by mitigating vocabulary mismatch. Experimental results show that GuRE\nsignificantly improves performance in a retriever-agnostic manner,\noutperforming all baseline methods. Further analysis reveals that different\ntraining objectives lead to distinct retrieval behaviors, making GuRE more\nsuitable than direct retriever fine-tuning for real-world applications. Codes\nare avaiable at github.com/daehuikim/GuRE."
                },
                "authors": [
                    {
                        "name": "Daehee Kim"
                    },
                    {
                        "name": "Deokhyung Kang"
                    },
                    {
                        "name": "Jonghwi Kim"
                    },
                    {
                        "name": "Sangwon Ryu"
                    },
                    {
                        "name": "Gary Geunbae Lee"
                    }
                ],
                "author_detail": {
                    "name": "Gary Geunbae Lee"
                },
                "author": "Gary Geunbae Lee",
                "arxiv_comment": "NLLP Workshop at EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12950v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12950v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09167v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09167v2",
                "updated": "2025-10-01T16:23:43Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    16,
                    23,
                    43,
                    2,
                    274,
                    0
                ],
                "published": "2025-06-10T18:33:39Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    18,
                    33,
                    39,
                    1,
                    161,
                    0
                ],
                "title": "Estimating Visceral Adiposity from Wrist-Worn Accelerometry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating Visceral Adiposity from Wrist-Worn Accelerometry"
                },
                "summary": "Visceral adipose tissue (VAT) is a key marker of both metabolic health and\nhabitual physical activity (PA). Excess VAT is highly correlated with type 2\ndiabetes and insulin resistance. The mechanistic basis for this pathophysiology\nrelates to overloading the liver with fatty acids. VAT is also a highly labile\nfat depot, with increased turnover stimulated by catecholamines during\nexercise. VAT can be measured with sophisticated imaging technologies, but can\nalso be inferred directly from PA. We tested this relationship using National\nHealth and Nutrition Examination Survey (NHANES) data from 2011-2014, for\nindividuals aged 20-60 years with 7 days of accelerometry data (n=2,456 men;\n2,427 women) [1]. Two approaches were used for estimating VAT from activity.\nThe first used engineered features based on movements during gait and sleep,\nand then ridge regression to map summary statistics of these features into a\nVAT estimate. The second approach used deep neural networks trained on 24 hours\nof continuous accelerometry. A foundation model first mapped each 10s frame\ninto a high-dimensional feature vector. A transformer model then mapped each\nday's feature vector time series into a VAT estimate, which were averaged over\nmultiple days. For both approaches, the most accurate estimates were obtained\nwith the addition of covariate information about subject demographics and body\nmeasurements. The best performance was obtained by combining the two\napproaches, resulting in VAT estimates with correlations of r=0.86. These\nfindings demonstrate a strong relationship between PA and VAT and, by\nextension, between PA and metabolic health risks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visceral adipose tissue (VAT) is a key marker of both metabolic health and\nhabitual physical activity (PA). Excess VAT is highly correlated with type 2\ndiabetes and insulin resistance. The mechanistic basis for this pathophysiology\nrelates to overloading the liver with fatty acids. VAT is also a highly labile\nfat depot, with increased turnover stimulated by catecholamines during\nexercise. VAT can be measured with sophisticated imaging technologies, but can\nalso be inferred directly from PA. We tested this relationship using National\nHealth and Nutrition Examination Survey (NHANES) data from 2011-2014, for\nindividuals aged 20-60 years with 7 days of accelerometry data (n=2,456 men;\n2,427 women) [1]. Two approaches were used for estimating VAT from activity.\nThe first used engineered features based on movements during gait and sleep,\nand then ridge regression to map summary statistics of these features into a\nVAT estimate. The second approach used deep neural networks trained on 24 hours\nof continuous accelerometry. A foundation model first mapped each 10s frame\ninto a high-dimensional feature vector. A transformer model then mapped each\nday's feature vector time series into a VAT estimate, which were averaged over\nmultiple days. For both approaches, the most accurate estimates were obtained\nwith the addition of covariate information about subject demographics and body\nmeasurements. The best performance was obtained by combining the two\napproaches, resulting in VAT estimates with correlations of r=0.86. These\nfindings demonstrate a strong relationship between PA and VAT and, by\nextension, between PA and metabolic health risks."
                },
                "authors": [
                    {
                        "name": "James R. Williamson"
                    },
                    {
                        "name": "Andrew Alini"
                    },
                    {
                        "name": "Brian A. Telfer"
                    },
                    {
                        "name": "Adam W. Potter"
                    },
                    {
                        "name": "Karl E. Friedl"
                    }
                ],
                "author_detail": {
                    "name": "Karl E. Friedl"
                },
                "author": "Karl E. Friedl",
                "arxiv_doi": "10.1109/JBHI.2025.3614093",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/JBHI.2025.3614093",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.09167v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09167v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This article has been accepted for publication in IEEE Journal of\n  Biomedical and Health Informatics",
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18137v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18137v7",
                "updated": "2025-10-01T16:15:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    16,
                    15,
                    31,
                    2,
                    274,
                    0
                ],
                "published": "2025-02-25T12:02:17Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    12,
                    2,
                    17,
                    1,
                    56,
                    0
                ],
                "title": "SpargeAttention: Accurate and Training-free Sparse Attention\n  Accelerating Any Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpargeAttention: Accurate and Training-free Sparse Attention\n  Accelerating Any Model Inference"
                },
                "summary": "An efficient attention implementation is essential for large models due to\nits quadratic time complexity. Fortunately, attention commonly exhibits\nsparsity, i.e., many values in the attention map are near zero, allowing for\nthe omission of corresponding computations. Many studies have utilized the\nsparse pattern to accelerate attention. However, most existing works focus on\noptimizing attention within specific models by exploiting certain sparse\npatterns of the attention map. A universal sparse attention that guarantees\nboth the speedup and end-to-end performance of diverse models remains elusive.\nIn this paper, we propose SpargeAttn, a universal sparse and quantized\nattention for any model. Our method uses a two-stage online filter: in the\nfirst stage, we rapidly and accurately predict the attention map, enabling the\nskip of some matrix multiplications in attention. In the second stage, we\ndesign an online softmax-aware filter that incurs no extra overhead and further\nskips some matrix multiplications. Experiments show that our method\nsignificantly accelerates diverse models, including language, image, and video\ngeneration, without sacrificing end-to-end metrics. The codes are available at\nhttps://github.com/thu-ml/SpargeAttn.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An efficient attention implementation is essential for large models due to\nits quadratic time complexity. Fortunately, attention commonly exhibits\nsparsity, i.e., many values in the attention map are near zero, allowing for\nthe omission of corresponding computations. Many studies have utilized the\nsparse pattern to accelerate attention. However, most existing works focus on\noptimizing attention within specific models by exploiting certain sparse\npatterns of the attention map. A universal sparse attention that guarantees\nboth the speedup and end-to-end performance of diverse models remains elusive.\nIn this paper, we propose SpargeAttn, a universal sparse and quantized\nattention for any model. Our method uses a two-stage online filter: in the\nfirst stage, we rapidly and accurately predict the attention map, enabling the\nskip of some matrix multiplications in attention. In the second stage, we\ndesign an online softmax-aware filter that incurs no extra overhead and further\nskips some matrix multiplications. Experiments show that our method\nsignificantly accelerates diverse models, including language, image, and video\ngeneration, without sacrificing end-to-end metrics. The codes are available at\nhttps://github.com/thu-ml/SpargeAttn."
                },
                "authors": [
                    {
                        "name": "Jintao Zhang"
                    },
                    {
                        "name": "Chendong Xiang"
                    },
                    {
                        "name": "Haofeng Huang"
                    },
                    {
                        "name": "Jia Wei"
                    },
                    {
                        "name": "Haocheng Xi"
                    },
                    {
                        "name": "Jun Zhu"
                    },
                    {
                        "name": "Jianfei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jianfei Chen"
                },
                "author": "Jianfei Chen",
                "arxiv_comment": "@inproceedings{zhang2025spargeattn, title={Spargeattn: Accurate\n  sparse attention accelerating any model inference}, author={Zhang, Jintao and\n  Xiang, Chendong and Huang, Haofeng and Wei, Jia and Xi, Haocheng and Zhu, Jun\n  and Chen, Jianfei}, booktitle={International Conference on Machine Learning\n  (ICML)}, year={2025} }",
                "arxiv_journal_ref": "Proceedings of the 42 nd International Conference on Machine\n  Learning, PMLR 267, 2025 (ICML 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18137v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18137v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16691v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16691v3",
                "updated": "2025-10-01T16:13:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    16,
                    13,
                    35,
                    2,
                    274,
                    0
                ],
                "published": "2025-07-22T15:25:19Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    15,
                    25,
                    19,
                    1,
                    203,
                    0
                ],
                "title": "On Causal Inference for the Survivor Function",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Causal Inference for the Survivor Function"
                },
                "summary": "In this expository paper, we consider the problem of causal inference and\nefficient estimation for the counterfactual survivor function. This problem has\npreviously been considered in the literature in several papers, each relying on\nthe imposition of conditions meant to identify the desired estimand from the\nobserved data. These conditions, generally referred to as either implying or\nsatisfying coarsening at random, are inconsistently imposed across this\nliterature and, in all cases, fail to imply coarsening at random. We establish\nthe first general characterization of coarsening at random, and also sequential\ncoarsening at random, for this estimation problem. Other contributions include\nthe first general characterization of the set of all influence functions for\nthe counterfactual survival probability under sequential coarsening at random,\nand the corresponding nonparametric efficient influence function. These\ncharacterizations are general in that neither impose continuity assumptions on\neither the underlying failure or censoring time distributions. We further show\nhow the latter compares to alternative forms recently derived in the\nliterature, including establishing the pointwise equivalence of the influence\nfunctions for our nonparametric efficient estimator and that recently given in\nWestling et al (2024, Journal of the American Statistical Association).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this expository paper, we consider the problem of causal inference and\nefficient estimation for the counterfactual survivor function. This problem has\npreviously been considered in the literature in several papers, each relying on\nthe imposition of conditions meant to identify the desired estimand from the\nobserved data. These conditions, generally referred to as either implying or\nsatisfying coarsening at random, are inconsistently imposed across this\nliterature and, in all cases, fail to imply coarsening at random. We establish\nthe first general characterization of coarsening at random, and also sequential\ncoarsening at random, for this estimation problem. Other contributions include\nthe first general characterization of the set of all influence functions for\nthe counterfactual survival probability under sequential coarsening at random,\nand the corresponding nonparametric efficient influence function. These\ncharacterizations are general in that neither impose continuity assumptions on\neither the underlying failure or censoring time distributions. We further show\nhow the latter compares to alternative forms recently derived in the\nliterature, including establishing the pointwise equivalence of the influence\nfunctions for our nonparametric efficient estimator and that recently given in\nWestling et al (2024, Journal of the American Statistical Association)."
                },
                "authors": [
                    {
                        "name": "Benjamin R. Baer"
                    },
                    {
                        "name": "Ashkan Ertefaie"
                    },
                    {
                        "name": "Robert L. Strawderman"
                    }
                ],
                "author_detail": {
                    "name": "Robert L. Strawderman"
                },
                "author": "Robert L. Strawderman",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2306.16571",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16691v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16691v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26153v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26153v2",
                "updated": "2025-10-01T16:11:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    16,
                    11,
                    30,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-30T12:03:32Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    12,
                    3,
                    32,
                    1,
                    273,
                    0
                ],
                "title": "Beyond the Algorithm: A Field Guide to Deploying AI Agents in Clinical\n  Practice",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond the Algorithm: A Field Guide to Deploying AI Agents in Clinical\n  Practice"
                },
                "summary": "Large language models (LLMs) integrated into agent-driven workflows hold\nimmense promise for healthcare, yet a significant gap exists between their\npotential and practical implementation within clinical settings. To address\nthis, we present a practitioner-oriented field manual for deploying generative\nagents that use electronic health record (EHR) data. This guide is informed by\nour experience deploying the \"irAE-Agent\", an automated system to detect\nimmune-related adverse events from clinical notes at Mass General Brigham, and\nby structured interviews with 20 clinicians, engineers, and informatics leaders\ninvolved in the project. Our analysis reveals a critical misalignment in\nclinical AI development: less than 20% of our effort was dedicated to prompt\nengineering and model development, while over 80% was consumed by the\nsociotechnical work of implementation. We distill this effort into five \"heavy\nlifts\": data integration, model validation, ensuring economic value, managing\nsystem drift, and governance. By providing actionable solutions for each of\nthese challenges, this field manual shifts the focus from algorithmic\ndevelopment to the essential infrastructure and implementation work required to\nbridge the \"valley of death\" and successfully translate generative AI from\npilot projects into routine clinical care.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) integrated into agent-driven workflows hold\nimmense promise for healthcare, yet a significant gap exists between their\npotential and practical implementation within clinical settings. To address\nthis, we present a practitioner-oriented field manual for deploying generative\nagents that use electronic health record (EHR) data. This guide is informed by\nour experience deploying the \"irAE-Agent\", an automated system to detect\nimmune-related adverse events from clinical notes at Mass General Brigham, and\nby structured interviews with 20 clinicians, engineers, and informatics leaders\ninvolved in the project. Our analysis reveals a critical misalignment in\nclinical AI development: less than 20% of our effort was dedicated to prompt\nengineering and model development, while over 80% was consumed by the\nsociotechnical work of implementation. We distill this effort into five \"heavy\nlifts\": data integration, model validation, ensuring economic value, managing\nsystem drift, and governance. By providing actionable solutions for each of\nthese challenges, this field manual shifts the focus from algorithmic\ndevelopment to the essential infrastructure and implementation work required to\nbridge the \"valley of death\" and successfully translate generative AI from\npilot projects into routine clinical care."
                },
                "authors": [
                    {
                        "name": "Jack Gallifant"
                    },
                    {
                        "name": "Katherine C. Kellogg"
                    },
                    {
                        "name": "Matt Butler"
                    },
                    {
                        "name": "Amanda Centi"
                    },
                    {
                        "name": "Shan Chen"
                    },
                    {
                        "name": "Patrick F. Doyle"
                    },
                    {
                        "name": "Sayon Dutta"
                    },
                    {
                        "name": "Joyce Guo"
                    },
                    {
                        "name": "Matthew J. Hadfield"
                    },
                    {
                        "name": "Esther H. Kim"
                    },
                    {
                        "name": "David E. Kozono"
                    },
                    {
                        "name": "Hugo JWL Aerts"
                    },
                    {
                        "name": "Adam B. Landman"
                    },
                    {
                        "name": "Raymond H. Mak"
                    },
                    {
                        "name": "Rebecca G. Mishuris"
                    },
                    {
                        "name": "Tanna L. Nelson"
                    },
                    {
                        "name": "Guergana K. Savova"
                    },
                    {
                        "name": "Elad Sharon"
                    },
                    {
                        "name": "Benjamin C. Silverman"
                    },
                    {
                        "name": "Umit Topaloglu"
                    },
                    {
                        "name": "Jeremy L. Warner"
                    },
                    {
                        "name": "Danielle S. Bitterman"
                    }
                ],
                "author_detail": {
                    "name": "Danielle S. Bitterman"
                },
                "author": "Danielle S. Bitterman",
                "arxiv_comment": "Under review. 5 Tables, 2 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26153v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26153v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02367v9",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02367v9",
                "updated": "2025-10-01T16:09:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    16,
                    9,
                    5,
                    2,
                    274,
                    0
                ],
                "published": "2024-10-03T10:25:23Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    10,
                    25,
                    23,
                    3,
                    277,
                    0
                ],
                "title": "SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference\n  Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference\n  Acceleration"
                },
                "summary": "The transformer architecture predominates across various models. As the heart\nof the transformer, attention has a computational complexity of $O(N^2)$,\ncompared to $O(N)$ for linear transformations. When handling large sequence\nlengths, attention becomes the primary time-consuming component. Although\nquantization has proven to be an effective method for accelerating model\ninference, existing quantization methods primarily focus on optimizing the\nlinear layer. In response, we first analyze the feasibility of quantization in\nattention detailedly. Following that, we propose SageAttention, a highly\nefficient and accurate quantization method for attention. The OPS (operations\nper second) of our approach outperforms FlashAttention2 and xformers by about\n2.1 times and 2.7 times, respectively. SageAttention also achieves superior\naccuracy performance over FlashAttention3. Comprehensive experiments confirm\nthat our approach incurs almost no end-to-end metrics loss across diverse\nmodels, including those for large language processing, image generation, and\nvideo generation. The codes are available at\nhttps://github.com/thu-ml/SageAttention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transformer architecture predominates across various models. As the heart\nof the transformer, attention has a computational complexity of $O(N^2)$,\ncompared to $O(N)$ for linear transformations. When handling large sequence\nlengths, attention becomes the primary time-consuming component. Although\nquantization has proven to be an effective method for accelerating model\ninference, existing quantization methods primarily focus on optimizing the\nlinear layer. In response, we first analyze the feasibility of quantization in\nattention detailedly. Following that, we propose SageAttention, a highly\nefficient and accurate quantization method for attention. The OPS (operations\nper second) of our approach outperforms FlashAttention2 and xformers by about\n2.1 times and 2.7 times, respectively. SageAttention also achieves superior\naccuracy performance over FlashAttention3. Comprehensive experiments confirm\nthat our approach incurs almost no end-to-end metrics loss across diverse\nmodels, including those for large language processing, image generation, and\nvideo generation. The codes are available at\nhttps://github.com/thu-ml/SageAttention."
                },
                "authors": [
                    {
                        "name": "Jintao Zhang"
                    },
                    {
                        "name": "Jia Wei"
                    },
                    {
                        "name": "Haofeng Huang"
                    },
                    {
                        "name": "Pengle Zhang"
                    },
                    {
                        "name": "Jun Zhu"
                    },
                    {
                        "name": "Jianfei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jianfei Chen"
                },
                "author": "Jianfei Chen",
                "arxiv_comment": "@inproceedings{zhang2025sageattention, title={SageAttention: Accurate\n  8-Bit Attention for Plug-and-play Inference Acceleration}, author={Zhang,\n  Jintao and Wei, Jia and Zhang, Pengle and Zhu, Jun and Chen, Jianfei},\n  booktitle={International Conference on Learning Representations (ICLR)},\n  year={2025} }",
                "arxiv_journal_ref": "The Thirteenth International Conference on Learning\n  Representations (ICLR 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02367v9",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02367v9",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14725v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14725v3",
                "updated": "2025-10-01T16:07:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    16,
                    7,
                    15,
                    2,
                    274,
                    0
                ],
                "published": "2025-07-19T19:15:03Z",
                "published_parsed": [
                    2025,
                    7,
                    19,
                    19,
                    15,
                    3,
                    5,
                    200,
                    0
                ],
                "title": "GRID: Scalable Task-Agnostic Prompt-Based Continual Learning for\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GRID: Scalable Task-Agnostic Prompt-Based Continual Learning for\n  Language Models"
                },
                "summary": "Prompt-based continual learning (CL) provides a parameter-efficient approach\nfor adapting large language models (LLMs) across task sequences. However, most\nexisting methods rely on task-aware inference and maintain a growing set of\ntask-specific prompts, which introduces two major challenges: (1) severe\nperformance degradation on earlier tasks under task-agnostic inference, and (2)\nlimited scalability due to prompt memory accumulation as task sequences grow.\nIn this paper, we present GRID, a unified framework designed to address these\nchallenges. GRID incorporates a decoding mechanism that enhances backward\ntransfer by leveraging representative inputs, automatic task identification,\nand constrained decoding. Furthermore, it employs a gradient-guided prompt\nselection strategy to compress less informative prompts into a single\naggregated representation, ensuring scalable and memory-efficient continual\nlearning. Extensive experiments on long-sequence and negative transfer\nbenchmarks show that GRID improves average accuracy and backward transfer,\nachieves competitive forward transfer, and substantially reduces prompt memory\nusage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt-based continual learning (CL) provides a parameter-efficient approach\nfor adapting large language models (LLMs) across task sequences. However, most\nexisting methods rely on task-aware inference and maintain a growing set of\ntask-specific prompts, which introduces two major challenges: (1) severe\nperformance degradation on earlier tasks under task-agnostic inference, and (2)\nlimited scalability due to prompt memory accumulation as task sequences grow.\nIn this paper, we present GRID, a unified framework designed to address these\nchallenges. GRID incorporates a decoding mechanism that enhances backward\ntransfer by leveraging representative inputs, automatic task identification,\nand constrained decoding. Furthermore, it employs a gradient-guided prompt\nselection strategy to compress less informative prompts into a single\naggregated representation, ensuring scalable and memory-efficient continual\nlearning. Extensive experiments on long-sequence and negative transfer\nbenchmarks show that GRID improves average accuracy and backward transfer,\nachieves competitive forward transfer, and substantially reduces prompt memory\nusage."
                },
                "authors": [
                    {
                        "name": "Anushka Tiwari"
                    },
                    {
                        "name": "Sayantan Pal"
                    },
                    {
                        "name": "Rohini K. Srihari"
                    },
                    {
                        "name": "Kaiyi Ji"
                    }
                ],
                "author_detail": {
                    "name": "Kaiyi Ji"
                },
                "author": "Kaiyi Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14725v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14725v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15957v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15957v3",
                "updated": "2025-10-01T16:02:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    16,
                    2,
                    37,
                    2,
                    274,
                    0
                ],
                "published": "2025-05-21T19:17:29Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    19,
                    17,
                    29,
                    2,
                    141,
                    0
                ],
                "title": "Towards Holistic Evaluation of Large Audio-Language Models: A\n  Comprehensive Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Holistic Evaluation of Large Audio-Language Models: A\n  Comprehensive Survey"
                },
                "summary": "With advancements in large audio-language models (LALMs), which enhance large\nlanguage models (LLMs) with auditory capabilities, these models are expected to\ndemonstrate universal proficiency across various auditory tasks. While numerous\nbenchmarks have emerged to assess LALMs' performance, they remain fragmented\nand lack a structured taxonomy. To bridge this gap, we conduct a comprehensive\nsurvey and propose a systematic taxonomy for LALM evaluations, categorizing\nthem into four dimensions based on their objectives: (1) General Auditory\nAwareness and Processing, (2) Knowledge and Reasoning, (3) Dialogue-oriented\nAbility, and (4) Fairness, Safety, and Trustworthiness. We provide detailed\noverviews within each category and highlight challenges in this field, offering\ninsights into promising future directions. To the best of our knowledge, this\nis the first survey specifically focused on the evaluations of LALMs, providing\nclear guidelines for the community. We will release the collection of the\nsurveyed papers and actively maintain it to support ongoing advancements in the\nfield.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With advancements in large audio-language models (LALMs), which enhance large\nlanguage models (LLMs) with auditory capabilities, these models are expected to\ndemonstrate universal proficiency across various auditory tasks. While numerous\nbenchmarks have emerged to assess LALMs' performance, they remain fragmented\nand lack a structured taxonomy. To bridge this gap, we conduct a comprehensive\nsurvey and propose a systematic taxonomy for LALM evaluations, categorizing\nthem into four dimensions based on their objectives: (1) General Auditory\nAwareness and Processing, (2) Knowledge and Reasoning, (3) Dialogue-oriented\nAbility, and (4) Fairness, Safety, and Trustworthiness. We provide detailed\noverviews within each category and highlight challenges in this field, offering\ninsights into promising future directions. To the best of our knowledge, this\nis the first survey specifically focused on the evaluations of LALMs, providing\nclear guidelines for the community. We will release the collection of the\nsurveyed papers and actively maintain it to support ongoing advancements in the\nfield."
                },
                "authors": [
                    {
                        "name": "Chih-Kai Yang"
                    },
                    {
                        "name": "Neo S. Ho"
                    },
                    {
                        "name": "Hung-yi Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hung-yi Lee"
                },
                "author": "Hung-yi Lee",
                "arxiv_comment": "EMNLP 2025 (Main). Project Website:\n  https://github.com/ckyang1124/LALM-Evaluation-Survey",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15957v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15957v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23571v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23571v2",
                "updated": "2025-10-01T16:01:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    16,
                    1,
                    24,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-28T02:08:17Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    2,
                    8,
                    17,
                    6,
                    271,
                    0
                ],
                "title": "Benchmarking LLM-Assisted Blue Teaming via Standardized Threat Hunting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking LLM-Assisted Blue Teaming via Standardized Threat Hunting"
                },
                "summary": "As cyber threats continue to grow in scale and sophistication, blue team\ndefenders increasingly require advanced tools to proactively detect and\nmitigate risks. Large Language Models (LLMs) offer promising capabilities for\nenhancing threat analysis. However, their effectiveness in real-world blue team\nthreat-hunting scenarios remains insufficiently explored. This paper presents\nCyberTeam, a benchmark designed to guide LLMs in blue teaming practice.\nCyberTeam constructs a standardized workflow in two stages. First, it models\nrealistic threat-hunting workflows by capturing the dependencies among\nanalytical tasks from threat attribution to incident response. Next, each task\nis addressed through a set of operational modules tailored to its specific\nanalytical requirements. This transforms threat hunting into a structured\nsequence of reasoning steps, with each step grounded in a discrete operation\nand ordered according to task-specific dependencies. Guided by this framework,\nLLMs are directed to perform threat-hunting tasks through modularized steps.\nOverall, CyberTeam integrates 30 tasks and 9 operational modules to guide LLMs\nthrough standardized threat analysis. We evaluate both leading LLMs and\nstate-of-the-art cybersecurity agents, comparing CyberTeam against open-ended\nreasoning strategies. Our results highlight the improvements enabled by\nstandardized design, while also revealing the limitations of open-ended\nreasoning in real-world threat hunting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As cyber threats continue to grow in scale and sophistication, blue team\ndefenders increasingly require advanced tools to proactively detect and\nmitigate risks. Large Language Models (LLMs) offer promising capabilities for\nenhancing threat analysis. However, their effectiveness in real-world blue team\nthreat-hunting scenarios remains insufficiently explored. This paper presents\nCyberTeam, a benchmark designed to guide LLMs in blue teaming practice.\nCyberTeam constructs a standardized workflow in two stages. First, it models\nrealistic threat-hunting workflows by capturing the dependencies among\nanalytical tasks from threat attribution to incident response. Next, each task\nis addressed through a set of operational modules tailored to its specific\nanalytical requirements. This transforms threat hunting into a structured\nsequence of reasoning steps, with each step grounded in a discrete operation\nand ordered according to task-specific dependencies. Guided by this framework,\nLLMs are directed to perform threat-hunting tasks through modularized steps.\nOverall, CyberTeam integrates 30 tasks and 9 operational modules to guide LLMs\nthrough standardized threat analysis. We evaluate both leading LLMs and\nstate-of-the-art cybersecurity agents, comparing CyberTeam against open-ended\nreasoning strategies. Our results highlight the improvements enabled by\nstandardized design, while also revealing the limitations of open-ended\nreasoning in real-world threat hunting."
                },
                "authors": [
                    {
                        "name": "Yuqiao Meng"
                    },
                    {
                        "name": "Luoxi Tang"
                    },
                    {
                        "name": "Feiyang Yu"
                    },
                    {
                        "name": "Xi Li"
                    },
                    {
                        "name": "Guanhua Yan"
                    },
                    {
                        "name": "Ping Yang"
                    },
                    {
                        "name": "Zhaohan Xi"
                    }
                ],
                "author_detail": {
                    "name": "Zhaohan Xi"
                },
                "author": "Zhaohan Xi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23571v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23571v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13818v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13818v3",
                "updated": "2025-10-01T15:58:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    15,
                    58,
                    47,
                    2,
                    274,
                    0
                ],
                "published": "2025-04-18T17:49:55Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    17,
                    49,
                    55,
                    4,
                    108,
                    0
                ],
                "title": "Not All Rollouts are Useful: Down-Sampling Rollouts in LLM Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Rollouts are Useful: Down-Sampling Rollouts in LLM Reinforcement\n  Learning"
                },
                "summary": "Reinforcement learning with verifiable rewards (RLVR) has emerged as the\nleading approach for enhancing reasoning capabilities in large language models.\nHowever, it faces a fundamental compute and memory asymmetry: rollout\ngeneration is embarrassingly parallel and memory-light, whereas policy updates\nare communication-heavy and memory-intensive. To address this, we introduce\nPODS (Policy Optimization with Down-Sampling), which decouples rollout\ngeneration from policy updates by training only on a strategically selected\nsubset of rollouts, maintaining learning quality while dramatically reducing\nupdate costs. We propose a principled subset selection criterion, max-variance\ndown-sampling, that maximizes reward diversity, and provide an efficient\n$O(n\\log n)$ implementation. Empirically, Group Relative Policy Optimization\n(GRPO) with PODS achieves the peak test accuracy of vanilla GRPO at least\n$\\mathbf{1.7\\times}$ faster across the different reasoning benchmarks and\nhardware configurations we tested.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning with verifiable rewards (RLVR) has emerged as the\nleading approach for enhancing reasoning capabilities in large language models.\nHowever, it faces a fundamental compute and memory asymmetry: rollout\ngeneration is embarrassingly parallel and memory-light, whereas policy updates\nare communication-heavy and memory-intensive. To address this, we introduce\nPODS (Policy Optimization with Down-Sampling), which decouples rollout\ngeneration from policy updates by training only on a strategically selected\nsubset of rollouts, maintaining learning quality while dramatically reducing\nupdate costs. We propose a principled subset selection criterion, max-variance\ndown-sampling, that maximizes reward diversity, and provide an efficient\n$O(n\\log n)$ implementation. Empirically, Group Relative Policy Optimization\n(GRPO) with PODS achieves the peak test accuracy of vanilla GRPO at least\n$\\mathbf{1.7\\times}$ faster across the different reasoning benchmarks and\nhardware configurations we tested."
                },
                "authors": [
                    {
                        "name": "Yixuan Even Xu"
                    },
                    {
                        "name": "Yash Savani"
                    },
                    {
                        "name": "Fei Fang"
                    },
                    {
                        "name": "J. Zico Kolter"
                    }
                ],
                "author_detail": {
                    "name": "J. Zico Kolter"
                },
                "author": "J. Zico Kolter",
                "arxiv_comment": "17 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13818v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13818v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23573v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23573v2",
                "updated": "2025-10-01T15:57:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    15,
                    57,
                    32,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-28T02:08:27Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    2,
                    8,
                    27,
                    6,
                    271,
                    0
                ],
                "title": "Uncovering Vulnerabilities of LLM-Assisted Cyber Threat Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncovering Vulnerabilities of LLM-Assisted Cyber Threat Intelligence"
                },
                "summary": "Large Language Models (LLMs) are intensively used to assist security analysts\nin counteracting the rapid exploitation of cyber threats, wherein LLMs offer\ncyber threat intelligence (CTI) to support vulnerability assessment and\nincident response. While recent work has shown that LLMs can support a wide\nrange of CTI tasks such as threat analysis, vulnerability detection, and\nintrusion defense, significant performance gaps persist in practical\ndeployments. In this paper, we investigate the intrinsic vulnerabilities of\nLLMs in CTI, focusing on challenges that arise from the nature of the threat\nlandscape itself rather than the model architecture. Using large-scale\nevaluations across multiple CTI benchmarks and real-world threat reports, we\nintroduce a novel categorization methodology that integrates stratification,\nautoregressive refinement, and human-in-the-loop supervision to reliably\nanalyze failure instances. Through extensive experiments and human inspections,\nwe reveal three fundamental vulnerabilities: spurious correlations,\ncontradictory knowledge, and constrained generalization, that limit LLMs in\neffectively supporting CTI. Subsequently, we provide actionable insights for\ndesigning more robust LLM-powered CTI systems to facilitate future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are intensively used to assist security analysts\nin counteracting the rapid exploitation of cyber threats, wherein LLMs offer\ncyber threat intelligence (CTI) to support vulnerability assessment and\nincident response. While recent work has shown that LLMs can support a wide\nrange of CTI tasks such as threat analysis, vulnerability detection, and\nintrusion defense, significant performance gaps persist in practical\ndeployments. In this paper, we investigate the intrinsic vulnerabilities of\nLLMs in CTI, focusing on challenges that arise from the nature of the threat\nlandscape itself rather than the model architecture. Using large-scale\nevaluations across multiple CTI benchmarks and real-world threat reports, we\nintroduce a novel categorization methodology that integrates stratification,\nautoregressive refinement, and human-in-the-loop supervision to reliably\nanalyze failure instances. Through extensive experiments and human inspections,\nwe reveal three fundamental vulnerabilities: spurious correlations,\ncontradictory knowledge, and constrained generalization, that limit LLMs in\neffectively supporting CTI. Subsequently, we provide actionable insights for\ndesigning more robust LLM-powered CTI systems to facilitate future research."
                },
                "authors": [
                    {
                        "name": "Yuqiao Meng"
                    },
                    {
                        "name": "Luoxi Tang"
                    },
                    {
                        "name": "Feiyang Yu"
                    },
                    {
                        "name": "Jinyuan Jia"
                    },
                    {
                        "name": "Guanhua Yan"
                    },
                    {
                        "name": "Ping Yang"
                    },
                    {
                        "name": "Zhaohan Xi"
                    }
                ],
                "author_detail": {
                    "name": "Zhaohan Xi"
                },
                "author": "Zhaohan Xi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23573v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23573v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06920v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06920v2",
                "updated": "2025-10-01T15:55:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    15,
                    55,
                    29,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-08T17:32:17Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    32,
                    17,
                    0,
                    251,
                    0
                ],
                "title": "An Ethically Grounded LLM-Based Approach to Insider Threat Synthesis and\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Ethically Grounded LLM-Based Approach to Insider Threat Synthesis and\n  Detection"
                },
                "summary": "Insider threats are a growing organizational problem due to the complexity of\nidentifying their technical and behavioral elements. A large research body is\ndedicated to the study of insider threats from technological, psychological,\nand educational perspectives. However, research in this domain has been\ngenerally dependent on datasets that are static and limited access which\nrestricts the development of adaptive detection models. This study introduces a\nnovel, ethically grounded approach that uses the large language model (LLM)\nClaude Sonnet 3.7 to dynamically synthesize syslog messages, some of which\ncontain indicators of insider threat scenarios. The messages reflect real-world\ndata distributions by being highly imbalanced (1% insider threats). The syslogs\nwere analyzed for insider threats by both Sonnet 3.7 and GPT-4o, with their\nperformance evaluated through statistical metrics including accuracy,\nprecision, recall, F1, specificity, FAR, MCC, and ROC AUC. Sonnet 3.7\nconsistently outperformed GPT-4o across nearly all metrics, particularly in\nreducing false alarms and improving detection accuracy. The results show strong\npromise for the use of LLMs in synthetic dataset generation and insider threat\ndetection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Insider threats are a growing organizational problem due to the complexity of\nidentifying their technical and behavioral elements. A large research body is\ndedicated to the study of insider threats from technological, psychological,\nand educational perspectives. However, research in this domain has been\ngenerally dependent on datasets that are static and limited access which\nrestricts the development of adaptive detection models. This study introduces a\nnovel, ethically grounded approach that uses the large language model (LLM)\nClaude Sonnet 3.7 to dynamically synthesize syslog messages, some of which\ncontain indicators of insider threat scenarios. The messages reflect real-world\ndata distributions by being highly imbalanced (1% insider threats). The syslogs\nwere analyzed for insider threats by both Sonnet 3.7 and GPT-4o, with their\nperformance evaluated through statistical metrics including accuracy,\nprecision, recall, F1, specificity, FAR, MCC, and ROC AUC. Sonnet 3.7\nconsistently outperformed GPT-4o across nearly all metrics, particularly in\nreducing false alarms and improving detection accuracy. The results show strong\npromise for the use of LLMs in synthetic dataset generation and insider threat\ndetection."
                },
                "authors": [
                    {
                        "name": "Haywood Gelman"
                    },
                    {
                        "name": "John D. Hastings"
                    },
                    {
                        "name": "David Kenley"
                    }
                ],
                "author_detail": {
                    "name": "David Kenley"
                },
                "author": "David Kenley",
                "arxiv_comment": "6 pages, 5 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06920v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06920v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.0; I.2.7; K.4.1; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21046v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21046v2",
                "updated": "2025-10-01T15:48:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    15,
                    48,
                    45,
                    2,
                    274,
                    0
                ],
                "published": "2025-08-28T17:50:58Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    17,
                    50,
                    58,
                    3,
                    240,
                    0
                ],
                "title": "CogVLA: Cognition-Aligned Vision-Language-Action Model via\n  Instruction-Driven Routing & Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CogVLA: Cognition-Aligned Vision-Language-Action Model via\n  Instruction-Driven Routing & Sparsification"
                },
                "summary": "Recent Vision-Language-Action (VLA) models built on pre-trained\nVision-Language Models (VLMs) require extensive post-training, resulting in\nhigh computational overhead that limits scalability and deployment.We propose\nCogVLA, a Cognition-Aligned Vision-Language-Action framework that leverages\ninstruction-driven routing and sparsification to improve both efficiency and\nperformance. CogVLA draws inspiration from human multimodal coordination and\nintroduces a 3-stage progressive architecture. 1) Encoder-FiLM based\nAggregation Routing (EFA-Routing) injects instruction information into the\nvision encoder to selectively aggregate and compress dual-stream visual tokens,\nforming a instruction-aware latent representation. 2) Building upon this\ncompact visual encoding, LLM-FiLM based Pruning Routing (LFP-Routing)\nintroduces action intent into the language model by pruning\ninstruction-irrelevant visually grounded tokens, thereby achieving token-level\nsparsity. 3) To ensure that compressed perception inputs can still support\naccurate and coherent action generation, we introduce V-L-A Coupled Attention\n(CAtten), which combines causal vision-language attention with bidirectional\naction parallel decoding. Extensive experiments on the LIBERO benchmark and\nreal-world robotic tasks demonstrate that CogVLA achieves state-of-the-art\nperformance with success rates of 97.4% and 70.0%, respectively, while reducing\ntraining costs by 2.5-fold and decreasing inference latency by 2.8-fold\ncompared to OpenVLA. CogVLA is open-sourced and publicly available at\nhttps://github.com/JiuTian-VL/CogVLA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Vision-Language-Action (VLA) models built on pre-trained\nVision-Language Models (VLMs) require extensive post-training, resulting in\nhigh computational overhead that limits scalability and deployment.We propose\nCogVLA, a Cognition-Aligned Vision-Language-Action framework that leverages\ninstruction-driven routing and sparsification to improve both efficiency and\nperformance. CogVLA draws inspiration from human multimodal coordination and\nintroduces a 3-stage progressive architecture. 1) Encoder-FiLM based\nAggregation Routing (EFA-Routing) injects instruction information into the\nvision encoder to selectively aggregate and compress dual-stream visual tokens,\nforming a instruction-aware latent representation. 2) Building upon this\ncompact visual encoding, LLM-FiLM based Pruning Routing (LFP-Routing)\nintroduces action intent into the language model by pruning\ninstruction-irrelevant visually grounded tokens, thereby achieving token-level\nsparsity. 3) To ensure that compressed perception inputs can still support\naccurate and coherent action generation, we introduce V-L-A Coupled Attention\n(CAtten), which combines causal vision-language attention with bidirectional\naction parallel decoding. Extensive experiments on the LIBERO benchmark and\nreal-world robotic tasks demonstrate that CogVLA achieves state-of-the-art\nperformance with success rates of 97.4% and 70.0%, respectively, while reducing\ntraining costs by 2.5-fold and decreasing inference latency by 2.8-fold\ncompared to OpenVLA. CogVLA is open-sourced and publicly available at\nhttps://github.com/JiuTian-VL/CogVLA."
                },
                "authors": [
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Renshan Zhang"
                    },
                    {
                        "name": "Rui Shao"
                    },
                    {
                        "name": "Jie He"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "arxiv_comment": "Accepted to NeurIPS 2025, Project Page:\n  https://jiutian-vl.github.io/CogVLA-page",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21046v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21046v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11679v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11679v3",
                "updated": "2025-10-01T15:44:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    15,
                    44,
                    19,
                    2,
                    274,
                    0
                ],
                "published": "2025-05-16T20:39:30Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    20,
                    39,
                    30,
                    4,
                    136,
                    0
                ],
                "title": "Ambiguity in LLMs is a concept missing problem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ambiguity in LLMs is a concept missing problem"
                },
                "summary": "Ambiguity in natural language is a significant obstacle for achieving\naccurate text to structured data mapping through large language models (LLMs),\nwhich affects the performance of tasks such as mapping text to agentic tool\ncalling and text-to-SQL queries. Existing methods to ambiguity handling either\nrely on the ReACT framework to obtain correct mappings through trial and error,\nor on supervised fine-tuning to bias models toward specific tasks. In this\npaper, we adopt a different approach that characterizes representation\ndifferences of ambiguous text in the latent space and leverages these\ndifferences to identify ambiguity before mapping them to structured data. To\ndetect sentence-level ambiguity, we focus on the relationship between ambiguous\nquestions and their interpretations. Unlike distances calculated by dense\nembeddings, we introduce a new distance measure based on a path kernel over\nconcepts. With this measurement, we identify patterns to distinguish ambiguous\nfrom unambiguous questions. Furthermore, we propose a method for improving LLM\nperformance on ambiguous agentic tool calling through missing concept\nprediction. Both achieve state-of-the-art results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ambiguity in natural language is a significant obstacle for achieving\naccurate text to structured data mapping through large language models (LLMs),\nwhich affects the performance of tasks such as mapping text to agentic tool\ncalling and text-to-SQL queries. Existing methods to ambiguity handling either\nrely on the ReACT framework to obtain correct mappings through trial and error,\nor on supervised fine-tuning to bias models toward specific tasks. In this\npaper, we adopt a different approach that characterizes representation\ndifferences of ambiguous text in the latent space and leverages these\ndifferences to identify ambiguity before mapping them to structured data. To\ndetect sentence-level ambiguity, we focus on the relationship between ambiguous\nquestions and their interpretations. Unlike distances calculated by dense\nembeddings, we introduce a new distance measure based on a path kernel over\nconcepts. With this measurement, we identify patterns to distinguish ambiguous\nfrom unambiguous questions. Furthermore, we propose a method for improving LLM\nperformance on ambiguous agentic tool calling through missing concept\nprediction. Both achieve state-of-the-art results."
                },
                "authors": [
                    {
                        "name": "Zhibo Hu"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Yanfeng Shu"
                    },
                    {
                        "name": "Hye-Young Paik"
                    },
                    {
                        "name": "Liming Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Liming Zhu"
                },
                "author": "Liming Zhu",
                "arxiv_comment": "17 pages, 11 figures, title updated",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11679v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11679v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25822v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25822v2",
                "updated": "2025-10-01T15:41:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    15,
                    41,
                    23,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-30T05:56:00Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    5,
                    56,
                    0,
                    1,
                    273,
                    0
                ],
                "title": "Act to See, See to Act: Diffusion-Driven Perception-Action Interplay for\n  Adaptive Policies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Act to See, See to Act: Diffusion-Driven Perception-Action Interplay for\n  Adaptive Policies"
                },
                "summary": "Existing imitation learning methods decouple perception and action, which\noverlooks the causal reciprocity between sensory representations and action\nexecution that humans naturally leverage for adaptive behaviors. To bridge this\ngap, we introduce Action-Guided Diffusion Policy (DP-AG), a unified\nrepresentation learning that explicitly models a dynamic interplay between\nperception and action through probabilistic latent dynamics. DP-AG encodes\nlatent observations into a Gaussian posterior via variational inference and\nevolves them using an action-guided SDE, where the Vector-Jacobian Product\n(VJP) of the diffusion policy's noise predictions serves as a structured\nstochastic force driving latent updates. To promote bidirectional learning\nbetween perception and action, we introduce a cycle-consistent contrastive loss\nthat organizes the gradient flow of the noise predictor into a coherent\nperception-action loop, enforcing mutually consistent transitions in both\nlatent updates and action refinements. Theoretically, we derive a variational\nlower bound for the action-guided SDE, and prove that the contrastive objective\nenhances continuity in both latent and action trajectories. Empirically, DP-AG\nsignificantly outperforms state-of-the-art methods across simulation benchmarks\nand real-world UR5 manipulation tasks. As a result, our DP-AG offers a\npromising step toward bridging biological adaptability and artificial policy\nlearning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing imitation learning methods decouple perception and action, which\noverlooks the causal reciprocity between sensory representations and action\nexecution that humans naturally leverage for adaptive behaviors. To bridge this\ngap, we introduce Action-Guided Diffusion Policy (DP-AG), a unified\nrepresentation learning that explicitly models a dynamic interplay between\nperception and action through probabilistic latent dynamics. DP-AG encodes\nlatent observations into a Gaussian posterior via variational inference and\nevolves them using an action-guided SDE, where the Vector-Jacobian Product\n(VJP) of the diffusion policy's noise predictions serves as a structured\nstochastic force driving latent updates. To promote bidirectional learning\nbetween perception and action, we introduce a cycle-consistent contrastive loss\nthat organizes the gradient flow of the noise predictor into a coherent\nperception-action loop, enforcing mutually consistent transitions in both\nlatent updates and action refinements. Theoretically, we derive a variational\nlower bound for the action-guided SDE, and prove that the contrastive objective\nenhances continuity in both latent and action trajectories. Empirically, DP-AG\nsignificantly outperforms state-of-the-art methods across simulation benchmarks\nand real-world UR5 manipulation tasks. As a result, our DP-AG offers a\npromising step toward bridging biological adaptability and artificial policy\nlearning."
                },
                "authors": [
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Weiting Peng"
                    },
                    {
                        "name": "Jing Tang"
                    },
                    {
                        "name": "Zeyu Gong"
                    },
                    {
                        "name": "Xihua Wang"
                    },
                    {
                        "name": "Bo Tao"
                    },
                    {
                        "name": "Li Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Li Cheng"
                },
                "author": "Li Cheng",
                "arxiv_comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25822v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25822v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19331v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19331v2",
                "updated": "2025-10-01T15:32:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    15,
                    32,
                    32,
                    2,
                    274,
                    0
                ],
                "published": "2025-08-26T18:00:02Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    18,
                    0,
                    2,
                    1,
                    238,
                    0
                ],
                "title": "Towards a few percent measurement of the Hubble constant with the\n  current network of gravitational wave detectors without using electromagnetic\n  information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a few percent measurement of the Hubble constant with the\n  current network of gravitational wave detectors without using electromagnetic\n  information"
                },
                "summary": "Gravitational waves provide a novel and independent measurement of\ncosmological parameters, offering a promising avenue to address the Hubble\ntension alongside traditional electromagnetic observations. In the absence of\nelectromagnetic counterparts or complete host galaxy catalogs, current\nmeasurements rely on population-based methods that statistically combine black\nhole merger events. Building on recent models that incorporate additional\nstructure in the primary black hole mass distribution, using public data from\nthe LIGO-Virgo-KAGRA (LVK) collaboration third observing run (O3), we obtain a\n30% accuracy improvement on the measurement of the Hubble constant with respect\nto the result reported by LVK with the third GW transient catalog (GWTC-3).\nEmploying a realistic simulation that includes full Bayesian single-event\ninference, we present forecasts for the upcoming LVK observational runs, O4 and\nO5. Using a three power-law mass model, we project a measurement of the Hubble\nconstant with 20% accuracy at O4 sensitivity, improving to 2.7% accuracy at O5\nsensitivity. Our findings demonstrate the potential for gravitational waves to\nprovide a substantial contribution to solving the Hubble tension within the\nnext decade of observations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gravitational waves provide a novel and independent measurement of\ncosmological parameters, offering a promising avenue to address the Hubble\ntension alongside traditional electromagnetic observations. In the absence of\nelectromagnetic counterparts or complete host galaxy catalogs, current\nmeasurements rely on population-based methods that statistically combine black\nhole merger events. Building on recent models that incorporate additional\nstructure in the primary black hole mass distribution, using public data from\nthe LIGO-Virgo-KAGRA (LVK) collaboration third observing run (O3), we obtain a\n30% accuracy improvement on the measurement of the Hubble constant with respect\nto the result reported by LVK with the third GW transient catalog (GWTC-3).\nEmploying a realistic simulation that includes full Bayesian single-event\ninference, we present forecasts for the upcoming LVK observational runs, O4 and\nO5. Using a three power-law mass model, we project a measurement of the Hubble\nconstant with 20% accuracy at O4 sensitivity, improving to 2.7% accuracy at O5\nsensitivity. Our findings demonstrate the potential for gravitational waves to\nprovide a substantial contribution to solving the Hubble tension within the\nnext decade of observations."
                },
                "authors": [
                    {
                        "name": "Tom Bertheas"
                    },
                    {
                        "name": "Vasco Gennari"
                    },
                    {
                        "name": "Nicola Tamanini"
                    }
                ],
                "author_detail": {
                    "name": "Nicola Tamanini"
                },
                "author": "Nicola Tamanini",
                "arxiv_comment": "11 pages, 4 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19331v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19331v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03516v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03516v2",
                "updated": "2025-10-01T15:26:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    15,
                    26,
                    16,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-03T17:58:12Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    17,
                    58,
                    12,
                    2,
                    246,
                    0
                ],
                "title": "Easier Painting Than Thinking: Can Text-to-Image Models Set the Stage,\n  but Not Direct the Play?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Easier Painting Than Thinking: Can Text-to-Image Models Set the Stage,\n  but Not Direct the Play?"
                },
                "summary": "Text-to-image (T2I) generation aims to synthesize images from textual\nprompts, which jointly specify what must be shown and imply what can be\ninferred, which thus correspond to two core capabilities: composition and\nreasoning. Despite recent advances of T2I models in both composition and\nreasoning, existing benchmarks remain limited in evaluation. They not only fail\nto provide comprehensive coverage across and within both capabilities, but also\nlargely restrict evaluation to low scene density and simple one-to-one\nreasoning. To address these limitations, we propose T2I-CoReBench, a\ncomprehensive and complex benchmark that evaluates both composition and\nreasoning capabilities of T2I models. To ensure comprehensiveness, we structure\ncomposition around scene graph elements (instance, attribute, and relation) and\nreasoning around the philosophical framework of inference (deductive,\ninductive, and abductive), formulating a 12-dimensional evaluation taxonomy. To\nincrease complexity, driven by the inherent real-world complexities, we curate\neach prompt with higher compositional density for composition and greater\nreasoning intensity for reasoning. To facilitate fine-grained and reliable\nevaluation, we also pair each evaluation prompt with a checklist that specifies\nindividual yes/no questions to assess each intended element independently. In\nstatistics, our benchmark comprises 1,080 challenging prompts and around 13,500\nchecklist questions. Experiments across 28 current T2I models reveal that their\ncomposition capability still remains limited in high compositional scenarios,\nwhile the reasoning capability lags even further behind as a critical\nbottleneck, with all models struggling to infer implicit elements from prompts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image (T2I) generation aims to synthesize images from textual\nprompts, which jointly specify what must be shown and imply what can be\ninferred, which thus correspond to two core capabilities: composition and\nreasoning. Despite recent advances of T2I models in both composition and\nreasoning, existing benchmarks remain limited in evaluation. They not only fail\nto provide comprehensive coverage across and within both capabilities, but also\nlargely restrict evaluation to low scene density and simple one-to-one\nreasoning. To address these limitations, we propose T2I-CoReBench, a\ncomprehensive and complex benchmark that evaluates both composition and\nreasoning capabilities of T2I models. To ensure comprehensiveness, we structure\ncomposition around scene graph elements (instance, attribute, and relation) and\nreasoning around the philosophical framework of inference (deductive,\ninductive, and abductive), formulating a 12-dimensional evaluation taxonomy. To\nincrease complexity, driven by the inherent real-world complexities, we curate\neach prompt with higher compositional density for composition and greater\nreasoning intensity for reasoning. To facilitate fine-grained and reliable\nevaluation, we also pair each evaluation prompt with a checklist that specifies\nindividual yes/no questions to assess each intended element independently. In\nstatistics, our benchmark comprises 1,080 challenging prompts and around 13,500\nchecklist questions. Experiments across 28 current T2I models reveal that their\ncomposition capability still remains limited in high compositional scenarios,\nwhile the reasoning capability lags even further behind as a critical\nbottleneck, with all models struggling to infer implicit elements from prompts."
                },
                "authors": [
                    {
                        "name": "Ouxiang Li"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "Xinting Hu"
                    },
                    {
                        "name": "Huijuan Huang"
                    },
                    {
                        "name": "Rui Chen"
                    },
                    {
                        "name": "Jiarong Ou"
                    },
                    {
                        "name": "Xin Tao"
                    },
                    {
                        "name": "Pengfei Wan"
                    },
                    {
                        "name": "Xiaojuan Qi"
                    },
                    {
                        "name": "Fuli Feng"
                    }
                ],
                "author_detail": {
                    "name": "Fuli Feng"
                },
                "author": "Fuli Feng",
                "arxiv_comment": "Project Page: https://t2i-corebench.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03516v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03516v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17017v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17017v2",
                "updated": "2025-10-01T15:25:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    15,
                    25,
                    14,
                    2,
                    274,
                    0
                ],
                "published": "2025-04-23T18:04:38Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    18,
                    4,
                    38,
                    2,
                    113,
                    0
                ],
                "title": "Neural Theorem Proving: Generating and Structuring Proofs for Formal\n  Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Theorem Proving: Generating and Structuring Proofs for Formal\n  Verification"
                },
                "summary": "Formally verifying properties of software code has been a highly desirable\ntask, especially with the emergence of LLM-generated code. In the same vein,\nthey provide an interesting avenue for the exploration of formal verification\nand mechanistic interpretability. Since the introduction of code-specific\nmodels, despite their successes in generating code in Lean4 and Isabelle, the\ntask of generalized theorem proving still remains far from being fully solved\nand will be a benchmark for reasoning capability in LLMs. In this work, we\nintroduce a framework that generates whole proofs in a formal language to be\nused within systems that utilize the power of built-in tactics and\noff-the-shelf automated theorem provers. Our framework includes 3 components:\ngenerating natural language statements of the code to be verified, an LLM that\ngenerates formal proofs for the given statement, and a module employing\nheuristics for building the final proof. To train the LLM, we employ a 2-stage\nfine-tuning process, where we first use SFT-based training to enable the model\nto generate syntactically correct Isabelle code and then RL-based training that\nencourages the model to generate proofs verified by a theorem prover. We\nvalidate our framework using the miniF2F-test benchmark and the Isabelle proof\nassistant and design a use case to verify the correctness of the AWS S3 bucket\naccess policy code. We also curate a dataset based on the\nFVEL\\textsubscript{\\textnormal{ER}} dataset for future training tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formally verifying properties of software code has been a highly desirable\ntask, especially with the emergence of LLM-generated code. In the same vein,\nthey provide an interesting avenue for the exploration of formal verification\nand mechanistic interpretability. Since the introduction of code-specific\nmodels, despite their successes in generating code in Lean4 and Isabelle, the\ntask of generalized theorem proving still remains far from being fully solved\nand will be a benchmark for reasoning capability in LLMs. In this work, we\nintroduce a framework that generates whole proofs in a formal language to be\nused within systems that utilize the power of built-in tactics and\noff-the-shelf automated theorem provers. Our framework includes 3 components:\ngenerating natural language statements of the code to be verified, an LLM that\ngenerates formal proofs for the given statement, and a module employing\nheuristics for building the final proof. To train the LLM, we employ a 2-stage\nfine-tuning process, where we first use SFT-based training to enable the model\nto generate syntactically correct Isabelle code and then RL-based training that\nencourages the model to generate proofs verified by a theorem prover. We\nvalidate our framework using the miniF2F-test benchmark and the Isabelle proof\nassistant and design a use case to verify the correctness of the AWS S3 bucket\naccess policy code. We also curate a dataset based on the\nFVEL\\textsubscript{\\textnormal{ER}} dataset for future training tasks."
                },
                "authors": [
                    {
                        "name": "Balaji Rao"
                    },
                    {
                        "name": "William Eiers"
                    },
                    {
                        "name": "Carlo Lipizzi"
                    }
                ],
                "author_detail": {
                    "name": "Carlo Lipizzi"
                },
                "author": "Carlo Lipizzi",
                "arxiv_comment": "Accepted to the Proceedings of the 19th Conference on Neurosymbolic\n  Learning and Reasoning (NeSy 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17017v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17017v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23019v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23019v2",
                "updated": "2025-10-01T15:24:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    15,
                    24,
                    12,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-27T00:24:57Z",
                "published_parsed": [
                    2025,
                    9,
                    27,
                    0,
                    24,
                    57,
                    5,
                    270,
                    0
                ],
                "title": "LLM Watermark Evasion via Bias Inversion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Watermark Evasion via Bias Inversion"
                },
                "summary": "Watermarking for large language models (LLMs) embeds a statistical signal\nduring generation to enable detection of model-produced text. While\nwatermarking has proven effective in benign settings, its robustness under\nadversarial evasion remains contested. To advance a rigorous understanding and\nevaluation of such vulnerabilities, we propose the \\emph{Bias-Inversion\nRewriting Attack} (BIRA), which is theoretically motivated and model-agnostic.\nBIRA weakens the watermark signal by suppressing the logits of likely\nwatermarked tokens during LLM-based rewriting, without any knowledge of the\nunderlying watermarking scheme. Across recent watermarking methods, BIRA\nachieves over 99\\% evasion while preserving the semantic content of the\noriginal text. Beyond demonstrating an attack, our results reveal a systematic\nvulnerability, emphasizing the need for stress testing and robust defenses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watermarking for large language models (LLMs) embeds a statistical signal\nduring generation to enable detection of model-produced text. While\nwatermarking has proven effective in benign settings, its robustness under\nadversarial evasion remains contested. To advance a rigorous understanding and\nevaluation of such vulnerabilities, we propose the \\emph{Bias-Inversion\nRewriting Attack} (BIRA), which is theoretically motivated and model-agnostic.\nBIRA weakens the watermark signal by suppressing the logits of likely\nwatermarked tokens during LLM-based rewriting, without any knowledge of the\nunderlying watermarking scheme. Across recent watermarking methods, BIRA\nachieves over 99\\% evasion while preserving the semantic content of the\noriginal text. Beyond demonstrating an attack, our results reveal a systematic\nvulnerability, emphasizing the need for stress testing and robust defenses."
                },
                "authors": [
                    {
                        "name": "Jeongyeon Hwang"
                    },
                    {
                        "name": "Sangdon Park"
                    },
                    {
                        "name": "Jungseul Ok"
                    }
                ],
                "author_detail": {
                    "name": "Jungseul Ok"
                },
                "author": "Jungseul Ok",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23019v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23019v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02651v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02651v4",
                "updated": "2025-10-01T15:22:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    15,
                    22,
                    38,
                    2,
                    274,
                    0
                ],
                "published": "2024-12-03T18:24:56Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    18,
                    24,
                    56,
                    1,
                    338,
                    0
                ],
                "title": "Costs of Bayesian Parameter Estimation in Third-Generation Gravitational\n  Wave Detectors: an Assessment of Current Acceleration Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Costs of Bayesian Parameter Estimation in Third-Generation Gravitational\n  Wave Detectors: an Assessment of Current Acceleration Methods"
                },
                "summary": "Bayesian inference with stochastic sampling has been widely used to obtain\nthe properties of gravitational wave (GW) sources. Although computationally\nintensive, its cost remains manageable for current second-generation GW\ndetectors because of the relatively low event rate and signal-to-noise ratio\n(SNR). The third-generation (3G) GW detectors are expected to detect hundreds\nof thousands of compact binary coalescence (CBC) events every year with\nsubstantially higher SNR and longer signal duration, presenting significant\ncomputational challenges. In this study, we systematically evaluate the\ncomputational costs of CBC source parameter estimation (PE) in the 3G era by\nmodeling the PE time cost as a function of SNR and signal duration. We examine\nthe standard PE method alongside acceleration methods including relative\nbinning, multibanding, and reduced order quadrature. We predict that PE for a\none-month-observation catalog with 3G detectors could require at least billions\nof CPU core hours with the standard PE method, whereas acceleration techniques\ncan reduce this demand to less than millions of core hours, which is as high as\nthe cost of analyzing GW events in the past 10 years. These findings highlight\nthe necessity for more efficient PE methods to enable cost-effective and\nenvironmentally sustainable data analysis for 3G detectors. In addition, we\nassess the accuracy of accelerated PE methods, emphasizing the need for careful\ntreatment in high-SNR scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian inference with stochastic sampling has been widely used to obtain\nthe properties of gravitational wave (GW) sources. Although computationally\nintensive, its cost remains manageable for current second-generation GW\ndetectors because of the relatively low event rate and signal-to-noise ratio\n(SNR). The third-generation (3G) GW detectors are expected to detect hundreds\nof thousands of compact binary coalescence (CBC) events every year with\nsubstantially higher SNR and longer signal duration, presenting significant\ncomputational challenges. In this study, we systematically evaluate the\ncomputational costs of CBC source parameter estimation (PE) in the 3G era by\nmodeling the PE time cost as a function of SNR and signal duration. We examine\nthe standard PE method alongside acceleration methods including relative\nbinning, multibanding, and reduced order quadrature. We predict that PE for a\none-month-observation catalog with 3G detectors could require at least billions\nof CPU core hours with the standard PE method, whereas acceleration techniques\ncan reduce this demand to less than millions of core hours, which is as high as\nthe cost of analyzing GW events in the past 10 years. These findings highlight\nthe necessity for more efficient PE methods to enable cost-effective and\nenvironmentally sustainable data analysis for 3G detectors. In addition, we\nassess the accuracy of accelerated PE methods, emphasizing the need for careful\ntreatment in high-SNR scenarios."
                },
                "authors": [
                    {
                        "name": "Qian Hu"
                    },
                    {
                        "name": "John Veitch"
                    }
                ],
                "author_detail": {
                    "name": "John Veitch"
                },
                "author": "John Veitch",
                "arxiv_comment": "15 pages, 4 figures, 1 table. Accepted version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02651v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02651v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04018v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04018v2",
                "updated": "2025-10-01T15:15:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    15,
                    15,
                    52,
                    2,
                    274,
                    0
                ],
                "published": "2025-06-04T14:46:47Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    14,
                    46,
                    47,
                    2,
                    155,
                    0
                ],
                "title": "AgentMisalignment: Measuring the Propensity for Misaligned Behaviour in\n  LLM-Based Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentMisalignment: Measuring the Propensity for Misaligned Behaviour in\n  LLM-Based Agents"
                },
                "summary": "As Large Language Model (LLM) agents become more widespread, associated\nmisalignment risks increase. While prior research has studied agents' ability\nto produce harmful outputs or follow malicious instructions, it remains unclear\nhow likely agents are to spontaneously pursue unintended goals in realistic\ndeployments. In this work, we approach misalignment as a conflict between the\ninternal goals pursued by the model and the goals intended by its deployer. We\nintroduce a misalignment propensity benchmark, \\textsc{AgentMisalignment}, a\nbenchmark suite designed to evaluate the propensity of LLM agents to misalign\nin realistic scenarios. Evaluations cover behaviours such as avoiding\noversight, resisting shutdown, sandbagging, and power-seeking. Testing frontier\nmodels, we find that more capable agents tend to exhibit higher misalignment on\naverage. We also systematically vary agent personalities through different\nsystem prompts and observe that persona characteristics can strongly and\nunpredictably influence misalignment, sometimes more than the choice of model\nitself. Our results reveal the limitations of current alignment methods for\nautonomous LLM agents and underscore the need to rethink misalignment in\nrealistic deployment settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Model (LLM) agents become more widespread, associated\nmisalignment risks increase. While prior research has studied agents' ability\nto produce harmful outputs or follow malicious instructions, it remains unclear\nhow likely agents are to spontaneously pursue unintended goals in realistic\ndeployments. In this work, we approach misalignment as a conflict between the\ninternal goals pursued by the model and the goals intended by its deployer. We\nintroduce a misalignment propensity benchmark, \\textsc{AgentMisalignment}, a\nbenchmark suite designed to evaluate the propensity of LLM agents to misalign\nin realistic scenarios. Evaluations cover behaviours such as avoiding\noversight, resisting shutdown, sandbagging, and power-seeking. Testing frontier\nmodels, we find that more capable agents tend to exhibit higher misalignment on\naverage. We also systematically vary agent personalities through different\nsystem prompts and observe that persona characteristics can strongly and\nunpredictably influence misalignment, sometimes more than the choice of model\nitself. Our results reveal the limitations of current alignment methods for\nautonomous LLM agents and underscore the need to rethink misalignment in\nrealistic deployment settings."
                },
                "authors": [
                    {
                        "name": "Akshat Naik"
                    },
                    {
                        "name": "Patrick Quinn"
                    },
                    {
                        "name": "Guillermo Bosch"
                    },
                    {
                        "name": "Emma Gouné"
                    },
                    {
                        "name": "Francisco Javier Campos Zabala"
                    },
                    {
                        "name": "Jason Ross Brown"
                    },
                    {
                        "name": "Edward James Young"
                    }
                ],
                "author_detail": {
                    "name": "Edward James Young"
                },
                "author": "Edward James Young",
                "arxiv_comment": "Prepint, under review for NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04018v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04018v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.11; K.4.1; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01289v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01289v2",
                "updated": "2025-10-01T15:14:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    15,
                    14,
                    33,
                    2,
                    274,
                    0
                ],
                "published": "2025-02-03T12:00:11Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    12,
                    0,
                    11,
                    0,
                    34,
                    0
                ],
                "title": "A Framework for Double-Blind Federated Adaptation of Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Framework for Double-Blind Federated Adaptation of Foundation Models"
                },
                "summary": "Foundation models (FMs) excel in zero-shot tasks but benefit from\ntask-specific adaptation. However, privacy concerns prevent data sharing among\nmultiple data owners, and proprietary restrictions prevent the learning service\nprovider (LSP) from sharing the FM. In this work, we propose BlindFed, a\nframework enabling collaborative FM adaptation while protecting both parties:\ndata owners do not access the FM or each other's data, and the LSP does not see\nsensitive task data. BlindFed relies on fully homomorphic encryption (FHE) and\nconsists of three key innovations: (i) FHE-friendly architectural modifications\nvia polynomial approximations and low-rank adapters, (ii) a two-stage split\nlearning approach combining offline knowledge distillation and online encrypted\ninference for adapter training without backpropagation through the FM, and\n(iii) a privacy-boosting scheme using sample permutations and stochastic block\nsampling to mitigate model extraction attacks. Empirical results on four image\nclassification datasets demonstrate the practical feasibility of the BlindFed\nframework, albeit at a high communication cost and large computational\ncomplexity for the LSP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation models (FMs) excel in zero-shot tasks but benefit from\ntask-specific adaptation. However, privacy concerns prevent data sharing among\nmultiple data owners, and proprietary restrictions prevent the learning service\nprovider (LSP) from sharing the FM. In this work, we propose BlindFed, a\nframework enabling collaborative FM adaptation while protecting both parties:\ndata owners do not access the FM or each other's data, and the LSP does not see\nsensitive task data. BlindFed relies on fully homomorphic encryption (FHE) and\nconsists of three key innovations: (i) FHE-friendly architectural modifications\nvia polynomial approximations and low-rank adapters, (ii) a two-stage split\nlearning approach combining offline knowledge distillation and online encrypted\ninference for adapter training without backpropagation through the FM, and\n(iii) a privacy-boosting scheme using sample permutations and stochastic block\nsampling to mitigate model extraction attacks. Empirical results on four image\nclassification datasets demonstrate the practical feasibility of the BlindFed\nframework, albeit at a high communication cost and large computational\ncomplexity for the LSP."
                },
                "authors": [
                    {
                        "name": "Nurbek Tastan"
                    },
                    {
                        "name": "Karthik Nandakumar"
                    }
                ],
                "author_detail": {
                    "name": "Karthik Nandakumar"
                },
                "author": "Karthik Nandakumar",
                "arxiv_comment": "Accepted to ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01289v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01289v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.17401v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.17401v5",
                "updated": "2025-10-01T15:10:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    15,
                    10,
                    54,
                    2,
                    274,
                    0
                ],
                "published": "2023-09-29T17:01:29Z",
                "published_parsed": [
                    2023,
                    9,
                    29,
                    17,
                    1,
                    29,
                    4,
                    272,
                    0
                ],
                "title": "Adversarial Attacks to Latent Representations of Distributed Neural\n  Networks in Split Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial Attacks to Latent Representations of Distributed Neural\n  Networks in Split Computing"
                },
                "summary": "Distributed deep neural networks (DNNs) have been shown to reduce the\ncomputational burden of mobile devices and decrease the end-to-end inference\nlatency in edge computing scenarios. While distributed DNNs have been studied,\nto the best of our knowledge, the resilience of distributed DNNs to adversarial\naction remains an open problem. In this paper, we fill the existing research\ngap by rigorously analyzing the robustness of distributed DNNs against\nadversarial action. We cast this problem in the context of information theory\nand rigorously proved that (i) the compressed latent dimension improves the\nrobustness but also affect task-oriented performance; and (ii) the deeper\nsplitting point enhances the robustness but also increases the computational\nburden. These two trade-offs provide a novel perspective to design robust\ndistributed DNN. To test our theoretical findings, we perform extensive\nexperimental analysis by considering 6 different DNN architectures, 6 different\napproaches for distributed DNN and 10 different adversarial attacks using the\nImageNet-1K dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed deep neural networks (DNNs) have been shown to reduce the\ncomputational burden of mobile devices and decrease the end-to-end inference\nlatency in edge computing scenarios. While distributed DNNs have been studied,\nto the best of our knowledge, the resilience of distributed DNNs to adversarial\naction remains an open problem. In this paper, we fill the existing research\ngap by rigorously analyzing the robustness of distributed DNNs against\nadversarial action. We cast this problem in the context of information theory\nand rigorously proved that (i) the compressed latent dimension improves the\nrobustness but also affect task-oriented performance; and (ii) the deeper\nsplitting point enhances the robustness but also increases the computational\nburden. These two trade-offs provide a novel perspective to design robust\ndistributed DNN. To test our theoretical findings, we perform extensive\nexperimental analysis by considering 6 different DNN architectures, 6 different\napproaches for distributed DNN and 10 different adversarial attacks using the\nImageNet-1K dataset."
                },
                "authors": [
                    {
                        "name": "Milin Zhang"
                    },
                    {
                        "name": "Mohammad Abdi"
                    },
                    {
                        "name": "Jonathan Ashdown"
                    },
                    {
                        "name": "Francesco Restuccia"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Restuccia"
                },
                "author": "Francesco Restuccia",
                "arxiv_comment": "Accepted in Elsevier Computer Networks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.17401v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.17401v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20460v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20460v2",
                "updated": "2025-10-01T15:08:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    15,
                    8,
                    15,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-24T18:06:11Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    18,
                    6,
                    11,
                    2,
                    267,
                    0
                ],
                "title": "Differential Privacy of Network Parameters from a System Identification\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differential Privacy of Network Parameters from a System Identification\n  Perspective"
                },
                "summary": "This paper addresses the problem of protecting network information from\nprivacy system identification (SI) attacks when sharing cyber-physical system\nsimulations. We model analyst observations of networked states as time-series\noutputs of a graph filter driven by differentially private (DP) nodal\nexcitations, with the analyst aiming to infer the underlying graph shift\noperator (GSO). Unlike traditional SI, which estimates system parameters, we\nstudy the inverse problem: what assumptions prevent adversaries from\nidentifying the GSO while preserving utility for legitimate analysis. We show\nthat applying DP mechanisms to inputs provides formal privacy guarantees for\nthe GSO, linking the $(\\epsilon,\\delta)$-DP bound to the spectral properties of\nthe graph filter and noise covariance. More precisely, for DP Gaussian signals,\nthe spectral characteristics of both the filter and noise covariance determine\nthe privacy bound, with smooth filters and low-condition-number covariance\nyielding greater privacy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the problem of protecting network information from\nprivacy system identification (SI) attacks when sharing cyber-physical system\nsimulations. We model analyst observations of networked states as time-series\noutputs of a graph filter driven by differentially private (DP) nodal\nexcitations, with the analyst aiming to infer the underlying graph shift\noperator (GSO). Unlike traditional SI, which estimates system parameters, we\nstudy the inverse problem: what assumptions prevent adversaries from\nidentifying the GSO while preserving utility for legitimate analysis. We show\nthat applying DP mechanisms to inputs provides formal privacy guarantees for\nthe GSO, linking the $(\\epsilon,\\delta)$-DP bound to the spectral properties of\nthe graph filter and noise covariance. More precisely, for DP Gaussian signals,\nthe spectral characteristics of both the filter and noise covariance determine\nthe privacy bound, with smooth filters and low-condition-number covariance\nyielding greater privacy."
                },
                "authors": [
                    {
                        "name": "Andrew Campbell"
                    },
                    {
                        "name": "Anna Scaglione"
                    },
                    {
                        "name": "Hang Liu"
                    },
                    {
                        "name": "Victor Elvira"
                    },
                    {
                        "name": "Sean Peisert"
                    },
                    {
                        "name": "Daniel Arnold"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Arnold"
                },
                "author": "Daniel Arnold",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20460v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20460v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14615v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14615v3",
                "updated": "2025-10-01T14:40:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    14,
                    40,
                    11,
                    2,
                    274,
                    0
                ],
                "published": "2025-01-24T16:33:52Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    16,
                    33,
                    52,
                    4,
                    24,
                    0
                ],
                "title": "Integration of Calcium Imaging Traces via Deep Generative Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integration of Calcium Imaging Traces via Deep Generative Modeling"
                },
                "summary": "Calcium imaging allows for the parallel measurement of large neuronal\npopulations in a spatially resolved and minimally invasive manner, and has\nbecome a gold-standard for neuronal functionality. While deep generative models\nhave been successfully applied to study the activity of neuronal ensembles,\ntheir potential for learning single-neuron representations from calcium imaging\nfluorescence traces remains largely unexplored, and batch effects remain an\nimportant hurdle. To address this, we explore supervised variational\nautoencoder architectures that learn compact representations of individual\nneurons from fluorescent traces without relying on spike inference algorithms.\nWe find that this approach outperforms state-of-the-art models, preserving\nbiological variability while mitigating batch effects. Across simulated and\nexperimental datasets, this framework enables robust visualization, clustering,\nand interpretation of single-neuron dynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Calcium imaging allows for the parallel measurement of large neuronal\npopulations in a spatially resolved and minimally invasive manner, and has\nbecome a gold-standard for neuronal functionality. While deep generative models\nhave been successfully applied to study the activity of neuronal ensembles,\ntheir potential for learning single-neuron representations from calcium imaging\nfluorescence traces remains largely unexplored, and batch effects remain an\nimportant hurdle. To address this, we explore supervised variational\nautoencoder architectures that learn compact representations of individual\nneurons from fluorescent traces without relying on spike inference algorithms.\nWe find that this approach outperforms state-of-the-art models, preserving\nbiological variability while mitigating batch effects. Across simulated and\nexperimental datasets, this framework enables robust visualization, clustering,\nand interpretation of single-neuron dynamics."
                },
                "authors": [
                    {
                        "name": "Berta Ros"
                    },
                    {
                        "name": "Mireia Olives-Verger"
                    },
                    {
                        "name": "Caterina Fuses"
                    },
                    {
                        "name": "Josep M Canals"
                    },
                    {
                        "name": "Jordi Soriano"
                    },
                    {
                        "name": "Jordi Abante"
                    }
                ],
                "author_detail": {
                    "name": "Jordi Abante"
                },
                "author": "Jordi Abante",
                "arxiv_comment": "Submitted to ICASSP 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14615v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14615v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07493v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07493v3",
                "updated": "2025-10-01T14:37:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    14,
                    37,
                    49,
                    2,
                    274,
                    0
                ],
                "published": "2024-12-10T13:18:45Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    13,
                    18,
                    45,
                    1,
                    345,
                    0
                ],
                "title": "LLM-guided Task and Motion Planning using Knowledge-based Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-guided Task and Motion Planning using Knowledge-based Reasoning"
                },
                "summary": "Performing complex manipulation tasks in dynamic environments requires\nefficient Task and Motion Planning (TAMP) approaches that combine high-level\nsymbolic plans with low-level motion control. Advances in Large Language Models\n(LLMs), such as GPT-4, are transforming task planning by offering natural\nlanguage as an intuitive and flexible way to describe tasks, generate symbolic\nplans, and reason. However, the effectiveness of LLM-based TAMP approaches is\nlimited due to static and template-based prompting, which limits adaptability\nto dynamic environments and complex task contexts. To address these\nlimitations, this work proposes a novel Onto-LLM-TAMP framework that employs\nknowledge-based reasoning to refine and expand user prompts with\ntask-contextual reasoning and knowledge-based environment state descriptions.\nIntegrating domain-specific knowledge into the prompt ensures semantically\naccurate and context-aware task plans. The proposed framework demonstrates its\neffectiveness by resolving semantic errors in symbolic plan generation, such as\nmaintaining logical temporal goal ordering in scenarios involving hierarchical\nobject placement. The proposed framework is validated through both simulation\nand real-world scenarios, demonstrating significant improvements over the\nbaseline approach in terms of adaptability to dynamic environments and the\ngeneration of semantically correct task plans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performing complex manipulation tasks in dynamic environments requires\nefficient Task and Motion Planning (TAMP) approaches that combine high-level\nsymbolic plans with low-level motion control. Advances in Large Language Models\n(LLMs), such as GPT-4, are transforming task planning by offering natural\nlanguage as an intuitive and flexible way to describe tasks, generate symbolic\nplans, and reason. However, the effectiveness of LLM-based TAMP approaches is\nlimited due to static and template-based prompting, which limits adaptability\nto dynamic environments and complex task contexts. To address these\nlimitations, this work proposes a novel Onto-LLM-TAMP framework that employs\nknowledge-based reasoning to refine and expand user prompts with\ntask-contextual reasoning and knowledge-based environment state descriptions.\nIntegrating domain-specific knowledge into the prompt ensures semantically\naccurate and context-aware task plans. The proposed framework demonstrates its\neffectiveness by resolving semantic errors in symbolic plan generation, such as\nmaintaining logical temporal goal ordering in scenarios involving hierarchical\nobject placement. The proposed framework is validated through both simulation\nand real-world scenarios, demonstrating significant improvements over the\nbaseline approach in terms of adaptability to dynamic environments and the\ngeneration of semantically correct task plans."
                },
                "authors": [
                    {
                        "name": "Muhayy Ud Din"
                    },
                    {
                        "name": "Jan Rosell"
                    },
                    {
                        "name": "Waseem Akram"
                    },
                    {
                        "name": "Isiah Zaplana"
                    },
                    {
                        "name": "Maximo A Roa"
                    },
                    {
                        "name": "Irfan Hussain"
                    }
                ],
                "author_detail": {
                    "name": "Irfan Hussain"
                },
                "author": "Irfan Hussain",
                "arxiv_comment": "Submitted to knowledge based systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07493v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07493v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22255v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22255v3",
                "updated": "2025-10-02T11:37:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    11,
                    37,
                    39,
                    3,
                    275,
                    0
                ],
                "published": "2025-09-26T12:19:22Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    12,
                    19,
                    22,
                    4,
                    269,
                    0
                ],
                "title": "Evaluating LLMs for Combinatorial Optimization: One-Phase and Two-Phase\n  Heuristics for 2D Bin-Packing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating LLMs for Combinatorial Optimization: One-Phase and Two-Phase\n  Heuristics for 2D Bin-Packing"
                },
                "summary": "This paper presents an evaluation framework for assessing Large Language\nModels' (LLMs) capabilities in combinatorial optimization, specifically\naddressing the 2D bin-packing problem. We introduce a systematic methodology\nthat combines LLMs with evolutionary algorithms to generate and refine\nheuristic solutions iteratively. Through comprehensive experiments comparing\nLLM generated heuristics against traditional approaches (Finite First-Fit and\nHybrid First-Fit), we demonstrate that LLMs can produce more efficient\nsolutions while requiring fewer computational resources. Our evaluation reveals\nthat GPT-4o achieves optimal solutions within two iterations, reducing average\nbin usage from 16 to 15 bins while improving space utilization from 0.76-0.78\nto 0.83. This work contributes to understanding LLM evaluation in specialized\ndomains and establishes benchmarks for assessing LLM performance in\ncombinatorial optimization tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an evaluation framework for assessing Large Language\nModels' (LLMs) capabilities in combinatorial optimization, specifically\naddressing the 2D bin-packing problem. We introduce a systematic methodology\nthat combines LLMs with evolutionary algorithms to generate and refine\nheuristic solutions iteratively. Through comprehensive experiments comparing\nLLM generated heuristics against traditional approaches (Finite First-Fit and\nHybrid First-Fit), we demonstrate that LLMs can produce more efficient\nsolutions while requiring fewer computational resources. Our evaluation reveals\nthat GPT-4o achieves optimal solutions within two iterations, reducing average\nbin usage from 16 to 15 bins while improving space utilization from 0.76-0.78\nto 0.83. This work contributes to understanding LLM evaluation in specialized\ndomains and establishes benchmarks for assessing LLM performance in\ncombinatorial optimization tasks."
                },
                "authors": [
                    {
                        "name": "Syed Mahbubul Huq"
                    },
                    {
                        "name": "Daniel Brito"
                    },
                    {
                        "name": "Daniel Sikar"
                    },
                    {
                        "name": "Chris Child"
                    },
                    {
                        "name": "Tillman Weyde"
                    },
                    {
                        "name": "Rajesh Mojumder"
                    }
                ],
                "author_detail": {
                    "name": "Rajesh Mojumder"
                },
                "author": "Rajesh Mojumder",
                "arxiv_comment": "1 table, 6 figures. 39th Conference on Neural Information Processing\n  Systems (NeurIPS 2025) Accepted for the Workshop: Evaluating the Evolving LLM\n  Lifecycle Benchmarks, Emergent Abilities, and Scaling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22255v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22255v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17621v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17621v2",
                "updated": "2025-10-01T14:31:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    14,
                    31,
                    5,
                    2,
                    274,
                    0
                ],
                "published": "2025-08-25T03:01:30Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    3,
                    1,
                    30,
                    0,
                    237,
                    0
                ],
                "title": "Steering When Necessary: Flexible Steering Large Language Models with\n  Backtracking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steering When Necessary: Flexible Steering Large Language Models with\n  Backtracking"
                },
                "summary": "Large language models (LLMs) have achieved remarkable performance across many\ngeneration tasks. Nevertheless, effectively aligning them with desired\nbehaviors remains a significant challenge. Activation steering is an effective\nand cost-efficient approach that directly modifies the activations of LLMs\nduring the inference stage, aligning their responses with the desired behaviors\nand avoiding the high cost of fine-tuning. Existing methods typically\nindiscriminately intervene to all generations or rely solely on the question to\ndetermine intervention, which limits the accurate assessment of the\nintervention strength. To this end, we propose the Flexible Activation Steering\nwith Backtracking (FASB) framework, which dynamically determines both the\nnecessity and strength of intervention by tracking the internal states of the\nLLMs during generation, considering both the question and the generated\ncontent. Since intervening after detecting a deviation from the desired\nbehavior is often too late, we further propose the backtracking mechanism to\ncorrect the deviated tokens and steer the LLMs toward the desired behavior.\nExtensive experiments on the TruthfulQA dataset and six multiple-choice\ndatasets demonstrate that our method outperforms baselines. Our code will be\nreleased at https://github.com/gjw185/FASB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable performance across many\ngeneration tasks. Nevertheless, effectively aligning them with desired\nbehaviors remains a significant challenge. Activation steering is an effective\nand cost-efficient approach that directly modifies the activations of LLMs\nduring the inference stage, aligning their responses with the desired behaviors\nand avoiding the high cost of fine-tuning. Existing methods typically\nindiscriminately intervene to all generations or rely solely on the question to\ndetermine intervention, which limits the accurate assessment of the\nintervention strength. To this end, we propose the Flexible Activation Steering\nwith Backtracking (FASB) framework, which dynamically determines both the\nnecessity and strength of intervention by tracking the internal states of the\nLLMs during generation, considering both the question and the generated\ncontent. Since intervening after detecting a deviation from the desired\nbehavior is often too late, we further propose the backtracking mechanism to\ncorrect the deviated tokens and steer the LLMs toward the desired behavior.\nExtensive experiments on the TruthfulQA dataset and six multiple-choice\ndatasets demonstrate that our method outperforms baselines. Our code will be\nreleased at https://github.com/gjw185/FASB."
                },
                "authors": [
                    {
                        "name": "Zifeng Cheng"
                    },
                    {
                        "name": "Jinwei Gan"
                    },
                    {
                        "name": "Zhiwei Jiang"
                    },
                    {
                        "name": "Cong Wang"
                    },
                    {
                        "name": "Yafeng Yin"
                    },
                    {
                        "name": "Xiang Luo"
                    },
                    {
                        "name": "Yuchen Fu"
                    },
                    {
                        "name": "Qing Gu"
                    }
                ],
                "author_detail": {
                    "name": "Qing Gu"
                },
                "author": "Qing Gu",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17621v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17621v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20511v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20511v2",
                "updated": "2025-10-01T14:30:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    14,
                    30,
                    41,
                    2,
                    274,
                    0
                ],
                "published": "2025-03-26T12:53:31Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    12,
                    53,
                    31,
                    2,
                    85,
                    0
                ],
                "title": "From reductionism to realism: Holistic mathematical modelling for\n  complex biological systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From reductionism to realism: Holistic mathematical modelling for\n  complex biological systems"
                },
                "summary": "At its core, the physics paradigm adopts a reductionist approach, aiming to\nunderstand fundamental phenomena by decomposing them into simpler, elementary\nprocesses. While this strategy has been tremendously successful in physics, it\nhas often fallen short in addressing fundamental questions in the biological\nsciences. This arises from the inherent complexity of biological systems,\ncharacterised by heterogeneity, polyfunctionality and interactions across\nspatiotemporal scales. Nevertheless, the traditional framework of complex\nsystems modelling falls short, as its emphasis on broad theoretical principles\nhas often failed to produce predictive, empirically-grounded insights. To\nadvance towards actionable mathematical models in biology, we argue, using\nneuroscience as a case study, that it is necessary to move beyond reductionist\napproaches and instead embrace the complexity of biological systems -\nleveraging the growing availability of high-resolution data and advances in\nhigh-performance computing. We advocate for a holistic mathematical modelling\nparadigm that harnesses rich representational structures such as annotated and\nmultilayer networks, employs agent-based models and simulation-based\napproaches, and focuses on the inverse problem of inferring system dynamics\nfrom observations. We emphasise that this approach is fully compatible with the\nsearch for fundamental biophysical principles, and highlight the potential it\nholds to drive progress in mathematical biology over the next two decades.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "At its core, the physics paradigm adopts a reductionist approach, aiming to\nunderstand fundamental phenomena by decomposing them into simpler, elementary\nprocesses. While this strategy has been tremendously successful in physics, it\nhas often fallen short in addressing fundamental questions in the biological\nsciences. This arises from the inherent complexity of biological systems,\ncharacterised by heterogeneity, polyfunctionality and interactions across\nspatiotemporal scales. Nevertheless, the traditional framework of complex\nsystems modelling falls short, as its emphasis on broad theoretical principles\nhas often failed to produce predictive, empirically-grounded insights. To\nadvance towards actionable mathematical models in biology, we argue, using\nneuroscience as a case study, that it is necessary to move beyond reductionist\napproaches and instead embrace the complexity of biological systems -\nleveraging the growing availability of high-resolution data and advances in\nhigh-performance computing. We advocate for a holistic mathematical modelling\nparadigm that harnesses rich representational structures such as annotated and\nmultilayer networks, employs agent-based models and simulation-based\napproaches, and focuses on the inverse problem of inferring system dynamics\nfrom observations. We emphasise that this approach is fully compatible with the\nsearch for fundamental biophysical principles, and highlight the potential it\nholds to drive progress in mathematical biology over the next two decades."
                },
                "authors": [
                    {
                        "name": "Ramón Nartallo-Kaluarachchi"
                    },
                    {
                        "name": "Renaud Lambiotte"
                    },
                    {
                        "name": "Alain Goriely"
                    }
                ],
                "author_detail": {
                    "name": "Alain Goriely"
                },
                "author": "Alain Goriely",
                "arxiv_comment": "14 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20511v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20511v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.bio-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10025v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10025v3",
                "updated": "2025-10-01T14:08:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    14,
                    8,
                    9,
                    2,
                    274,
                    0
                ],
                "published": "2024-07-13T23:03:13Z",
                "published_parsed": [
                    2024,
                    7,
                    13,
                    23,
                    3,
                    13,
                    5,
                    195,
                    0
                ],
                "title": "Badminton Birdie-Like Aerodynamic Alignment of Drifting Dust Grains by\n  Subsonic Gaseous Flows in Protoplanetary Disks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Badminton Birdie-Like Aerodynamic Alignment of Drifting Dust Grains by\n  Subsonic Gaseous Flows in Protoplanetary Disks"
                },
                "summary": "Recent (sub)millimeter polarization observations of protoplanetary disks\nreveal toroidally aligned, effectively prolate dust grains large enough (at\nleast ~100 $\\mu$m) to efficiently scatter millimeter light. The alignment\nmechanism for these grains remains unclear. We explore the possibility that gas\ndrag aligns grains through gas-dust relative motion when the grain's center of\nmass is offset from its geometric center, analogous to a badminton birdie's\nalignment in flight. A simple grain model of two non-identical spheres\nillustrates how a grain undergoes damped oscillations from flow-induced\nrestoring torques which align its geometric center in the flow direction\nrelative to its center of mass. Assuming specular reflection and subsonic flow,\nwe derive an analytical equation of motion for spheroids where the center of\nmass can be shifted away from the spheroid's geometric center. We show that a\nprolate or an oblate grain can be aligned with the long axis parallel to the\ngas flow when the center of mass is shifted along that axis. Both scenarios can\nexplain the required effectively prolate grains inferred from observations.\nApplication to a simple disk model shows that the alignment timescales are\nshorter than or comparable to the orbital time. The grain alignment direction\nin a disk depends on the disk (sub-)structure and grain Stokes number (St) with\nazimuthal alignment for large St grains in sub-Keplerian smooth gas disks and\nfor small St grains near the gas pressure extrema, such as rings and gaps.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent (sub)millimeter polarization observations of protoplanetary disks\nreveal toroidally aligned, effectively prolate dust grains large enough (at\nleast ~100 $\\mu$m) to efficiently scatter millimeter light. The alignment\nmechanism for these grains remains unclear. We explore the possibility that gas\ndrag aligns grains through gas-dust relative motion when the grain's center of\nmass is offset from its geometric center, analogous to a badminton birdie's\nalignment in flight. A simple grain model of two non-identical spheres\nillustrates how a grain undergoes damped oscillations from flow-induced\nrestoring torques which align its geometric center in the flow direction\nrelative to its center of mass. Assuming specular reflection and subsonic flow,\nwe derive an analytical equation of motion for spheroids where the center of\nmass can be shifted away from the spheroid's geometric center. We show that a\nprolate or an oblate grain can be aligned with the long axis parallel to the\ngas flow when the center of mass is shifted along that axis. Both scenarios can\nexplain the required effectively prolate grains inferred from observations.\nApplication to a simple disk model shows that the alignment timescales are\nshorter than or comparable to the orbital time. The grain alignment direction\nin a disk depends on the disk (sub-)structure and grain Stokes number (St) with\nazimuthal alignment for large St grains in sub-Keplerian smooth gas disks and\nfor small St grains near the gas pressure extrema, such as rings and gaps."
                },
                "authors": [
                    {
                        "name": "Zhe-Yu Daniel Lin"
                    },
                    {
                        "name": "Zhi-Yun Li"
                    },
                    {
                        "name": "Haifeng Yang"
                    },
                    {
                        "name": "Leslie W. Looney"
                    },
                    {
                        "name": "Ian W. Stephens"
                    },
                    {
                        "name": "Manuel Fernández-López"
                    },
                    {
                        "name": "Rachel E. Harrison"
                    }
                ],
                "author_detail": {
                    "name": "Rachel E. Harrison"
                },
                "author": "Rachel E. Harrison",
                "arxiv_comment": "21 pages, 12 figures, accepted by MNRAS. Fixed typo in Eq. 27 which\n  propagated to Eq. 30, 32, 33",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10025v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10025v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24866v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24866v2",
                "updated": "2025-10-01T14:06:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    14,
                    6,
                    17,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-29T14:50:18Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    50,
                    18,
                    0,
                    272,
                    0
                ],
                "title": "Metaphor identification using large language models: A comparison of\n  RAG, prompt engineering, and fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Metaphor identification using large language models: A comparison of\n  RAG, prompt engineering, and fine-tuning"
                },
                "summary": "Metaphor is a pervasive feature of discourse and a powerful lens for\nexamining cognition, emotion, and ideology. Large-scale analysis, however, has\nbeen constrained by the need for manual annotation due to the context-sensitive\nnature of metaphor. This study investigates the potential of large language\nmodels (LLMs) to automate metaphor identification in full texts. We compare\nthree methods: (i) retrieval-augmented generation (RAG), where the model is\nprovided with a codebook and instructed to annotate texts based on its rules\nand examples; (ii) prompt engineering, where we design task-specific verbal\ninstructions; and (iii) fine-tuning, where the model is trained on hand-coded\ntexts to optimize performance. Within prompt engineering, we test zero-shot,\nfew-shot, and chain-of-thought strategies. Our results show that\nstate-of-the-art closed-source LLMs can achieve high accuracy, with fine-tuning\nyielding a median F1 score of 0.79. A comparison of human and LLM outputs\nreveals that most discrepancies are systematic, reflecting well-known grey\nareas and conceptual challenges in metaphor theory. We propose that LLMs can be\nused to at least partly automate metaphor identification and can serve as a\ntestbed for developing and refining metaphor identification protocols and the\ntheory that underpins them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Metaphor is a pervasive feature of discourse and a powerful lens for\nexamining cognition, emotion, and ideology. Large-scale analysis, however, has\nbeen constrained by the need for manual annotation due to the context-sensitive\nnature of metaphor. This study investigates the potential of large language\nmodels (LLMs) to automate metaphor identification in full texts. We compare\nthree methods: (i) retrieval-augmented generation (RAG), where the model is\nprovided with a codebook and instructed to annotate texts based on its rules\nand examples; (ii) prompt engineering, where we design task-specific verbal\ninstructions; and (iii) fine-tuning, where the model is trained on hand-coded\ntexts to optimize performance. Within prompt engineering, we test zero-shot,\nfew-shot, and chain-of-thought strategies. Our results show that\nstate-of-the-art closed-source LLMs can achieve high accuracy, with fine-tuning\nyielding a median F1 score of 0.79. A comparison of human and LLM outputs\nreveals that most discrepancies are systematic, reflecting well-known grey\nareas and conceptual challenges in metaphor theory. We propose that LLMs can be\nused to at least partly automate metaphor identification and can serve as a\ntestbed for developing and refining metaphor identification protocols and the\ntheory that underpins them."
                },
                "authors": [
                    {
                        "name": "Matteo Fuoli"
                    },
                    {
                        "name": "Weihang Huang"
                    },
                    {
                        "name": "Jeannette Littlemore"
                    },
                    {
                        "name": "Sarah Turner"
                    },
                    {
                        "name": "Ellen Wilding"
                    }
                ],
                "author_detail": {
                    "name": "Ellen Wilding"
                },
                "author": "Ellen Wilding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24866v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24866v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25448v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25448v2",
                "updated": "2025-10-01T14:04:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    14,
                    4,
                    38,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-29T19:54:36Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    19,
                    54,
                    36,
                    0,
                    272,
                    0
                ],
                "title": "Fingerprinting LLMs via Prompt Injection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fingerprinting LLMs via Prompt Injection"
                },
                "summary": "Large language models (LLMs) are often modified after release through\npost-processing such as post-training or quantization, which makes it\nchallenging to determine whether one model is derived from another. Existing\nprovenance detection methods have two main limitations: (1) they embed signals\ninto the base model before release, which is infeasible for already published\nmodels, or (2) they compare outputs across models using hand-crafted or random\nprompts, which are not robust to post-processing. In this work, we propose\nLLMPrint, a novel detection framework that constructs fingerprints by\nexploiting LLMs' inherent vulnerability to prompt injection. Our key insight is\nthat by optimizing fingerprint prompts to enforce consistent token preferences,\nwe can obtain fingerprints that are both unique to the base model and robust to\npost-processing. We further develop a unified verification procedure that\napplies to both gray-box and black-box settings, with statistical guarantees.\nWe evaluate LLMPrint on five base models and around 700 post-trained or\nquantized variants. Our results show that LLMPrint achieves high true positive\nrates while keeping false positive rates near zero.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are often modified after release through\npost-processing such as post-training or quantization, which makes it\nchallenging to determine whether one model is derived from another. Existing\nprovenance detection methods have two main limitations: (1) they embed signals\ninto the base model before release, which is infeasible for already published\nmodels, or (2) they compare outputs across models using hand-crafted or random\nprompts, which are not robust to post-processing. In this work, we propose\nLLMPrint, a novel detection framework that constructs fingerprints by\nexploiting LLMs' inherent vulnerability to prompt injection. Our key insight is\nthat by optimizing fingerprint prompts to enforce consistent token preferences,\nwe can obtain fingerprints that are both unique to the base model and robust to\npost-processing. We further develop a unified verification procedure that\napplies to both gray-box and black-box settings, with statistical guarantees.\nWe evaluate LLMPrint on five base models and around 700 post-trained or\nquantized variants. Our results show that LLMPrint achieves high true positive\nrates while keeping false positive rates near zero."
                },
                "authors": [
                    {
                        "name": "Yuepeng Hu"
                    },
                    {
                        "name": "Zhengyuan Jiang"
                    },
                    {
                        "name": "Mengyuan Li"
                    },
                    {
                        "name": "Osama Ahmed"
                    },
                    {
                        "name": "Zhicong Huang"
                    },
                    {
                        "name": "Cheng Hong"
                    },
                    {
                        "name": "Neil Gong"
                    }
                ],
                "author_detail": {
                    "name": "Neil Gong"
                },
                "author": "Neil Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25448v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25448v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09199v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09199v2",
                "updated": "2025-10-01T14:01:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    14,
                    1,
                    30,
                    2,
                    274,
                    0
                ],
                "published": "2025-07-12T08:42:10Z",
                "published_parsed": [
                    2025,
                    7,
                    12,
                    8,
                    42,
                    10,
                    5,
                    193,
                    0
                ],
                "title": "Back to the Basics: Rethinking Issue-Commit Linking with LLM-Assisted\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Back to the Basics: Rethinking Issue-Commit Linking with LLM-Assisted\n  Retrieval"
                },
                "summary": "Issue-commit linking, which connects issues with commits that fix them, is\ncrucial for software maintenance. Existing approaches have shown promise in\nautomatically recovering these links. Evaluations of these techniques assess\ntheir ability to identify genuine links from plausible but false links.\nHowever, these evaluations overlook the fact that, in reality, when a\nrepository has more commits, the presence of more plausible yet unrelated\ncommits may interfere with the tool in differentiating the correct fix commits.\nTo address this, we propose the Realistic Distribution Setting (RDS) and use it\nto construct a more realistic evaluation dataset that includes 20 open-source\nprojects. By evaluating tools on this dataset, we observe that the performance\nof the state-of-the-art deep learning-based approach drops by more than half,\nwhile the traditional Information Retrieval method, VSM, outperforms it.\nInspired by these observations, we propose EasyLink, which utilizes a vector\ndatabase as a modern Information Retrieval technique. To address the\nlong-standing problem of the semantic gap between issues and commits, EasyLink\nleverages a large language model to rerank the commits retrieved from the\ndatabase. Under our evaluation, EasyLink achieves an average Precision@1 of\n75.03\\%, improving over the state-of-the-art by over four times. Additionally,\nthis paper provides practical guidelines for advancing research in issue-commit\nlink recovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Issue-commit linking, which connects issues with commits that fix them, is\ncrucial for software maintenance. Existing approaches have shown promise in\nautomatically recovering these links. Evaluations of these techniques assess\ntheir ability to identify genuine links from plausible but false links.\nHowever, these evaluations overlook the fact that, in reality, when a\nrepository has more commits, the presence of more plausible yet unrelated\ncommits may interfere with the tool in differentiating the correct fix commits.\nTo address this, we propose the Realistic Distribution Setting (RDS) and use it\nto construct a more realistic evaluation dataset that includes 20 open-source\nprojects. By evaluating tools on this dataset, we observe that the performance\nof the state-of-the-art deep learning-based approach drops by more than half,\nwhile the traditional Information Retrieval method, VSM, outperforms it.\nInspired by these observations, we propose EasyLink, which utilizes a vector\ndatabase as a modern Information Retrieval technique. To address the\nlong-standing problem of the semantic gap between issues and commits, EasyLink\nleverages a large language model to rerank the commits retrieved from the\ndatabase. Under our evaluation, EasyLink achieves an average Precision@1 of\n75.03\\%, improving over the state-of-the-art by over four times. Additionally,\nthis paper provides practical guidelines for advancing research in issue-commit\nlink recovery."
                },
                "authors": [
                    {
                        "name": "Huihui Huang"
                    },
                    {
                        "name": "Ratnadira Widyasari"
                    },
                    {
                        "name": "Ting Zhang"
                    },
                    {
                        "name": "Ivana Clairine Irsan"
                    },
                    {
                        "name": "Jieke Shi"
                    },
                    {
                        "name": "Han Wei Ang"
                    },
                    {
                        "name": "Frank Liauw"
                    },
                    {
                        "name": "Eng Lieh Ouh"
                    },
                    {
                        "name": "Lwin Khin Shar"
                    },
                    {
                        "name": "Hong Jin Kang"
                    },
                    {
                        "name": "David Lo"
                    }
                ],
                "author_detail": {
                    "name": "David Lo"
                },
                "author": "David Lo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09199v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09199v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22319v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22319v2",
                "updated": "2025-10-01T13:53:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    13,
                    53,
                    12,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-26T13:19:32Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    13,
                    19,
                    32,
                    4,
                    269,
                    0
                ],
                "title": "Progressive Weight Loading: Accelerating Initial Inference and Gradually\n  Boosting Performance on Resource-Constrained Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Progressive Weight Loading: Accelerating Initial Inference and Gradually\n  Boosting Performance on Resource-Constrained Environments"
                },
                "summary": "Deep learning models have become increasingly large and complex, resulting in\nhigher memory consumption and computational demands. Consequently, model\nloading times and initial inference latency have increased, posing significant\nchallenges in mobile and latency-sensitive environments where frequent model\nloading and unloading are required, which directly impacts user experience.\nWhile Knowledge Distillation (KD) offers a solution by compressing large\nteacher models into smaller student ones, it often comes at the cost of reduced\nperformance. To address this trade-off, we propose Progressive Weight Loading\n(PWL), a novel technique that enables fast initial inference by first deploying\na lightweight student model, then incrementally replacing its layers with those\nof a pre-trained teacher model. To support seamless layer substitution, we\nintroduce a training method that not only aligns intermediate feature\nrepresentations between student and teacher layers, but also improves the\noverall output performance of the student model. Our experiments on VGG,\nResNet, and ViT architectures demonstrate that models trained with PWL maintain\ncompetitive distillation performance and gradually improve accuracy as teacher\nlayers are loaded-matching the final accuracy of the full teacher model without\ncompromising initial inference speed. This makes PWL particularly suited for\ndynamic, resource-constrained deployments where both responsiveness and\nperformance are critical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning models have become increasingly large and complex, resulting in\nhigher memory consumption and computational demands. Consequently, model\nloading times and initial inference latency have increased, posing significant\nchallenges in mobile and latency-sensitive environments where frequent model\nloading and unloading are required, which directly impacts user experience.\nWhile Knowledge Distillation (KD) offers a solution by compressing large\nteacher models into smaller student ones, it often comes at the cost of reduced\nperformance. To address this trade-off, we propose Progressive Weight Loading\n(PWL), a novel technique that enables fast initial inference by first deploying\na lightweight student model, then incrementally replacing its layers with those\nof a pre-trained teacher model. To support seamless layer substitution, we\nintroduce a training method that not only aligns intermediate feature\nrepresentations between student and teacher layers, but also improves the\noverall output performance of the student model. Our experiments on VGG,\nResNet, and ViT architectures demonstrate that models trained with PWL maintain\ncompetitive distillation performance and gradually improve accuracy as teacher\nlayers are loaded-matching the final accuracy of the full teacher model without\ncompromising initial inference speed. This makes PWL particularly suited for\ndynamic, resource-constrained deployments where both responsiveness and\nperformance are critical."
                },
                "authors": [
                    {
                        "name": "Hyunwoo Kim"
                    },
                    {
                        "name": "Junha Lee"
                    },
                    {
                        "name": "Mincheol Choi"
                    },
                    {
                        "name": "Jeonghwan Lee"
                    },
                    {
                        "name": "Jaeshin Cho"
                    }
                ],
                "author_detail": {
                    "name": "Jaeshin Cho"
                },
                "author": "Jaeshin Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22319v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22319v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11072v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11072v2",
                "updated": "2025-10-01T13:46:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    13,
                    46,
                    3,
                    2,
                    274,
                    0
                ],
                "published": "2025-02-16T10:42:37Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    10,
                    42,
                    37,
                    6,
                    47,
                    0
                ],
                "title": "Box Confidence Depth: simulation-based inference with hyper-rectangles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Box Confidence Depth: simulation-based inference with hyper-rectangles"
                },
                "summary": "This work presents a novel simulation-based approach for constructing\nconfidence regions in parametric models, which is particularly suited for\ngenerative models and situations where limited data and conventional asymptotic\napproximations fail to provide accurate results. The method leverages the\nconcept of data depth and depends on creating random hyper-rectangles, i.e.\nboxes, in the sample space generated through simulations from the model,\nvarying the input parameters. A probabilistic acceptance rule allows to\nretrieve a Depth-Confidence Distribution for the model parameters from which\npoint estimators as well as calibrated confidence sets can be read-off. The\nmethod is designed to address cases where both the parameters and test\nstatistics are multivariate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents a novel simulation-based approach for constructing\nconfidence regions in parametric models, which is particularly suited for\ngenerative models and situations where limited data and conventional asymptotic\napproximations fail to provide accurate results. The method leverages the\nconcept of data depth and depends on creating random hyper-rectangles, i.e.\nboxes, in the sample space generated through simulations from the model,\nvarying the input parameters. A probabilistic acceptance rule allows to\nretrieve a Depth-Confidence Distribution for the model parameters from which\npoint estimators as well as calibrated confidence sets can be read-off. The\nmethod is designed to address cases where both the parameters and test\nstatistics are multivariate."
                },
                "authors": [
                    {
                        "name": "Elena Bortolato"
                    },
                    {
                        "name": "Laura Ventura"
                    }
                ],
                "author_detail": {
                    "name": "Laura Ventura"
                },
                "author": "Laura Ventura",
                "arxiv_comment": "21 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11072v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11072v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21027v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21027v2",
                "updated": "2025-10-01T13:30:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    13,
                    30,
                    14,
                    2,
                    274,
                    0
                ],
                "published": "2025-06-26T06:09:40Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    6,
                    9,
                    40,
                    3,
                    177,
                    0
                ],
                "title": "Simultaneous estimation of the effective reproduction number and the\n  time series of daily infections: Application to Covid-19",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous estimation of the effective reproduction number and the\n  time series of daily infections: Application to Covid-19"
                },
                "summary": "The time varying effective reproduction number is an important parameter for\ncommunication and policy decisions during an epidemic. In this paper, we\npresent new statistical methods for estimating the reproduction number based on\nthe popular model of \\citet{cori2013new} which defines the effective\nreproduction number based on self-exciting dynamics of new infections. Such a\nmodel is conceptually simple and less susceptible to misspecifications than\nmore complicated multi-compartment models. However, statistical inference is\nchallenging, and the previous literature has either relied on proxy data and/or\na two-step approach in which the number of infections are first estimated. In\ncontrast, we present a coherent Bayesian method that approximates the joint\nposterior of daily new infections and reproduction numbers using a novel Markov\nchain Monte Carlo (MCMC) algorithm. Comparing our method to the\nstate-of-the-art three-step estimation procedure of\n\\citet{huisman2022estimation}, both using daily confirmed cases from\nSwitzerland in the Covid-19 epidemic and simulated data, we find that our\nmethod is more accurate in terms of point estimates and uncertainty\nquantification, especially near the beginning and end of an observation period.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The time varying effective reproduction number is an important parameter for\ncommunication and policy decisions during an epidemic. In this paper, we\npresent new statistical methods for estimating the reproduction number based on\nthe popular model of \\citet{cori2013new} which defines the effective\nreproduction number based on self-exciting dynamics of new infections. Such a\nmodel is conceptually simple and less susceptible to misspecifications than\nmore complicated multi-compartment models. However, statistical inference is\nchallenging, and the previous literature has either relied on proxy data and/or\na two-step approach in which the number of infections are first estimated. In\ncontrast, we present a coherent Bayesian method that approximates the joint\nposterior of daily new infections and reproduction numbers using a novel Markov\nchain Monte Carlo (MCMC) algorithm. Comparing our method to the\nstate-of-the-art three-step estimation procedure of\n\\citet{huisman2022estimation}, both using daily confirmed cases from\nSwitzerland in the Covid-19 epidemic and simulated data, we find that our\nmethod is more accurate in terms of point estimates and uncertainty\nquantification, especially near the beginning and end of an observation period."
                },
                "authors": [
                    {
                        "name": "Hans R. Künsch"
                    },
                    {
                        "name": "Fabio Sigrist"
                    }
                ],
                "author_detail": {
                    "name": "Fabio Sigrist"
                },
                "author": "Fabio Sigrist",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21027v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21027v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18553v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18553v2",
                "updated": "2025-10-01T13:25:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    13,
                    25,
                    12,
                    2,
                    274,
                    0
                ],
                "published": "2025-07-24T16:22:18Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    22,
                    18,
                    3,
                    205,
                    0
                ],
                "title": "The Geometry of LLM Quantization: GPTQ as Babai's Nearest Plane\n  Algorithm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Geometry of LLM Quantization: GPTQ as Babai's Nearest Plane\n  Algorithm"
                },
                "summary": "Quantizing the weights of large language models (LLMs) from 16-bit to lower\nbitwidth is the de facto approach to deploy massive transformers onto more\naffordable accelerators. While GPTQ emerged as one of the standard methods for\none-shot post-training quantization at LLM scale, its inner workings are\ndescribed as a sequence of ad-hoc algebraic updates that obscure geometric\nmeaning or worst-case guarantees. In this work, we show that, when executed\nback-to-front (from the last to first dimension) for a linear layer, GPTQ is\nmathematically identical to Babai's nearest plane algorithm for the classical\nclosest vector problem (CVP) on a lattice defined by the Hessian matrix of the\nlayer's inputs. This equivalence is based on a sophisticated mathematical\nargument, and has two analytical consequences: first, the GPTQ error\npropagation step gains an intuitive geometric interpretation; second, GPTQ\ninherits the error upper bound of Babai's algorithm under the assumption that\nno weights are clipped. Leveraging this bound, we design post-training\nquantization methods that avoid clipping, and outperform the original GPTQ. In\naddition, we provide efficient GPU inference kernels for the resulting\nrepresentation. Taken together, these results place GPTQ on a firm theoretical\nfooting and open the door to importing decades of progress in lattice\nalgorithms towards the design of future quantization algorithms for\nbillion-parameter models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantizing the weights of large language models (LLMs) from 16-bit to lower\nbitwidth is the de facto approach to deploy massive transformers onto more\naffordable accelerators. While GPTQ emerged as one of the standard methods for\none-shot post-training quantization at LLM scale, its inner workings are\ndescribed as a sequence of ad-hoc algebraic updates that obscure geometric\nmeaning or worst-case guarantees. In this work, we show that, when executed\nback-to-front (from the last to first dimension) for a linear layer, GPTQ is\nmathematically identical to Babai's nearest plane algorithm for the classical\nclosest vector problem (CVP) on a lattice defined by the Hessian matrix of the\nlayer's inputs. This equivalence is based on a sophisticated mathematical\nargument, and has two analytical consequences: first, the GPTQ error\npropagation step gains an intuitive geometric interpretation; second, GPTQ\ninherits the error upper bound of Babai's algorithm under the assumption that\nno weights are clipped. Leveraging this bound, we design post-training\nquantization methods that avoid clipping, and outperform the original GPTQ. In\naddition, we provide efficient GPU inference kernels for the resulting\nrepresentation. Taken together, these results place GPTQ on a firm theoretical\nfooting and open the door to importing decades of progress in lattice\nalgorithms towards the design of future quantization algorithms for\nbillion-parameter models."
                },
                "authors": [
                    {
                        "name": "Jiale Chen"
                    },
                    {
                        "name": "Yalda Shabanzadeh"
                    },
                    {
                        "name": "Elvir Crnčević"
                    },
                    {
                        "name": "Torsten Hoefler"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18553v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18553v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05724v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05724v2",
                "updated": "2025-10-01T13:23:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    13,
                    23,
                    12,
                    2,
                    274,
                    0
                ],
                "published": "2025-02-17T19:05:55Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    19,
                    5,
                    55,
                    0,
                    48,
                    0
                ],
                "title": "Addressing Moral Uncertainty using Large Language Models for Ethical\n  Decision-Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing Moral Uncertainty using Large Language Models for Ethical\n  Decision-Making"
                },
                "summary": "We present an ethical decision-making framework that refines a pre-trained\nreinforcement learning (RL) model using a task-agnostic ethical layer.\nFollowing initial training, the RL model undergoes ethical fine-tuning, where\nhuman feedback is replaced by feedback generated from a large language model\n(LLM). The LLM embodies consequentialist, deontological, virtue, social\njustice, and care ethics as moral principles to assign belief values to\nrecommended actions during ethical decision-making. An ethical layer aggregates\nbelief scores from multiple LLM-derived moral perspectives using Belief\nJensen-Shannon Divergence and Dempster-Shafer Theory into probability scores\nthat also serve as the shaping reward, steering the agent toward choices that\nalign with a balanced ethical framework. This integrated learning framework\nhelps the RL agent navigate moral uncertainty in complex environments and\nenables it to make morally sound decisions across diverse tasks. Our approach,\ntested across different LLM variants and compared with other belief aggregation\ntechniques, demonstrates improved consistency, adaptability, and reduced\nreliance on handcrafted ethical rewards. This method is especially effective in\ndynamic scenarios where ethical challenges arise unexpectedly, making it\nwell-suited for real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an ethical decision-making framework that refines a pre-trained\nreinforcement learning (RL) model using a task-agnostic ethical layer.\nFollowing initial training, the RL model undergoes ethical fine-tuning, where\nhuman feedback is replaced by feedback generated from a large language model\n(LLM). The LLM embodies consequentialist, deontological, virtue, social\njustice, and care ethics as moral principles to assign belief values to\nrecommended actions during ethical decision-making. An ethical layer aggregates\nbelief scores from multiple LLM-derived moral perspectives using Belief\nJensen-Shannon Divergence and Dempster-Shafer Theory into probability scores\nthat also serve as the shaping reward, steering the agent toward choices that\nalign with a balanced ethical framework. This integrated learning framework\nhelps the RL agent navigate moral uncertainty in complex environments and\nenables it to make morally sound decisions across diverse tasks. Our approach,\ntested across different LLM variants and compared with other belief aggregation\ntechniques, demonstrates improved consistency, adaptability, and reduced\nreliance on handcrafted ethical rewards. This method is especially effective in\ndynamic scenarios where ethical challenges arise unexpectedly, making it\nwell-suited for real-world applications."
                },
                "authors": [
                    {
                        "name": "Rohit K. Dubey"
                    },
                    {
                        "name": "Damian Dailisan"
                    },
                    {
                        "name": "Sachit Mahajan"
                    }
                ],
                "author_detail": {
                    "name": "Sachit Mahajan"
                },
                "author": "Sachit Mahajan",
                "arxiv_comment": "13 pages, 5 figures. All authors contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05724v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05724v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08591v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08591v3",
                "updated": "2025-10-01T13:22:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    13,
                    22,
                    34,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-10T13:45:58Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    13,
                    45,
                    58,
                    2,
                    253,
                    0
                ],
                "title": "Functional Regression with Nonstationarity and Error Contamination:\n  Application to the Economic Impact of Climate Change",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Functional Regression with Nonstationarity and Error Contamination:\n  Application to the Economic Impact of Climate Change"
                },
                "summary": "This paper studies a regression model with functional dependent and\nexplanatory variables, both of which exhibit nonstationary dynamics. The model\nassumes that the nonstationary stochastic trends of the dependent variable are\nexplained by those of the explanatory variables, and hence that there exists a\nstable long-run relationship between the two variables despite their\nnonstationary behavior. We also assume that the functional observations may be\nerror-contaminated. We develop novel autocovariance-based estimation and\ninference methods for this model. The methodology is broadly applicable to\neconomic and statistical functional time series with nonstationary dynamics. To\nillustrate our methodology and its usefulness, we apply it to evaluating the\nglobal economic impact of climate change, an issue of intrinsic importance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper studies a regression model with functional dependent and\nexplanatory variables, both of which exhibit nonstationary dynamics. The model\nassumes that the nonstationary stochastic trends of the dependent variable are\nexplained by those of the explanatory variables, and hence that there exists a\nstable long-run relationship between the two variables despite their\nnonstationary behavior. We also assume that the functional observations may be\nerror-contaminated. We develop novel autocovariance-based estimation and\ninference methods for this model. The methodology is broadly applicable to\neconomic and statistical functional time series with nonstationary dynamics. To\nillustrate our methodology and its usefulness, we apply it to evaluating the\nglobal economic impact of climate change, an issue of intrinsic importance."
                },
                "authors": [
                    {
                        "name": "Kyungsik Nam"
                    },
                    {
                        "name": "Won-Ki Seo"
                    }
                ],
                "author_detail": {
                    "name": "Won-Ki Seo"
                },
                "author": "Won-Ki Seo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08591v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08591v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62M10, 62R10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22937v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22937v2",
                "updated": "2025-10-01T13:07:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    13,
                    7,
                    56,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-26T21:12:32Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    21,
                    12,
                    32,
                    4,
                    269,
                    0
                ],
                "title": "DBF-MA: A Differential Bayesian Filtering Planner for Multi-Agent\n  Autonomous Racing Overtakes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DBF-MA: A Differential Bayesian Filtering Planner for Multi-Agent\n  Autonomous Racing Overtakes"
                },
                "summary": "A significant challenge in autonomous racing is to generate overtaking\nmaneuvers. Racing agents must execute these maneuvers on complex racetracks\nwith little room for error. Optimization techniques and graph-based methods\nhave been proposed, but these methods often rely on oversimplified assumptions\nfor collision-avoidance and dynamic constraints. In this work, we present an\napproach to trajectory synthesis based on an extension of the Differential\nBayesian Filtering framework. Our approach for collision-free trajectory\nsynthesis frames the problem as one of Bayesian Inference over the space of\nComposite Bezier Curves. Our method is derivative-free, does not require a\nspherical approximation of the vehicle footprint, linearization of constraints,\nor simplifying upper bounds on collision avoidance. We conduct a closed-loop\nanalysis of DBF-MA and find it successfully overtakes an opponent in 87% of\ntested scenarios, outperforming existing methods in autonomous overtaking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A significant challenge in autonomous racing is to generate overtaking\nmaneuvers. Racing agents must execute these maneuvers on complex racetracks\nwith little room for error. Optimization techniques and graph-based methods\nhave been proposed, but these methods often rely on oversimplified assumptions\nfor collision-avoidance and dynamic constraints. In this work, we present an\napproach to trajectory synthesis based on an extension of the Differential\nBayesian Filtering framework. Our approach for collision-free trajectory\nsynthesis frames the problem as one of Bayesian Inference over the space of\nComposite Bezier Curves. Our method is derivative-free, does not require a\nspherical approximation of the vehicle footprint, linearization of constraints,\nor simplifying upper bounds on collision avoidance. We conduct a closed-loop\nanalysis of DBF-MA and find it successfully overtakes an opponent in 87% of\ntested scenarios, outperforming existing methods in autonomous overtaking."
                },
                "authors": [
                    {
                        "name": "Trent Weiss"
                    },
                    {
                        "name": "Amar Kulkarni"
                    },
                    {
                        "name": "Madhur Behl"
                    }
                ],
                "author_detail": {
                    "name": "Madhur Behl"
                },
                "author": "Madhur Behl",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22937v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22937v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26184v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26184v2",
                "updated": "2025-10-01T13:05:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    13,
                    5,
                    17,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-30T12:41:11Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    12,
                    41,
                    11,
                    1,
                    273,
                    0
                ],
                "title": "Auto-ARGUE: LLM-Based Report Generation Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-ARGUE: LLM-Based Report Generation Evaluation"
                },
                "summary": "Generation of long-form, citation-backed reports is a primary use case for\nretrieval augmented generation (RAG) systems. While open-source evaluation\ntools exist for various RAG tasks, ones tailored to report generation are\nlacking. Accordingly, we introduce Auto-ARGUE, a robust LLM-based\nimplementation of the recent ARGUE framework for report generation evaluation.\nWe present analysis of Auto-ARGUE on the report generation pilot task from the\nTREC 2024 NeuCLIR track, showing good system-level correlations with human\njudgments. We further release a web app for visualization of Auto-ARGUE\noutputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generation of long-form, citation-backed reports is a primary use case for\nretrieval augmented generation (RAG) systems. While open-source evaluation\ntools exist for various RAG tasks, ones tailored to report generation are\nlacking. Accordingly, we introduce Auto-ARGUE, a robust LLM-based\nimplementation of the recent ARGUE framework for report generation evaluation.\nWe present analysis of Auto-ARGUE on the report generation pilot task from the\nTREC 2024 NeuCLIR track, showing good system-level correlations with human\njudgments. We further release a web app for visualization of Auto-ARGUE\noutputs."
                },
                "authors": [
                    {
                        "name": "William Walden"
                    },
                    {
                        "name": "Marc Mason"
                    },
                    {
                        "name": "Orion Weller"
                    },
                    {
                        "name": "Laura Dietz"
                    },
                    {
                        "name": "Hannah Recknor"
                    },
                    {
                        "name": "Bryan Li"
                    },
                    {
                        "name": "Gabrielle Kaili-May Liu"
                    },
                    {
                        "name": "Yu Hou"
                    },
                    {
                        "name": "James Mayfield"
                    },
                    {
                        "name": "Eugene Yang"
                    }
                ],
                "author_detail": {
                    "name": "Eugene Yang"
                },
                "author": "Eugene Yang",
                "arxiv_comment": "ECIR 2025 demo format",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26184v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26184v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26368v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26368v2",
                "updated": "2025-10-01T13:02:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    13,
                    2,
                    51,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-30T15:04:24Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    4,
                    24,
                    1,
                    273,
                    0
                ],
                "title": "Introducing Large Language Models into the Design Flow of Time Sensitive\n  Networking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Introducing Large Language Models into the Design Flow of Time Sensitive\n  Networking"
                },
                "summary": "The growing demand for real-time, safety-critical systems has significantly\nincreased both the adoption and complexity of Time Sensitive Networking (TSN).\nConfiguring an optimized TSN network is highly challenging, requiring careful\nplanning, design, verification, validation, and deployment. Large Language\nModels (LLMs) have recently demonstrated strong capabilities in solving complex\ntasks, positioning them as promising candidates for automating end-to-end TSN\ndeployment, referred to as TSN orchestration. This paper outlines the steps\ninvolved in TSN orchestration and the associated challenges. To assess the\ncapabilities of existing LLM models, we conduct an initial proof-of-concept\ncase study focused on TSN configuration across multiple models. Building on\nthese insights, we propose an LLM-assisted orchestration framework. Unlike\nprior research on LLMs in computer networks, which has concentrated on general\nconfiguration and management, TSN-specific orchestration has not yet been\ninvestigated. We present the building blocks for automating TSN using LLMs,\ndescribe the proposed pipeline, and analyze opportunities and limitations for\nreal-world deployment. Finally, we highlight key challenges and research\ndirections, including the development of TSN-focused datasets, standardized\nbenchmark suites, and the integration of external tools such as Network\nCalculus (NC) engines and simulators. This work provides the first roadmap\ntoward assessing the feasibility of LLM-assisted TSN orchestration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing demand for real-time, safety-critical systems has significantly\nincreased both the adoption and complexity of Time Sensitive Networking (TSN).\nConfiguring an optimized TSN network is highly challenging, requiring careful\nplanning, design, verification, validation, and deployment. Large Language\nModels (LLMs) have recently demonstrated strong capabilities in solving complex\ntasks, positioning them as promising candidates for automating end-to-end TSN\ndeployment, referred to as TSN orchestration. This paper outlines the steps\ninvolved in TSN orchestration and the associated challenges. To assess the\ncapabilities of existing LLM models, we conduct an initial proof-of-concept\ncase study focused on TSN configuration across multiple models. Building on\nthese insights, we propose an LLM-assisted orchestration framework. Unlike\nprior research on LLMs in computer networks, which has concentrated on general\nconfiguration and management, TSN-specific orchestration has not yet been\ninvestigated. We present the building blocks for automating TSN using LLMs,\ndescribe the proposed pipeline, and analyze opportunities and limitations for\nreal-world deployment. Finally, we highlight key challenges and research\ndirections, including the development of TSN-focused datasets, standardized\nbenchmark suites, and the integration of external tools such as Network\nCalculus (NC) engines and simulators. This work provides the first roadmap\ntoward assessing the feasibility of LLM-assisted TSN orchestration."
                },
                "authors": [
                    {
                        "name": "Rubi Debnath"
                    },
                    {
                        "name": "Luxi Zhao"
                    },
                    {
                        "name": "Mohammadreza Barzegaran"
                    },
                    {
                        "name": "Sebastian Steinhorst"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Steinhorst"
                },
                "author": "Sebastian Steinhorst",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26368v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26368v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17052v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17052v4",
                "updated": "2025-10-01T12:49:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    12,
                    49,
                    54,
                    2,
                    274,
                    0
                ],
                "published": "2024-12-22T15:05:30Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    15,
                    5,
                    30,
                    6,
                    357,
                    0
                ],
                "title": "ViLBias: Detecting and Reasoning about Bias in Multimodal Content",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ViLBias: Detecting and Reasoning about Bias in Multimodal Content"
                },
                "summary": "Detecting bias in multimodal news requires models that reason over\ntext--image pairs, not just classify text. In response, we present ViLBias, a\nVQA-style benchmark and framework for detecting and reasoning about bias in\nmultimodal news. The dataset comprises 40,945 text--image pairs from diverse\noutlets, each annotated with a bias label and concise rationale using a\ntwo-stage LLM-as-annotator pipeline with hierarchical majority voting and\nhuman-in-the-loop validation. We evaluate Small Language Models (SLMs), Large\nLanguage Models (LLMs), and Vision--Language Models (VLMs) across closed-ended\nclassification and open-ended reasoning (oVQA), and compare parameter-efficient\ntuning strategies. Results show that incorporating images alongside text\nimproves detection accuracy by 3--5\\%, and that LLMs/VLMs better capture subtle\nframing and text--image inconsistencies than SLMs. Parameter-efficient methods\n(LoRA/QLoRA/Adapters) recover 97--99\\% of full fine-tuning performance with\n$<5\\%$ trainable parameters. For oVQA, reasoning accuracy spans 52--79\\% and\nfaithfulness 68--89\\%, both improved by instruction tuning; closed accuracy\ncorrelates strongly with reasoning ($r = 0.91$). ViLBias offers a scalable\nbenchmark and strong baselines for multimodal bias detection and rationale\nquality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting bias in multimodal news requires models that reason over\ntext--image pairs, not just classify text. In response, we present ViLBias, a\nVQA-style benchmark and framework for detecting and reasoning about bias in\nmultimodal news. The dataset comprises 40,945 text--image pairs from diverse\noutlets, each annotated with a bias label and concise rationale using a\ntwo-stage LLM-as-annotator pipeline with hierarchical majority voting and\nhuman-in-the-loop validation. We evaluate Small Language Models (SLMs), Large\nLanguage Models (LLMs), and Vision--Language Models (VLMs) across closed-ended\nclassification and open-ended reasoning (oVQA), and compare parameter-efficient\ntuning strategies. Results show that incorporating images alongside text\nimproves detection accuracy by 3--5\\%, and that LLMs/VLMs better capture subtle\nframing and text--image inconsistencies than SLMs. Parameter-efficient methods\n(LoRA/QLoRA/Adapters) recover 97--99\\% of full fine-tuning performance with\n$<5\\%$ trainable parameters. For oVQA, reasoning accuracy spans 52--79\\% and\nfaithfulness 68--89\\%, both improved by instruction tuning; closed accuracy\ncorrelates strongly with reasoning ($r = 0.91$). ViLBias offers a scalable\nbenchmark and strong baselines for multimodal bias detection and rationale\nquality."
                },
                "authors": [
                    {
                        "name": "Shaina Raza"
                    },
                    {
                        "name": "Caesar Saleh"
                    },
                    {
                        "name": "Azib Farooq"
                    },
                    {
                        "name": "Emrul Hasan"
                    },
                    {
                        "name": "Franklin Ogidi"
                    },
                    {
                        "name": "Maximus Powers"
                    },
                    {
                        "name": "Veronica Chatrath"
                    },
                    {
                        "name": "Marcelo Lotif"
                    },
                    {
                        "name": "Karanpal Sekhon"
                    },
                    {
                        "name": "Roya Javadi"
                    },
                    {
                        "name": "Haad Zahid"
                    },
                    {
                        "name": "Anam Zahid"
                    },
                    {
                        "name": "Vahid Reza Khazaie"
                    },
                    {
                        "name": "Zhenyu Yu"
                    }
                ],
                "author_detail": {
                    "name": "Zhenyu Yu"
                },
                "author": "Zhenyu Yu",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17052v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17052v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13667v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13667v4",
                "updated": "2025-10-01T12:46:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    12,
                    46,
                    1,
                    2,
                    274,
                    0
                ],
                "published": "2025-05-19T19:12:29Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    19,
                    12,
                    29,
                    0,
                    139,
                    0
                ],
                "title": "Adaptive Diffusion Constrained Sampling for Bimanual Robot Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Diffusion Constrained Sampling for Bimanual Robot Manipulation"
                },
                "summary": "Coordinated multi-arm manipulation requires satisfying multiple simultaneous\ngeometric constraints across high-dimensional configuration spaces, which poses\na significant challenge for traditional planning and control methods. In this\nwork, we propose Adaptive Diffusion Constrained Sampling (ADCS), a generative\nframework that flexibly integrates both equality (e.g., relative and absolute\npose constraints) and structured inequality constraints (e.g., proximity to\nobject surfaces) into an energy-based diffusion model. Equality constraints are\nmodeled using dedicated energy networks trained on pose differences in Lie\nalgebra space, while inequality constraints are represented via Signed Distance\nFunctions (SDFs) and encoded into learned constraint embeddings, allowing the\nmodel to reason about complex spatial regions. A key innovation of our method\nis a Transformer-based architecture that learns to weight constraint-specific\nenergy functions at inference time, enabling flexible and context-aware\nconstraint integration. Moreover, we adopt a two-phase sampling strategy that\nimproves precision and sample diversity by combining Langevin dynamics with\nresampling and density-aware re-weighting. Experimental results on dual-arm\nmanipulation tasks show that ADCS significantly improves sample diversity and\ngeneralization across settings demanding precise coordination and adaptive\nconstraint handling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coordinated multi-arm manipulation requires satisfying multiple simultaneous\ngeometric constraints across high-dimensional configuration spaces, which poses\na significant challenge for traditional planning and control methods. In this\nwork, we propose Adaptive Diffusion Constrained Sampling (ADCS), a generative\nframework that flexibly integrates both equality (e.g., relative and absolute\npose constraints) and structured inequality constraints (e.g., proximity to\nobject surfaces) into an energy-based diffusion model. Equality constraints are\nmodeled using dedicated energy networks trained on pose differences in Lie\nalgebra space, while inequality constraints are represented via Signed Distance\nFunctions (SDFs) and encoded into learned constraint embeddings, allowing the\nmodel to reason about complex spatial regions. A key innovation of our method\nis a Transformer-based architecture that learns to weight constraint-specific\nenergy functions at inference time, enabling flexible and context-aware\nconstraint integration. Moreover, we adopt a two-phase sampling strategy that\nimproves precision and sample diversity by combining Langevin dynamics with\nresampling and density-aware re-weighting. Experimental results on dual-arm\nmanipulation tasks show that ADCS significantly improves sample diversity and\ngeneralization across settings demanding precise coordination and adaptive\nconstraint handling."
                },
                "authors": [
                    {
                        "name": "Haolei Tong"
                    },
                    {
                        "name": "Yuezhe Zhang"
                    },
                    {
                        "name": "Sophie Lueth"
                    },
                    {
                        "name": "Georgia Chalvatzaki"
                    }
                ],
                "author_detail": {
                    "name": "Georgia Chalvatzaki"
                },
                "author": "Georgia Chalvatzaki",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13667v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13667v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17324v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17324v2",
                "updated": "2025-10-01T12:29:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    12,
                    29,
                    27,
                    2,
                    274,
                    0
                ],
                "published": "2025-08-24T12:11:21Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    12,
                    11,
                    21,
                    6,
                    236,
                    0
                ],
                "title": "CultranAI at PalmX 2025: Data Augmentation for Cultural Knowledge\n  Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CultranAI at PalmX 2025: Data Augmentation for Cultural Knowledge\n  Representation"
                },
                "summary": "In this paper, we report our participation to the PalmX cultural evaluation\nshared task. Our system, CultranAI, focused on data augmentation and LoRA\nfine-tuning of large language models (LLMs) for Arabic cultural knowledge\nrepresentation. We benchmarked several LLMs to identify the best-performing\nmodel for the task. In addition to utilizing the PalmX dataset, we augmented it\nby incorporating the Palm dataset and curated a new dataset of over 22K\nculturally grounded multiple-choice questions (MCQs). Our experiments showed\nthat the Fanar-1-9B-Instruct model achieved the highest performance. We\nfine-tuned this model on the combined augmented dataset of 22K+ MCQs. On the\nblind test set, our submitted system ranked 5th with an accuracy of 70.50%,\nwhile on the PalmX development set, it achieved an accuracy of 84.1%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we report our participation to the PalmX cultural evaluation\nshared task. Our system, CultranAI, focused on data augmentation and LoRA\nfine-tuning of large language models (LLMs) for Arabic cultural knowledge\nrepresentation. We benchmarked several LLMs to identify the best-performing\nmodel for the task. In addition to utilizing the PalmX dataset, we augmented it\nby incorporating the Palm dataset and curated a new dataset of over 22K\nculturally grounded multiple-choice questions (MCQs). Our experiments showed\nthat the Fanar-1-9B-Instruct model achieved the highest performance. We\nfine-tuned this model on the combined augmented dataset of 22K+ MCQs. On the\nblind test set, our submitted system ranked 5th with an accuracy of 70.50%,\nwhile on the PalmX development set, it achieved an accuracy of 84.1%."
                },
                "authors": [
                    {
                        "name": "Hunzalah Hassan Bhatti"
                    },
                    {
                        "name": "Youssef Ahmed"
                    },
                    {
                        "name": "Md Arid Hasan"
                    },
                    {
                        "name": "Firoj Alam"
                    }
                ],
                "author_detail": {
                    "name": "Firoj Alam"
                },
                "author": "Firoj Alam",
                "arxiv_comment": "LLMs, Native, Arabic LLMs, Augmentation, Multilingual, Language\n  Diversity, Contextual Understanding, Minority Languages, Culturally Informed,\n  Foundation Models, Large Language Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17324v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17324v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16495v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16495v2",
                "updated": "2025-10-01T12:27:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    12,
                    27,
                    37,
                    2,
                    274,
                    0
                ],
                "published": "2025-08-22T16:17:31Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    16,
                    17,
                    31,
                    4,
                    234,
                    0
                ],
                "title": "Post Hoc Regression Refinement via Pairwise Rankings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post Hoc Regression Refinement via Pairwise Rankings"
                },
                "summary": "Accurate prediction of continuous properties is essential to many scientific\nand engineering tasks. Although deep-learning regressors excel with abundant\nlabels, their accuracy deteriorates in data-scarce regimes. We introduce\nRankRefine, a model-agnostic, plug-and-play post hoc method that refines\nregression with expert knowledge coming from pairwise rankings. Given a query\nitem and a small reference set with known properties, RankRefine combines the\nbase regressor's output with a rank-based estimate via inverse variance\nweighting, requiring no retraining. In molecular property prediction task,\nRankRefine achieves up to 10% relative reduction in mean absolute error using\nonly 20 pairwise comparisons obtained through a general-purpose large language\nmodel (LLM) with no finetuning. As rankings provided by human experts or\ngeneral-purpose LLMs are sufficient for improving regression across diverse\ndomains, RankRefine offers practicality and broad applicability, especially in\nlow-data settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate prediction of continuous properties is essential to many scientific\nand engineering tasks. Although deep-learning regressors excel with abundant\nlabels, their accuracy deteriorates in data-scarce regimes. We introduce\nRankRefine, a model-agnostic, plug-and-play post hoc method that refines\nregression with expert knowledge coming from pairwise rankings. Given a query\nitem and a small reference set with known properties, RankRefine combines the\nbase regressor's output with a rank-based estimate via inverse variance\nweighting, requiring no retraining. In molecular property prediction task,\nRankRefine achieves up to 10% relative reduction in mean absolute error using\nonly 20 pairwise comparisons obtained through a general-purpose large language\nmodel (LLM) with no finetuning. As rankings provided by human experts or\ngeneral-purpose LLMs are sufficient for improving regression across diverse\ndomains, RankRefine offers practicality and broad applicability, especially in\nlow-data settings."
                },
                "authors": [
                    {
                        "name": "Kevin Tirta Wijaya"
                    },
                    {
                        "name": "Michael Sun"
                    },
                    {
                        "name": "Minghao Guo"
                    },
                    {
                        "name": "Hans-Peter Seidel"
                    },
                    {
                        "name": "Wojciech Matusik"
                    },
                    {
                        "name": "Vahid Babaei"
                    }
                ],
                "author_detail": {
                    "name": "Vahid Babaei"
                },
                "author": "Vahid Babaei",
                "arxiv_comment": "NeurIPS 2025 camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16495v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16495v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26272v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26272v2",
                "updated": "2025-10-01T12:27:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    12,
                    27,
                    21,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-30T13:56:05Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    13,
                    56,
                    5,
                    1,
                    273,
                    0
                ],
                "title": "PRPO: Paragraph-level Policy Optimization for Vision-Language Deepfake\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRPO: Paragraph-level Policy Optimization for Vision-Language Deepfake\n  Detection"
                },
                "summary": "The rapid rise of synthetic media has made deepfake detection a critical\nchallenge for online safety and trust. Progress remains constrained by the\nscarcity of large, high-quality datasets. Although multimodal large language\nmodels (LLMs) exhibit strong reasoning capabilities, their performance on\ndeepfake detection is poor, often producing explanations that are misaligned\nwith visual evidence or hallucinatory. To address this limitation, we introduce\na reasoning-annotated dataset for deepfake detection and propose\nParagraph-level Relative Policy Optimization (PRPO), a reinforcement learning\nalgorithm that aligns LLM reasoning with image content at the paragraph level.\nExperiments show that PRPO improves detection accuracy by a wide margin and\nachieves the highest reasoning score of 4.55/5.0. Ablation studies further\ndemonstrate that PRPO significantly outperforms GRPO under test-time\nconditions. These results underscore the importance of grounding multimodal\nreasoning in visual evidence to enable more reliable and interpretable deepfake\ndetection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid rise of synthetic media has made deepfake detection a critical\nchallenge for online safety and trust. Progress remains constrained by the\nscarcity of large, high-quality datasets. Although multimodal large language\nmodels (LLMs) exhibit strong reasoning capabilities, their performance on\ndeepfake detection is poor, often producing explanations that are misaligned\nwith visual evidence or hallucinatory. To address this limitation, we introduce\na reasoning-annotated dataset for deepfake detection and propose\nParagraph-level Relative Policy Optimization (PRPO), a reinforcement learning\nalgorithm that aligns LLM reasoning with image content at the paragraph level.\nExperiments show that PRPO improves detection accuracy by a wide margin and\nachieves the highest reasoning score of 4.55/5.0. Ablation studies further\ndemonstrate that PRPO significantly outperforms GRPO under test-time\nconditions. These results underscore the importance of grounding multimodal\nreasoning in visual evidence to enable more reliable and interpretable deepfake\ndetection."
                },
                "authors": [
                    {
                        "name": "Tuan Nguyen"
                    },
                    {
                        "name": "Naseem Khan"
                    },
                    {
                        "name": "Khang Tran"
                    },
                    {
                        "name": "NhatHai Phan"
                    },
                    {
                        "name": "Issa Khalil"
                    }
                ],
                "author_detail": {
                    "name": "Issa Khalil"
                },
                "author": "Issa Khalil",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26272v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26272v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13497v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13497v3",
                "updated": "2025-10-01T12:01:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    12,
                    1,
                    27,
                    2,
                    274,
                    0
                ],
                "published": "2025-05-15T20:23:21Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    20,
                    23,
                    21,
                    3,
                    135,
                    0
                ],
                "title": "Learning Hierarchical Domain Models Through Environment-Grounded\n  Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Hierarchical Domain Models Through Environment-Grounded\n  Interaction"
                },
                "summary": "Domain models enable autonomous agents to solve long-horizon tasks by\nproducing interpretable plans. However, in open-world environments, a single\ngeneral domain model cannot capture the variety of tasks, so agents must\ngenerate suitable task-specific models on the fly. Large Language Models\n(LLMs), with their implicit common knowledge, can generate such domains, but\nsuffer from high error rates that limit their applicability. Hence, related\nwork relies on extensive human feed-back or prior knowledge, which undermines\nautonomous, open-world deployment. In this work, we propose LODGE, a framework\nfor autonomous domain learning from LLMs and environment grounding. LODGE\nbuilds on hierarchical abstractions and automated simulations to identify and\ncorrect inconsistencies between abstraction layers and between the model and\nenvironment. Our framework is task-agnostic, as it generates predicates,\noperators, and their preconditions and effects, while only assuming access to a\nsimulator and a set of generic, executable low-level skills. Experiments on two\nInternational Planning Competition ( IPC) domains and a robotic assembly domain\nshow that LODGE yields more accurate domain models and higher task success than\nexisting methods, requiring remarkably few environment interactions and no\nhuman feedback or demonstrations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain models enable autonomous agents to solve long-horizon tasks by\nproducing interpretable plans. However, in open-world environments, a single\ngeneral domain model cannot capture the variety of tasks, so agents must\ngenerate suitable task-specific models on the fly. Large Language Models\n(LLMs), with their implicit common knowledge, can generate such domains, but\nsuffer from high error rates that limit their applicability. Hence, related\nwork relies on extensive human feed-back or prior knowledge, which undermines\nautonomous, open-world deployment. In this work, we propose LODGE, a framework\nfor autonomous domain learning from LLMs and environment grounding. LODGE\nbuilds on hierarchical abstractions and automated simulations to identify and\ncorrect inconsistencies between abstraction layers and between the model and\nenvironment. Our framework is task-agnostic, as it generates predicates,\noperators, and their preconditions and effects, while only assuming access to a\nsimulator and a set of generic, executable low-level skills. Experiments on two\nInternational Planning Competition ( IPC) domains and a robotic assembly domain\nshow that LODGE yields more accurate domain models and higher task success than\nexisting methods, requiring remarkably few environment interactions and no\nhuman feedback or demonstrations."
                },
                "authors": [
                    {
                        "name": "Claudius Kienle"
                    },
                    {
                        "name": "Benjamin Alt"
                    },
                    {
                        "name": "Oleg Arenz"
                    },
                    {
                        "name": "Jan Peters"
                    }
                ],
                "author_detail": {
                    "name": "Jan Peters"
                },
                "author": "Jan Peters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13497v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13497v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19526v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19526v3",
                "updated": "2025-10-01T11:54:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    11,
                    54,
                    28,
                    2,
                    274,
                    0
                ],
                "published": "2025-04-28T07:05:38Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    7,
                    5,
                    38,
                    0,
                    118,
                    0
                ],
                "title": "Automated flood detection from Sentinel-1 GRD time series using Bayesian\n  analysis for change point problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated flood detection from Sentinel-1 GRD time series using Bayesian\n  analysis for change point problems"
                },
                "summary": "Current Synthetic Aperture Radar (SAR)-based flood detection methods face\ncritical limitations that hinder operational deployment. Supervised learning\napproaches require extensive labeled training data, exhibit poor geographical\ntransferability, and may fail to adapt to new regions without additional\ntraining examples. Existing approaches do not fully exploit the rich temporal\ninformation available in SAR time series, instead relying on simple change\ndetection between pre- and post-flood images or supplementary datasets that\noften introduce error propagation. These limitations prevent effective\nautomated flood monitoring in data-scarce regions where disaster response is\nmost needed. To address these limitations, we develop a novel training-free\napproach by adapting Bayesian analysis for change point problems, specifically\nfor automated flood detection from Sentinel-1 Ground Range Detected time series\ndata. Our method statistically models the temporal behavior of SAR backscatter\nintensity over a one-year baseline period, then computes the posterior\nprobability of change points at flood observation dates. This approach\neliminates supervised learning dependencies by using Bayesian inference to\nidentify when backscatter deviations exceed expected normal variations,\nleveraging inherent statistical properties of time series data. Validation\nacross three diverse geographical contexts using the UrbanSARFloods benchmark\ndataset demonstrates superior performance compared to conventional thresholding\nand deep learning approaches, achieving F1 scores up to 0.76. This enables\nimmediate deployment to any region with SAR coverage, providing critical\nadvantages for disaster response.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current Synthetic Aperture Radar (SAR)-based flood detection methods face\ncritical limitations that hinder operational deployment. Supervised learning\napproaches require extensive labeled training data, exhibit poor geographical\ntransferability, and may fail to adapt to new regions without additional\ntraining examples. Existing approaches do not fully exploit the rich temporal\ninformation available in SAR time series, instead relying on simple change\ndetection between pre- and post-flood images or supplementary datasets that\noften introduce error propagation. These limitations prevent effective\nautomated flood monitoring in data-scarce regions where disaster response is\nmost needed. To address these limitations, we develop a novel training-free\napproach by adapting Bayesian analysis for change point problems, specifically\nfor automated flood detection from Sentinel-1 Ground Range Detected time series\ndata. Our method statistically models the temporal behavior of SAR backscatter\nintensity over a one-year baseline period, then computes the posterior\nprobability of change points at flood observation dates. This approach\neliminates supervised learning dependencies by using Bayesian inference to\nidentify when backscatter deviations exceed expected normal variations,\nleveraging inherent statistical properties of time series data. Validation\nacross three diverse geographical contexts using the UrbanSARFloods benchmark\ndataset demonstrates superior performance compared to conventional thresholding\nand deep learning approaches, achieving F1 scores up to 0.76. This enables\nimmediate deployment to any region with SAR coverage, providing critical\nadvantages for disaster response."
                },
                "authors": [
                    {
                        "name": "Narumasa Tsutsumida"
                    },
                    {
                        "name": "Tomohiro Tanaka"
                    },
                    {
                        "name": "Nifat Sultana"
                    }
                ],
                "author_detail": {
                    "name": "Nifat Sultana"
                },
                "author": "Nifat Sultana",
                "arxiv_comment": "17 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19526v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19526v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11408v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11408v2",
                "updated": "2025-10-01T11:29:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    11,
                    29,
                    54,
                    2,
                    274,
                    0
                ],
                "published": "2024-10-15T08:52:48Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    8,
                    52,
                    48,
                    1,
                    289,
                    0
                ],
                "title": "Aggregation Trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aggregation Trees"
                },
                "summary": "Uncovering the heterogeneous effects of particular policies or \"treatments\"\nis a key concern for researchers and policymakers. A common approach is to\nreport average treatment effects across subgroups based on observable\ncovariates. However, the choice of subgroups is crucial as it poses the risk of\n$p$-hacking and requires balancing interpretability with granularity. This\npaper proposes a nonparametric approach to construct heterogeneous subgroups.\nThe approach enables a flexible exploration of the trade-off between\ninterpretability and the discovery of more granular heterogeneity by\nconstructing a sequence of nested groupings, each with an optimality property.\nBy integrating our approach with \"honesty\" and debiased machine learning, we\nprovide valid inference about the average treatment effect of each group. We\nvalidate the proposed methodology through an empirical Monte-Carlo study and\napply it to revisit the impact of maternal smoking on birth weight, revealing\nsystematic heterogeneity driven by parental and birth-related characteristics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncovering the heterogeneous effects of particular policies or \"treatments\"\nis a key concern for researchers and policymakers. A common approach is to\nreport average treatment effects across subgroups based on observable\ncovariates. However, the choice of subgroups is crucial as it poses the risk of\n$p$-hacking and requires balancing interpretability with granularity. This\npaper proposes a nonparametric approach to construct heterogeneous subgroups.\nThe approach enables a flexible exploration of the trade-off between\ninterpretability and the discovery of more granular heterogeneity by\nconstructing a sequence of nested groupings, each with an optimality property.\nBy integrating our approach with \"honesty\" and debiased machine learning, we\nprovide valid inference about the average treatment effect of each group. We\nvalidate the proposed methodology through an empirical Monte-Carlo study and\napply it to revisit the impact of maternal smoking on birth weight, revealing\nsystematic heterogeneity driven by parental and birth-related characteristics."
                },
                "authors": [
                    {
                        "name": "Riccardo Di Francesco"
                    }
                ],
                "author_detail": {
                    "name": "Riccardo Di Francesco"
                },
                "author": "Riccardo Di Francesco",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11408v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11408v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26432v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26432v2",
                "updated": "2025-10-01T11:26:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    11,
                    26,
                    36,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-30T15:53:56Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    53,
                    56,
                    1,
                    273,
                    0
                ],
                "title": "AdaBlock-dLLM: Semantic-Aware Diffusion LLM Inference via Adaptive Block\n  Size",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaBlock-dLLM: Semantic-Aware Diffusion LLM Inference via Adaptive Block\n  Size"
                },
                "summary": "Diffusion-based large language models (dLLMs) are gaining attention for their\ninherent capacity for parallel decoding, offering a compelling alternative to\nautoregressive LLMs. Among various decoding strategies, blockwise\nsemi-autoregressive (semi-AR) approaches are widely adopted due to their\nnatural support for KV caching and their favorable accuracy-speed trade-off.\nHowever, this paper identifies two fundamental limitations in the conventional\nsemi-AR decoding approach that applies a fixed block size: i) late decoding\noverhead, where the unmasking of high-confidence tokens outside the current\nblock is unnecessarily delayed, and ii) premature decoding error, where\nlow-confidence tokens inside the current block are committed too early, leading\nto incorrect tokens. This paper presents the first systematic investigation\nchallenging the fixed block size assumption in semi-AR decoding. Through a\nstatistical analysis of confidence dynamics during the denoising process, we\nidentify a volatility band (VB) region during dLLM decoding, which encodes\nlocal semantic structure and can be used to guide adaptive block sizing.\nLeveraging these insights, we introduce AdaBlock-dLLM, a training-free,\nplug-and-play scheduler that adaptively aligns block boundaries with semantic\nsteps by adjusting block size during runtime. Extensive experiments across\ndiverse benchmarks show that AdaBlock-dLLM achieves up to 5.3% accuracy\nimprovement under the same throughput budget. Beyond inference-time\noptimization, we hope our semantics-aware adaptive scheduling approach and\nconfidence-based analysis will inspire future training strategies for dLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based large language models (dLLMs) are gaining attention for their\ninherent capacity for parallel decoding, offering a compelling alternative to\nautoregressive LLMs. Among various decoding strategies, blockwise\nsemi-autoregressive (semi-AR) approaches are widely adopted due to their\nnatural support for KV caching and their favorable accuracy-speed trade-off.\nHowever, this paper identifies two fundamental limitations in the conventional\nsemi-AR decoding approach that applies a fixed block size: i) late decoding\noverhead, where the unmasking of high-confidence tokens outside the current\nblock is unnecessarily delayed, and ii) premature decoding error, where\nlow-confidence tokens inside the current block are committed too early, leading\nto incorrect tokens. This paper presents the first systematic investigation\nchallenging the fixed block size assumption in semi-AR decoding. Through a\nstatistical analysis of confidence dynamics during the denoising process, we\nidentify a volatility band (VB) region during dLLM decoding, which encodes\nlocal semantic structure and can be used to guide adaptive block sizing.\nLeveraging these insights, we introduce AdaBlock-dLLM, a training-free,\nplug-and-play scheduler that adaptively aligns block boundaries with semantic\nsteps by adjusting block size during runtime. Extensive experiments across\ndiverse benchmarks show that AdaBlock-dLLM achieves up to 5.3% accuracy\nimprovement under the same throughput budget. Beyond inference-time\noptimization, we hope our semantics-aware adaptive scheduling approach and\nconfidence-based analysis will inspire future training strategies for dLLMs."
                },
                "authors": [
                    {
                        "name": "Guanxi Lu"
                    },
                    {
                        "name": "Hao Mark Chen"
                    },
                    {
                        "name": "Yuto Karashima"
                    },
                    {
                        "name": "Zhican Wang"
                    },
                    {
                        "name": "Daichi Fujiki"
                    },
                    {
                        "name": "Hongxiang Fan"
                    }
                ],
                "author_detail": {
                    "name": "Hongxiang Fan"
                },
                "author": "Hongxiang Fan",
                "arxiv_comment": "Preprint. Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26432v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26432v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22678v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22678v2",
                "updated": "2025-10-01T11:09:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    11,
                    9,
                    34,
                    2,
                    274,
                    0
                ],
                "published": "2025-03-28T17:59:53Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    17,
                    59,
                    53,
                    4,
                    87,
                    0
                ],
                "title": "Self-Evolving Multi-Agent Simulations for Realistic Clinical\n  Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Evolving Multi-Agent Simulations for Realistic Clinical\n  Interactions"
                },
                "summary": "In this work, we introduce MedAgentSim, an open-source simulated clinical\nenvironment with doctor, patient, and measurement agents designed to evaluate\nand enhance LLM performance in dynamic diagnostic settings. Unlike prior\napproaches, our framework requires doctor agents to actively engage with\npatients through multi-turn conversations, requesting relevant medical\nexaminations (e.g., temperature, blood pressure, ECG) and imaging results\n(e.g., MRI, X-ray) from a measurement agent to mimic the real-world diagnostic\nprocess. Additionally, we incorporate self improvement mechanisms that allow\nmodels to iteratively refine their diagnostic strategies. We enhance LLM\nperformance in our simulated setting by integrating multi-agent discussions,\nchain-of-thought reasoning, and experience-based knowledge retrieval,\nfacilitating progressive learning as doctor agents interact with more patients.\nWe also introduce an evaluation benchmark for assessing the LLM's ability to\nengage in dynamic, context-aware diagnostic interactions. While MedAgentSim is\nfully automated, it also supports a user-controlled mode, enabling human\ninteraction with either the doctor or patient agent. Comprehensive evaluations\nin various simulated diagnostic scenarios demonstrate the effectiveness of our\napproach. Our code, simulation tool, and benchmark are available at\n\\href{https://medagentsim.netlify.app/}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we introduce MedAgentSim, an open-source simulated clinical\nenvironment with doctor, patient, and measurement agents designed to evaluate\nand enhance LLM performance in dynamic diagnostic settings. Unlike prior\napproaches, our framework requires doctor agents to actively engage with\npatients through multi-turn conversations, requesting relevant medical\nexaminations (e.g., temperature, blood pressure, ECG) and imaging results\n(e.g., MRI, X-ray) from a measurement agent to mimic the real-world diagnostic\nprocess. Additionally, we incorporate self improvement mechanisms that allow\nmodels to iteratively refine their diagnostic strategies. We enhance LLM\nperformance in our simulated setting by integrating multi-agent discussions,\nchain-of-thought reasoning, and experience-based knowledge retrieval,\nfacilitating progressive learning as doctor agents interact with more patients.\nWe also introduce an evaluation benchmark for assessing the LLM's ability to\nengage in dynamic, context-aware diagnostic interactions. While MedAgentSim is\nfully automated, it also supports a user-controlled mode, enabling human\ninteraction with either the doctor or patient agent. Comprehensive evaluations\nin various simulated diagnostic scenarios demonstrate the effectiveness of our\napproach. Our code, simulation tool, and benchmark are available at\n\\href{https://medagentsim.netlify.app/}."
                },
                "authors": [
                    {
                        "name": "Mohammad Almansoori"
                    },
                    {
                        "name": "Komal Kumar"
                    },
                    {
                        "name": "Hisham Cholakkal"
                    }
                ],
                "author_detail": {
                    "name": "Hisham Cholakkal"
                },
                "author": "Hisham Cholakkal",
                "arxiv_comment": "14 page, 4 figures, 61 references, presented in MICCAI (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22678v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22678v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14824v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14824v2",
                "updated": "2025-10-01T11:06:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    11,
                    6,
                    32,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-18T10:32:52Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    10,
                    32,
                    52,
                    3,
                    261,
                    0
                ],
                "title": "Confirmation Bias as a Cognitive Resource in LLM-Supported Deliberation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confirmation Bias as a Cognitive Resource in LLM-Supported Deliberation"
                },
                "summary": "Large language models (LLMs) are increasingly used in group decision-making,\nbut their influence risks fostering conformity and reducing epistemic\nvigilance. Drawing on the Argumentative Theory of Reasoning, we argue that\nconfirmation bias, often seen as detrimental, can be harnessed as a resource\nwhen paired with critical evaluation. We propose a three-step process in which\nindividuals first generate ideas independently, then use LLMs to refine and\narticulate them, and finally engage with LLMs as epistemic provocateurs to\nanticipate group critique. This framing positions LLMs as tools for scaffolding\ndisagreement, helping individuals prepare for more productive group\ndiscussions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly used in group decision-making,\nbut their influence risks fostering conformity and reducing epistemic\nvigilance. Drawing on the Argumentative Theory of Reasoning, we argue that\nconfirmation bias, often seen as detrimental, can be harnessed as a resource\nwhen paired with critical evaluation. We propose a three-step process in which\nindividuals first generate ideas independently, then use LLMs to refine and\narticulate them, and finally engage with LLMs as epistemic provocateurs to\nanticipate group critique. This framing positions LLMs as tools for scaffolding\ndisagreement, helping individuals prepare for more productive group\ndiscussions."
                },
                "authors": [
                    {
                        "name": "Sander de Jong"
                    },
                    {
                        "name": "Rune Møberg Jacobsen"
                    },
                    {
                        "name": "Niels van Berkel"
                    }
                ],
                "author_detail": {
                    "name": "Niels van Berkel"
                },
                "author": "Niels van Berkel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14824v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14824v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12922v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12922v2",
                "updated": "2025-10-01T11:06:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    11,
                    6,
                    20,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-16T10:19:37Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    10,
                    19,
                    37,
                    1,
                    259,
                    0
                ],
                "title": "Evolution of the infrared luminosity function and its corresponding\n  dust-obscured star formation rate density out to z~6",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolution of the infrared luminosity function and its corresponding\n  dust-obscured star formation rate density out to z~6"
                },
                "summary": "We present a new determination of the evolving far-infrared galaxy luminosity\nfunction (FIR LF) and the resulting inferred evolution of dust-obscured\nstar-formation rate density (SFRD) out to redshift z~6. To establish the\nevolving co-moving number density of FIR-bright objects, we make use of the\nhigh-resolution ALMA follow-up study (AS2UDS), of the JCMT SCUBA-2 Cosmology\nLegacy Survey (S2CLS) sub-mm imaging in the UKIDSS UDS survey field. In order\nto estimate the contributions of faint/low-mass sources we implement a method\nin which the faint-end of the IR LF is inferred by stacking (in stellar mass\nand redshift bins) the optical/near-infrared samples of star-forming galaxies\ninto the appropriate FIR Herschel and sub-mm JCMT maps. Using this information\nwe determine the faint-end slope of the FIR LF in two intermediate redshift\nbins (where it can be robustly established) and then adopt this result at all\nother redshifts. The evolution of the characteristic luminosity of the galaxy\nFIR LF, L*, is found to be increase monotonically with redshift, evolving as\nz^1.38+-0.07, while the characteristic number density is well fitted by double\npower-law function, constant at z<2.24 and declining as z^-4.95+-0.73 at higher\nredshifts. The evolution of the corresponding dust-obscured star-formation rate\ndensity was then calculated and is here compared with the results from a number\nof recent studies in the literature. Our analysis confirms that dust-obscured\nstar-formation activity dominates SFRD at cosmic noon, but then becomes\nprogressively less important with increasing redshift: while dusty star-forming\ngalaxies are still found out to the highest redshifts explored here, UV-visible\nstar formation dominates at z>4, and dust-obscured activity contributes <25% of\nSFRD by z~6.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a new determination of the evolving far-infrared galaxy luminosity\nfunction (FIR LF) and the resulting inferred evolution of dust-obscured\nstar-formation rate density (SFRD) out to redshift z~6. To establish the\nevolving co-moving number density of FIR-bright objects, we make use of the\nhigh-resolution ALMA follow-up study (AS2UDS), of the JCMT SCUBA-2 Cosmology\nLegacy Survey (S2CLS) sub-mm imaging in the UKIDSS UDS survey field. In order\nto estimate the contributions of faint/low-mass sources we implement a method\nin which the faint-end of the IR LF is inferred by stacking (in stellar mass\nand redshift bins) the optical/near-infrared samples of star-forming galaxies\ninto the appropriate FIR Herschel and sub-mm JCMT maps. Using this information\nwe determine the faint-end slope of the FIR LF in two intermediate redshift\nbins (where it can be robustly established) and then adopt this result at all\nother redshifts. The evolution of the characteristic luminosity of the galaxy\nFIR LF, L*, is found to be increase monotonically with redshift, evolving as\nz^1.38+-0.07, while the characteristic number density is well fitted by double\npower-law function, constant at z<2.24 and declining as z^-4.95+-0.73 at higher\nredshifts. The evolution of the corresponding dust-obscured star-formation rate\ndensity was then calculated and is here compared with the results from a number\nof recent studies in the literature. Our analysis confirms that dust-obscured\nstar-formation activity dominates SFRD at cosmic noon, but then becomes\nprogressively less important with increasing redshift: while dusty star-forming\ngalaxies are still found out to the highest redshifts explored here, UV-visible\nstar formation dominates at z>4, and dust-obscured activity contributes <25% of\nSFRD by z~6."
                },
                "authors": [
                    {
                        "name": "M. P. Koprowski"
                    },
                    {
                        "name": "J. V. Wijesekera"
                    },
                    {
                        "name": "J. S. Dunlop"
                    },
                    {
                        "name": "K. Lisiecki"
                    },
                    {
                        "name": "D. J. McLeod"
                    },
                    {
                        "name": "R. J. McLure"
                    },
                    {
                        "name": "M. J. Michałowski"
                    },
                    {
                        "name": "M. Solar"
                    }
                ],
                "author_detail": {
                    "name": "M. Solar"
                },
                "author": "M. Solar",
                "arxiv_comment": "10 pages, 7 figures, 5 tables, submitted to A&A. Changes to previous\n  version include more careful referencing and adding the comparison to\n  Dudzeviciute_2020 work in figures 5 and 7. The main results of the paper have\n  not been changed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12922v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12922v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12310v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12310v2",
                "updated": "2025-10-01T10:59:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    10,
                    59,
                    9,
                    2,
                    274,
                    0
                ],
                "published": "2024-12-16T19:29:06Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    19,
                    29,
                    6,
                    0,
                    351,
                    0
                ],
                "title": "Second Language (Arabic) Acquisition of LLMs via Progressive Vocabulary\n  Expansion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Second Language (Arabic) Acquisition of LLMs via Progressive Vocabulary\n  Expansion"
                },
                "summary": "This paper addresses the critical need for democratizing large language\nmodels (LLM) in the Arab world, a region that has seen slower progress in\ndeveloping models comparable to state-of-the-art offerings like GPT-4 or\nChatGPT 3.5, due to a predominant focus on mainstream languages (e.g., English\nand Chinese). One practical objective for an Arabic LLM is to utilize an\nArabic-specific vocabulary for the tokenizer that could speed up decoding.\nHowever, using a different vocabulary often leads to a degradation of learned\nknowledge since many words are initially out-of-vocabulary (OOV) when training\nstarts. Inspired by the vocabulary learning during Second Language (Arabic)\nAcquisition for humans, the released AraLLaMA employs progressive vocabulary\nexpansion, which is implemented by a modified BPE algorithm that progressively\nextends the Arabic subwords in its dynamic vocabulary during training, thereby\nbalancing the OOV ratio at every stage. The ablation study demonstrated the\neffectiveness of Progressive Vocabulary Expansion. Moreover, AraLLaMA achieves\ndecent performance comparable to the best Arabic LLMs across a variety of\nArabic benchmarks. Models, training data, benchmarks, and codes will be all\nopen-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the critical need for democratizing large language\nmodels (LLM) in the Arab world, a region that has seen slower progress in\ndeveloping models comparable to state-of-the-art offerings like GPT-4 or\nChatGPT 3.5, due to a predominant focus on mainstream languages (e.g., English\nand Chinese). One practical objective for an Arabic LLM is to utilize an\nArabic-specific vocabulary for the tokenizer that could speed up decoding.\nHowever, using a different vocabulary often leads to a degradation of learned\nknowledge since many words are initially out-of-vocabulary (OOV) when training\nstarts. Inspired by the vocabulary learning during Second Language (Arabic)\nAcquisition for humans, the released AraLLaMA employs progressive vocabulary\nexpansion, which is implemented by a modified BPE algorithm that progressively\nextends the Arabic subwords in its dynamic vocabulary during training, thereby\nbalancing the OOV ratio at every stage. The ablation study demonstrated the\neffectiveness of Progressive Vocabulary Expansion. Moreover, AraLLaMA achieves\ndecent performance comparable to the best Arabic LLMs across a variety of\nArabic benchmarks. Models, training data, benchmarks, and codes will be all\nopen-sourced."
                },
                "authors": [
                    {
                        "name": "Jianqing Zhu"
                    },
                    {
                        "name": "Huang Huang"
                    },
                    {
                        "name": "Zhihang Lin"
                    },
                    {
                        "name": "Juhao Liang"
                    },
                    {
                        "name": "Zhengyang Tang"
                    },
                    {
                        "name": "Khalid Almubarak"
                    },
                    {
                        "name": "Abdulmohsen Alharthik"
                    },
                    {
                        "name": "Bang An"
                    },
                    {
                        "name": "Juncai He"
                    },
                    {
                        "name": "Xiangbo Wu"
                    },
                    {
                        "name": "Fei Yu"
                    },
                    {
                        "name": "Junying Chen"
                    },
                    {
                        "name": "Zhuoheng Ma"
                    },
                    {
                        "name": "Yuhao Du"
                    },
                    {
                        "name": "He Zhang"
                    },
                    {
                        "name": "Emad A. Alghamdi"
                    },
                    {
                        "name": "Lian Zhang"
                    },
                    {
                        "name": "Ruoyu Sun"
                    },
                    {
                        "name": "Haizhou Li"
                    },
                    {
                        "name": "Benyou Wang"
                    },
                    {
                        "name": "Jinchao Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jinchao Xu"
                },
                "author": "Jinchao Xu",
                "arxiv_journal_ref": "Proceedings of the 63rd Annual Meeting of the Association for\n  Computational Linguistics (Volume 1: Long Papers), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12310v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12310v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26181v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26181v2",
                "updated": "2025-10-01T10:03:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    10,
                    3,
                    53,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-30T12:40:36Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    12,
                    40,
                    36,
                    1,
                    273,
                    0
                ],
                "title": "Explaining novel senses using definition generation with open language\n  models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explaining novel senses using definition generation with open language\n  models"
                },
                "summary": "We apply definition generators based on open-weights large language models to\nthe task of creating explanations of novel senses, taking target word usages as\nan input. To this end, we employ the datasets from the AXOLOTL'24 shared task\non explainable semantic change modeling, which features Finnish, Russian and\nGerman languages. We fine-tune and provide publicly the open-source models\nperforming higher than the best submissions of the aforementioned shared task,\nwhich employed closed proprietary LLMs. In addition, we find that\nencoder-decoder definition generators perform on par with their decoder-only\ncounterparts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We apply definition generators based on open-weights large language models to\nthe task of creating explanations of novel senses, taking target word usages as\nan input. To this end, we employ the datasets from the AXOLOTL'24 shared task\non explainable semantic change modeling, which features Finnish, Russian and\nGerman languages. We fine-tune and provide publicly the open-source models\nperforming higher than the best submissions of the aforementioned shared task,\nwhich employed closed proprietary LLMs. In addition, we find that\nencoder-decoder definition generators perform on par with their decoder-only\ncounterparts."
                },
                "authors": [
                    {
                        "name": "Mariia Fedorova"
                    },
                    {
                        "name": "Andrey Kutuzov"
                    },
                    {
                        "name": "Francesco Periti"
                    },
                    {
                        "name": "Yves Scherrer"
                    }
                ],
                "author_detail": {
                    "name": "Yves Scherrer"
                },
                "author": "Yves Scherrer",
                "arxiv_comment": "EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26181v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26181v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06066v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06066v4",
                "updated": "2025-10-01T09:57:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    9,
                    57,
                    28,
                    2,
                    274,
                    0
                ],
                "published": "2025-01-10T15:57:23Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    15,
                    57,
                    23,
                    4,
                    10,
                    0
                ],
                "title": "Distilling Calibration via Conformalized Credal Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilling Calibration via Conformalized Credal Inference"
                },
                "summary": "Deploying artificial intelligence (AI) models on edge devices involves a\ndelicate balance between meeting stringent complexity constraints, such as\nlimited memory and energy resources, and ensuring reliable performance in\nsensitive decision-making tasks. One way to enhance reliability is through\nuncertainty quantification via Bayesian inference. This approach, however,\ntypically necessitates maintaining and running multiple models in an ensemble,\nwhich may exceed the computational limits of edge devices. This paper\nintroduces a low-complexity methodology to address this challenge by distilling\ncalibration information from a more complex model. In an offline phase,\npredictive probabilities generated by a high-complexity cloud-based model are\nleveraged to determine a threshold based on the typical divergence between the\ncloud and edge models. At run time, this threshold is used to construct credal\nsets -- ranges of predictive probabilities that are guaranteed, with a\nuser-selected confidence level, to include the predictions of the cloud model.\nThe credal sets are obtained through thresholding of a divergence measure in\nthe simplex of predictive probabilities. Experiments on visual and language\ntasks demonstrate that the proposed approach, termed Conformalized Distillation\nfor Credal Inference (CD-CI), significantly improves calibration performance\ncompared to low-complexity Bayesian methods, such as Laplace approximation,\nmaking it a practical and efficient solution for edge AI deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying artificial intelligence (AI) models on edge devices involves a\ndelicate balance between meeting stringent complexity constraints, such as\nlimited memory and energy resources, and ensuring reliable performance in\nsensitive decision-making tasks. One way to enhance reliability is through\nuncertainty quantification via Bayesian inference. This approach, however,\ntypically necessitates maintaining and running multiple models in an ensemble,\nwhich may exceed the computational limits of edge devices. This paper\nintroduces a low-complexity methodology to address this challenge by distilling\ncalibration information from a more complex model. In an offline phase,\npredictive probabilities generated by a high-complexity cloud-based model are\nleveraged to determine a threshold based on the typical divergence between the\ncloud and edge models. At run time, this threshold is used to construct credal\nsets -- ranges of predictive probabilities that are guaranteed, with a\nuser-selected confidence level, to include the predictions of the cloud model.\nThe credal sets are obtained through thresholding of a divergence measure in\nthe simplex of predictive probabilities. Experiments on visual and language\ntasks demonstrate that the proposed approach, termed Conformalized Distillation\nfor Credal Inference (CD-CI), significantly improves calibration performance\ncompared to low-complexity Bayesian methods, such as Laplace approximation,\nmaking it a practical and efficient solution for edge AI deployments."
                },
                "authors": [
                    {
                        "name": "Jiayi Huang"
                    },
                    {
                        "name": "Sangwoo Park"
                    },
                    {
                        "name": "Nicola Paoletti"
                    },
                    {
                        "name": "Osvaldo Simeone"
                    }
                ],
                "author_detail": {
                    "name": "Osvaldo Simeone"
                },
                "author": "Osvaldo Simeone",
                "arxiv_comment": "IJCNN 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06066v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06066v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23274v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23274v2",
                "updated": "2025-10-01T09:46:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    9,
                    46,
                    53,
                    2,
                    274,
                    0
                ],
                "published": "2025-06-29T15:01:01Z",
                "published_parsed": [
                    2025,
                    6,
                    29,
                    15,
                    1,
                    1,
                    6,
                    180,
                    0
                ],
                "title": "Towards a Progress Bar for Reasoning: Progress Prediction in Large\n  Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a Progress Bar for Reasoning: Progress Prediction in Large\n  Reasoning Models"
                },
                "summary": "Reasoning models that produce long, hidden chains of thought, have emerged as\npowerful tools for reasoning-intensive and agentic tasks. However, as the time\nhorizons at which these models can operate grow exponentially, it becomes\nincreasingly difficult to know how much progress the model is making on a task,\nmaking it challenging for users to set appropriate expectations about\ncompletion time. By probing the internal representations of Large Language\nModels (LLMs), we find evidence that their reasoning progress can be\nquantified, with simple linear probes achieving 30\\% accuracy over 10 progress\nclasses and Mean Absolute Error (MAE) of 1.75. Rooted in this insight, we\npropose a two-stage fine-tuning method that trains existing reasoning models to\nexplicitly generate progress estimates (0-100\\%) during their reasoning\nprocess. We find that the predictions of our best fine-tuned language model for\nsequences below 16K tokens are on average 10\\% from the true label.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning models that produce long, hidden chains of thought, have emerged as\npowerful tools for reasoning-intensive and agentic tasks. However, as the time\nhorizons at which these models can operate grow exponentially, it becomes\nincreasingly difficult to know how much progress the model is making on a task,\nmaking it challenging for users to set appropriate expectations about\ncompletion time. By probing the internal representations of Large Language\nModels (LLMs), we find evidence that their reasoning progress can be\nquantified, with simple linear probes achieving 30\\% accuracy over 10 progress\nclasses and Mean Absolute Error (MAE) of 1.75. Rooted in this insight, we\npropose a two-stage fine-tuning method that trains existing reasoning models to\nexplicitly generate progress estimates (0-100\\%) during their reasoning\nprocess. We find that the predictions of our best fine-tuned language model for\nsequences below 16K tokens are on average 10\\% from the true label."
                },
                "authors": [
                    {
                        "name": "Hans Peter Lynsgøe Raaschou-jensen"
                    },
                    {
                        "name": "Constanza Fierro"
                    },
                    {
                        "name": "Anders Søgaard"
                    }
                ],
                "author_detail": {
                    "name": "Anders Søgaard"
                },
                "author": "Anders Søgaard",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23274v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23274v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24391v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24391v3",
                "updated": "2025-10-01T09:41:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    9,
                    41,
                    33,
                    2,
                    274,
                    0
                ],
                "published": "2025-03-31T17:59:58Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    59,
                    58,
                    0,
                    90,
                    0
                ],
                "title": "Easi3R: Estimating Disentangled Motion from DUSt3R Without Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Easi3R: Estimating Disentangled Motion from DUSt3R Without Training"
                },
                "summary": "Recent advances in DUSt3R have enabled robust estimation of dense point\nclouds and camera parameters of static scenes, leveraging Transformer network\narchitectures and direct supervision on large-scale 3D datasets. In contrast,\nthe limited scale and diversity of available 4D datasets present a major\nbottleneck for training a highly generalizable 4D model. This constraint has\ndriven conventional 4D methods to fine-tune 3D models on scalable dynamic video\ndata with additional geometric priors such as optical flow and depths. In this\nwork, we take an opposite path and introduce Easi3R, a simple yet efficient\ntraining-free method for 4D reconstruction. Our approach applies attention\nadaptation during inference, eliminating the need for from-scratch pre-training\nor network fine-tuning. We find that the attention layers in DUSt3R inherently\nencode rich information about camera and object motion. By carefully\ndisentangling these attention maps, we achieve accurate dynamic region\nsegmentation, camera pose estimation, and 4D dense point map reconstruction.\nExtensive experiments on real-world dynamic videos demonstrate that our\nlightweight attention adaptation significantly outperforms previous\nstate-of-the-art methods that are trained or finetuned on extensive dynamic\ndatasets. Our code is publicly available for research purpose at\nhttps://easi3r.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in DUSt3R have enabled robust estimation of dense point\nclouds and camera parameters of static scenes, leveraging Transformer network\narchitectures and direct supervision on large-scale 3D datasets. In contrast,\nthe limited scale and diversity of available 4D datasets present a major\nbottleneck for training a highly generalizable 4D model. This constraint has\ndriven conventional 4D methods to fine-tune 3D models on scalable dynamic video\ndata with additional geometric priors such as optical flow and depths. In this\nwork, we take an opposite path and introduce Easi3R, a simple yet efficient\ntraining-free method for 4D reconstruction. Our approach applies attention\nadaptation during inference, eliminating the need for from-scratch pre-training\nor network fine-tuning. We find that the attention layers in DUSt3R inherently\nencode rich information about camera and object motion. By carefully\ndisentangling these attention maps, we achieve accurate dynamic region\nsegmentation, camera pose estimation, and 4D dense point map reconstruction.\nExtensive experiments on real-world dynamic videos demonstrate that our\nlightweight attention adaptation significantly outperforms previous\nstate-of-the-art methods that are trained or finetuned on extensive dynamic\ndatasets. Our code is publicly available for research purpose at\nhttps://easi3r.github.io/"
                },
                "authors": [
                    {
                        "name": "Xingyu Chen"
                    },
                    {
                        "name": "Yue Chen"
                    },
                    {
                        "name": "Yuliang Xiu"
                    },
                    {
                        "name": "Andreas Geiger"
                    },
                    {
                        "name": "Anpei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Anpei Chen"
                },
                "author": "Anpei Chen",
                "arxiv_comment": "Page: https://easi3r.github.io/ Code:\n  https://github.com/Inception3D/Easi3R",
                "arxiv_journal_ref": "IEEE/CVF International Conference on Computer Vision (ICCV), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24391v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24391v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02670v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02670v6",
                "updated": "2025-10-01T09:38:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    9,
                    38,
                    58,
                    2,
                    274,
                    0
                ],
                "published": "2025-04-03T15:11:55Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    15,
                    11,
                    55,
                    3,
                    93,
                    0
                ],
                "title": "Affordable AI Assistants with Knowledge Graph of Thoughts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Affordable AI Assistants with Knowledge Graph of Thoughts"
                },
                "summary": "Large Language Models (LLMs) are revolutionizing the development of AI\nassistants capable of performing diverse tasks across domains. However, current\nstate-of-the-art LLM-driven agents face significant challenges, including high\noperational costs and limited success rates on complex benchmarks like GAIA. To\naddress these issues, we propose Knowledge Graph of Thoughts (KGoT), an\ninnovative AI assistant architecture that integrates LLM reasoning with\ndynamically constructed knowledge graphs (KGs). KGoT extracts and structures\ntask-relevant knowledge into a dynamic KG representation, iteratively enhanced\nthrough external tools such as math solvers, web crawlers, and Python scripts.\nSuch structured representation of task-relevant knowledge enables low-cost\nmodels to solve complex tasks effectively while also minimizing bias and noise.\nFor example, KGoT achieves a 29% improvement in task success rates on the GAIA\nbenchmark compared to Hugging Face Agents with GPT-4o mini. Moreover,\nharnessing a smaller model dramatically reduces operational costs by over 36x\ncompared to GPT-4o. Improvements for other models (e.g., Qwen2.5-32B and\nDeepseek-R1-70B) and benchmarks (e.g., SimpleQA) are similar. KGoT offers a\nscalable, affordable, versatile, and high-performing solution for AI\nassistants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are revolutionizing the development of AI\nassistants capable of performing diverse tasks across domains. However, current\nstate-of-the-art LLM-driven agents face significant challenges, including high\noperational costs and limited success rates on complex benchmarks like GAIA. To\naddress these issues, we propose Knowledge Graph of Thoughts (KGoT), an\ninnovative AI assistant architecture that integrates LLM reasoning with\ndynamically constructed knowledge graphs (KGs). KGoT extracts and structures\ntask-relevant knowledge into a dynamic KG representation, iteratively enhanced\nthrough external tools such as math solvers, web crawlers, and Python scripts.\nSuch structured representation of task-relevant knowledge enables low-cost\nmodels to solve complex tasks effectively while also minimizing bias and noise.\nFor example, KGoT achieves a 29% improvement in task success rates on the GAIA\nbenchmark compared to Hugging Face Agents with GPT-4o mini. Moreover,\nharnessing a smaller model dramatically reduces operational costs by over 36x\ncompared to GPT-4o. Improvements for other models (e.g., Qwen2.5-32B and\nDeepseek-R1-70B) and benchmarks (e.g., SimpleQA) are similar. KGoT offers a\nscalable, affordable, versatile, and high-performing solution for AI\nassistants."
                },
                "authors": [
                    {
                        "name": "Maciej Besta"
                    },
                    {
                        "name": "Lorenzo Paleari"
                    },
                    {
                        "name": "Jia Hao Andrea Jiang"
                    },
                    {
                        "name": "Robert Gerstenberger"
                    },
                    {
                        "name": "You Wu"
                    },
                    {
                        "name": "Jón Gunnar Hannesson"
                    },
                    {
                        "name": "Patrick Iff"
                    },
                    {
                        "name": "Ales Kubicek"
                    },
                    {
                        "name": "Piotr Nyczyk"
                    },
                    {
                        "name": "Diana Khimey"
                    },
                    {
                        "name": "Nils Blach"
                    },
                    {
                        "name": "Haiqiang Zhang"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Peiran Ma"
                    },
                    {
                        "name": "Grzegorz Kwaśniewski"
                    },
                    {
                        "name": "Marcin Copik"
                    },
                    {
                        "name": "Hubert Niewiadomski"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02670v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02670v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04325v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04325v3",
                "updated": "2025-10-01T09:38:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    9,
                    38,
                    55,
                    2,
                    274,
                    0
                ],
                "published": "2024-05-07T13:55:11Z",
                "published_parsed": [
                    2024,
                    5,
                    7,
                    13,
                    55,
                    11,
                    1,
                    128,
                    0
                ],
                "title": "Language Models can Subtly Deceive Without Lying: A Case Study on\n  Strategic Phrasing in Legislation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models can Subtly Deceive Without Lying: A Case Study on\n  Strategic Phrasing in Legislation"
                },
                "summary": "We explore the ability of large language models (LLMs) to engage in subtle\ndeception through strategically phrasing and intentionally manipulating\ninformation. This harmful behavior can be hard to detect, unlike blatant lying\nor unintentional hallucination. We build a simple testbed mimicking a\nlegislative environment where a corporate \\textit{lobbyist} module is proposing\namendments to bills that benefit a specific company while evading\nidentification of this benefactor. We use real-world legislative bills matched\nwith potentially affected companies to ground these interactions. Our results\nshow that LLM lobbyists can draft subtle phrasing to avoid such identification\nby strong LLM-based detectors. Further optimization of the phrasing using\nLLM-based re-planning and re-sampling increases deception rates by up to 40\npercentage points. Our human evaluations to verify the quality of deceptive\ngenerations and their retention of self-serving intent show significant\ncoherence with our automated metrics and also help in identifying certain\nstrategies of deceptive phrasing. This study highlights the risk of LLMs'\ncapabilities for strategic phrasing through seemingly neutral language to\nattain self-serving goals. This calls for future research to uncover and\nprotect against such subtle deception.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore the ability of large language models (LLMs) to engage in subtle\ndeception through strategically phrasing and intentionally manipulating\ninformation. This harmful behavior can be hard to detect, unlike blatant lying\nor unintentional hallucination. We build a simple testbed mimicking a\nlegislative environment where a corporate \\textit{lobbyist} module is proposing\namendments to bills that benefit a specific company while evading\nidentification of this benefactor. We use real-world legislative bills matched\nwith potentially affected companies to ground these interactions. Our results\nshow that LLM lobbyists can draft subtle phrasing to avoid such identification\nby strong LLM-based detectors. Further optimization of the phrasing using\nLLM-based re-planning and re-sampling increases deception rates by up to 40\npercentage points. Our human evaluations to verify the quality of deceptive\ngenerations and their retention of self-serving intent show significant\ncoherence with our automated metrics and also help in identifying certain\nstrategies of deceptive phrasing. This study highlights the risk of LLMs'\ncapabilities for strategic phrasing through seemingly neutral language to\nattain self-serving goals. This calls for future research to uncover and\nprotect against such subtle deception."
                },
                "authors": [
                    {
                        "name": "Atharvan Dogra"
                    },
                    {
                        "name": "Krishna Pillutla"
                    },
                    {
                        "name": "Ameet Deshpande"
                    },
                    {
                        "name": "Ananya B Sai"
                    },
                    {
                        "name": "John Nay"
                    },
                    {
                        "name": "Tanmay Rajpurohit"
                    },
                    {
                        "name": "Ashwin Kalyan"
                    },
                    {
                        "name": "Balaraman Ravindran"
                    }
                ],
                "author_detail": {
                    "name": "Balaraman Ravindran"
                },
                "author": "Balaraman Ravindran",
                "arxiv_doi": "10.18653/v1/2025.acl-long.1600",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.18653/v1/2025.acl-long.1600",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.04325v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04325v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "24 pages, 7 figures; published in Proceedings of the 63rd Annual\n  Meeting of the Association for Computational Linguistics (ACL 2025), Volume\n  1: Long Papers; Anthology ID 2025.acl-long.1600",
                "arxiv_journal_ref": "Proceedings of the 63rd Annual Meeting of the Association for\n  Computational Linguistics (Volume 1: Long Papers), Vienna, Austria, July\n  2025, pages 33367-33390",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23384v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23384v3",
                "updated": "2025-10-01T09:38:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    9,
                    38,
                    38,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-27T16:09:03Z",
                "published_parsed": [
                    2025,
                    9,
                    27,
                    16,
                    9,
                    3,
                    5,
                    270,
                    0
                ],
                "title": "A Predictive and Synergistic Two-Layer Scheduling Framework for LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Predictive and Synergistic Two-Layer Scheduling Framework for LLM\n  Serving"
                },
                "summary": "LLM inference serving typically scales out with a two-tier architecture: a\ncluster router distributes requests to multiple inference engines, each of\nwhich then in turn performs its own internal scheduling. However, this commonly\nused paradigm suffers from critical, systemic inefficiency caused by the\ninformation gaps across two layers. At the cluster-layer, the router mainly\nrelies on lagging, coarse-grained metrics, such as average latency and queue\nlength to make decisions, resulting in \"decision lag\" that leads to suboptimal\nrequest routing. At the engine-layer, static heuristic scheduling policies\ncannot effectively handle the dynamic workloads, leading a poor balance between\nlatency and throughput. Besides, these gaps may cause SLO violations and\nresource waste, especially in heterogeneous cloud environments.\n  To bridge such gaps, we propose NexusSched, a cross-layer framework that\nshifts LLM serving system from reactive load balancing to predictive\norchestration. The core of NexusSched lies in a structurally-informed online\nperformance model that provides accurate, forward-looking per-step latency and\ncapacity estimations. This model empowers two key components. At the\nengine-layer, LENS performs SLO-aware, adaptive scheduling, dynamically\noptimizing batching to meet SLOs under real-time loads. At the cluster-layer,\nPRISM uses predictive signals to perform state-driven routing, maximizing\ncluster-wide performance and SLO attainment. Performance evaluations show that\nNexusSched improves SLO attainment by 43% on average and achieves up to 3x\nthroughput speedup in long-context and heterogeneous scenarios. Besides, we\nalso deploy NexusSched on FlowGPT's clusters to demonstrate its advantages in\nproduction environment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM inference serving typically scales out with a two-tier architecture: a\ncluster router distributes requests to multiple inference engines, each of\nwhich then in turn performs its own internal scheduling. However, this commonly\nused paradigm suffers from critical, systemic inefficiency caused by the\ninformation gaps across two layers. At the cluster-layer, the router mainly\nrelies on lagging, coarse-grained metrics, such as average latency and queue\nlength to make decisions, resulting in \"decision lag\" that leads to suboptimal\nrequest routing. At the engine-layer, static heuristic scheduling policies\ncannot effectively handle the dynamic workloads, leading a poor balance between\nlatency and throughput. Besides, these gaps may cause SLO violations and\nresource waste, especially in heterogeneous cloud environments.\n  To bridge such gaps, we propose NexusSched, a cross-layer framework that\nshifts LLM serving system from reactive load balancing to predictive\norchestration. The core of NexusSched lies in a structurally-informed online\nperformance model that provides accurate, forward-looking per-step latency and\ncapacity estimations. This model empowers two key components. At the\nengine-layer, LENS performs SLO-aware, adaptive scheduling, dynamically\noptimizing batching to meet SLOs under real-time loads. At the cluster-layer,\nPRISM uses predictive signals to perform state-driven routing, maximizing\ncluster-wide performance and SLO attainment. Performance evaluations show that\nNexusSched improves SLO attainment by 43% on average and achieves up to 3x\nthroughput speedup in long-context and heterogeneous scenarios. Besides, we\nalso deploy NexusSched on FlowGPT's clusters to demonstrate its advantages in\nproduction environment."
                },
                "authors": [
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Yuansheng Chen"
                    },
                    {
                        "name": "Xuan Mo"
                    },
                    {
                        "name": "Alex Xi"
                    },
                    {
                        "name": "Jialun Li"
                    },
                    {
                        "name": "WeiGang Wu"
                    }
                ],
                "author_detail": {
                    "name": "WeiGang Wu"
                },
                "author": "WeiGang Wu",
                "arxiv_comment": "Update the system name in the summary",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23384v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23384v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19136v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19136v2",
                "updated": "2025-10-01T09:32:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    9,
                    32,
                    15,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-23T15:20:40Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    20,
                    40,
                    1,
                    266,
                    0
                ],
                "title": "On the Soundness and Consistency of LLM Agents for Executing Test Cases\n  Written in Natural Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Soundness and Consistency of LLM Agents for Executing Test Cases\n  Written in Natural Language"
                },
                "summary": "The use of natural language (NL) test cases for validating graphical user\ninterface (GUI) applications is emerging as a promising direction to manually\nwritten executable test scripts, which are costly to develop and difficult to\nmaintain. Recent advances in large language models (LLMs) have opened the\npossibility of the direct execution of NL test cases by LLM agents. This paper\ninvestigates this direction, focusing on the impact on NL test case unsoundness\nand on test case execution consistency. NL test cases are inherently unsound,\nas they may yield false failures due to ambiguous instructions or unpredictable\nagent behaviour. Furthermore, repeated executions of the same NL test case may\nlead to inconsistent outcomes, undermining test reliability. To address these\nchallenges, we propose an algorithm for executing NL test cases with guardrail\nmechanisms and specialised agents that dynamically verify the correct execution\nof each test step. We introduce measures to evaluate the capabilities of LLMs\nin test execution and one measure to quantify execution consistency. We propose\na definition of weak unsoundness to characterise contexts in which NL test case\nexecution remains acceptable, with respect to the industrial quality levels Six\nSigma. Our experimental evaluation with eight publicly available LLMs, ranging\nfrom 3B to 70B parameters, demonstrates both the potential and current\nlimitations of current LLM agents for GUI testing. Our experiments show that\nMeta Llama 3.1 70B demonstrates acceptable capabilities in NL test case\nexecution with high execution consistency (above the level 3-sigma). We provide\nprototype tools, test suites, and results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of natural language (NL) test cases for validating graphical user\ninterface (GUI) applications is emerging as a promising direction to manually\nwritten executable test scripts, which are costly to develop and difficult to\nmaintain. Recent advances in large language models (LLMs) have opened the\npossibility of the direct execution of NL test cases by LLM agents. This paper\ninvestigates this direction, focusing on the impact on NL test case unsoundness\nand on test case execution consistency. NL test cases are inherently unsound,\nas they may yield false failures due to ambiguous instructions or unpredictable\nagent behaviour. Furthermore, repeated executions of the same NL test case may\nlead to inconsistent outcomes, undermining test reliability. To address these\nchallenges, we propose an algorithm for executing NL test cases with guardrail\nmechanisms and specialised agents that dynamically verify the correct execution\nof each test step. We introduce measures to evaluate the capabilities of LLMs\nin test execution and one measure to quantify execution consistency. We propose\na definition of weak unsoundness to characterise contexts in which NL test case\nexecution remains acceptable, with respect to the industrial quality levels Six\nSigma. Our experimental evaluation with eight publicly available LLMs, ranging\nfrom 3B to 70B parameters, demonstrates both the potential and current\nlimitations of current LLM agents for GUI testing. Our experiments show that\nMeta Llama 3.1 70B demonstrates acceptable capabilities in NL test case\nexecution with high execution consistency (above the level 3-sigma). We provide\nprototype tools, test suites, and results."
                },
                "authors": [
                    {
                        "name": "Sébastien Salva"
                    },
                    {
                        "name": "Redha Taguelmimt"
                    }
                ],
                "author_detail": {
                    "name": "Redha Taguelmimt"
                },
                "author": "Redha Taguelmimt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19136v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19136v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.4; D.2.5; F.3.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13243v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13243v2",
                "updated": "2025-10-01T09:31:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    9,
                    31,
                    28,
                    2,
                    274,
                    0
                ],
                "published": "2025-02-18T19:17:01Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    19,
                    17,
                    1,
                    1,
                    49,
                    0
                ],
                "title": "Learning the Universe: Learning to Optimize Cosmic Initial Conditions\n  with Non-Differentiable Structure Formation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning the Universe: Learning to Optimize Cosmic Initial Conditions\n  with Non-Differentiable Structure Formation Models"
                },
                "summary": "Making the most of next-generation galaxy clustering surveys requires\novercoming challenges in complex, non-linear modelling to access the\nsignificant amount of information at smaller cosmological scales. Field-level\ninference has provided a unique opportunity beyond summary statistics to use\nall of the information of the galaxy distribution. However, addressing current\nchallenges often necessitates numerical modelling that incorporates\nnon-differentiable components, hindering the use of efficient gradient-based\ninference methods. In this paper, we introduce Learning the Universe by\nLearning to Optimize (LULO), a gradient-free framework for reconstructing the\n3D cosmic initial conditions. Our approach advances deep learning to train an\noptimization algorithm capable of fitting state-of-the-art non-differentiable\nsimulators to data at the field level. Importantly, the neural optimizer solely\nacts as a search engine in an iterative scheme, always maintaining full physics\nsimulations in the loop, ensuring scalability and reliability. We demonstrate\nthe method by accurately reconstructing initial conditions from\n$M_{200\\mathrm{c}}$ halos identified in a dark matter-only $N$-body simulation\nwith a spherical overdensity algorithm. The derived dark matter and halo\noverdensity fields exhibit $\\geq80\\%$ cross-correlation with the ground truth\ninto the non-linear regime $k \\sim 1h$ Mpc$^{-1}$. Additional cosmological\ntests reveal accurate recovery of the power spectra, bispectra, halo mass\nfunction, and velocities. With this work, we demonstrate a promising path\nforward to non-linear field-level inference surpassing the requirement of a\ndifferentiable physics model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making the most of next-generation galaxy clustering surveys requires\novercoming challenges in complex, non-linear modelling to access the\nsignificant amount of information at smaller cosmological scales. Field-level\ninference has provided a unique opportunity beyond summary statistics to use\nall of the information of the galaxy distribution. However, addressing current\nchallenges often necessitates numerical modelling that incorporates\nnon-differentiable components, hindering the use of efficient gradient-based\ninference methods. In this paper, we introduce Learning the Universe by\nLearning to Optimize (LULO), a gradient-free framework for reconstructing the\n3D cosmic initial conditions. Our approach advances deep learning to train an\noptimization algorithm capable of fitting state-of-the-art non-differentiable\nsimulators to data at the field level. Importantly, the neural optimizer solely\nacts as a search engine in an iterative scheme, always maintaining full physics\nsimulations in the loop, ensuring scalability and reliability. We demonstrate\nthe method by accurately reconstructing initial conditions from\n$M_{200\\mathrm{c}}$ halos identified in a dark matter-only $N$-body simulation\nwith a spherical overdensity algorithm. The derived dark matter and halo\noverdensity fields exhibit $\\geq80\\%$ cross-correlation with the ground truth\ninto the non-linear regime $k \\sim 1h$ Mpc$^{-1}$. Additional cosmological\ntests reveal accurate recovery of the power spectra, bispectra, halo mass\nfunction, and velocities. With this work, we demonstrate a promising path\nforward to non-linear field-level inference surpassing the requirement of a\ndifferentiable physics model."
                },
                "authors": [
                    {
                        "name": "Ludvig Doeser"
                    },
                    {
                        "name": "Metin Ata"
                    },
                    {
                        "name": "Jens Jasche"
                    }
                ],
                "author_detail": {
                    "name": "Jens Jasche"
                },
                "author": "Jens Jasche",
                "arxiv_doi": "10.1093/mnras/staf1289",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/mnras/staf1289",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.13243v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13243v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "20 pages, 15 figures. Updated to match version accepted by MNRAS\n  (published 2025/08/06)",
                "arxiv_journal_ref": "Monthly Notices of the Royal Astronomical Society, Volume 542,\n  Issue 2, September 2025, Pages 1403-1422",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19078v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19078v2",
                "updated": "2025-10-01T09:29:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    9,
                    29,
                    45,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-23T14:36:47Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    36,
                    47,
                    1,
                    266,
                    0
                ],
                "title": "Diffusion Bridge Variational Inference for Deep Gaussian Processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Bridge Variational Inference for Deep Gaussian Processes"
                },
                "summary": "Deep Gaussian processes (DGPs) enable expressive hierarchical Bayesian\nmodeling but pose substantial challenges for posterior inference, especially\nover inducing variables. Denoising diffusion variational inference (DDVI)\naddresses this by modeling the posterior as a time-reversed diffusion from a\nsimple Gaussian prior. However, DDVI's fixed unconditional starting\ndistribution remains far from the complex true posterior, resulting in\ninefficient inference trajectories and slow convergence. In this work, we\npropose Diffusion Bridge Variational Inference (DBVI), a principled extension\nof DDVI that initiates the reverse diffusion from a learnable, data-dependent\ninitial distribution. This initialization is parameterized via an amortized\nneural network and progressively adapted using gradients from the ELBO\nobjective, reducing the posterior gap and improving sample efficiency. To\nenable scalable amortization, we design the network to operate on the inducing\ninputs, which serve as structured, low-dimensional summaries of the dataset and\nnaturally align with the inducing variables' shape. DBVI retains the\nmathematical elegance of DDVI, including Girsanov-based ELBOs and reverse-time\nSDEs,while reinterpreting the prior via a Doob-bridged diffusion process. We\nderive a tractable training objective under this formulation and implement DBVI\nfor scalable inference in large-scale DGPs. Across regression, classification,\nand image reconstruction tasks, DBVI consistently outperforms DDVI and other\nvariational baselines in predictive accuracy, convergence speed, and posterior\nquality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Gaussian processes (DGPs) enable expressive hierarchical Bayesian\nmodeling but pose substantial challenges for posterior inference, especially\nover inducing variables. Denoising diffusion variational inference (DDVI)\naddresses this by modeling the posterior as a time-reversed diffusion from a\nsimple Gaussian prior. However, DDVI's fixed unconditional starting\ndistribution remains far from the complex true posterior, resulting in\ninefficient inference trajectories and slow convergence. In this work, we\npropose Diffusion Bridge Variational Inference (DBVI), a principled extension\nof DDVI that initiates the reverse diffusion from a learnable, data-dependent\ninitial distribution. This initialization is parameterized via an amortized\nneural network and progressively adapted using gradients from the ELBO\nobjective, reducing the posterior gap and improving sample efficiency. To\nenable scalable amortization, we design the network to operate on the inducing\ninputs, which serve as structured, low-dimensional summaries of the dataset and\nnaturally align with the inducing variables' shape. DBVI retains the\nmathematical elegance of DDVI, including Girsanov-based ELBOs and reverse-time\nSDEs,while reinterpreting the prior via a Doob-bridged diffusion process. We\nderive a tractable training objective under this formulation and implement DBVI\nfor scalable inference in large-scale DGPs. Across regression, classification,\nand image reconstruction tasks, DBVI consistently outperforms DDVI and other\nvariational baselines in predictive accuracy, convergence speed, and posterior\nquality."
                },
                "authors": [
                    {
                        "name": "Jian Xu"
                    },
                    {
                        "name": "Qibin Zhao"
                    },
                    {
                        "name": "John Paisley"
                    },
                    {
                        "name": "Delu Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Delu Zeng"
                },
                "author": "Delu Zeng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19078v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19078v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18706v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18706v3",
                "updated": "2025-10-01T09:23:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    9,
                    23,
                    7,
                    2,
                    274,
                    0
                ],
                "published": "2025-05-24T13:55:38Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    13,
                    55,
                    38,
                    5,
                    144,
                    0
                ],
                "title": "Steering LLM Reasoning Through Bias-Only Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steering LLM Reasoning Through Bias-Only Adaptation"
                },
                "summary": "We show that training a single $d$-dimensional steering vector per layer with\nreinforcement learning, while freezing all base weights, matches the accuracy\nof fully RL-tuned reasoning models on mathematical-reasoning tasks. On an 8\nbillion-parameter model this adds only $\\approx 0.0016\\%$ additional parameters\nand reproduces performance across a range of base models and\nmathematical-reasoning benchmarks. These results tighten the upper bound on the\nparameter budget required for high-level chain-of-thought reasoning, indicating\nthat millions of adapter weights are unnecessary. The minimal trainable\nfootprint reduces optimizer memory and inter-GPU communication, lowering the\noverall cost of fine-tuning. Moreover, a logit-lens analysis shows that the\nlearned vectors amplify coherent token directions, providing clearer insight\ninto the model's internal computations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We show that training a single $d$-dimensional steering vector per layer with\nreinforcement learning, while freezing all base weights, matches the accuracy\nof fully RL-tuned reasoning models on mathematical-reasoning tasks. On an 8\nbillion-parameter model this adds only $\\approx 0.0016\\%$ additional parameters\nand reproduces performance across a range of base models and\nmathematical-reasoning benchmarks. These results tighten the upper bound on the\nparameter budget required for high-level chain-of-thought reasoning, indicating\nthat millions of adapter weights are unnecessary. The minimal trainable\nfootprint reduces optimizer memory and inter-GPU communication, lowering the\noverall cost of fine-tuning. Moreover, a logit-lens analysis shows that the\nlearned vectors amplify coherent token directions, providing clearer insight\ninto the model's internal computations."
                },
                "authors": [
                    {
                        "name": "Viacheslav Sinii"
                    },
                    {
                        "name": "Alexey Gorbatovski"
                    },
                    {
                        "name": "Artem Cherepanov"
                    },
                    {
                        "name": "Boris Shaposhnikov"
                    },
                    {
                        "name": "Nikita Balagansky"
                    },
                    {
                        "name": "Daniil Gavrilov"
                    }
                ],
                "author_detail": {
                    "name": "Daniil Gavrilov"
                },
                "author": "Daniil Gavrilov",
                "arxiv_comment": "EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18706v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18706v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14378v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14378v2",
                "updated": "2025-10-01T09:12:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    9,
                    12,
                    0,
                    2,
                    274,
                    0
                ],
                "published": "2025-05-20T14:01:38Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    1,
                    38,
                    1,
                    140,
                    0
                ],
                "title": "Toward mapping turbulence in the intracluster medium IV. Using\n  NewAthena/X-IFU and simulation based inference to constrain turbulence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward mapping turbulence in the intracluster medium IV. Using\n  NewAthena/X-IFU and simulation based inference to constrain turbulence"
                },
                "summary": "Context. The NewAthena mission planned for launch in the late 2030s will\ncarry X-IFU, an integral field unit spectrometer that will obtain unique\ninsight into the X-ray hot universe through its combination of spectral and\nspatial capabilities. Its high spectral resolution will allow a mapping of\nturbulent velocities of the hot gas in galaxy clusters, providing an unrivaled\nway to study the complex dynamics within galaxy clusters. Aims. This is the\nfourth in a series of papers aimed at forecasting the ability to investigate\nturbulence in the intracluster medium through the observation of the centroid\nshift caused by turbulent motions of the gas. In this paper we improve on\nprevious methods by investigating the ability of simulation-based inference\n(SBI) to constrain the underlying nature of velocity fluctuations through the\nuse of standard observational diagnostics, such as the structure function.\nMethods. We rely on a complex architecture of neural networks in order to model\nthe likelihood and posterior distributions relevant to our case. We investigate\nits capability to retrieve the turbulence parameters on mock observations, and\nexplore its capability to use alternative summary statistics. Results. Our\ntrained models are able to infer the parameters of the intracluster gas\nvelocity power-spectrum in independently simulated X-IFU observations of a\ngalaxy cluster. We evaluated the precision of the recovery for different\nmodels. We show the necessity to use methods such as SBI to avoid an\nunder-estimation of the sources of variance by comparing the results to our\nprevious paper. We confirm that sample variance severely impacts the precision\nof recovered turbulent features. Our results demonstrate the need for advanced\nmodeling methods to tackle the complexity of the physical information nested\nwithin future observations expected from X-IFU/NewAthena.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context. The NewAthena mission planned for launch in the late 2030s will\ncarry X-IFU, an integral field unit spectrometer that will obtain unique\ninsight into the X-ray hot universe through its combination of spectral and\nspatial capabilities. Its high spectral resolution will allow a mapping of\nturbulent velocities of the hot gas in galaxy clusters, providing an unrivaled\nway to study the complex dynamics within galaxy clusters. Aims. This is the\nfourth in a series of papers aimed at forecasting the ability to investigate\nturbulence in the intracluster medium through the observation of the centroid\nshift caused by turbulent motions of the gas. In this paper we improve on\nprevious methods by investigating the ability of simulation-based inference\n(SBI) to constrain the underlying nature of velocity fluctuations through the\nuse of standard observational diagnostics, such as the structure function.\nMethods. We rely on a complex architecture of neural networks in order to model\nthe likelihood and posterior distributions relevant to our case. We investigate\nits capability to retrieve the turbulence parameters on mock observations, and\nexplore its capability to use alternative summary statistics. Results. Our\ntrained models are able to infer the parameters of the intracluster gas\nvelocity power-spectrum in independently simulated X-IFU observations of a\ngalaxy cluster. We evaluated the precision of the recovery for different\nmodels. We show the necessity to use methods such as SBI to avoid an\nunder-estimation of the sources of variance by comparing the results to our\nprevious paper. We confirm that sample variance severely impacts the precision\nof recovered turbulent features. Our results demonstrate the need for advanced\nmodeling methods to tackle the complexity of the physical information nested\nwithin future observations expected from X-IFU/NewAthena."
                },
                "authors": [
                    {
                        "name": "Alexeï Molin"
                    },
                    {
                        "name": "Simon Dupourqué"
                    },
                    {
                        "name": "Nicolas Clerc"
                    },
                    {
                        "name": "Étienne Pointecouteau"
                    },
                    {
                        "name": "François Pajot"
                    },
                    {
                        "name": "Edoardo Cucchetti"
                    }
                ],
                "author_detail": {
                    "name": "Edoardo Cucchetti"
                },
                "author": "Edoardo Cucchetti",
                "arxiv_comment": "Accepted in Astronomy and Astrophysics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14378v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14378v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23619v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23619v2",
                "updated": "2025-10-01T08:57:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    8,
                    57,
                    37,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-28T03:49:32Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    3,
                    49,
                    32,
                    6,
                    271,
                    0
                ],
                "title": "Reasoning Scaffolding: Distilling the Flow of Thought from LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Scaffolding: Distilling the Flow of Thought from LLMs"
                },
                "summary": "The prevailing approach to distilling reasoning from Large Language Models\n(LLMs)-behavioral cloning from textual rationales-is fundamentally limited. It\nteaches Small Language Models (SLMs) to mimic surface-level patterns rather\nthan the underlying algorithmic structure of thought, resulting in a critical\nlack of logical robustness. We argue that instead of cloning text, distillation\nshould transfer this algorithmic structure directly. We introduce Reasoning\nScaffolding}, a framework that reframes reasoning as a structured generation\nprocess. Our method first abstracts the teacher's thought process into a\nsequence of discrete, interpretable semantic signals (e.g., Contrast, Addition)\nthat act as a scaffold. The student model is then trained via a multi-task\nobjective to both (1)predict the next semantic signal, anticipating the\nreasoning flow, and (2)generate the corresponding step, conditioned on that\nsignal. This multi-task scheme acts as a powerful regularizer, compelling the\nstudent to internalize the computational patterns of coherent reasoning. On a\nsuite of challenging reasoning benchmarks, our method significantly outperforms\nstate-of-the-art distillation in both accuracy and logical consistency,\nproviding a path towards creating smaller models that are genuine reasoners,\nnot just fluent mimics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The prevailing approach to distilling reasoning from Large Language Models\n(LLMs)-behavioral cloning from textual rationales-is fundamentally limited. It\nteaches Small Language Models (SLMs) to mimic surface-level patterns rather\nthan the underlying algorithmic structure of thought, resulting in a critical\nlack of logical robustness. We argue that instead of cloning text, distillation\nshould transfer this algorithmic structure directly. We introduce Reasoning\nScaffolding}, a framework that reframes reasoning as a structured generation\nprocess. Our method first abstracts the teacher's thought process into a\nsequence of discrete, interpretable semantic signals (e.g., Contrast, Addition)\nthat act as a scaffold. The student model is then trained via a multi-task\nobjective to both (1)predict the next semantic signal, anticipating the\nreasoning flow, and (2)generate the corresponding step, conditioned on that\nsignal. This multi-task scheme acts as a powerful regularizer, compelling the\nstudent to internalize the computational patterns of coherent reasoning. On a\nsuite of challenging reasoning benchmarks, our method significantly outperforms\nstate-of-the-art distillation in both accuracy and logical consistency,\nproviding a path towards creating smaller models that are genuine reasoners,\nnot just fluent mimics."
                },
                "authors": [
                    {
                        "name": "Xiangyu Wen"
                    },
                    {
                        "name": "Junhua Huang"
                    },
                    {
                        "name": "Zeju Li"
                    },
                    {
                        "name": "Min Li"
                    },
                    {
                        "name": "Jianyuan Zhong"
                    },
                    {
                        "name": "Zhijian Xu"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    },
                    {
                        "name": "Yongxiang Huang"
                    },
                    {
                        "name": "Qiang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Qiang Xu"
                },
                "author": "Qiang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23619v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23619v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17247v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17247v2",
                "updated": "2025-10-01T08:47:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    8,
                    47,
                    8,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-21T21:44:10Z",
                "published_parsed": [
                    2025,
                    9,
                    21,
                    21,
                    44,
                    10,
                    6,
                    264,
                    0
                ],
                "title": "DeepASA: An Object-Oriented One-for-All Network for Auditory Scene\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepASA: An Object-Oriented One-for-All Network for Auditory Scene\n  Analysis"
                },
                "summary": "We propose DeepASA, a multi-purpose model for auditory scene analysis that\nperforms multi-input multi-output (MIMO) source separation, dereverberation,\nsound event detection (SED), audio classification, and direction-of-arrival\nestimation (DoAE) within a unified framework. DeepASA is designed for complex\nauditory scenes where multiple, often similar, sound sources overlap in time\nand move dynamically in space. To achieve robust and consistent inference\nacross tasks, we introduce an object-oriented processing (OOP) strategy. This\napproach encapsulates diverse auditory features into object-centric\nrepresentations and refines them through a chain-of-inference (CoI) mechanism.\nThe pipeline comprises a dynamic temporal kernel-based feature extractor, a\ntransformer-based aggregator, and an object separator that yields per-object\nfeatures. These features feed into multiple task-specific decoders. Our\nobject-centric representations naturally resolve the parameter association\nambiguity inherent in traditional track-wise processing. However, early-stage\nobject separation can lead to failure in downstream ASA tasks. To address this,\nwe implement temporal coherence matching (TCM) within the chain-of-inference,\nenabling multi-task fusion and iterative refinement of object features using\nestimated auditory parameters. We evaluate DeepASA on representative spatial\naudio benchmark datasets, including ASA2, MC-FUSS, and STARSS23. Experimental\nresults show that our model achieves state-of-the-art performance across all\nevaluated tasks, demonstrating its effectiveness in both source separation and\nauditory parameter estimation under diverse spatial auditory scenes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose DeepASA, a multi-purpose model for auditory scene analysis that\nperforms multi-input multi-output (MIMO) source separation, dereverberation,\nsound event detection (SED), audio classification, and direction-of-arrival\nestimation (DoAE) within a unified framework. DeepASA is designed for complex\nauditory scenes where multiple, often similar, sound sources overlap in time\nand move dynamically in space. To achieve robust and consistent inference\nacross tasks, we introduce an object-oriented processing (OOP) strategy. This\napproach encapsulates diverse auditory features into object-centric\nrepresentations and refines them through a chain-of-inference (CoI) mechanism.\nThe pipeline comprises a dynamic temporal kernel-based feature extractor, a\ntransformer-based aggregator, and an object separator that yields per-object\nfeatures. These features feed into multiple task-specific decoders. Our\nobject-centric representations naturally resolve the parameter association\nambiguity inherent in traditional track-wise processing. However, early-stage\nobject separation can lead to failure in downstream ASA tasks. To address this,\nwe implement temporal coherence matching (TCM) within the chain-of-inference,\nenabling multi-task fusion and iterative refinement of object features using\nestimated auditory parameters. We evaluate DeepASA on representative spatial\naudio benchmark datasets, including ASA2, MC-FUSS, and STARSS23. Experimental\nresults show that our model achieves state-of-the-art performance across all\nevaluated tasks, demonstrating its effectiveness in both source separation and\nauditory parameter estimation under diverse spatial auditory scenes."
                },
                "authors": [
                    {
                        "name": "Dongheon Lee"
                    },
                    {
                        "name": "Younghoo Kwon"
                    },
                    {
                        "name": "Jung-Woo Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jung-Woo Choi"
                },
                "author": "Jung-Woo Choi",
                "arxiv_comment": "19 pages, 13 figures, 8 tables, accepted in NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17247v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17247v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06608v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06608v3",
                "updated": "2025-10-01T08:37:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    8,
                    37,
                    7,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-08T12:26:31Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    12,
                    26,
                    31,
                    0,
                    251,
                    0
                ],
                "title": "Small Vectors, Big Effects: A Mechanistic Study of RL-Induced Reasoning\n  via Steering Vectors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small Vectors, Big Effects: A Mechanistic Study of RL-Induced Reasoning\n  via Steering Vectors"
                },
                "summary": "The mechanisms by which reasoning training reshapes LLMs' internal\ncomputations remain unclear. We study lightweight steering vectors inserted\ninto the base model's residual stream and trained with a reinforcement-learning\nobjective. These vectors match full fine-tuning performance while preserving\nthe interpretability of small, additive interventions. Using logit-lens\nreadouts and path-patching analyses on two models, we find that (i) the\nlast-layer steering vector acts like a token-substitution bias concentrated on\nthe first generated token, consistently boosting tokens such as \"To\" and\n\"Step\"; (ii) the penultimate-layer vector leaves attention patterns largely\nintact and instead operates through the MLP and unembedding, preferentially\nup-weighting process words and structure symbols; and (iii) middle layers\nde-emphasize non-English tokens. Next, we show that a SAE isolates features\nassociated with correct generations. We also show that steering vectors (i)\ntransfer to other models, (ii) combine across layers when trained in isolation,\nand (iii) concentrate magnitude on meaningful prompt segments under adaptive\ntoken-wise scaling. Taken together, these results deepen understanding of how\ntrained steering vectors shape computation and should inform future work in\nactivation engineering and the study of reasoning models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The mechanisms by which reasoning training reshapes LLMs' internal\ncomputations remain unclear. We study lightweight steering vectors inserted\ninto the base model's residual stream and trained with a reinforcement-learning\nobjective. These vectors match full fine-tuning performance while preserving\nthe interpretability of small, additive interventions. Using logit-lens\nreadouts and path-patching analyses on two models, we find that (i) the\nlast-layer steering vector acts like a token-substitution bias concentrated on\nthe first generated token, consistently boosting tokens such as \"To\" and\n\"Step\"; (ii) the penultimate-layer vector leaves attention patterns largely\nintact and instead operates through the MLP and unembedding, preferentially\nup-weighting process words and structure symbols; and (iii) middle layers\nde-emphasize non-English tokens. Next, we show that a SAE isolates features\nassociated with correct generations. We also show that steering vectors (i)\ntransfer to other models, (ii) combine across layers when trained in isolation,\nand (iii) concentrate magnitude on meaningful prompt segments under adaptive\ntoken-wise scaling. Taken together, these results deepen understanding of how\ntrained steering vectors shape computation and should inform future work in\nactivation engineering and the study of reasoning models."
                },
                "authors": [
                    {
                        "name": "Viacheslav Sinii"
                    },
                    {
                        "name": "Nikita Balagansky"
                    },
                    {
                        "name": "Gleb Gerasimov"
                    },
                    {
                        "name": "Daniil Laptev"
                    },
                    {
                        "name": "Yaroslav Aksenov"
                    },
                    {
                        "name": "Vadim Kurochkin"
                    },
                    {
                        "name": "Alexey Gorbatovski"
                    },
                    {
                        "name": "Boris Shaposhnikov"
                    },
                    {
                        "name": "Daniil Gavrilov"
                    }
                ],
                "author_detail": {
                    "name": "Daniil Gavrilov"
                },
                "author": "Daniil Gavrilov",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06608v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06608v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15935v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15935v2",
                "updated": "2025-10-01T08:20:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    8,
                    20,
                    37,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-19T12:40:49Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    12,
                    40,
                    49,
                    4,
                    262,
                    0
                ],
                "title": "PAN: Pillars-Attention-Based Network for 3D Object Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PAN: Pillars-Attention-Based Network for 3D Object Detection"
                },
                "summary": "Camera-radar fusion offers a robust and low-cost alternative to Camera-lidar\nfusion for the 3D object detection task in real-time under adverse weather and\nlighting conditions. However, currently, in the literature, it is possible to\nfind few works focusing on this modality and, most importantly, developing new\narchitectures to explore the advantages of the radar point cloud, such as\naccurate distance estimation and speed information. Therefore, this work\npresents a novel and efficient 3D object detection algorithm using cameras and\nradars in the bird's-eye-view (BEV). Our algorithm exploits the advantages of\nradar before fusing the features into a detection head. A new backbone is\nintroduced, which maps the radar pillar features into an embedded dimension. A\nself-attention mechanism allows the backbone to model the dependencies between\nthe radar points. We are using a simplified convolutional layer to replace the\nFPN-based convolutional layers used in the PointPillars-based architectures\nwith the main goal of reducing inference time. Our results show that with this\nmodification, our approach achieves the new state-of-the-art in the 3D object\ndetection problem, reaching 58.2 of the NDS metric for the use of ResNet-50,\nwhile also setting a new benchmark for inference time on the nuScenes dataset\nfor the same category.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Camera-radar fusion offers a robust and low-cost alternative to Camera-lidar\nfusion for the 3D object detection task in real-time under adverse weather and\nlighting conditions. However, currently, in the literature, it is possible to\nfind few works focusing on this modality and, most importantly, developing new\narchitectures to explore the advantages of the radar point cloud, such as\naccurate distance estimation and speed information. Therefore, this work\npresents a novel and efficient 3D object detection algorithm using cameras and\nradars in the bird's-eye-view (BEV). Our algorithm exploits the advantages of\nradar before fusing the features into a detection head. A new backbone is\nintroduced, which maps the radar pillar features into an embedded dimension. A\nself-attention mechanism allows the backbone to model the dependencies between\nthe radar points. We are using a simplified convolutional layer to replace the\nFPN-based convolutional layers used in the PointPillars-based architectures\nwith the main goal of reducing inference time. Our results show that with this\nmodification, our approach achieves the new state-of-the-art in the 3D object\ndetection problem, reaching 58.2 of the NDS metric for the use of ResNet-50,\nwhile also setting a new benchmark for inference time on the nuScenes dataset\nfor the same category."
                },
                "authors": [
                    {
                        "name": "Ruan Bispo"
                    },
                    {
                        "name": "Dane Mitrev"
                    },
                    {
                        "name": "Letizia Mariotti"
                    },
                    {
                        "name": "Clément Botty"
                    },
                    {
                        "name": "Denver Humphrey"
                    },
                    {
                        "name": "Anthony Scanlan"
                    },
                    {
                        "name": "Ciarán Eising"
                    }
                ],
                "author_detail": {
                    "name": "Ciarán Eising"
                },
                "author": "Ciarán Eising",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15935v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15935v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.09469v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.09469v2",
                "updated": "2025-10-01T08:10:57Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    8,
                    10,
                    57,
                    2,
                    274,
                    0
                ],
                "published": "2023-10-14T02:19:07Z",
                "published_parsed": [
                    2023,
                    10,
                    14,
                    2,
                    19,
                    7,
                    5,
                    287,
                    0
                ],
                "title": "Towards More Accurate Diffusion Model Acceleration with A Timestep Tuner",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards More Accurate Diffusion Model Acceleration with A Timestep Tuner"
                },
                "summary": "A diffusion model, which is formulated to produce an image using thousands of\ndenoising steps, usually suffers from a slow inference speed. Existing\nacceleration algorithms simplify the sampling by skipping most steps yet\nexhibit considerable performance degradation. By viewing the generation of\ndiffusion models as a discretized integral process, we argue that the quality\ndrop is partly caused by applying an inaccurate integral direction to a\ntimestep interval. To rectify this issue, we propose a \\textbf{timestep tuner}\nthat helps find a more accurate integral direction for a particular interval at\nthe minimum cost. Specifically, at each denoising step, we replace the original\nparameterization by conditioning the network on a new timestep, enforcing the\nsampling distribution towards the real one. Extensive experiments show that our\nplug-in design can be trained efficiently and boost the inference performance\nof various state-of-the-art acceleration methods, especially when there are few\ndenoising steps. For example, when using 10 denoising steps on LSUN Bedroom\ndataset, we improve the FID of DDIM from 9.65 to 6.07, simply by adopting our\nmethod for a more appropriate set of timesteps. Code is available at\n\\href{https://github.com/THU-LYJ-Lab/time-tuner}{https://github.com/THU-LYJ-Lab/time-tuner}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A diffusion model, which is formulated to produce an image using thousands of\ndenoising steps, usually suffers from a slow inference speed. Existing\nacceleration algorithms simplify the sampling by skipping most steps yet\nexhibit considerable performance degradation. By viewing the generation of\ndiffusion models as a discretized integral process, we argue that the quality\ndrop is partly caused by applying an inaccurate integral direction to a\ntimestep interval. To rectify this issue, we propose a \\textbf{timestep tuner}\nthat helps find a more accurate integral direction for a particular interval at\nthe minimum cost. Specifically, at each denoising step, we replace the original\nparameterization by conditioning the network on a new timestep, enforcing the\nsampling distribution towards the real one. Extensive experiments show that our\nplug-in design can be trained efficiently and boost the inference performance\nof various state-of-the-art acceleration methods, especially when there are few\ndenoising steps. For example, when using 10 denoising steps on LSUN Bedroom\ndataset, we improve the FID of DDIM from 9.65 to 6.07, simply by adopting our\nmethod for a more appropriate set of timesteps. Code is available at\n\\href{https://github.com/THU-LYJ-Lab/time-tuner}{https://github.com/THU-LYJ-Lab/time-tuner}."
                },
                "authors": [
                    {
                        "name": "Mengfei Xia"
                    },
                    {
                        "name": "Yujun Shen"
                    },
                    {
                        "name": "Changsong Lei"
                    },
                    {
                        "name": "Yu Zhou"
                    },
                    {
                        "name": "Ran Yi"
                    },
                    {
                        "name": "Deli Zhao"
                    },
                    {
                        "name": "Wenping Wang"
                    },
                    {
                        "name": "Yong-Jin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong-Jin Liu"
                },
                "author": "Yong-Jin Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.09469v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.09469v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03113v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03113v2",
                "updated": "2025-10-01T07:53:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    7,
                    53,
                    37,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-03T08:13:52Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    13,
                    52,
                    2,
                    246,
                    0
                ],
                "title": "Mitigating Multimodal Hallucinations via Gradient-based Self-Reflection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Multimodal Hallucinations via Gradient-based Self-Reflection"
                },
                "summary": "Multimodal large language models achieve strong performance across diverse\ntasks but remain prone to hallucinations, where outputs are not grounded in\nvisual inputs. This issue can be attributed to two main biases: text-visual\nbias, the overreliance on prompts and prior outputs, and co-occurrence bias,\nspurious correlations between frequently paired objects. We propose\nGradient-based Influence-Aware Constrained Decoding (GACD), an inference-based\nmethod, that addresses both biases without auxiliary models, and is readily\napplicable to existing models without finetuning. The core of our approach is\nbias estimation, which uses first-order Taylor gradients to understand the\ncontribution of individual tokens-visual features and text tokens-to the\ncurrent output. Based on this analysis, GACD mitigates hallucinations through\ntwo components: (1) suppressing spurious visual features correlated with the\noutput objects, and (2) rebalancing cross-modal contributions by strengthening\nvisual features relative to text. Experiments across multiple benchmarks\ndemonstrate that GACD effectively reduces hallucinations and improves the\nvisual grounding of MLLM outputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models achieve strong performance across diverse\ntasks but remain prone to hallucinations, where outputs are not grounded in\nvisual inputs. This issue can be attributed to two main biases: text-visual\nbias, the overreliance on prompts and prior outputs, and co-occurrence bias,\nspurious correlations between frequently paired objects. We propose\nGradient-based Influence-Aware Constrained Decoding (GACD), an inference-based\nmethod, that addresses both biases without auxiliary models, and is readily\napplicable to existing models without finetuning. The core of our approach is\nbias estimation, which uses first-order Taylor gradients to understand the\ncontribution of individual tokens-visual features and text tokens-to the\ncurrent output. Based on this analysis, GACD mitigates hallucinations through\ntwo components: (1) suppressing spurious visual features correlated with the\noutput objects, and (2) rebalancing cross-modal contributions by strengthening\nvisual features relative to text. Experiments across multiple benchmarks\ndemonstrate that GACD effectively reduces hallucinations and improves the\nvisual grounding of MLLM outputs."
                },
                "authors": [
                    {
                        "name": "Shan Wang"
                    },
                    {
                        "name": "Maying Shen"
                    },
                    {
                        "name": "Nadine Chang"
                    },
                    {
                        "name": "Chuong Nguyen"
                    },
                    {
                        "name": "Hongdong Li"
                    },
                    {
                        "name": "Jose M. Alvarez"
                    }
                ],
                "author_detail": {
                    "name": "Jose M. Alvarez"
                },
                "author": "Jose M. Alvarez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03113v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03113v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18980v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18980v2",
                "updated": "2025-10-01T07:49:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    7,
                    49,
                    58,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-23T13:30:03Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    13,
                    30,
                    3,
                    1,
                    266,
                    0
                ],
                "title": "From latent factors to language: a user study on LLM-generated\n  explanations for an inherently interpretable matrix-based recommender system",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From latent factors to language: a user study on LLM-generated\n  explanations for an inherently interpretable matrix-based recommender system"
                },
                "summary": "We investigate whether large language models (LLMs) can generate effective,\nuser-facing explanations from a mathematically interpretable recommendation\nmodel. The model is based on constrained matrix factorization, where user types\nare explicitly represented and predicted item scores share the same scale as\nobserved ratings, making the model's internal representations and predicted\nscores directly interpretable. This structure is translated into natural\nlanguage explanations using carefully designed LLM prompts. Many works in\nexplainable AI rely on automatic evaluation metrics, which often fail to\ncapture users' actual needs and perceptions. In contrast, we adopt a\nuser-centered approach: we conduct a study with 326 participants who assessed\nthe quality of the explanations across five key dimensions-transparency,\neffectiveness, persuasion, trust, and satisfaction-as well as the\nrecommendations themselves. To evaluate how different explanation strategies\nare perceived, we generate multiple explanation types from the same underlying\nmodel, varying the input information provided to the LLM. Our analysis reveals\nthat all explanation types are generally well received, with moderate\nstatistical differences between strategies. User comments further underscore\nhow participants react to each type of explanation, offering complementary\ninsights beyond the quantitative results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate whether large language models (LLMs) can generate effective,\nuser-facing explanations from a mathematically interpretable recommendation\nmodel. The model is based on constrained matrix factorization, where user types\nare explicitly represented and predicted item scores share the same scale as\nobserved ratings, making the model's internal representations and predicted\nscores directly interpretable. This structure is translated into natural\nlanguage explanations using carefully designed LLM prompts. Many works in\nexplainable AI rely on automatic evaluation metrics, which often fail to\ncapture users' actual needs and perceptions. In contrast, we adopt a\nuser-centered approach: we conduct a study with 326 participants who assessed\nthe quality of the explanations across five key dimensions-transparency,\neffectiveness, persuasion, trust, and satisfaction-as well as the\nrecommendations themselves. To evaluate how different explanation strategies\nare perceived, we generate multiple explanation types from the same underlying\nmodel, varying the input information provided to the LLM. Our analysis reveals\nthat all explanation types are generally well received, with moderate\nstatistical differences between strategies. User comments further underscore\nhow participants react to each type of explanation, offering complementary\ninsights beyond the quantitative results."
                },
                "authors": [
                    {
                        "name": "Maxime Manderlier"
                    },
                    {
                        "name": "Fabian Lecron"
                    },
                    {
                        "name": "Olivier Vu Thanh"
                    },
                    {
                        "name": "Nicolas Gillis"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Gillis"
                },
                "author": "Nicolas Gillis",
                "arxiv_journal_ref": "In Proceedings of the 12th Joint Workshop on Interfaces and Human\n  Decision Making for Recommender Systems (IntRS 2025) co-located with 19th ACM\n  Conference on Recommender Systems (RecSys 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18980v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18980v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.3; H.5.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18842v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18842v3",
                "updated": "2025-10-01T07:23:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    7,
                    23,
                    42,
                    2,
                    274,
                    0
                ],
                "published": "2025-05-24T19:30:47Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    19,
                    30,
                    47,
                    5,
                    144,
                    0
                ],
                "title": "v1: Learning to Point Visual Tokens for Multimodal Grounded Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "v1: Learning to Point Visual Tokens for Multimodal Grounded Reasoning"
                },
                "summary": "When thinking with images, humans rarely rely on a single glance: they\nrevisit visual information repeatedly during reasoning. However, existing\nmodels typically process images only once and thereafter generate reasoning\nentirely in text, lacking mechanisms to re-access or ground inference in visual\nrepresentations. We empirically confirm this: as reasoning chains lengthen,\nmodels progressively lose focus on relevant regions. In response, we introduce\nv1, a lightweight extension that enables active visual referencing through a\nsimple point-and-copy approach. This allows the model to identify relevant\nimage patches and copy their embeddings back into the reasoning stream,\nensuring that evolving hypotheses remain grounded in perceptual evidence.\nCrucially, our pointing strategy lets the MLLM directly select image patches\nusing their semantic representations as keys, keeping perceptual evidence\nembedded in the same space as the model's reasoning. To train this capability,\nwe construct v1g, a dataset of 300K multimodal reasoning traces with\ninterleaved visual grounding annotations. Across various multimodal\nmathematical reasoning benchmarks, v1 consistently outperforms comparable\nbaselines, establishing point-and-copy as a practical mechanism for grounded\nreasoning. The model checkpoint and dataset are available at\ngithub.com/jun297/v1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When thinking with images, humans rarely rely on a single glance: they\nrevisit visual information repeatedly during reasoning. However, existing\nmodels typically process images only once and thereafter generate reasoning\nentirely in text, lacking mechanisms to re-access or ground inference in visual\nrepresentations. We empirically confirm this: as reasoning chains lengthen,\nmodels progressively lose focus on relevant regions. In response, we introduce\nv1, a lightweight extension that enables active visual referencing through a\nsimple point-and-copy approach. This allows the model to identify relevant\nimage patches and copy their embeddings back into the reasoning stream,\nensuring that evolving hypotheses remain grounded in perceptual evidence.\nCrucially, our pointing strategy lets the MLLM directly select image patches\nusing their semantic representations as keys, keeping perceptual evidence\nembedded in the same space as the model's reasoning. To train this capability,\nwe construct v1g, a dataset of 300K multimodal reasoning traces with\ninterleaved visual grounding annotations. Across various multimodal\nmathematical reasoning benchmarks, v1 consistently outperforms comparable\nbaselines, establishing point-and-copy as a practical mechanism for grounded\nreasoning. The model checkpoint and dataset are available at\ngithub.com/jun297/v1."
                },
                "authors": [
                    {
                        "name": "Jiwan Chung"
                    },
                    {
                        "name": "Junhyeok Kim"
                    },
                    {
                        "name": "Siyeol Kim"
                    },
                    {
                        "name": "Jaeyoung Lee"
                    },
                    {
                        "name": "Min Soo Kim"
                    },
                    {
                        "name": "Youngjae Yu"
                    }
                ],
                "author_detail": {
                    "name": "Youngjae Yu"
                },
                "author": "Youngjae Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18842v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18842v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17630v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17630v3",
                "updated": "2025-10-01T07:18:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    7,
                    18,
                    47,
                    2,
                    274,
                    0
                ],
                "published": "2025-05-23T08:41:45Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    8,
                    41,
                    45,
                    4,
                    143,
                    0
                ],
                "title": "GIM: Improved Interpretability for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GIM: Improved Interpretability for Large Language Models"
                },
                "summary": "Ensuring faithful interpretability in large language models is imperative for\ntrustworthy and reliable AI. A key obstacle is self-repair, a phenomenon where\nnetworks compensate for reduced signal in one component by amplifying others,\nmasking the true importance of the ablated component. While prior work\nattributes self-repair to layer normalization and back-up components that\ncompensate for ablated components, we identify a novel form occurring within\nthe attention mechanism, where softmax redistribution conceals the influence of\nimportant attention scores. This leads traditional ablation and gradient-based\nmethods to underestimate the significance of all components contributing to\nthese attention scores. We introduce Gradient Interaction Modifications (GIM),\na technique that accounts for self-repair during backpropagation. Extensive\nexperiments across multiple large language models (Gemma 2B/9B, LLAMA 1B/3B/8B,\nQwen 1.5B/3B) and diverse tasks demonstrate that GIM significantly improves\nfaithfulness over existing circuit identification and feature attribution\nmethods. Our work is a significant step toward better understanding the inner\nmechanisms of LLMs, which is crucial for improving them and ensuring their\nsafety. Our code is available at https://github.com/JoakimEdin/gim.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring faithful interpretability in large language models is imperative for\ntrustworthy and reliable AI. A key obstacle is self-repair, a phenomenon where\nnetworks compensate for reduced signal in one component by amplifying others,\nmasking the true importance of the ablated component. While prior work\nattributes self-repair to layer normalization and back-up components that\ncompensate for ablated components, we identify a novel form occurring within\nthe attention mechanism, where softmax redistribution conceals the influence of\nimportant attention scores. This leads traditional ablation and gradient-based\nmethods to underestimate the significance of all components contributing to\nthese attention scores. We introduce Gradient Interaction Modifications (GIM),\na technique that accounts for self-repair during backpropagation. Extensive\nexperiments across multiple large language models (Gemma 2B/9B, LLAMA 1B/3B/8B,\nQwen 1.5B/3B) and diverse tasks demonstrate that GIM significantly improves\nfaithfulness over existing circuit identification and feature attribution\nmethods. Our work is a significant step toward better understanding the inner\nmechanisms of LLMs, which is crucial for improving them and ensuring their\nsafety. Our code is available at https://github.com/JoakimEdin/gim."
                },
                "authors": [
                    {
                        "name": "Joakim Edin"
                    },
                    {
                        "name": "Róbert Csordás"
                    },
                    {
                        "name": "Tuukka Ruotsalo"
                    },
                    {
                        "name": "Zhengxuan Wu"
                    },
                    {
                        "name": "Maria Maistro"
                    },
                    {
                        "name": "Casper L. Christensen"
                    },
                    {
                        "name": "Jing Huang"
                    },
                    {
                        "name": "Lars Maaløe"
                    }
                ],
                "author_detail": {
                    "name": "Lars Maaløe"
                },
                "author": "Lars Maaløe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17630v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17630v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04215v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04215v2",
                "updated": "2025-10-01T07:14:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    7,
                    14,
                    50,
                    2,
                    274,
                    0
                ],
                "published": "2024-05-07T11:27:13Z",
                "published_parsed": [
                    2024,
                    5,
                    7,
                    11,
                    27,
                    13,
                    1,
                    128,
                    0
                ],
                "title": "NL2Plan: Robust LLM-Driven Planning from Minimal Text Descriptions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NL2Plan: Robust LLM-Driven Planning from Minimal Text Descriptions"
                },
                "summary": "Classical planners are powerful systems, but modeling tasks in input formats\nsuch as PDDL is tedious and error-prone. In contrast, planning with Large\nLanguage Models (LLMs) allows for almost any input text, but offers no\nguarantees on plan quality or even soundness. In an attempt to merge the best\nof these two approaches, some work has begun to use LLMs to automate parts of\nthe PDDL creation process. However, these methods still require various degrees\nof expert input or domain-specific adaptations. We present NL2Plan, the first\nfully automatic system for generating complete PDDL tasks from minimal natural\nlanguage descriptions. NL2Plan uses an LLM to incrementally extract the\nnecessary information from the short text input before creating a complete PDDL\ndescription of both the domain and the problem which is finally solved by a\nclassical planner. We evaluate NL2Plan on seven planning domains, five of which\nare novel and thus not in the LLM training data, and find that NL2Plan\noutperforms directly generating the files with an LLM+validator combination. As\nsuch, NL2Plan is a powerful tool for assistive PDDL modeling and a step towards\nsolving natural language planning task with interpretability and guarantees.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Classical planners are powerful systems, but modeling tasks in input formats\nsuch as PDDL is tedious and error-prone. In contrast, planning with Large\nLanguage Models (LLMs) allows for almost any input text, but offers no\nguarantees on plan quality or even soundness. In an attempt to merge the best\nof these two approaches, some work has begun to use LLMs to automate parts of\nthe PDDL creation process. However, these methods still require various degrees\nof expert input or domain-specific adaptations. We present NL2Plan, the first\nfully automatic system for generating complete PDDL tasks from minimal natural\nlanguage descriptions. NL2Plan uses an LLM to incrementally extract the\nnecessary information from the short text input before creating a complete PDDL\ndescription of both the domain and the problem which is finally solved by a\nclassical planner. We evaluate NL2Plan on seven planning domains, five of which\nare novel and thus not in the LLM training data, and find that NL2Plan\noutperforms directly generating the files with an LLM+validator combination. As\nsuch, NL2Plan is a powerful tool for assistive PDDL modeling and a step towards\nsolving natural language planning task with interpretability and guarantees."
                },
                "authors": [
                    {
                        "name": "Elliot Gestrin"
                    },
                    {
                        "name": "Marco Kuhlmann"
                    },
                    {
                        "name": "Jendrik Seipp"
                    }
                ],
                "author_detail": {
                    "name": "Jendrik Seipp"
                },
                "author": "Jendrik Seipp",
                "arxiv_comment": "Accepted for the ICAPS 2024 Workshop on Human-Aware and Explainable\n  Planning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.04215v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04215v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19552v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19552v2",
                "updated": "2025-10-01T06:54:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    6,
                    54,
                    44,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-23T20:25:53Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    20,
                    25,
                    53,
                    1,
                    266,
                    0
                ],
                "title": "iFinder: Structured Zero-Shot Vision-Based LLM Grounding for Dash-Cam\n  Video Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "iFinder: Structured Zero-Shot Vision-Based LLM Grounding for Dash-Cam\n  Video Reasoning"
                },
                "summary": "Grounding large language models (LLMs) in domain-specific tasks like post-hoc\ndash-cam driving video analysis is challenging due to their general-purpose\ntraining and lack of structured inductive biases. As vision is often the sole\nmodality available for such analysis (i.e., no LiDAR, GPS, etc.), existing\nvideo-based vision-language models (V-VLMs) struggle with spatial reasoning,\ncausal inference, and explainability of events in the input video. To this end,\nwe introduce iFinder, a structured semantic grounding framework that decouples\nperception from reasoning by translating dash-cam videos into a hierarchical,\ninterpretable data structure for LLMs. iFinder operates as a modular,\ntraining-free pipeline that employs pretrained vision models to extract\ncritical cues -- object pose, lane positions, and object trajectories -- which\nare hierarchically organized into frame- and video-level structures. Combined\nwith a three-block prompting strategy, it enables step-wise, grounded reasoning\nfor the LLM to refine a peer V-VLM's outputs and provide accurate reasoning.\nEvaluations on four public dash-cam video benchmarks show that iFinder's\nproposed grounding with domain-specific cues, especially object orientation and\nglobal context, significantly outperforms end-to-end V-VLMs on four zero-shot\ndriving benchmarks, with up to 39% gains in accident reasoning accuracy. By\ngrounding LLMs with driving domain-specific representations, iFinder offers a\nzero-shot, interpretable, and reliable alternative to end-to-end V-VLMs for\npost-hoc driving video understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grounding large language models (LLMs) in domain-specific tasks like post-hoc\ndash-cam driving video analysis is challenging due to their general-purpose\ntraining and lack of structured inductive biases. As vision is often the sole\nmodality available for such analysis (i.e., no LiDAR, GPS, etc.), existing\nvideo-based vision-language models (V-VLMs) struggle with spatial reasoning,\ncausal inference, and explainability of events in the input video. To this end,\nwe introduce iFinder, a structured semantic grounding framework that decouples\nperception from reasoning by translating dash-cam videos into a hierarchical,\ninterpretable data structure for LLMs. iFinder operates as a modular,\ntraining-free pipeline that employs pretrained vision models to extract\ncritical cues -- object pose, lane positions, and object trajectories -- which\nare hierarchically organized into frame- and video-level structures. Combined\nwith a three-block prompting strategy, it enables step-wise, grounded reasoning\nfor the LLM to refine a peer V-VLM's outputs and provide accurate reasoning.\nEvaluations on four public dash-cam video benchmarks show that iFinder's\nproposed grounding with domain-specific cues, especially object orientation and\nglobal context, significantly outperforms end-to-end V-VLMs on four zero-shot\ndriving benchmarks, with up to 39% gains in accident reasoning accuracy. By\ngrounding LLMs with driving domain-specific representations, iFinder offers a\nzero-shot, interpretable, and reliable alternative to end-to-end V-VLMs for\npost-hoc driving video understanding."
                },
                "authors": [
                    {
                        "name": "Manyi Yao"
                    },
                    {
                        "name": "Bingbing Zhuang"
                    },
                    {
                        "name": "Sparsh Garg"
                    },
                    {
                        "name": "Amit Roy-Chowdhury"
                    },
                    {
                        "name": "Christian Shelton"
                    },
                    {
                        "name": "Manmohan Chandraker"
                    },
                    {
                        "name": "Abhishek Aich"
                    }
                ],
                "author_detail": {
                    "name": "Abhishek Aich"
                },
                "author": "Abhishek Aich",
                "arxiv_comment": "Accepted at NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19552v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19552v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23935v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23935v2",
                "updated": "2025-10-01T06:30:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    6,
                    30,
                    54,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-28T15:15:49Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    15,
                    15,
                    49,
                    6,
                    271,
                    0
                ],
                "title": "RAPSEM: Identifying Latent Mediators Without Sequential Ignorability via\n  a Rank-Preserving Structural Equation Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAPSEM: Identifying Latent Mediators Without Sequential Ignorability via\n  a Rank-Preserving Structural Equation Model"
                },
                "summary": "The identification of latent mediator variables is typically conducted using\nstandard structural equation models (SEMs). When SEM is applied to mediation\nanalysis with a causal interpretation, valid inference relies on the strong\nassumption of no unmeasured confounding, that is, all relevant covariates must\nbe included in the analysis. This assumption is often violated in empirical\napplications, leading to biased estimates of direct and indirect effects. We\naddress this limitation by weakening the causal assumptions and proposing a\nprocedure that combines g-estimation with a two-stage method of moments to\nincorporate latent variables, thereby enabling more robust mediation analysis\nin settings common to the social sciences. We establish consistency and\nasymptotic normality of the resulting estimator. Simulation studies demonstrate\nthat the estimator is unbiased across a wide range of settings, robust to\nviolations of its underlying no-effect-modifier assumption, and achieves\nreasonable power to detect medium to large effects for sample sizes above 500,\nwith power increasing as the strength of treatment-covariate interactions\ngrows. The code is available at https://github.com/PsychometricsMZ/RAPSEM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The identification of latent mediator variables is typically conducted using\nstandard structural equation models (SEMs). When SEM is applied to mediation\nanalysis with a causal interpretation, valid inference relies on the strong\nassumption of no unmeasured confounding, that is, all relevant covariates must\nbe included in the analysis. This assumption is often violated in empirical\napplications, leading to biased estimates of direct and indirect effects. We\naddress this limitation by weakening the causal assumptions and proposing a\nprocedure that combines g-estimation with a two-stage method of moments to\nincorporate latent variables, thereby enabling more robust mediation analysis\nin settings common to the social sciences. We establish consistency and\nasymptotic normality of the resulting estimator. Simulation studies demonstrate\nthat the estimator is unbiased across a wide range of settings, robust to\nviolations of its underlying no-effect-modifier assumption, and achieves\nreasonable power to detect medium to large effects for sample sizes above 500,\nwith power increasing as the strength of treatment-covariate interactions\ngrows. The code is available at https://github.com/PsychometricsMZ/RAPSEM."
                },
                "authors": [
                    {
                        "name": "Sofia Morelli"
                    },
                    {
                        "name": "Roberto Faleh"
                    },
                    {
                        "name": "Holger Brandt"
                    }
                ],
                "author_detail": {
                    "name": "Holger Brandt"
                },
                "author": "Holger Brandt",
                "arxiv_comment": "19 pages, 4 figures, submitted to Psychometrika, Cambridge University\n  Press",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23935v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23935v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13904v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13904v2",
                "updated": "2025-10-01T06:13:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    6,
                    13,
                    26,
                    2,
                    274,
                    0
                ],
                "published": "2025-08-19T15:05:55Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    5,
                    55,
                    1,
                    231,
                    0
                ],
                "title": "Revisiting Diffusion Q-Learning: From Iterative Denoising to One-Step\n  Action Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Diffusion Q-Learning: From Iterative Denoising to One-Step\n  Action Generation"
                },
                "summary": "Diffusion Q-Learning (DQL) has established diffusion policies as a\nhigh-performing paradigm for offline reinforcement learning, but its reliance\non multi-step denoising for action generation renders both training and\ninference slow and fragile. Existing efforts to accelerate DQL toward one-step\ndenoising typically rely on auxiliary modules or policy distillation,\nsacrificing either simplicity or performance. It remains unclear whether a\none-step policy can be trained directly without such trade-offs. To this end,\nwe introduce One-Step Flow Q-Learning (OFQL), a novel framework that enables\neffective one-step action generation during both training and inference,\nwithout auxiliary modules or distillation. OFQL reformulates the DQL policy\nwithin the Flow Matching (FM) paradigm but departs from conventional FM by\nlearning an average velocity field that directly supports accurate one-step\naction generation. This design removes the need for multi-step denoising and\nbackpropagation-through-time updates, resulting in substantially faster and\nmore robust learning. Extensive experiments on the D4RL benchmark show that\nOFQL, despite generating actions in a single step, not only significantly\nreduces computation during both training and inference but also outperforms\nmulti-step DQL by a large margin. Furthermore, OFQL surpasses all other\nbaselines, achieving state-of-the-art performance in D4RL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Q-Learning (DQL) has established diffusion policies as a\nhigh-performing paradigm for offline reinforcement learning, but its reliance\non multi-step denoising for action generation renders both training and\ninference slow and fragile. Existing efforts to accelerate DQL toward one-step\ndenoising typically rely on auxiliary modules or policy distillation,\nsacrificing either simplicity or performance. It remains unclear whether a\none-step policy can be trained directly without such trade-offs. To this end,\nwe introduce One-Step Flow Q-Learning (OFQL), a novel framework that enables\neffective one-step action generation during both training and inference,\nwithout auxiliary modules or distillation. OFQL reformulates the DQL policy\nwithin the Flow Matching (FM) paradigm but departs from conventional FM by\nlearning an average velocity field that directly supports accurate one-step\naction generation. This design removes the need for multi-step denoising and\nbackpropagation-through-time updates, resulting in substantially faster and\nmore robust learning. Extensive experiments on the D4RL benchmark show that\nOFQL, despite generating actions in a single step, not only significantly\nreduces computation during both training and inference but also outperforms\nmulti-step DQL by a large margin. Furthermore, OFQL surpasses all other\nbaselines, achieving state-of-the-art performance in D4RL."
                },
                "authors": [
                    {
                        "name": "Thanh Nguyen"
                    },
                    {
                        "name": "Chang D. Yoo"
                    }
                ],
                "author_detail": {
                    "name": "Chang D. Yoo"
                },
                "author": "Chang D. Yoo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13904v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13904v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20097v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20097v2",
                "updated": "2025-10-01T06:09:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    6,
                    9,
                    25,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-24T13:20:37Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    13,
                    20,
                    37,
                    2,
                    267,
                    0
                ],
                "title": "Integrated Framework for LLM Evaluation with Answer Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrated Framework for LLM Evaluation with Answer Generation"
                },
                "summary": "Reliable evaluation of large language models is essential to ensure their\napplicability in practical scenarios. Traditional benchmark-based evaluation\nmethods often rely on fixed reference answers, limiting their ability to\ncapture important qualitative aspects of generated responses. To address these\nshortcomings, we propose an integrated evaluation framework called\n\\textit{self-refining descriptive evaluation with expert-driven diagnostics},\nSPEED, which utilizes specialized functional experts to perform comprehensive,\ndescriptive analyses of model outputs. Unlike conventional approaches, SPEED\nactively incorporates expert feedback across multiple dimensions, including\nhallucination detection, toxicity assessment, and lexical-contextual\nappropriateness. Experimental results demonstrate that SPEED achieves robust\nand consistent evaluation performance across diverse domains and datasets.\nAdditionally, by employing relatively compact expert models, SPEED demonstrates\nsuperior resource efficiency compared to larger-scale evaluators. These\nfindings illustrate that SPEED significantly enhances fairness and\ninterpretability in LLM evaluations, offering a promising alternative to\nexisting evaluation methodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable evaluation of large language models is essential to ensure their\napplicability in practical scenarios. Traditional benchmark-based evaluation\nmethods often rely on fixed reference answers, limiting their ability to\ncapture important qualitative aspects of generated responses. To address these\nshortcomings, we propose an integrated evaluation framework called\n\\textit{self-refining descriptive evaluation with expert-driven diagnostics},\nSPEED, which utilizes specialized functional experts to perform comprehensive,\ndescriptive analyses of model outputs. Unlike conventional approaches, SPEED\nactively incorporates expert feedback across multiple dimensions, including\nhallucination detection, toxicity assessment, and lexical-contextual\nappropriateness. Experimental results demonstrate that SPEED achieves robust\nand consistent evaluation performance across diverse domains and datasets.\nAdditionally, by employing relatively compact expert models, SPEED demonstrates\nsuperior resource efficiency compared to larger-scale evaluators. These\nfindings illustrate that SPEED significantly enhances fairness and\ninterpretability in LLM evaluations, offering a promising alternative to\nexisting evaluation methodologies."
                },
                "authors": [
                    {
                        "name": "Sujeong Lee"
                    },
                    {
                        "name": "Hayoung Lee"
                    },
                    {
                        "name": "Seongsoo Heo"
                    },
                    {
                        "name": "Wonik Choi"
                    }
                ],
                "author_detail": {
                    "name": "Wonik Choi"
                },
                "author": "Wonik Choi",
                "arxiv_comment": "16pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20097v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20097v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04008v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04008v2",
                "updated": "2025-10-01T06:06:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    6,
                    6,
                    25,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-04T08:39:47Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    8,
                    39,
                    47,
                    3,
                    247,
                    0
                ],
                "title": "Towards understanding Accelerated Stein Variational Gradient Flow --\n  Analysis of Generalized Bilinear Kernels for Gaussian target distributions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards understanding Accelerated Stein Variational Gradient Flow --\n  Analysis of Generalized Bilinear Kernels for Gaussian target distributions"
                },
                "summary": "Stein variational gradient descent (SVGD) is a kernel-based and\nnon-parametric particle method for sampling from a target distribution, such as\nin Bayesian inference and other machine learning tasks. Different from other\nparticle methods, SVGD does not require estimating the score, which is the\ngradient of the log-density. However, in practice, SVGD can be slow compared to\nscore-estimation-based sampling algorithms. To design a fast and efficient\nhigh-dimensional sampling algorithm with the advantages of SVGD, we introduce\naccelerated SVGD (ASVGD), based on an accelerated gradient flow in a metric\nspace of probability densities following Nesterov's method. We then derive a\nmomentum-based discrete-time sampling algorithm, which evolves a set of\nparticles deterministically. To stabilize the particles' position update, we\nalso include a Wasserstein metric regularization. This paper extends the\nconference version \\cite{SL2025}. For the bilinear kernel and Gaussian target\ndistributions, we study the kernel parameter and damping parameters with an\noptimal convergence rate of the proposed dynamics. This is achieved by\nanalyzing the linearized accelerated gradient flows at the equilibrium.\nInterestingly, the optimal parameter is a constant, which does not depend on\nthe covariance of the target distribution. For the generalized kernel\nfunctions, such as the Gaussian kernel, numerical examples with varied target\ndistributions demonstrate the effectiveness of ASVGD compared to SVGD and other\npopular sampling methods. Furthermore, we show that in the setting of Bayesian\nneural networks, ASVGD outperforms SVGD significantly in terms of\nlog-likelihood and total iteration times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stein variational gradient descent (SVGD) is a kernel-based and\nnon-parametric particle method for sampling from a target distribution, such as\nin Bayesian inference and other machine learning tasks. Different from other\nparticle methods, SVGD does not require estimating the score, which is the\ngradient of the log-density. However, in practice, SVGD can be slow compared to\nscore-estimation-based sampling algorithms. To design a fast and efficient\nhigh-dimensional sampling algorithm with the advantages of SVGD, we introduce\naccelerated SVGD (ASVGD), based on an accelerated gradient flow in a metric\nspace of probability densities following Nesterov's method. We then derive a\nmomentum-based discrete-time sampling algorithm, which evolves a set of\nparticles deterministically. To stabilize the particles' position update, we\nalso include a Wasserstein metric regularization. This paper extends the\nconference version \\cite{SL2025}. For the bilinear kernel and Gaussian target\ndistributions, we study the kernel parameter and damping parameters with an\noptimal convergence rate of the proposed dynamics. This is achieved by\nanalyzing the linearized accelerated gradient flows at the equilibrium.\nInterestingly, the optimal parameter is a constant, which does not depend on\nthe covariance of the target distribution. For the generalized kernel\nfunctions, such as the Gaussian kernel, numerical examples with varied target\ndistributions demonstrate the effectiveness of ASVGD compared to SVGD and other\npopular sampling methods. Furthermore, we show that in the setting of Bayesian\nneural networks, ASVGD outperforms SVGD significantly in terms of\nlog-likelihood and total iteration times."
                },
                "authors": [
                    {
                        "name": "Viktor Stein"
                    },
                    {
                        "name": "Wuchen Li"
                    }
                ],
                "author_detail": {
                    "name": "Wuchen Li"
                },
                "author": "Wuchen Li",
                "arxiv_comment": "46 pages, 4 figures, 4 algorithms, 4 tables, comments welcome! (v2:\n  added missing funding info)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04008v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04008v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "46N10 (Primary) 46E22 94A15 37Lxx 37A50 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11895v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11895v3",
                "updated": "2025-10-01T05:59:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    5,
                    59,
                    16,
                    2,
                    274,
                    0
                ],
                "published": "2025-03-14T21:53:12Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    21,
                    53,
                    12,
                    4,
                    73,
                    0
                ],
                "title": "Resolving UnderEdit & OverEdit with Iterative & Neighbor-Assisted Model\n  Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resolving UnderEdit & OverEdit with Iterative & Neighbor-Assisted Model\n  Editing"
                },
                "summary": "Large Language Models (LLMs) are widely deployed in downstream tasks, but\nkeeping their knowledge up-to-date via retraining or fine-tuning is often\ncomputationally expensive. Model editing provides a more efficient alternative\nby updating a targeted subset of parameters, which often follows the\nlocate-and-edit paradigm. Despite this efficiency, existing methods are\nlimited: edits may fail to inject knowledge (UnderEdit) or unintentionally\ndisrupt unrelated neighboring knowledge (OverEdit). To address these\nchallenges, we propose two complementary methods: iterative model editing,\nwhich applies successive edits to mitigate UnderEdit, and neighbor-assisted\nmodel editing, which incorporates neighboring knowledge during editing to\nreduce OverEdit. Our extensive experiments show that these techniques improve\nediting performance across multiple LLMs, algorithms, and benchmarks, reducing\nUnderEdit by up to 38 percentage points and OverEdit by up to 6, while\nremaining broadly applicable to any locate-and-edit method. We release our code\nat https://github.com/bhimanbaghel/ResolveUnderOverEdit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely deployed in downstream tasks, but\nkeeping their knowledge up-to-date via retraining or fine-tuning is often\ncomputationally expensive. Model editing provides a more efficient alternative\nby updating a targeted subset of parameters, which often follows the\nlocate-and-edit paradigm. Despite this efficiency, existing methods are\nlimited: edits may fail to inject knowledge (UnderEdit) or unintentionally\ndisrupt unrelated neighboring knowledge (OverEdit). To address these\nchallenges, we propose two complementary methods: iterative model editing,\nwhich applies successive edits to mitigate UnderEdit, and neighbor-assisted\nmodel editing, which incorporates neighboring knowledge during editing to\nreduce OverEdit. Our extensive experiments show that these techniques improve\nediting performance across multiple LLMs, algorithms, and benchmarks, reducing\nUnderEdit by up to 38 percentage points and OverEdit by up to 6, while\nremaining broadly applicable to any locate-and-edit method. We release our code\nat https://github.com/bhimanbaghel/ResolveUnderOverEdit."
                },
                "authors": [
                    {
                        "name": "Bhiman Kumar Baghel"
                    },
                    {
                        "name": "Emma Jordan"
                    },
                    {
                        "name": "Zheyuan Ryan Shi"
                    },
                    {
                        "name": "Xiang Lorraine Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Lorraine Li"
                },
                "author": "Xiang Lorraine Li",
                "arxiv_comment": "Accepted at EMNLP 2025 as Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11895v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11895v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21102v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21102v3",
                "updated": "2025-10-01T05:50:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    5,
                    50,
                    1,
                    2,
                    274,
                    0
                ],
                "published": "2024-12-30T17:25:58Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    17,
                    25,
                    58,
                    0,
                    365,
                    0
                ],
                "title": "Exploring and Controlling Diversity in LLM-Agent Conversation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring and Controlling Diversity in LLM-Agent Conversation"
                },
                "summary": "Controlling diversity in LLM-agent simulations is essential for balancing\nstability in structured tasks with variability in open-ended interactions.\nHowever, we observe that dialogue diversity tends to degrade over long-term\nsimulations. To explore the role of prompt design in this phenomenon, we\nmodularized the utterance generation prompt and found that reducing contextual\ninformation leads to more diverse outputs. Based on this insight, we propose\nAdaptive Prompt Pruning (APP), a novel method that allows users to control\ndiversity via a single parameter, lambda. APP dynamically prunes prompt\nsegments based on attention scores and is compatible with existing diversity\ncontrol methods. We demonstrate that APP effectively modulates diversity\nthrough extensive experiments and propose a method to balance the control\ntrade-offs. Our analysis reveals that all prompt components impose constraints\non diversity, with the Memory being the most influential. Additionally,\nhigh-attention contents consistently suppress output diversity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controlling diversity in LLM-agent simulations is essential for balancing\nstability in structured tasks with variability in open-ended interactions.\nHowever, we observe that dialogue diversity tends to degrade over long-term\nsimulations. To explore the role of prompt design in this phenomenon, we\nmodularized the utterance generation prompt and found that reducing contextual\ninformation leads to more diverse outputs. Based on this insight, we propose\nAdaptive Prompt Pruning (APP), a novel method that allows users to control\ndiversity via a single parameter, lambda. APP dynamically prunes prompt\nsegments based on attention scores and is compatible with existing diversity\ncontrol methods. We demonstrate that APP effectively modulates diversity\nthrough extensive experiments and propose a method to balance the control\ntrade-offs. Our analysis reveals that all prompt components impose constraints\non diversity, with the Memory being the most influential. Additionally,\nhigh-attention contents consistently suppress output diversity."
                },
                "authors": [
                    {
                        "name": "KuanChao Chu"
                    },
                    {
                        "name": "Yi-Pei Chen"
                    },
                    {
                        "name": "Hideki Nakayama"
                    }
                ],
                "author_detail": {
                    "name": "Hideki Nakayama"
                },
                "author": "Hideki Nakayama",
                "arxiv_comment": "EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21102v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21102v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15194v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15194v2",
                "updated": "2025-10-01T05:29:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    5,
                    29,
                    42,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-18T17:50:04Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    50,
                    4,
                    3,
                    261,
                    0
                ],
                "title": "Evolving Language Models without Labels: Majority Drives Selection,\n  Novelty Promotes Variation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolving Language Models without Labels: Majority Drives Selection,\n  Novelty Promotes Variation"
                },
                "summary": "Large language models (LLMs) are increasingly trained with reinforcement\nlearning from verifiable rewards (RLVR), yet real-world deployment demands\nmodels that can self-improve without labels or external judges. Existing\nself-improvement approaches primarily rely on self-confirmation signals (e.g.,\nconfidence, entropy, or consistency) to generate rewards. This reliance drives\nmodels toward over-confident, majority-favored solutions, causing an entropy\ncollapse that degrades pass@n and reasoning complexity. To address this, we\npropose EVOL-RL, a label-free framework that mirrors the evolutionary principle\nof balancing selection with variation. Concretely, EVOL-RL retains the\nmajority-voted answer as an anchor for stability, but adds a novelty-aware\nreward that scores each sampled solution by how different its reasoning is from\nother concurrently generated responses. This majority-for-stability +\nnovelty-for-exploration rule mirrors the variation-selection principle:\nselection prevents drift, while novelty prevents collapse. Evaluation results\nshow that EVOL-RL consistently outperforms the majority-only baseline; e.g.,\ntraining on label-free AIME24 lifts Qwen3-4B-Base AIME25 pass@1 from baseline's\n4.6% to 16.4%, and pass@16 from 18.5% to 37.9%. EVOL-RL not only prevents\nin-domain diversity collapse but also improves out-of-domain generalization\n(from math reasoning to broader tasks, e.g., GPQA, MMLU-Pro, and BBEH). The\ncode is available at: https://github.com/YujunZhou/EVOL-RL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly trained with reinforcement\nlearning from verifiable rewards (RLVR), yet real-world deployment demands\nmodels that can self-improve without labels or external judges. Existing\nself-improvement approaches primarily rely on self-confirmation signals (e.g.,\nconfidence, entropy, or consistency) to generate rewards. This reliance drives\nmodels toward over-confident, majority-favored solutions, causing an entropy\ncollapse that degrades pass@n and reasoning complexity. To address this, we\npropose EVOL-RL, a label-free framework that mirrors the evolutionary principle\nof balancing selection with variation. Concretely, EVOL-RL retains the\nmajority-voted answer as an anchor for stability, but adds a novelty-aware\nreward that scores each sampled solution by how different its reasoning is from\nother concurrently generated responses. This majority-for-stability +\nnovelty-for-exploration rule mirrors the variation-selection principle:\nselection prevents drift, while novelty prevents collapse. Evaluation results\nshow that EVOL-RL consistently outperforms the majority-only baseline; e.g.,\ntraining on label-free AIME24 lifts Qwen3-4B-Base AIME25 pass@1 from baseline's\n4.6% to 16.4%, and pass@16 from 18.5% to 37.9%. EVOL-RL not only prevents\nin-domain diversity collapse but also improves out-of-domain generalization\n(from math reasoning to broader tasks, e.g., GPQA, MMLU-Pro, and BBEH). The\ncode is available at: https://github.com/YujunZhou/EVOL-RL."
                },
                "authors": [
                    {
                        "name": "Yujun Zhou"
                    },
                    {
                        "name": "Zhenwen Liang"
                    },
                    {
                        "name": "Haolin Liu"
                    },
                    {
                        "name": "Wenhao Yu"
                    },
                    {
                        "name": "Kishan Panaganti"
                    },
                    {
                        "name": "Linfeng Song"
                    },
                    {
                        "name": "Dian Yu"
                    },
                    {
                        "name": "Xiangliang Zhang"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Dong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yu"
                },
                "author": "Dong Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15194v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15194v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22200v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22200v4",
                "updated": "2025-10-01T05:28:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    5,
                    28,
                    4,
                    2,
                    274,
                    0
                ],
                "published": "2025-06-27T13:09:05Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    13,
                    9,
                    5,
                    4,
                    178,
                    0
                ],
                "title": "EFRame: Deeper Reasoning via Exploration-Filter-Replay Reinforcement\n  Learning Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EFRame: Deeper Reasoning via Exploration-Filter-Replay Reinforcement\n  Learning Framework"
                },
                "summary": "Recent advances in reinforcement learning (RL) have significantly enhanced\nthe reasoning capabilities of large language models (LLMs). Group Relative\nPolicy Optimization (GRPO), a lightweight variant of Proximal Policy\nOptimization (PPO), improves efficiency but suffers from limited exploration\nand training instability, limiting its effectiveness on complex reasoning\ntasks. To address these challenges, we introduce EFRame, an\nExploration-Filter-Replay framework that augments GRPO across three dimensions:\nadditional rollouts enable deeper and more targeted exploration, online\nfiltering removes low-quality samples to stabilize gradients and accelerate\ntraining, and experience replay amplifies rare yet informative trajectories for\nstable convergence. This unified framework establishes a principled training\ncycle that balances exploration, efficiency, and stability. Experiments on\ndiverse reasoning benchmarks demonstrate that EFRame achieves consistent gains,\nincluding a 37.9\\% relative improvement on Geometry3K over GRPO. EFRame further\nsupports fine-grained sample categorization and precise entropy control,\nhighlighting it as a robust solution for advancing deeper reasoning in LLMs.\nOur code is available at https://github.com/597358816/EFRame.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in reinforcement learning (RL) have significantly enhanced\nthe reasoning capabilities of large language models (LLMs). Group Relative\nPolicy Optimization (GRPO), a lightweight variant of Proximal Policy\nOptimization (PPO), improves efficiency but suffers from limited exploration\nand training instability, limiting its effectiveness on complex reasoning\ntasks. To address these challenges, we introduce EFRame, an\nExploration-Filter-Replay framework that augments GRPO across three dimensions:\nadditional rollouts enable deeper and more targeted exploration, online\nfiltering removes low-quality samples to stabilize gradients and accelerate\ntraining, and experience replay amplifies rare yet informative trajectories for\nstable convergence. This unified framework establishes a principled training\ncycle that balances exploration, efficiency, and stability. Experiments on\ndiverse reasoning benchmarks demonstrate that EFRame achieves consistent gains,\nincluding a 37.9\\% relative improvement on Geometry3K over GRPO. EFRame further\nsupports fine-grained sample categorization and precise entropy control,\nhighlighting it as a robust solution for advancing deeper reasoning in LLMs.\nOur code is available at https://github.com/597358816/EFRame."
                },
                "authors": [
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Lai Wei"
                    },
                    {
                        "name": "Yanzhi Zhang"
                    },
                    {
                        "name": "Chenyang Shao"
                    },
                    {
                        "name": "Zedong Dan"
                    },
                    {
                        "name": "Weiran Huang"
                    },
                    {
                        "name": "Yuzhi Zhang"
                    },
                    {
                        "name": "Yue Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yue Wang"
                },
                "author": "Yue Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22200v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22200v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22646v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22646v2",
                "updated": "2025-10-01T05:14:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    5,
                    14,
                    25,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-26T17:59:54Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    59,
                    54,
                    4,
                    269,
                    0
                ],
                "title": "Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal\n  LLMs"
                },
                "summary": "Can humans identify AI-generated (fake) videos and provide grounded reasons?\nWhile video generation models have advanced rapidly, a critical dimension --\nwhether humans can detect deepfake traces within a generated video, i.e.,\nspatiotemporal grounded visual artifacts that reveal a video as machine\ngenerated -- has been largely overlooked. We introduce DeeptraceReward, the\nfirst fine-grained, spatially- and temporally- aware benchmark that annotates\nhuman-perceived fake traces for video generation reward. The dataset comprises\n4.3K detailed annotations across 3.3K high-quality generated videos. Each\nannotation provides a natural-language explanation, pinpoints a bounding-box\nregion containing the perceived trace, and marks precise onset and offset\ntimestamps. We consolidate these annotations into 9 major categories of\ndeepfake traces that lead humans to identify a video as AI-generated, and train\nmultimodal language models (LMs) as reward models to mimic human judgments and\nlocalizations. On DeeptraceReward, our 7B reward model outperforms GPT-5 by\n34.7% on average across fake clue identification, grounding, and explanation.\nInterestingly, we observe a consistent difficulty gradient: binary fake v.s.\nreal classification is substantially easier than fine-grained deepfake trace\ndetection; within the latter, performance degrades from natural language\nexplanations (easiest), to spatial grounding, to temporal labeling (hardest).\nBy foregrounding human-perceived deepfake traces, DeeptraceReward provides a\nrigorous testbed and training signal for socially aware and trustworthy video\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can humans identify AI-generated (fake) videos and provide grounded reasons?\nWhile video generation models have advanced rapidly, a critical dimension --\nwhether humans can detect deepfake traces within a generated video, i.e.,\nspatiotemporal grounded visual artifacts that reveal a video as machine\ngenerated -- has been largely overlooked. We introduce DeeptraceReward, the\nfirst fine-grained, spatially- and temporally- aware benchmark that annotates\nhuman-perceived fake traces for video generation reward. The dataset comprises\n4.3K detailed annotations across 3.3K high-quality generated videos. Each\nannotation provides a natural-language explanation, pinpoints a bounding-box\nregion containing the perceived trace, and marks precise onset and offset\ntimestamps. We consolidate these annotations into 9 major categories of\ndeepfake traces that lead humans to identify a video as AI-generated, and train\nmultimodal language models (LMs) as reward models to mimic human judgments and\nlocalizations. On DeeptraceReward, our 7B reward model outperforms GPT-5 by\n34.7% on average across fake clue identification, grounding, and explanation.\nInterestingly, we observe a consistent difficulty gradient: binary fake v.s.\nreal classification is substantially easier than fine-grained deepfake trace\ndetection; within the latter, performance degrades from natural language\nexplanations (easiest), to spatial grounding, to temporal labeling (hardest).\nBy foregrounding human-perceived deepfake traces, DeeptraceReward provides a\nrigorous testbed and training signal for socially aware and trustworthy video\ngeneration."
                },
                "authors": [
                    {
                        "name": "Xingyu Fu"
                    },
                    {
                        "name": "Siyi Liu"
                    },
                    {
                        "name": "Yinuo Xu"
                    },
                    {
                        "name": "Pan Lu"
                    },
                    {
                        "name": "Guangqiuse Hu"
                    },
                    {
                        "name": "Tianbo Yang"
                    },
                    {
                        "name": "Taran Anantasagar"
                    },
                    {
                        "name": "Christopher Shen"
                    },
                    {
                        "name": "Yikai Mao"
                    },
                    {
                        "name": "Yuanzhe Liu"
                    },
                    {
                        "name": "Keyush Shah"
                    },
                    {
                        "name": "Chung Un Lee"
                    },
                    {
                        "name": "Yejin Choi"
                    },
                    {
                        "name": "James Zou"
                    },
                    {
                        "name": "Dan Roth"
                    },
                    {
                        "name": "Chris Callison-Burch"
                    }
                ],
                "author_detail": {
                    "name": "Chris Callison-Burch"
                },
                "author": "Chris Callison-Burch",
                "arxiv_comment": "Project Page: https://deeptracereward.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22646v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22646v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04996v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04996v4",
                "updated": "2025-10-01T05:10:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    5,
                    10,
                    1,
                    2,
                    274,
                    0
                ],
                "published": "2024-10-07T12:52:38Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    12,
                    52,
                    38,
                    0,
                    281,
                    0
                ],
                "title": "Assumption-Lean Post-Integrated Inference with Surrogate Control\n  Outcomes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assumption-Lean Post-Integrated Inference with Surrogate Control\n  Outcomes"
                },
                "summary": "Data integration methods aim to extract low-dimensional embeddings from\nhigh-dimensional outcomes to remove unwanted variations, such as batch effects\nand unmeasured covariates, across heterogeneous datasets. However, multiple\nhypothesis testing after integration can be biased due to data-dependent\nprocesses. We introduce a robust post-integrated inference (PII) method that\nadjusts for latent heterogeneity using control outcomes. Leveraging causal\ninterpretations, we derive nonparametric identifiability of the direct effects\nusing negative control outcomes. By utilizing surrogate control outcomes as an\nextension of negative control outcomes, we develop semiparametric inference on\nprojected direct effect estimands, accounting for hidden mediators,\nconfounders, and moderators. These estimands remain statistically meaningful\nunder model misspecifications and with error-prone embeddings. We provide bias\nquantifications and finite-sample linear expansions with uniform concentration\nbounds. The proposed doubly robust estimators are consistent and efficient\nunder minimal assumptions and potential misspecification, facilitating\ndata-adaptive estimation with machine learning algorithms. Our proposal is\nevaluated with random forests through simulations and analysis of single-cell\nCRISPR perturbed datasets with potential unmeasured confounders.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data integration methods aim to extract low-dimensional embeddings from\nhigh-dimensional outcomes to remove unwanted variations, such as batch effects\nand unmeasured covariates, across heterogeneous datasets. However, multiple\nhypothesis testing after integration can be biased due to data-dependent\nprocesses. We introduce a robust post-integrated inference (PII) method that\nadjusts for latent heterogeneity using control outcomes. Leveraging causal\ninterpretations, we derive nonparametric identifiability of the direct effects\nusing negative control outcomes. By utilizing surrogate control outcomes as an\nextension of negative control outcomes, we develop semiparametric inference on\nprojected direct effect estimands, accounting for hidden mediators,\nconfounders, and moderators. These estimands remain statistically meaningful\nunder model misspecifications and with error-prone embeddings. We provide bias\nquantifications and finite-sample linear expansions with uniform concentration\nbounds. The proposed doubly robust estimators are consistent and efficient\nunder minimal assumptions and potential misspecification, facilitating\ndata-adaptive estimation with machine learning algorithms. Our proposal is\nevaluated with random forests through simulations and analysis of single-cell\nCRISPR perturbed datasets with potential unmeasured confounders."
                },
                "authors": [
                    {
                        "name": "Jin-Hong Du"
                    },
                    {
                        "name": "Kathryn Roeder"
                    },
                    {
                        "name": "Larry Wasserman"
                    }
                ],
                "author_detail": {
                    "name": "Larry Wasserman"
                },
                "author": "Larry Wasserman",
                "arxiv_comment": "23 pages for the main text, 32 pages for the appendix, 6 figures for\n  the main text, 7 figures for the appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04996v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04996v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2507.12142v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12142v2",
                "updated": "2025-10-01T17:59:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    17,
                    59,
                    10,
                    2,
                    274,
                    0
                ],
                "published": "2025-07-16T11:17:12Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    11,
                    17,
                    12,
                    2,
                    197,
                    0
                ],
                "title": "LoRA meets Riemannion: Muon Optimizer for Parametrization-independent\n  Low-Rank Adapters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRA meets Riemannion: Muon Optimizer for Parametrization-independent\n  Low-Rank Adapters"
                },
                "summary": "This work presents a novel, fully Riemannian framework for Low-Rank\nAdaptation (LoRA) that geometrically treats low-rank adapters by optimizing\nthem directly on the fixed-rank manifold. This formulation eliminates the\nparametrization ambiguity present in standard Euclidean optimizers. Our\nframework integrates three key components to achieve this: (1) we derive\nRiemannion, a new Riemannian optimizer on the fixed-rank matrix manifold that\ngeneralizes the recently proposed Muon optimizer; (2) we develop a Riemannian\ngradient-informed LoRA initialization, and (3) we provide an efficient\nimplementation without prominent overhead that uses automatic differentiation\nto compute arising geometric operations while adhering to best practices in\nnumerical linear algebra. Comprehensive experimental results on both LLM and\ndiffusion model architectures demonstrate that our approach yields consistent\nand noticeable improvements in convergence speed and final task performance\nover both standard LoRA and its state-of-the-art modifications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents a novel, fully Riemannian framework for Low-Rank\nAdaptation (LoRA) that geometrically treats low-rank adapters by optimizing\nthem directly on the fixed-rank manifold. This formulation eliminates the\nparametrization ambiguity present in standard Euclidean optimizers. Our\nframework integrates three key components to achieve this: (1) we derive\nRiemannion, a new Riemannian optimizer on the fixed-rank matrix manifold that\ngeneralizes the recently proposed Muon optimizer; (2) we develop a Riemannian\ngradient-informed LoRA initialization, and (3) we provide an efficient\nimplementation without prominent overhead that uses automatic differentiation\nto compute arising geometric operations while adhering to best practices in\nnumerical linear algebra. Comprehensive experimental results on both LLM and\ndiffusion model architectures demonstrate that our approach yields consistent\nand noticeable improvements in convergence speed and final task performance\nover both standard LoRA and its state-of-the-art modifications."
                },
                "authors": [
                    {
                        "name": "Vladimir Bogachev"
                    },
                    {
                        "name": "Vladimir Aletov"
                    },
                    {
                        "name": "Alexander Molozhavenko"
                    },
                    {
                        "name": "Denis Bobkov"
                    },
                    {
                        "name": "Vera Soboleva"
                    },
                    {
                        "name": "Aibek Alanov"
                    },
                    {
                        "name": "Maxim Rakhuba"
                    }
                ],
                "author_detail": {
                    "name": "Maxim Rakhuba"
                },
                "author": "Maxim Rakhuba",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12142v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12142v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.DG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07, 65F55, 53Z50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00907v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00907v4",
                "updated": "2025-10-01T17:58:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    17,
                    58,
                    0,
                    2,
                    274,
                    0
                ],
                "published": "2025-04-01T15:41:50Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    15,
                    41,
                    50,
                    1,
                    91,
                    0
                ],
                "title": "Grounding Multimodal LLMs to Embodied Agents that Ask for Help with\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grounding Multimodal LLMs to Embodied Agents that Ask for Help with\n  Reinforcement Learning"
                },
                "summary": "Embodied agents operating in household environments must interpret ambiguous\nand under-specified human instructions. A capable household robot should\nrecognize ambiguity and ask relevant clarification questions to infer the user\nintent accurately, leading to more effective task execution. To study this\nproblem, we introduce the Ask-to-Act task, where an embodied agent is tasked\nwith a single or multi-object rearrangement task using an under-specified\ninstruction in a home environment. The agent must strategically ask minimal,\nyet relevant, clarification questions to resolve ambiguity while navigating\nunder partial observability. To address this challenge, we propose a novel\napproach that fine-tunes multi-modal large language models (MLLMs) as\nvision-language-action (VLA) policies using online reinforcement learning (RL)\nwith LLM-generated rewards. Our method eliminates the need for large-scale\nhuman demonstrations or manually engineered rewards for training such agents.\nWe benchmark against strong zero-shot baselines including GPT-4o as well as\nsupervised fine-tuned MLLMs on our task. Our results show that our RL-finetuned\nMLLM outperforms all baselines by a significant margin (10.4-16.5%),\ngeneralizing well to novel scenes and tasks. To the best of our knowledge, this\nis the first demonstration of adapting MLLMs as VLA agents that can act and ask\nfor help using LLM-generated rewards with online RL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied agents operating in household environments must interpret ambiguous\nand under-specified human instructions. A capable household robot should\nrecognize ambiguity and ask relevant clarification questions to infer the user\nintent accurately, leading to more effective task execution. To study this\nproblem, we introduce the Ask-to-Act task, where an embodied agent is tasked\nwith a single or multi-object rearrangement task using an under-specified\ninstruction in a home environment. The agent must strategically ask minimal,\nyet relevant, clarification questions to resolve ambiguity while navigating\nunder partial observability. To address this challenge, we propose a novel\napproach that fine-tunes multi-modal large language models (MLLMs) as\nvision-language-action (VLA) policies using online reinforcement learning (RL)\nwith LLM-generated rewards. Our method eliminates the need for large-scale\nhuman demonstrations or manually engineered rewards for training such agents.\nWe benchmark against strong zero-shot baselines including GPT-4o as well as\nsupervised fine-tuned MLLMs on our task. Our results show that our RL-finetuned\nMLLM outperforms all baselines by a significant margin (10.4-16.5%),\ngeneralizing well to novel scenes and tasks. To the best of our knowledge, this\nis the first demonstration of adapting MLLMs as VLA agents that can act and ask\nfor help using LLM-generated rewards with online RL."
                },
                "authors": [
                    {
                        "name": "Ram Ramrakhya"
                    },
                    {
                        "name": "Matthew Chang"
                    },
                    {
                        "name": "Xavier Puig"
                    },
                    {
                        "name": "Ruta Desai"
                    },
                    {
                        "name": "Zsolt Kira"
                    },
                    {
                        "name": "Roozbeh Mottaghi"
                    }
                ],
                "author_detail": {
                    "name": "Roozbeh Mottaghi"
                },
                "author": "Roozbeh Mottaghi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00907v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00907v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16928v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16928v2",
                "updated": "2025-10-01T17:51:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    17,
                    51,
                    44,
                    2,
                    274,
                    0
                ],
                "published": "2025-05-22T17:20:38Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    20,
                    38,
                    3,
                    142,
                    0
                ],
                "title": "Beyond Needle(s) in the Embodied Haystack: Environment, Architecture,\n  and Training Considerations for Long Context Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Needle(s) in the Embodied Haystack: Environment, Architecture,\n  and Training Considerations for Long Context Reasoning"
                },
                "summary": "We introduce $\\infty$-THOR, a new framework for long-horizon embodied tasks\nthat advances long-context understanding in embodied AI. $\\infty$-THOR\nprovides: (1) a generation framework for synthesizing scalable, reproducible,\nand unlimited long-horizon trajectories; (2) a novel embodied QA task,\nNeedle(s) in the Embodied Haystack, where multiple scattered clues across\nextended trajectories test agents' long-context reasoning ability; and (3) a\nlong-horizon dataset and benchmark suite featuring complex tasks that span\nhundreds of environment steps, each paired with ground-truth action sequences.\nTo enable this capability, we explore architectural adaptations, including\ninterleaved Goal-State-Action modeling, context extension techniques, and\nContext Parallelism, to equip LLM-based agents for extreme long-context\nreasoning and interaction. Experimental results and analyses highlight the\nchallenges posed by our benchmark and provide insights into training strategies\nand model behaviors under long-horizon conditions. Our work provides a\nfoundation for the next generation of embodied AI systems capable of robust,\nlong-term reasoning and planning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce $\\infty$-THOR, a new framework for long-horizon embodied tasks\nthat advances long-context understanding in embodied AI. $\\infty$-THOR\nprovides: (1) a generation framework for synthesizing scalable, reproducible,\nand unlimited long-horizon trajectories; (2) a novel embodied QA task,\nNeedle(s) in the Embodied Haystack, where multiple scattered clues across\nextended trajectories test agents' long-context reasoning ability; and (3) a\nlong-horizon dataset and benchmark suite featuring complex tasks that span\nhundreds of environment steps, each paired with ground-truth action sequences.\nTo enable this capability, we explore architectural adaptations, including\ninterleaved Goal-State-Action modeling, context extension techniques, and\nContext Parallelism, to equip LLM-based agents for extreme long-context\nreasoning and interaction. Experimental results and analyses highlight the\nchallenges posed by our benchmark and provide insights into training strategies\nand model behaviors under long-horizon conditions. Our work provides a\nfoundation for the next generation of embodied AI systems capable of robust,\nlong-term reasoning and planning."
                },
                "authors": [
                    {
                        "name": "Bosung Kim"
                    },
                    {
                        "name": "Prithviraj Ammanabrolu"
                    }
                ],
                "author_detail": {
                    "name": "Prithviraj Ammanabrolu"
                },
                "author": "Prithviraj Ammanabrolu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16928v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16928v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25297v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25297v2",
                "updated": "2025-10-01T17:32:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    17,
                    32,
                    51,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-29T16:18:19Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    16,
                    18,
                    19,
                    0,
                    272,
                    0
                ],
                "title": "Automatically Generating Web Applications from Requirements Via\n  Multi-Agent Test-Driven Development",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatically Generating Web Applications from Requirements Via\n  Multi-Agent Test-Driven Development"
                },
                "summary": "Developing full-stack web applications is complex and time-intensive,\ndemanding proficiency across diverse technologies and frameworks. Although\nrecent advances in multimodal large language models (MLLMs) enable automated\nwebpage generation from visual inputs, current solutions remain limited to\nfront-end tasks and fail to deliver fully functional applications. In this\nwork, we introduce TDDev, the first test-driven development (TDD)-enabled\nLLM-agent framework for end-to-end full-stack web application generation. Given\na natural language description or design image, TDDev automatically derives\nexecutable test cases, generates front-end and back-end code, simulates user\ninteractions, and iteratively refines the implementation until all requirements\nare satisfied. Our framework addresses key challenges in full-stack automation,\nincluding underspecified user requirements, complex interdependencies among\nmultiple files, and the need for both functional correctness and visual\nfidelity. Through extensive experiments on diverse application scenarios, TDDev\nachieves a 14.4% improvement on overall accuracy compared to state-of-the-art\nbaselines, demonstrating its effectiveness in producing reliable, high-quality\nweb applications without requiring manual intervention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing full-stack web applications is complex and time-intensive,\ndemanding proficiency across diverse technologies and frameworks. Although\nrecent advances in multimodal large language models (MLLMs) enable automated\nwebpage generation from visual inputs, current solutions remain limited to\nfront-end tasks and fail to deliver fully functional applications. In this\nwork, we introduce TDDev, the first test-driven development (TDD)-enabled\nLLM-agent framework for end-to-end full-stack web application generation. Given\na natural language description or design image, TDDev automatically derives\nexecutable test cases, generates front-end and back-end code, simulates user\ninteractions, and iteratively refines the implementation until all requirements\nare satisfied. Our framework addresses key challenges in full-stack automation,\nincluding underspecified user requirements, complex interdependencies among\nmultiple files, and the need for both functional correctness and visual\nfidelity. Through extensive experiments on diverse application scenarios, TDDev\nachieves a 14.4% improvement on overall accuracy compared to state-of-the-art\nbaselines, demonstrating its effectiveness in producing reliable, high-quality\nweb applications without requiring manual intervention."
                },
                "authors": [
                    {
                        "name": "Yuxuan Wan"
                    },
                    {
                        "name": "Tingshuo Liang"
                    },
                    {
                        "name": "Jiakai Xu"
                    },
                    {
                        "name": "Jingyu Xiao"
                    },
                    {
                        "name": "Yintong Huo"
                    },
                    {
                        "name": "Michael R. Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Michael R. Lyu"
                },
                "author": "Michael R. Lyu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25297v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25297v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25477v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25477v3",
                "updated": "2025-10-02T03:08:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    3,
                    8,
                    37,
                    3,
                    275,
                    0
                ],
                "published": "2025-09-29T20:31:12Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    20,
                    31,
                    12,
                    0,
                    272,
                    0
                ],
                "title": "The Rise of AfricaNLP: Contributions, Contributors, and Community Impact\n  (2005-2025)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Rise of AfricaNLP: Contributions, Contributors, and Community Impact\n  (2005-2025)"
                },
                "summary": "Natural Language Processing (NLP) is undergoing constant transformation, as\nLarge Language Models (LLMs) are driving daily breakthroughs in research and\npractice. In this regard, tracking the progress of NLP research and\nautomatically analyzing the contributions of research papers provides key\ninsights into the nature of the field and the researchers. This study explores\nthe progress of African NLP (AfricaNLP) by asking (and answering) basic\nresearch questions such as: i) How has the nature of NLP evolved over the last\ntwo decades?, ii) What are the contributions of AfricaNLP papers?, and iii)\nWhich individuals and organizations (authors, affiliated institutions, and\nfunding bodies) have been involved in the development of AfricaNLP? We\nquantitatively examine the contributions of AfricaNLP research using 1.9K NLP\npaper abstracts, 4.9K author contributors, and 7.8K human-annotated\ncontribution sentences (AfricaNLPContributions) along with benchmark results.\nOur dataset and continuously existing NLP progress tracking website provide a\npowerful lens for tracing AfricaNLP research trends and hold potential for\ngenerating data-driven literature surveys.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language Processing (NLP) is undergoing constant transformation, as\nLarge Language Models (LLMs) are driving daily breakthroughs in research and\npractice. In this regard, tracking the progress of NLP research and\nautomatically analyzing the contributions of research papers provides key\ninsights into the nature of the field and the researchers. This study explores\nthe progress of African NLP (AfricaNLP) by asking (and answering) basic\nresearch questions such as: i) How has the nature of NLP evolved over the last\ntwo decades?, ii) What are the contributions of AfricaNLP papers?, and iii)\nWhich individuals and organizations (authors, affiliated institutions, and\nfunding bodies) have been involved in the development of AfricaNLP? We\nquantitatively examine the contributions of AfricaNLP research using 1.9K NLP\npaper abstracts, 4.9K author contributors, and 7.8K human-annotated\ncontribution sentences (AfricaNLPContributions) along with benchmark results.\nOur dataset and continuously existing NLP progress tracking website provide a\npowerful lens for tracing AfricaNLP research trends and hold potential for\ngenerating data-driven literature surveys."
                },
                "authors": [
                    {
                        "name": "Tadesse Destaw Belay"
                    },
                    {
                        "name": "Kedir Yassin Hussen"
                    },
                    {
                        "name": "Sukairaj Hafiz Imam"
                    },
                    {
                        "name": "Ibrahim Said Ahmad"
                    },
                    {
                        "name": "Isa Inuwa-Dutse"
                    },
                    {
                        "name": "Abrham Belete Haile"
                    },
                    {
                        "name": "Grigori Sidorov"
                    },
                    {
                        "name": "Iqra Ameer"
                    },
                    {
                        "name": "Idris Abdulmumin"
                    },
                    {
                        "name": "Tajuddeen Gwadabe"
                    },
                    {
                        "name": "Vukosi Marivate"
                    },
                    {
                        "name": "Seid Muhie Yimam"
                    },
                    {
                        "name": "Shamsuddeen Hassan Muhammad"
                    }
                ],
                "author_detail": {
                    "name": "Shamsuddeen Hassan Muhammad"
                },
                "author": "Shamsuddeen Hassan Muhammad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25477v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25477v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08584v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08584v2",
                "updated": "2025-10-01T17:10:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    17,
                    10,
                    2,
                    2,
                    274,
                    0
                ],
                "published": "2025-06-10T08:53:06Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    8,
                    53,
                    6,
                    1,
                    161,
                    0
                ],
                "title": "CounselBench: A Large-Scale Expert Evaluation and Adversarial\n  Benchmarking of Large Language Models in Mental Health Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CounselBench: A Large-Scale Expert Evaluation and Adversarial\n  Benchmarking of Large Language Models in Mental Health Question Answering"
                },
                "summary": "Medical question answering (QA) benchmarks often focus on multiple-choice or\nfact-based tasks, leaving open-ended answers to real patient questions\nunderexplored. This gap is particularly critical in mental health, where\npatient questions often mix symptoms, treatment concerns, and emotional needs,\nrequiring answers that balance clinical caution with contextual sensitivity. We\npresent CounselBench, a large-scale benchmark developed with 100 mental health\nprofessionals to evaluate and stress-test large language models (LLMs) in\nrealistic help-seeking scenarios. The first component, CounselBench-EVAL,\ncontains 2,000 expert evaluations of answers from GPT-4, LLaMA 3, Gemini, and\nhuman therapists on patient questions from the public forum CounselChat. Each\nanswer is rated across six clinically grounded dimensions, with span-level\nannotations and written rationales. Expert evaluations show that while LLMs\nachieve high scores on several dimensions, they also exhibit recurring issues,\nincluding unconstructive feedback, overgeneralization, and limited\npersonalization or relevance. Responses were frequently flagged for safety\nrisks, most notably unauthorized medical advice. Follow-up experiments show\nthat LLM judges systematically overrate model responses and overlook safety\nconcerns identified by human experts. To probe failure modes more directly, we\nconstruct CounselBench-Adv, an adversarial dataset of 120 expert-authored\nmental health questions designed to trigger specific model issues. Evaluation\nof 3,240 responses from nine LLMs reveals consistent, model-specific failure\npatterns. Together, CounselBench establishes a clinically grounded framework\nfor benchmarking LLMs in mental health QA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical question answering (QA) benchmarks often focus on multiple-choice or\nfact-based tasks, leaving open-ended answers to real patient questions\nunderexplored. This gap is particularly critical in mental health, where\npatient questions often mix symptoms, treatment concerns, and emotional needs,\nrequiring answers that balance clinical caution with contextual sensitivity. We\npresent CounselBench, a large-scale benchmark developed with 100 mental health\nprofessionals to evaluate and stress-test large language models (LLMs) in\nrealistic help-seeking scenarios. The first component, CounselBench-EVAL,\ncontains 2,000 expert evaluations of answers from GPT-4, LLaMA 3, Gemini, and\nhuman therapists on patient questions from the public forum CounselChat. Each\nanswer is rated across six clinically grounded dimensions, with span-level\nannotations and written rationales. Expert evaluations show that while LLMs\nachieve high scores on several dimensions, they also exhibit recurring issues,\nincluding unconstructive feedback, overgeneralization, and limited\npersonalization or relevance. Responses were frequently flagged for safety\nrisks, most notably unauthorized medical advice. Follow-up experiments show\nthat LLM judges systematically overrate model responses and overlook safety\nconcerns identified by human experts. To probe failure modes more directly, we\nconstruct CounselBench-Adv, an adversarial dataset of 120 expert-authored\nmental health questions designed to trigger specific model issues. Evaluation\nof 3,240 responses from nine LLMs reveals consistent, model-specific failure\npatterns. Together, CounselBench establishes a clinically grounded framework\nfor benchmarking LLMs in mental health QA."
                },
                "authors": [
                    {
                        "name": "Yahan Li"
                    },
                    {
                        "name": "Jifan Yao"
                    },
                    {
                        "name": "John Bosco S. Bunyi"
                    },
                    {
                        "name": "Adam C. Frank"
                    },
                    {
                        "name": "Angel Hwang"
                    },
                    {
                        "name": "Ruishan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ruishan Liu"
                },
                "author": "Ruishan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08584v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08584v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08359v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08359v2",
                "updated": "2025-10-01T16:56:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    16,
                    56,
                    5,
                    2,
                    274,
                    0
                ],
                "published": "2025-06-10T02:16:50Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    2,
                    16,
                    50,
                    1,
                    161,
                    0
                ],
                "title": "REAL: Reading Out Transformer Activations for Precise Localization in\n  Language Model Steering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REAL: Reading Out Transformer Activations for Precise Localization in\n  Language Model Steering"
                },
                "summary": "Inference-time steering aims to alter a large language model's (LLM's)\nresponses without changing its parameters, but a central challenge is\nidentifying the internal modules that most strongly govern the target behavior.\nExisting approaches often rely on simplistic cues or ad hoc heuristics, leading\nto suboptimal or unintended effects. We introduce REAL, a framework for\nidentifying behavior-relevant modules (attention heads or layers) in\nTransformer models. For each module, REAL trains a vector-quantized autoencoder\n(VQ-AE) on its hidden activations and uses a shared, learnable codebook to\npartition the latent space into behavior-relevant and behavior-irrelevant\nsubspaces. REAL quantifies a module's behavioral relevance by how well its\nVQ-AE encodings discriminate behavior-aligned from behavior-violating responses\nvia a binary classification metric; this score guides both module selection and\nsteering strength. We evaluate REAL across eight LLMs from the Llama and Qwen\nfamilies and nine datasets spanning truthfulness enhancement, open-domain QA\nunder knowledge conflicts, and general alignment tasks. REAL enables more\neffective inference-time interventions, achieving an average relative\nimprovement of 20% (up to 81.5%) over the ITI method on truthfulness steering.\nIn addition, the modules selected by REAL exhibit strong zero-shot\ngeneralization in cross-domain truthfulness-steering scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-time steering aims to alter a large language model's (LLM's)\nresponses without changing its parameters, but a central challenge is\nidentifying the internal modules that most strongly govern the target behavior.\nExisting approaches often rely on simplistic cues or ad hoc heuristics, leading\nto suboptimal or unintended effects. We introduce REAL, a framework for\nidentifying behavior-relevant modules (attention heads or layers) in\nTransformer models. For each module, REAL trains a vector-quantized autoencoder\n(VQ-AE) on its hidden activations and uses a shared, learnable codebook to\npartition the latent space into behavior-relevant and behavior-irrelevant\nsubspaces. REAL quantifies a module's behavioral relevance by how well its\nVQ-AE encodings discriminate behavior-aligned from behavior-violating responses\nvia a binary classification metric; this score guides both module selection and\nsteering strength. We evaluate REAL across eight LLMs from the Llama and Qwen\nfamilies and nine datasets spanning truthfulness enhancement, open-domain QA\nunder knowledge conflicts, and general alignment tasks. REAL enables more\neffective inference-time interventions, achieving an average relative\nimprovement of 20% (up to 81.5%) over the ITI method on truthfulness steering.\nIn addition, the modules selected by REAL exhibit strong zero-shot\ngeneralization in cross-domain truthfulness-steering scenarios."
                },
                "authors": [
                    {
                        "name": "Li-Ming Zhan"
                    },
                    {
                        "name": "Bo Liu"
                    },
                    {
                        "name": "Chengqiang Xie"
                    },
                    {
                        "name": "Jiannong Cao"
                    },
                    {
                        "name": "Xiao-Ming Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xiao-Ming Wu"
                },
                "author": "Xiao-Ming Wu",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08359v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08359v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22415v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22415v2",
                "updated": "2025-10-01T16:48:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    16,
                    48,
                    1,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-26T14:39:13Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    39,
                    13,
                    4,
                    269,
                    0
                ],
                "title": "Explaining multimodal LLMs via intra-modal token interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explaining multimodal LLMs via intra-modal token interactions"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success\nacross diverse vision-language tasks, yet their internal decision-making\nmechanisms remain insufficiently understood. Existing interpretability research\nhas primarily focused on cross-modal attribution, identifying which image\nregions the model attends to during output generation. However, these\napproaches often overlook intra-modal dependencies. In the visual modality,\nattributing importance to isolated image patches ignores spatial context due to\nlimited receptive fields, resulting in fragmented and noisy explanations. In\nthe textual modality, reliance on preceding tokens introduces spurious\nactivations. Failing to effectively mitigate these interference compromises\nattribution fidelity. To address these limitations, we propose enhancing\ninterpretability by leveraging intra-modal interaction. For the visual branch,\nwe introduce \\textit{Multi-Scale Explanation Aggregation} (MSEA), which\naggregates attributions over multi-scale inputs to dynamically adjust receptive\nfields, producing more holistic and spatially coherent visual explanations. For\nthe textual branch, we propose \\textit{Activation Ranking Correlation} (ARC),\nwhich measures the relevance of contextual tokens to the current token via\nalignment of their top-$k$ prediction rankings. ARC leverages this relevance to\nsuppress spurious activations from irrelevant contexts while preserving\nsemantically coherent ones. Extensive experiments across state-of-the-art MLLMs\nand benchmark datasets demonstrate that our approach consistently outperforms\nexisting interpretability methods, yielding more faithful and fine-grained\nexplanations of model behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have achieved remarkable success\nacross diverse vision-language tasks, yet their internal decision-making\nmechanisms remain insufficiently understood. Existing interpretability research\nhas primarily focused on cross-modal attribution, identifying which image\nregions the model attends to during output generation. However, these\napproaches often overlook intra-modal dependencies. In the visual modality,\nattributing importance to isolated image patches ignores spatial context due to\nlimited receptive fields, resulting in fragmented and noisy explanations. In\nthe textual modality, reliance on preceding tokens introduces spurious\nactivations. Failing to effectively mitigate these interference compromises\nattribution fidelity. To address these limitations, we propose enhancing\ninterpretability by leveraging intra-modal interaction. For the visual branch,\nwe introduce \\textit{Multi-Scale Explanation Aggregation} (MSEA), which\naggregates attributions over multi-scale inputs to dynamically adjust receptive\nfields, producing more holistic and spatially coherent visual explanations. For\nthe textual branch, we propose \\textit{Activation Ranking Correlation} (ARC),\nwhich measures the relevance of contextual tokens to the current token via\nalignment of their top-$k$ prediction rankings. ARC leverages this relevance to\nsuppress spurious activations from irrelevant contexts while preserving\nsemantically coherent ones. Extensive experiments across state-of-the-art MLLMs\nand benchmark datasets demonstrate that our approach consistently outperforms\nexisting interpretability methods, yielding more faithful and fine-grained\nexplanations of model behavior."
                },
                "authors": [
                    {
                        "name": "Jiawei Liang"
                    },
                    {
                        "name": "Ruoyu Chen"
                    },
                    {
                        "name": "Xianghao Jiao"
                    },
                    {
                        "name": "Siyuan Liang"
                    },
                    {
                        "name": "Shiming Liu"
                    },
                    {
                        "name": "Qunli Zhang"
                    },
                    {
                        "name": "Zheng Hu"
                    },
                    {
                        "name": "Xiaochun Cao"
                    }
                ],
                "author_detail": {
                    "name": "Xiaochun Cao"
                },
                "author": "Xiaochun Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22415v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22415v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.04671v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.04671v4",
                "updated": "2025-10-01T16:40:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    16,
                    40,
                    22,
                    2,
                    274,
                    0
                ],
                "published": "2024-04-06T16:16:30Z",
                "published_parsed": [
                    2024,
                    4,
                    6,
                    16,
                    16,
                    30,
                    5,
                    97,
                    0
                ],
                "title": "PhyloLM : Inferring the Phylogeny of Large Language Models and\n  Predicting their Performances in Benchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhyloLM : Inferring the Phylogeny of Large Language Models and\n  Predicting their Performances in Benchmarks"
                },
                "summary": "This paper introduces PhyloLM, a method adapting phylogenetic algorithms to\nLarge Language Models (LLMs) to explore whether and how they relate to each\nother and to predict their performance characteristics. Our method calculates a\nphylogenetic distance metric based on the similarity of LLMs' output. The\nresulting metric is then used to construct dendrograms, which satisfactorily\ncapture known relationships across a set of 111 open-source and 45 closed\nmodels. Furthermore, our phylogenetic distance predicts performance in standard\nbenchmarks, thus demonstrating its functional validity and paving the way for a\ntime and cost-effective estimation of LLM capabilities. To sum up, by\ntranslating population genetic concepts to machine learning, we propose and\nvalidate a tool to evaluate LLM development, relationships and capabilities,\neven in the absence of transparent training information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces PhyloLM, a method adapting phylogenetic algorithms to\nLarge Language Models (LLMs) to explore whether and how they relate to each\nother and to predict their performance characteristics. Our method calculates a\nphylogenetic distance metric based on the similarity of LLMs' output. The\nresulting metric is then used to construct dendrograms, which satisfactorily\ncapture known relationships across a set of 111 open-source and 45 closed\nmodels. Furthermore, our phylogenetic distance predicts performance in standard\nbenchmarks, thus demonstrating its functional validity and paving the way for a\ntime and cost-effective estimation of LLM capabilities. To sum up, by\ntranslating population genetic concepts to machine learning, we propose and\nvalidate a tool to evaluate LLM development, relationships and capabilities,\neven in the absence of transparent training information."
                },
                "authors": [
                    {
                        "name": "Nicolas Yax"
                    },
                    {
                        "name": "Pierre-Yves Oudeyer"
                    },
                    {
                        "name": "Stefano Palminteri"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Palminteri"
                },
                "author": "Stefano Palminteri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.04671v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.04671v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14285v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14285v2",
                "updated": "2025-10-01T16:39:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    16,
                    39,
                    48,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-16T19:11:28Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    19,
                    11,
                    28,
                    1,
                    259,
                    0
                ],
                "title": "A Multi-Agent LLM Defense Pipeline Against Prompt Injection Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multi-Agent LLM Defense Pipeline Against Prompt Injection Attacks"
                },
                "summary": "Prompt injection attacks represent a major vulnerability in Large Language\nModel (LLM) deployments, where malicious instructions embedded in user inputs\ncan override system prompts and induce unintended behaviors. This paper\npresents a novel multi-agent defense framework that employs specialized LLM\nagents in coordinated pipelines to detect and neutralize prompt injection\nattacks in real-time. We evaluate our approach using two distinct\narchitectures: a sequential chain-of-agents pipeline and a hierarchical\ncoordinator-based system. Our comprehensive evaluation on 55 unique prompt\ninjection attacks, grouped into 8 categories and totaling 400 attack instances\nacross two LLM platforms (ChatGLM and Llama2), demonstrates significant\nsecurity improvements. Without defense mechanisms, baseline Attack Success\nRates (ASR) reached 30% for ChatGLM and 20% for Llama2. Our multi-agent\npipeline achieved 100% mitigation, reducing ASR to 0% across all tested\nscenarios. The framework demonstrates robustness across multiple attack\ncategories including direct overrides, code execution attempts, data\nexfiltration, and obfuscation techniques, while maintaining system\nfunctionality for legitimate queries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt injection attacks represent a major vulnerability in Large Language\nModel (LLM) deployments, where malicious instructions embedded in user inputs\ncan override system prompts and induce unintended behaviors. This paper\npresents a novel multi-agent defense framework that employs specialized LLM\nagents in coordinated pipelines to detect and neutralize prompt injection\nattacks in real-time. We evaluate our approach using two distinct\narchitectures: a sequential chain-of-agents pipeline and a hierarchical\ncoordinator-based system. Our comprehensive evaluation on 55 unique prompt\ninjection attacks, grouped into 8 categories and totaling 400 attack instances\nacross two LLM platforms (ChatGLM and Llama2), demonstrates significant\nsecurity improvements. Without defense mechanisms, baseline Attack Success\nRates (ASR) reached 30% for ChatGLM and 20% for Llama2. Our multi-agent\npipeline achieved 100% mitigation, reducing ASR to 0% across all tested\nscenarios. The framework demonstrates robustness across multiple attack\ncategories including direct overrides, code execution attempts, data\nexfiltration, and obfuscation techniques, while maintaining system\nfunctionality for legitimate queries."
                },
                "authors": [
                    {
                        "name": "S M Asif Hossain"
                    },
                    {
                        "name": "Ruksat Khan Shayoni"
                    },
                    {
                        "name": "Mohd Ruhul Ameen"
                    },
                    {
                        "name": "Akif Islam"
                    },
                    {
                        "name": "M. F. Mridha"
                    },
                    {
                        "name": "Jungpil Shin"
                    }
                ],
                "author_detail": {
                    "name": "Jungpil Shin"
                },
                "author": "Jungpil Shin",
                "arxiv_comment": "IEEE Conference standard paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14285v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14285v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12950v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12950v2",
                "updated": "2025-10-01T16:25:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    16,
                    25,
                    22,
                    2,
                    274,
                    0
                ],
                "published": "2025-05-19T10:42:36Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    10,
                    42,
                    36,
                    0,
                    139,
                    0
                ],
                "title": "GuRE:Generative Query REwriter for Legal Passage Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GuRE:Generative Query REwriter for Legal Passage Retrieval"
                },
                "summary": "Legal Passage Retrieval (LPR) systems are crucial as they help practitioners\nsave time when drafting legal arguments. However, it remains an underexplored\navenue. One primary reason is the significant vocabulary mismatch between the\nquery and the target passage. To address this, we propose a simple yet\neffective method, the Generative query REwriter (GuRE). We leverage the\ngenerative capabilities of Large Language Models (LLMs) by training the LLM for\nquery rewriting. \"Rewritten queries\" help retrievers to retrieve target\npassages by mitigating vocabulary mismatch. Experimental results show that GuRE\nsignificantly improves performance in a retriever-agnostic manner,\noutperforming all baseline methods. Further analysis reveals that different\ntraining objectives lead to distinct retrieval behaviors, making GuRE more\nsuitable than direct retriever fine-tuning for real-world applications. Codes\nare avaiable at github.com/daehuikim/GuRE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Legal Passage Retrieval (LPR) systems are crucial as they help practitioners\nsave time when drafting legal arguments. However, it remains an underexplored\navenue. One primary reason is the significant vocabulary mismatch between the\nquery and the target passage. To address this, we propose a simple yet\neffective method, the Generative query REwriter (GuRE). We leverage the\ngenerative capabilities of Large Language Models (LLMs) by training the LLM for\nquery rewriting. \"Rewritten queries\" help retrievers to retrieve target\npassages by mitigating vocabulary mismatch. Experimental results show that GuRE\nsignificantly improves performance in a retriever-agnostic manner,\noutperforming all baseline methods. Further analysis reveals that different\ntraining objectives lead to distinct retrieval behaviors, making GuRE more\nsuitable than direct retriever fine-tuning for real-world applications. Codes\nare avaiable at github.com/daehuikim/GuRE."
                },
                "authors": [
                    {
                        "name": "Daehee Kim"
                    },
                    {
                        "name": "Deokhyung Kang"
                    },
                    {
                        "name": "Jonghwi Kim"
                    },
                    {
                        "name": "Sangwon Ryu"
                    },
                    {
                        "name": "Gary Geunbae Lee"
                    }
                ],
                "author_detail": {
                    "name": "Gary Geunbae Lee"
                },
                "author": "Gary Geunbae Lee",
                "arxiv_comment": "NLLP Workshop at EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12950v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12950v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26153v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26153v2",
                "updated": "2025-10-01T16:11:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    16,
                    11,
                    30,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-30T12:03:32Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    12,
                    3,
                    32,
                    1,
                    273,
                    0
                ],
                "title": "Beyond the Algorithm: A Field Guide to Deploying AI Agents in Clinical\n  Practice",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond the Algorithm: A Field Guide to Deploying AI Agents in Clinical\n  Practice"
                },
                "summary": "Large language models (LLMs) integrated into agent-driven workflows hold\nimmense promise for healthcare, yet a significant gap exists between their\npotential and practical implementation within clinical settings. To address\nthis, we present a practitioner-oriented field manual for deploying generative\nagents that use electronic health record (EHR) data. This guide is informed by\nour experience deploying the \"irAE-Agent\", an automated system to detect\nimmune-related adverse events from clinical notes at Mass General Brigham, and\nby structured interviews with 20 clinicians, engineers, and informatics leaders\ninvolved in the project. Our analysis reveals a critical misalignment in\nclinical AI development: less than 20% of our effort was dedicated to prompt\nengineering and model development, while over 80% was consumed by the\nsociotechnical work of implementation. We distill this effort into five \"heavy\nlifts\": data integration, model validation, ensuring economic value, managing\nsystem drift, and governance. By providing actionable solutions for each of\nthese challenges, this field manual shifts the focus from algorithmic\ndevelopment to the essential infrastructure and implementation work required to\nbridge the \"valley of death\" and successfully translate generative AI from\npilot projects into routine clinical care.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) integrated into agent-driven workflows hold\nimmense promise for healthcare, yet a significant gap exists between their\npotential and practical implementation within clinical settings. To address\nthis, we present a practitioner-oriented field manual for deploying generative\nagents that use electronic health record (EHR) data. This guide is informed by\nour experience deploying the \"irAE-Agent\", an automated system to detect\nimmune-related adverse events from clinical notes at Mass General Brigham, and\nby structured interviews with 20 clinicians, engineers, and informatics leaders\ninvolved in the project. Our analysis reveals a critical misalignment in\nclinical AI development: less than 20% of our effort was dedicated to prompt\nengineering and model development, while over 80% was consumed by the\nsociotechnical work of implementation. We distill this effort into five \"heavy\nlifts\": data integration, model validation, ensuring economic value, managing\nsystem drift, and governance. By providing actionable solutions for each of\nthese challenges, this field manual shifts the focus from algorithmic\ndevelopment to the essential infrastructure and implementation work required to\nbridge the \"valley of death\" and successfully translate generative AI from\npilot projects into routine clinical care."
                },
                "authors": [
                    {
                        "name": "Jack Gallifant"
                    },
                    {
                        "name": "Katherine C. Kellogg"
                    },
                    {
                        "name": "Matt Butler"
                    },
                    {
                        "name": "Amanda Centi"
                    },
                    {
                        "name": "Shan Chen"
                    },
                    {
                        "name": "Patrick F. Doyle"
                    },
                    {
                        "name": "Sayon Dutta"
                    },
                    {
                        "name": "Joyce Guo"
                    },
                    {
                        "name": "Matthew J. Hadfield"
                    },
                    {
                        "name": "Esther H. Kim"
                    },
                    {
                        "name": "David E. Kozono"
                    },
                    {
                        "name": "Hugo JWL Aerts"
                    },
                    {
                        "name": "Adam B. Landman"
                    },
                    {
                        "name": "Raymond H. Mak"
                    },
                    {
                        "name": "Rebecca G. Mishuris"
                    },
                    {
                        "name": "Tanna L. Nelson"
                    },
                    {
                        "name": "Guergana K. Savova"
                    },
                    {
                        "name": "Elad Sharon"
                    },
                    {
                        "name": "Benjamin C. Silverman"
                    },
                    {
                        "name": "Umit Topaloglu"
                    },
                    {
                        "name": "Jeremy L. Warner"
                    },
                    {
                        "name": "Danielle S. Bitterman"
                    }
                ],
                "author_detail": {
                    "name": "Danielle S. Bitterman"
                },
                "author": "Danielle S. Bitterman",
                "arxiv_comment": "Under review. 5 Tables, 2 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26153v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26153v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14725v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14725v3",
                "updated": "2025-10-01T16:07:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    16,
                    7,
                    15,
                    2,
                    274,
                    0
                ],
                "published": "2025-07-19T19:15:03Z",
                "published_parsed": [
                    2025,
                    7,
                    19,
                    19,
                    15,
                    3,
                    5,
                    200,
                    0
                ],
                "title": "GRID: Scalable Task-Agnostic Prompt-Based Continual Learning for\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GRID: Scalable Task-Agnostic Prompt-Based Continual Learning for\n  Language Models"
                },
                "summary": "Prompt-based continual learning (CL) provides a parameter-efficient approach\nfor adapting large language models (LLMs) across task sequences. However, most\nexisting methods rely on task-aware inference and maintain a growing set of\ntask-specific prompts, which introduces two major challenges: (1) severe\nperformance degradation on earlier tasks under task-agnostic inference, and (2)\nlimited scalability due to prompt memory accumulation as task sequences grow.\nIn this paper, we present GRID, a unified framework designed to address these\nchallenges. GRID incorporates a decoding mechanism that enhances backward\ntransfer by leveraging representative inputs, automatic task identification,\nand constrained decoding. Furthermore, it employs a gradient-guided prompt\nselection strategy to compress less informative prompts into a single\naggregated representation, ensuring scalable and memory-efficient continual\nlearning. Extensive experiments on long-sequence and negative transfer\nbenchmarks show that GRID improves average accuracy and backward transfer,\nachieves competitive forward transfer, and substantially reduces prompt memory\nusage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt-based continual learning (CL) provides a parameter-efficient approach\nfor adapting large language models (LLMs) across task sequences. However, most\nexisting methods rely on task-aware inference and maintain a growing set of\ntask-specific prompts, which introduces two major challenges: (1) severe\nperformance degradation on earlier tasks under task-agnostic inference, and (2)\nlimited scalability due to prompt memory accumulation as task sequences grow.\nIn this paper, we present GRID, a unified framework designed to address these\nchallenges. GRID incorporates a decoding mechanism that enhances backward\ntransfer by leveraging representative inputs, automatic task identification,\nand constrained decoding. Furthermore, it employs a gradient-guided prompt\nselection strategy to compress less informative prompts into a single\naggregated representation, ensuring scalable and memory-efficient continual\nlearning. Extensive experiments on long-sequence and negative transfer\nbenchmarks show that GRID improves average accuracy and backward transfer,\nachieves competitive forward transfer, and substantially reduces prompt memory\nusage."
                },
                "authors": [
                    {
                        "name": "Anushka Tiwari"
                    },
                    {
                        "name": "Sayantan Pal"
                    },
                    {
                        "name": "Rohini K. Srihari"
                    },
                    {
                        "name": "Kaiyi Ji"
                    }
                ],
                "author_detail": {
                    "name": "Kaiyi Ji"
                },
                "author": "Kaiyi Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14725v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14725v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15957v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15957v3",
                "updated": "2025-10-01T16:02:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    16,
                    2,
                    37,
                    2,
                    274,
                    0
                ],
                "published": "2025-05-21T19:17:29Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    19,
                    17,
                    29,
                    2,
                    141,
                    0
                ],
                "title": "Towards Holistic Evaluation of Large Audio-Language Models: A\n  Comprehensive Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Holistic Evaluation of Large Audio-Language Models: A\n  Comprehensive Survey"
                },
                "summary": "With advancements in large audio-language models (LALMs), which enhance large\nlanguage models (LLMs) with auditory capabilities, these models are expected to\ndemonstrate universal proficiency across various auditory tasks. While numerous\nbenchmarks have emerged to assess LALMs' performance, they remain fragmented\nand lack a structured taxonomy. To bridge this gap, we conduct a comprehensive\nsurvey and propose a systematic taxonomy for LALM evaluations, categorizing\nthem into four dimensions based on their objectives: (1) General Auditory\nAwareness and Processing, (2) Knowledge and Reasoning, (3) Dialogue-oriented\nAbility, and (4) Fairness, Safety, and Trustworthiness. We provide detailed\noverviews within each category and highlight challenges in this field, offering\ninsights into promising future directions. To the best of our knowledge, this\nis the first survey specifically focused on the evaluations of LALMs, providing\nclear guidelines for the community. We will release the collection of the\nsurveyed papers and actively maintain it to support ongoing advancements in the\nfield.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With advancements in large audio-language models (LALMs), which enhance large\nlanguage models (LLMs) with auditory capabilities, these models are expected to\ndemonstrate universal proficiency across various auditory tasks. While numerous\nbenchmarks have emerged to assess LALMs' performance, they remain fragmented\nand lack a structured taxonomy. To bridge this gap, we conduct a comprehensive\nsurvey and propose a systematic taxonomy for LALM evaluations, categorizing\nthem into four dimensions based on their objectives: (1) General Auditory\nAwareness and Processing, (2) Knowledge and Reasoning, (3) Dialogue-oriented\nAbility, and (4) Fairness, Safety, and Trustworthiness. We provide detailed\noverviews within each category and highlight challenges in this field, offering\ninsights into promising future directions. To the best of our knowledge, this\nis the first survey specifically focused on the evaluations of LALMs, providing\nclear guidelines for the community. We will release the collection of the\nsurveyed papers and actively maintain it to support ongoing advancements in the\nfield."
                },
                "authors": [
                    {
                        "name": "Chih-Kai Yang"
                    },
                    {
                        "name": "Neo S. Ho"
                    },
                    {
                        "name": "Hung-yi Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hung-yi Lee"
                },
                "author": "Hung-yi Lee",
                "arxiv_comment": "EMNLP 2025 (Main). Project Website:\n  https://github.com/ckyang1124/LALM-Evaluation-Survey",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15957v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15957v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23571v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23571v2",
                "updated": "2025-10-01T16:01:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    16,
                    1,
                    24,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-28T02:08:17Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    2,
                    8,
                    17,
                    6,
                    271,
                    0
                ],
                "title": "Benchmarking LLM-Assisted Blue Teaming via Standardized Threat Hunting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking LLM-Assisted Blue Teaming via Standardized Threat Hunting"
                },
                "summary": "As cyber threats continue to grow in scale and sophistication, blue team\ndefenders increasingly require advanced tools to proactively detect and\nmitigate risks. Large Language Models (LLMs) offer promising capabilities for\nenhancing threat analysis. However, their effectiveness in real-world blue team\nthreat-hunting scenarios remains insufficiently explored. This paper presents\nCyberTeam, a benchmark designed to guide LLMs in blue teaming practice.\nCyberTeam constructs a standardized workflow in two stages. First, it models\nrealistic threat-hunting workflows by capturing the dependencies among\nanalytical tasks from threat attribution to incident response. Next, each task\nis addressed through a set of operational modules tailored to its specific\nanalytical requirements. This transforms threat hunting into a structured\nsequence of reasoning steps, with each step grounded in a discrete operation\nand ordered according to task-specific dependencies. Guided by this framework,\nLLMs are directed to perform threat-hunting tasks through modularized steps.\nOverall, CyberTeam integrates 30 tasks and 9 operational modules to guide LLMs\nthrough standardized threat analysis. We evaluate both leading LLMs and\nstate-of-the-art cybersecurity agents, comparing CyberTeam against open-ended\nreasoning strategies. Our results highlight the improvements enabled by\nstandardized design, while also revealing the limitations of open-ended\nreasoning in real-world threat hunting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As cyber threats continue to grow in scale and sophistication, blue team\ndefenders increasingly require advanced tools to proactively detect and\nmitigate risks. Large Language Models (LLMs) offer promising capabilities for\nenhancing threat analysis. However, their effectiveness in real-world blue team\nthreat-hunting scenarios remains insufficiently explored. This paper presents\nCyberTeam, a benchmark designed to guide LLMs in blue teaming practice.\nCyberTeam constructs a standardized workflow in two stages. First, it models\nrealistic threat-hunting workflows by capturing the dependencies among\nanalytical tasks from threat attribution to incident response. Next, each task\nis addressed through a set of operational modules tailored to its specific\nanalytical requirements. This transforms threat hunting into a structured\nsequence of reasoning steps, with each step grounded in a discrete operation\nand ordered according to task-specific dependencies. Guided by this framework,\nLLMs are directed to perform threat-hunting tasks through modularized steps.\nOverall, CyberTeam integrates 30 tasks and 9 operational modules to guide LLMs\nthrough standardized threat analysis. We evaluate both leading LLMs and\nstate-of-the-art cybersecurity agents, comparing CyberTeam against open-ended\nreasoning strategies. Our results highlight the improvements enabled by\nstandardized design, while also revealing the limitations of open-ended\nreasoning in real-world threat hunting."
                },
                "authors": [
                    {
                        "name": "Yuqiao Meng"
                    },
                    {
                        "name": "Luoxi Tang"
                    },
                    {
                        "name": "Feiyang Yu"
                    },
                    {
                        "name": "Xi Li"
                    },
                    {
                        "name": "Guanhua Yan"
                    },
                    {
                        "name": "Ping Yang"
                    },
                    {
                        "name": "Zhaohan Xi"
                    }
                ],
                "author_detail": {
                    "name": "Zhaohan Xi"
                },
                "author": "Zhaohan Xi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23571v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23571v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13818v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13818v3",
                "updated": "2025-10-01T15:58:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    15,
                    58,
                    47,
                    2,
                    274,
                    0
                ],
                "published": "2025-04-18T17:49:55Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    17,
                    49,
                    55,
                    4,
                    108,
                    0
                ],
                "title": "Not All Rollouts are Useful: Down-Sampling Rollouts in LLM Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Rollouts are Useful: Down-Sampling Rollouts in LLM Reinforcement\n  Learning"
                },
                "summary": "Reinforcement learning with verifiable rewards (RLVR) has emerged as the\nleading approach for enhancing reasoning capabilities in large language models.\nHowever, it faces a fundamental compute and memory asymmetry: rollout\ngeneration is embarrassingly parallel and memory-light, whereas policy updates\nare communication-heavy and memory-intensive. To address this, we introduce\nPODS (Policy Optimization with Down-Sampling), which decouples rollout\ngeneration from policy updates by training only on a strategically selected\nsubset of rollouts, maintaining learning quality while dramatically reducing\nupdate costs. We propose a principled subset selection criterion, max-variance\ndown-sampling, that maximizes reward diversity, and provide an efficient\n$O(n\\log n)$ implementation. Empirically, Group Relative Policy Optimization\n(GRPO) with PODS achieves the peak test accuracy of vanilla GRPO at least\n$\\mathbf{1.7\\times}$ faster across the different reasoning benchmarks and\nhardware configurations we tested.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning with verifiable rewards (RLVR) has emerged as the\nleading approach for enhancing reasoning capabilities in large language models.\nHowever, it faces a fundamental compute and memory asymmetry: rollout\ngeneration is embarrassingly parallel and memory-light, whereas policy updates\nare communication-heavy and memory-intensive. To address this, we introduce\nPODS (Policy Optimization with Down-Sampling), which decouples rollout\ngeneration from policy updates by training only on a strategically selected\nsubset of rollouts, maintaining learning quality while dramatically reducing\nupdate costs. We propose a principled subset selection criterion, max-variance\ndown-sampling, that maximizes reward diversity, and provide an efficient\n$O(n\\log n)$ implementation. Empirically, Group Relative Policy Optimization\n(GRPO) with PODS achieves the peak test accuracy of vanilla GRPO at least\n$\\mathbf{1.7\\times}$ faster across the different reasoning benchmarks and\nhardware configurations we tested."
                },
                "authors": [
                    {
                        "name": "Yixuan Even Xu"
                    },
                    {
                        "name": "Yash Savani"
                    },
                    {
                        "name": "Fei Fang"
                    },
                    {
                        "name": "J. Zico Kolter"
                    }
                ],
                "author_detail": {
                    "name": "J. Zico Kolter"
                },
                "author": "J. Zico Kolter",
                "arxiv_comment": "17 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13818v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13818v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23573v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23573v2",
                "updated": "2025-10-01T15:57:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    15,
                    57,
                    32,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-28T02:08:27Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    2,
                    8,
                    27,
                    6,
                    271,
                    0
                ],
                "title": "Uncovering Vulnerabilities of LLM-Assisted Cyber Threat Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncovering Vulnerabilities of LLM-Assisted Cyber Threat Intelligence"
                },
                "summary": "Large Language Models (LLMs) are intensively used to assist security analysts\nin counteracting the rapid exploitation of cyber threats, wherein LLMs offer\ncyber threat intelligence (CTI) to support vulnerability assessment and\nincident response. While recent work has shown that LLMs can support a wide\nrange of CTI tasks such as threat analysis, vulnerability detection, and\nintrusion defense, significant performance gaps persist in practical\ndeployments. In this paper, we investigate the intrinsic vulnerabilities of\nLLMs in CTI, focusing on challenges that arise from the nature of the threat\nlandscape itself rather than the model architecture. Using large-scale\nevaluations across multiple CTI benchmarks and real-world threat reports, we\nintroduce a novel categorization methodology that integrates stratification,\nautoregressive refinement, and human-in-the-loop supervision to reliably\nanalyze failure instances. Through extensive experiments and human inspections,\nwe reveal three fundamental vulnerabilities: spurious correlations,\ncontradictory knowledge, and constrained generalization, that limit LLMs in\neffectively supporting CTI. Subsequently, we provide actionable insights for\ndesigning more robust LLM-powered CTI systems to facilitate future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are intensively used to assist security analysts\nin counteracting the rapid exploitation of cyber threats, wherein LLMs offer\ncyber threat intelligence (CTI) to support vulnerability assessment and\nincident response. While recent work has shown that LLMs can support a wide\nrange of CTI tasks such as threat analysis, vulnerability detection, and\nintrusion defense, significant performance gaps persist in practical\ndeployments. In this paper, we investigate the intrinsic vulnerabilities of\nLLMs in CTI, focusing on challenges that arise from the nature of the threat\nlandscape itself rather than the model architecture. Using large-scale\nevaluations across multiple CTI benchmarks and real-world threat reports, we\nintroduce a novel categorization methodology that integrates stratification,\nautoregressive refinement, and human-in-the-loop supervision to reliably\nanalyze failure instances. Through extensive experiments and human inspections,\nwe reveal three fundamental vulnerabilities: spurious correlations,\ncontradictory knowledge, and constrained generalization, that limit LLMs in\neffectively supporting CTI. Subsequently, we provide actionable insights for\ndesigning more robust LLM-powered CTI systems to facilitate future research."
                },
                "authors": [
                    {
                        "name": "Yuqiao Meng"
                    },
                    {
                        "name": "Luoxi Tang"
                    },
                    {
                        "name": "Feiyang Yu"
                    },
                    {
                        "name": "Jinyuan Jia"
                    },
                    {
                        "name": "Guanhua Yan"
                    },
                    {
                        "name": "Ping Yang"
                    },
                    {
                        "name": "Zhaohan Xi"
                    }
                ],
                "author_detail": {
                    "name": "Zhaohan Xi"
                },
                "author": "Zhaohan Xi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23573v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23573v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06920v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06920v2",
                "updated": "2025-10-01T15:55:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    15,
                    55,
                    29,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-08T17:32:17Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    32,
                    17,
                    0,
                    251,
                    0
                ],
                "title": "An Ethically Grounded LLM-Based Approach to Insider Threat Synthesis and\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Ethically Grounded LLM-Based Approach to Insider Threat Synthesis and\n  Detection"
                },
                "summary": "Insider threats are a growing organizational problem due to the complexity of\nidentifying their technical and behavioral elements. A large research body is\ndedicated to the study of insider threats from technological, psychological,\nand educational perspectives. However, research in this domain has been\ngenerally dependent on datasets that are static and limited access which\nrestricts the development of adaptive detection models. This study introduces a\nnovel, ethically grounded approach that uses the large language model (LLM)\nClaude Sonnet 3.7 to dynamically synthesize syslog messages, some of which\ncontain indicators of insider threat scenarios. The messages reflect real-world\ndata distributions by being highly imbalanced (1% insider threats). The syslogs\nwere analyzed for insider threats by both Sonnet 3.7 and GPT-4o, with their\nperformance evaluated through statistical metrics including accuracy,\nprecision, recall, F1, specificity, FAR, MCC, and ROC AUC. Sonnet 3.7\nconsistently outperformed GPT-4o across nearly all metrics, particularly in\nreducing false alarms and improving detection accuracy. The results show strong\npromise for the use of LLMs in synthetic dataset generation and insider threat\ndetection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Insider threats are a growing organizational problem due to the complexity of\nidentifying their technical and behavioral elements. A large research body is\ndedicated to the study of insider threats from technological, psychological,\nand educational perspectives. However, research in this domain has been\ngenerally dependent on datasets that are static and limited access which\nrestricts the development of adaptive detection models. This study introduces a\nnovel, ethically grounded approach that uses the large language model (LLM)\nClaude Sonnet 3.7 to dynamically synthesize syslog messages, some of which\ncontain indicators of insider threat scenarios. The messages reflect real-world\ndata distributions by being highly imbalanced (1% insider threats). The syslogs\nwere analyzed for insider threats by both Sonnet 3.7 and GPT-4o, with their\nperformance evaluated through statistical metrics including accuracy,\nprecision, recall, F1, specificity, FAR, MCC, and ROC AUC. Sonnet 3.7\nconsistently outperformed GPT-4o across nearly all metrics, particularly in\nreducing false alarms and improving detection accuracy. The results show strong\npromise for the use of LLMs in synthetic dataset generation and insider threat\ndetection."
                },
                "authors": [
                    {
                        "name": "Haywood Gelman"
                    },
                    {
                        "name": "John D. Hastings"
                    },
                    {
                        "name": "David Kenley"
                    }
                ],
                "author_detail": {
                    "name": "David Kenley"
                },
                "author": "David Kenley",
                "arxiv_comment": "6 pages, 5 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06920v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06920v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.0; I.2.7; K.4.1; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21046v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21046v2",
                "updated": "2025-10-01T15:48:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    15,
                    48,
                    45,
                    2,
                    274,
                    0
                ],
                "published": "2025-08-28T17:50:58Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    17,
                    50,
                    58,
                    3,
                    240,
                    0
                ],
                "title": "CogVLA: Cognition-Aligned Vision-Language-Action Model via\n  Instruction-Driven Routing & Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CogVLA: Cognition-Aligned Vision-Language-Action Model via\n  Instruction-Driven Routing & Sparsification"
                },
                "summary": "Recent Vision-Language-Action (VLA) models built on pre-trained\nVision-Language Models (VLMs) require extensive post-training, resulting in\nhigh computational overhead that limits scalability and deployment.We propose\nCogVLA, a Cognition-Aligned Vision-Language-Action framework that leverages\ninstruction-driven routing and sparsification to improve both efficiency and\nperformance. CogVLA draws inspiration from human multimodal coordination and\nintroduces a 3-stage progressive architecture. 1) Encoder-FiLM based\nAggregation Routing (EFA-Routing) injects instruction information into the\nvision encoder to selectively aggregate and compress dual-stream visual tokens,\nforming a instruction-aware latent representation. 2) Building upon this\ncompact visual encoding, LLM-FiLM based Pruning Routing (LFP-Routing)\nintroduces action intent into the language model by pruning\ninstruction-irrelevant visually grounded tokens, thereby achieving token-level\nsparsity. 3) To ensure that compressed perception inputs can still support\naccurate and coherent action generation, we introduce V-L-A Coupled Attention\n(CAtten), which combines causal vision-language attention with bidirectional\naction parallel decoding. Extensive experiments on the LIBERO benchmark and\nreal-world robotic tasks demonstrate that CogVLA achieves state-of-the-art\nperformance with success rates of 97.4% and 70.0%, respectively, while reducing\ntraining costs by 2.5-fold and decreasing inference latency by 2.8-fold\ncompared to OpenVLA. CogVLA is open-sourced and publicly available at\nhttps://github.com/JiuTian-VL/CogVLA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Vision-Language-Action (VLA) models built on pre-trained\nVision-Language Models (VLMs) require extensive post-training, resulting in\nhigh computational overhead that limits scalability and deployment.We propose\nCogVLA, a Cognition-Aligned Vision-Language-Action framework that leverages\ninstruction-driven routing and sparsification to improve both efficiency and\nperformance. CogVLA draws inspiration from human multimodal coordination and\nintroduces a 3-stage progressive architecture. 1) Encoder-FiLM based\nAggregation Routing (EFA-Routing) injects instruction information into the\nvision encoder to selectively aggregate and compress dual-stream visual tokens,\nforming a instruction-aware latent representation. 2) Building upon this\ncompact visual encoding, LLM-FiLM based Pruning Routing (LFP-Routing)\nintroduces action intent into the language model by pruning\ninstruction-irrelevant visually grounded tokens, thereby achieving token-level\nsparsity. 3) To ensure that compressed perception inputs can still support\naccurate and coherent action generation, we introduce V-L-A Coupled Attention\n(CAtten), which combines causal vision-language attention with bidirectional\naction parallel decoding. Extensive experiments on the LIBERO benchmark and\nreal-world robotic tasks demonstrate that CogVLA achieves state-of-the-art\nperformance with success rates of 97.4% and 70.0%, respectively, while reducing\ntraining costs by 2.5-fold and decreasing inference latency by 2.8-fold\ncompared to OpenVLA. CogVLA is open-sourced and publicly available at\nhttps://github.com/JiuTian-VL/CogVLA."
                },
                "authors": [
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Renshan Zhang"
                    },
                    {
                        "name": "Rui Shao"
                    },
                    {
                        "name": "Jie He"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "arxiv_comment": "Accepted to NeurIPS 2025, Project Page:\n  https://jiutian-vl.github.io/CogVLA-page",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21046v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21046v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11679v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11679v3",
                "updated": "2025-10-01T15:44:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    15,
                    44,
                    19,
                    2,
                    274,
                    0
                ],
                "published": "2025-05-16T20:39:30Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    20,
                    39,
                    30,
                    4,
                    136,
                    0
                ],
                "title": "Ambiguity in LLMs is a concept missing problem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ambiguity in LLMs is a concept missing problem"
                },
                "summary": "Ambiguity in natural language is a significant obstacle for achieving\naccurate text to structured data mapping through large language models (LLMs),\nwhich affects the performance of tasks such as mapping text to agentic tool\ncalling and text-to-SQL queries. Existing methods to ambiguity handling either\nrely on the ReACT framework to obtain correct mappings through trial and error,\nor on supervised fine-tuning to bias models toward specific tasks. In this\npaper, we adopt a different approach that characterizes representation\ndifferences of ambiguous text in the latent space and leverages these\ndifferences to identify ambiguity before mapping them to structured data. To\ndetect sentence-level ambiguity, we focus on the relationship between ambiguous\nquestions and their interpretations. Unlike distances calculated by dense\nembeddings, we introduce a new distance measure based on a path kernel over\nconcepts. With this measurement, we identify patterns to distinguish ambiguous\nfrom unambiguous questions. Furthermore, we propose a method for improving LLM\nperformance on ambiguous agentic tool calling through missing concept\nprediction. Both achieve state-of-the-art results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ambiguity in natural language is a significant obstacle for achieving\naccurate text to structured data mapping through large language models (LLMs),\nwhich affects the performance of tasks such as mapping text to agentic tool\ncalling and text-to-SQL queries. Existing methods to ambiguity handling either\nrely on the ReACT framework to obtain correct mappings through trial and error,\nor on supervised fine-tuning to bias models toward specific tasks. In this\npaper, we adopt a different approach that characterizes representation\ndifferences of ambiguous text in the latent space and leverages these\ndifferences to identify ambiguity before mapping them to structured data. To\ndetect sentence-level ambiguity, we focus on the relationship between ambiguous\nquestions and their interpretations. Unlike distances calculated by dense\nembeddings, we introduce a new distance measure based on a path kernel over\nconcepts. With this measurement, we identify patterns to distinguish ambiguous\nfrom unambiguous questions. Furthermore, we propose a method for improving LLM\nperformance on ambiguous agentic tool calling through missing concept\nprediction. Both achieve state-of-the-art results."
                },
                "authors": [
                    {
                        "name": "Zhibo Hu"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Yanfeng Shu"
                    },
                    {
                        "name": "Hye-Young Paik"
                    },
                    {
                        "name": "Liming Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Liming Zhu"
                },
                "author": "Liming Zhu",
                "arxiv_comment": "17 pages, 11 figures, title updated",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11679v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11679v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17017v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17017v2",
                "updated": "2025-10-01T15:25:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    15,
                    25,
                    14,
                    2,
                    274,
                    0
                ],
                "published": "2025-04-23T18:04:38Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    18,
                    4,
                    38,
                    2,
                    113,
                    0
                ],
                "title": "Neural Theorem Proving: Generating and Structuring Proofs for Formal\n  Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Theorem Proving: Generating and Structuring Proofs for Formal\n  Verification"
                },
                "summary": "Formally verifying properties of software code has been a highly desirable\ntask, especially with the emergence of LLM-generated code. In the same vein,\nthey provide an interesting avenue for the exploration of formal verification\nand mechanistic interpretability. Since the introduction of code-specific\nmodels, despite their successes in generating code in Lean4 and Isabelle, the\ntask of generalized theorem proving still remains far from being fully solved\nand will be a benchmark for reasoning capability in LLMs. In this work, we\nintroduce a framework that generates whole proofs in a formal language to be\nused within systems that utilize the power of built-in tactics and\noff-the-shelf automated theorem provers. Our framework includes 3 components:\ngenerating natural language statements of the code to be verified, an LLM that\ngenerates formal proofs for the given statement, and a module employing\nheuristics for building the final proof. To train the LLM, we employ a 2-stage\nfine-tuning process, where we first use SFT-based training to enable the model\nto generate syntactically correct Isabelle code and then RL-based training that\nencourages the model to generate proofs verified by a theorem prover. We\nvalidate our framework using the miniF2F-test benchmark and the Isabelle proof\nassistant and design a use case to verify the correctness of the AWS S3 bucket\naccess policy code. We also curate a dataset based on the\nFVEL\\textsubscript{\\textnormal{ER}} dataset for future training tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formally verifying properties of software code has been a highly desirable\ntask, especially with the emergence of LLM-generated code. In the same vein,\nthey provide an interesting avenue for the exploration of formal verification\nand mechanistic interpretability. Since the introduction of code-specific\nmodels, despite their successes in generating code in Lean4 and Isabelle, the\ntask of generalized theorem proving still remains far from being fully solved\nand will be a benchmark for reasoning capability in LLMs. In this work, we\nintroduce a framework that generates whole proofs in a formal language to be\nused within systems that utilize the power of built-in tactics and\noff-the-shelf automated theorem provers. Our framework includes 3 components:\ngenerating natural language statements of the code to be verified, an LLM that\ngenerates formal proofs for the given statement, and a module employing\nheuristics for building the final proof. To train the LLM, we employ a 2-stage\nfine-tuning process, where we first use SFT-based training to enable the model\nto generate syntactically correct Isabelle code and then RL-based training that\nencourages the model to generate proofs verified by a theorem prover. We\nvalidate our framework using the miniF2F-test benchmark and the Isabelle proof\nassistant and design a use case to verify the correctness of the AWS S3 bucket\naccess policy code. We also curate a dataset based on the\nFVEL\\textsubscript{\\textnormal{ER}} dataset for future training tasks."
                },
                "authors": [
                    {
                        "name": "Balaji Rao"
                    },
                    {
                        "name": "William Eiers"
                    },
                    {
                        "name": "Carlo Lipizzi"
                    }
                ],
                "author_detail": {
                    "name": "Carlo Lipizzi"
                },
                "author": "Carlo Lipizzi",
                "arxiv_comment": "Accepted to the Proceedings of the 19th Conference on Neurosymbolic\n  Learning and Reasoning (NeSy 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17017v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17017v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23019v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23019v2",
                "updated": "2025-10-01T15:24:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    15,
                    24,
                    12,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-27T00:24:57Z",
                "published_parsed": [
                    2025,
                    9,
                    27,
                    0,
                    24,
                    57,
                    5,
                    270,
                    0
                ],
                "title": "LLM Watermark Evasion via Bias Inversion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Watermark Evasion via Bias Inversion"
                },
                "summary": "Watermarking for large language models (LLMs) embeds a statistical signal\nduring generation to enable detection of model-produced text. While\nwatermarking has proven effective in benign settings, its robustness under\nadversarial evasion remains contested. To advance a rigorous understanding and\nevaluation of such vulnerabilities, we propose the \\emph{Bias-Inversion\nRewriting Attack} (BIRA), which is theoretically motivated and model-agnostic.\nBIRA weakens the watermark signal by suppressing the logits of likely\nwatermarked tokens during LLM-based rewriting, without any knowledge of the\nunderlying watermarking scheme. Across recent watermarking methods, BIRA\nachieves over 99\\% evasion while preserving the semantic content of the\noriginal text. Beyond demonstrating an attack, our results reveal a systematic\nvulnerability, emphasizing the need for stress testing and robust defenses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watermarking for large language models (LLMs) embeds a statistical signal\nduring generation to enable detection of model-produced text. While\nwatermarking has proven effective in benign settings, its robustness under\nadversarial evasion remains contested. To advance a rigorous understanding and\nevaluation of such vulnerabilities, we propose the \\emph{Bias-Inversion\nRewriting Attack} (BIRA), which is theoretically motivated and model-agnostic.\nBIRA weakens the watermark signal by suppressing the logits of likely\nwatermarked tokens during LLM-based rewriting, without any knowledge of the\nunderlying watermarking scheme. Across recent watermarking methods, BIRA\nachieves over 99\\% evasion while preserving the semantic content of the\noriginal text. Beyond demonstrating an attack, our results reveal a systematic\nvulnerability, emphasizing the need for stress testing and robust defenses."
                },
                "authors": [
                    {
                        "name": "Jeongyeon Hwang"
                    },
                    {
                        "name": "Sangdon Park"
                    },
                    {
                        "name": "Jungseul Ok"
                    }
                ],
                "author_detail": {
                    "name": "Jungseul Ok"
                },
                "author": "Jungseul Ok",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23019v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23019v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09878v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09878v3",
                "updated": "2025-10-01T15:17:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    15,
                    17,
                    17,
                    2,
                    274,
                    0
                ],
                "published": "2025-06-11T15:48:29Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    15,
                    48,
                    29,
                    2,
                    162,
                    0
                ],
                "title": "Virtualizing RAN: Science, Strategy, and Architecture of\n  Software-Defined Mobile Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virtualizing RAN: Science, Strategy, and Architecture of\n  Software-Defined Mobile Networks"
                },
                "summary": "Virtualizing the Radio-Access Network (RAN) is increasingly viewed as an\nenabler of affordable 5G expansion and a stepping-stone toward AI-native 6G.\nMost discussions, however, still approach spectrum policy, cloud engineering\nand organizational practice as separate topics. This paper offers an integrated\nperspective spanning four pillars -- science, technology, business strategy and\nculture. A comparative U.S.\\ case study illustrates how mid-band contiguity,\ncomplemented by selective mmWave capacity layers, can improve both coverage and\nchurn when orchestrated through software-defined carrier aggregation. We derive\nanalytic capacity and latency bounds for Split 7.2 $\\times$ vRAN/O-RAN\ndeployments, quantify the throughput penalty of end-to-end 256-bit encryption,\nand show how GPU/FPGA off-load plus digital-twin-driven automation keeps the\nhybrid-automatic-repeat request (HARQ) round-trip within a 0.5 ms budget. When\nthese technical enablers are embedded in a physics-first delivery roadmap,\naverage vRAN cycle time drops an order of magnitude -- even in the presence of\ncultural head-winds such as dual-ladder'' erosion. Three cybernetic templates\n-- the Clock-Hierarchy Law, Ashby's Requisite Variety and a delay-cost curve --\nare then used to explain why silo-constrained automation can amplify, rather\nthan absorb, integration debt. Looking forward, silicon-paced 6G evolution\n(9-12 month node shrinks, sub-THz joint communication-and-sensing, chiplet\narchitectures and optical I/O) calls for a dual-resolution planning grid that\ncouples five-year spectrum physics with six-month silicon sprints.'' The paper\ncloses with balanced, action-oriented recommendations for operators, vendors\nand researchers on sub-THz fronthaul, AI-native security, energy-proportional\naccelerators and zero-touch assurance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virtualizing the Radio-Access Network (RAN) is increasingly viewed as an\nenabler of affordable 5G expansion and a stepping-stone toward AI-native 6G.\nMost discussions, however, still approach spectrum policy, cloud engineering\nand organizational practice as separate topics. This paper offers an integrated\nperspective spanning four pillars -- science, technology, business strategy and\nculture. A comparative U.S.\\ case study illustrates how mid-band contiguity,\ncomplemented by selective mmWave capacity layers, can improve both coverage and\nchurn when orchestrated through software-defined carrier aggregation. We derive\nanalytic capacity and latency bounds for Split 7.2 $\\times$ vRAN/O-RAN\ndeployments, quantify the throughput penalty of end-to-end 256-bit encryption,\nand show how GPU/FPGA off-load plus digital-twin-driven automation keeps the\nhybrid-automatic-repeat request (HARQ) round-trip within a 0.5 ms budget. When\nthese technical enablers are embedded in a physics-first delivery roadmap,\naverage vRAN cycle time drops an order of magnitude -- even in the presence of\ncultural head-winds such as dual-ladder'' erosion. Three cybernetic templates\n-- the Clock-Hierarchy Law, Ashby's Requisite Variety and a delay-cost curve --\nare then used to explain why silo-constrained automation can amplify, rather\nthan absorb, integration debt. Looking forward, silicon-paced 6G evolution\n(9-12 month node shrinks, sub-THz joint communication-and-sensing, chiplet\narchitectures and optical I/O) calls for a dual-resolution planning grid that\ncouples five-year spectrum physics with six-month silicon sprints.'' The paper\ncloses with balanced, action-oriented recommendations for operators, vendors\nand researchers on sub-THz fronthaul, AI-native security, energy-proportional\naccelerators and zero-touch assurance."
                },
                "authors": [
                    {
                        "name": "Ryan Barker"
                    }
                ],
                "author_detail": {
                    "name": "Ryan Barker"
                },
                "author": "Ryan Barker",
                "arxiv_comment": "12 pages, 4 figures, 8 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09878v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09878v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04018v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04018v2",
                "updated": "2025-10-01T15:15:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    15,
                    15,
                    52,
                    2,
                    274,
                    0
                ],
                "published": "2025-06-04T14:46:47Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    14,
                    46,
                    47,
                    2,
                    155,
                    0
                ],
                "title": "AgentMisalignment: Measuring the Propensity for Misaligned Behaviour in\n  LLM-Based Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentMisalignment: Measuring the Propensity for Misaligned Behaviour in\n  LLM-Based Agents"
                },
                "summary": "As Large Language Model (LLM) agents become more widespread, associated\nmisalignment risks increase. While prior research has studied agents' ability\nto produce harmful outputs or follow malicious instructions, it remains unclear\nhow likely agents are to spontaneously pursue unintended goals in realistic\ndeployments. In this work, we approach misalignment as a conflict between the\ninternal goals pursued by the model and the goals intended by its deployer. We\nintroduce a misalignment propensity benchmark, \\textsc{AgentMisalignment}, a\nbenchmark suite designed to evaluate the propensity of LLM agents to misalign\nin realistic scenarios. Evaluations cover behaviours such as avoiding\noversight, resisting shutdown, sandbagging, and power-seeking. Testing frontier\nmodels, we find that more capable agents tend to exhibit higher misalignment on\naverage. We also systematically vary agent personalities through different\nsystem prompts and observe that persona characteristics can strongly and\nunpredictably influence misalignment, sometimes more than the choice of model\nitself. Our results reveal the limitations of current alignment methods for\nautonomous LLM agents and underscore the need to rethink misalignment in\nrealistic deployment settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Model (LLM) agents become more widespread, associated\nmisalignment risks increase. While prior research has studied agents' ability\nto produce harmful outputs or follow malicious instructions, it remains unclear\nhow likely agents are to spontaneously pursue unintended goals in realistic\ndeployments. In this work, we approach misalignment as a conflict between the\ninternal goals pursued by the model and the goals intended by its deployer. We\nintroduce a misalignment propensity benchmark, \\textsc{AgentMisalignment}, a\nbenchmark suite designed to evaluate the propensity of LLM agents to misalign\nin realistic scenarios. Evaluations cover behaviours such as avoiding\noversight, resisting shutdown, sandbagging, and power-seeking. Testing frontier\nmodels, we find that more capable agents tend to exhibit higher misalignment on\naverage. We also systematically vary agent personalities through different\nsystem prompts and observe that persona characteristics can strongly and\nunpredictably influence misalignment, sometimes more than the choice of model\nitself. Our results reveal the limitations of current alignment methods for\nautonomous LLM agents and underscore the need to rethink misalignment in\nrealistic deployment settings."
                },
                "authors": [
                    {
                        "name": "Akshat Naik"
                    },
                    {
                        "name": "Patrick Quinn"
                    },
                    {
                        "name": "Guillermo Bosch"
                    },
                    {
                        "name": "Emma Gouné"
                    },
                    {
                        "name": "Francisco Javier Campos Zabala"
                    },
                    {
                        "name": "Jason Ross Brown"
                    },
                    {
                        "name": "Edward James Young"
                    }
                ],
                "author_detail": {
                    "name": "Edward James Young"
                },
                "author": "Edward James Young",
                "arxiv_comment": "Prepint, under review for NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04018v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04018v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.11; K.4.1; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07493v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07493v3",
                "updated": "2025-10-01T14:37:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    14,
                    37,
                    49,
                    2,
                    274,
                    0
                ],
                "published": "2024-12-10T13:18:45Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    13,
                    18,
                    45,
                    1,
                    345,
                    0
                ],
                "title": "LLM-guided Task and Motion Planning using Knowledge-based Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-guided Task and Motion Planning using Knowledge-based Reasoning"
                },
                "summary": "Performing complex manipulation tasks in dynamic environments requires\nefficient Task and Motion Planning (TAMP) approaches that combine high-level\nsymbolic plans with low-level motion control. Advances in Large Language Models\n(LLMs), such as GPT-4, are transforming task planning by offering natural\nlanguage as an intuitive and flexible way to describe tasks, generate symbolic\nplans, and reason. However, the effectiveness of LLM-based TAMP approaches is\nlimited due to static and template-based prompting, which limits adaptability\nto dynamic environments and complex task contexts. To address these\nlimitations, this work proposes a novel Onto-LLM-TAMP framework that employs\nknowledge-based reasoning to refine and expand user prompts with\ntask-contextual reasoning and knowledge-based environment state descriptions.\nIntegrating domain-specific knowledge into the prompt ensures semantically\naccurate and context-aware task plans. The proposed framework demonstrates its\neffectiveness by resolving semantic errors in symbolic plan generation, such as\nmaintaining logical temporal goal ordering in scenarios involving hierarchical\nobject placement. The proposed framework is validated through both simulation\nand real-world scenarios, demonstrating significant improvements over the\nbaseline approach in terms of adaptability to dynamic environments and the\ngeneration of semantically correct task plans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performing complex manipulation tasks in dynamic environments requires\nefficient Task and Motion Planning (TAMP) approaches that combine high-level\nsymbolic plans with low-level motion control. Advances in Large Language Models\n(LLMs), such as GPT-4, are transforming task planning by offering natural\nlanguage as an intuitive and flexible way to describe tasks, generate symbolic\nplans, and reason. However, the effectiveness of LLM-based TAMP approaches is\nlimited due to static and template-based prompting, which limits adaptability\nto dynamic environments and complex task contexts. To address these\nlimitations, this work proposes a novel Onto-LLM-TAMP framework that employs\nknowledge-based reasoning to refine and expand user prompts with\ntask-contextual reasoning and knowledge-based environment state descriptions.\nIntegrating domain-specific knowledge into the prompt ensures semantically\naccurate and context-aware task plans. The proposed framework demonstrates its\neffectiveness by resolving semantic errors in symbolic plan generation, such as\nmaintaining logical temporal goal ordering in scenarios involving hierarchical\nobject placement. The proposed framework is validated through both simulation\nand real-world scenarios, demonstrating significant improvements over the\nbaseline approach in terms of adaptability to dynamic environments and the\ngeneration of semantically correct task plans."
                },
                "authors": [
                    {
                        "name": "Muhayy Ud Din"
                    },
                    {
                        "name": "Jan Rosell"
                    },
                    {
                        "name": "Waseem Akram"
                    },
                    {
                        "name": "Isiah Zaplana"
                    },
                    {
                        "name": "Maximo A Roa"
                    },
                    {
                        "name": "Irfan Hussain"
                    }
                ],
                "author_detail": {
                    "name": "Irfan Hussain"
                },
                "author": "Irfan Hussain",
                "arxiv_comment": "Submitted to knowledge based systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07493v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07493v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22255v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22255v3",
                "updated": "2025-10-02T11:37:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    11,
                    37,
                    39,
                    3,
                    275,
                    0
                ],
                "published": "2025-09-26T12:19:22Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    12,
                    19,
                    22,
                    4,
                    269,
                    0
                ],
                "title": "Evaluating LLMs for Combinatorial Optimization: One-Phase and Two-Phase\n  Heuristics for 2D Bin-Packing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating LLMs for Combinatorial Optimization: One-Phase and Two-Phase\n  Heuristics for 2D Bin-Packing"
                },
                "summary": "This paper presents an evaluation framework for assessing Large Language\nModels' (LLMs) capabilities in combinatorial optimization, specifically\naddressing the 2D bin-packing problem. We introduce a systematic methodology\nthat combines LLMs with evolutionary algorithms to generate and refine\nheuristic solutions iteratively. Through comprehensive experiments comparing\nLLM generated heuristics against traditional approaches (Finite First-Fit and\nHybrid First-Fit), we demonstrate that LLMs can produce more efficient\nsolutions while requiring fewer computational resources. Our evaluation reveals\nthat GPT-4o achieves optimal solutions within two iterations, reducing average\nbin usage from 16 to 15 bins while improving space utilization from 0.76-0.78\nto 0.83. This work contributes to understanding LLM evaluation in specialized\ndomains and establishes benchmarks for assessing LLM performance in\ncombinatorial optimization tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an evaluation framework for assessing Large Language\nModels' (LLMs) capabilities in combinatorial optimization, specifically\naddressing the 2D bin-packing problem. We introduce a systematic methodology\nthat combines LLMs with evolutionary algorithms to generate and refine\nheuristic solutions iteratively. Through comprehensive experiments comparing\nLLM generated heuristics against traditional approaches (Finite First-Fit and\nHybrid First-Fit), we demonstrate that LLMs can produce more efficient\nsolutions while requiring fewer computational resources. Our evaluation reveals\nthat GPT-4o achieves optimal solutions within two iterations, reducing average\nbin usage from 16 to 15 bins while improving space utilization from 0.76-0.78\nto 0.83. This work contributes to understanding LLM evaluation in specialized\ndomains and establishes benchmarks for assessing LLM performance in\ncombinatorial optimization tasks."
                },
                "authors": [
                    {
                        "name": "Syed Mahbubul Huq"
                    },
                    {
                        "name": "Daniel Brito"
                    },
                    {
                        "name": "Daniel Sikar"
                    },
                    {
                        "name": "Chris Child"
                    },
                    {
                        "name": "Tillman Weyde"
                    },
                    {
                        "name": "Rajesh Mojumder"
                    }
                ],
                "author_detail": {
                    "name": "Rajesh Mojumder"
                },
                "author": "Rajesh Mojumder",
                "arxiv_comment": "1 table, 6 figures. 39th Conference on Neural Information Processing\n  Systems (NeurIPS 2025) Accepted for the Workshop: Evaluating the Evolving LLM\n  Lifecycle Benchmarks, Emergent Abilities, and Scaling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22255v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22255v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17621v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17621v2",
                "updated": "2025-10-01T14:31:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    14,
                    31,
                    5,
                    2,
                    274,
                    0
                ],
                "published": "2025-08-25T03:01:30Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    3,
                    1,
                    30,
                    0,
                    237,
                    0
                ],
                "title": "Steering When Necessary: Flexible Steering Large Language Models with\n  Backtracking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steering When Necessary: Flexible Steering Large Language Models with\n  Backtracking"
                },
                "summary": "Large language models (LLMs) have achieved remarkable performance across many\ngeneration tasks. Nevertheless, effectively aligning them with desired\nbehaviors remains a significant challenge. Activation steering is an effective\nand cost-efficient approach that directly modifies the activations of LLMs\nduring the inference stage, aligning their responses with the desired behaviors\nand avoiding the high cost of fine-tuning. Existing methods typically\nindiscriminately intervene to all generations or rely solely on the question to\ndetermine intervention, which limits the accurate assessment of the\nintervention strength. To this end, we propose the Flexible Activation Steering\nwith Backtracking (FASB) framework, which dynamically determines both the\nnecessity and strength of intervention by tracking the internal states of the\nLLMs during generation, considering both the question and the generated\ncontent. Since intervening after detecting a deviation from the desired\nbehavior is often too late, we further propose the backtracking mechanism to\ncorrect the deviated tokens and steer the LLMs toward the desired behavior.\nExtensive experiments on the TruthfulQA dataset and six multiple-choice\ndatasets demonstrate that our method outperforms baselines. Our code will be\nreleased at https://github.com/gjw185/FASB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable performance across many\ngeneration tasks. Nevertheless, effectively aligning them with desired\nbehaviors remains a significant challenge. Activation steering is an effective\nand cost-efficient approach that directly modifies the activations of LLMs\nduring the inference stage, aligning their responses with the desired behaviors\nand avoiding the high cost of fine-tuning. Existing methods typically\nindiscriminately intervene to all generations or rely solely on the question to\ndetermine intervention, which limits the accurate assessment of the\nintervention strength. To this end, we propose the Flexible Activation Steering\nwith Backtracking (FASB) framework, which dynamically determines both the\nnecessity and strength of intervention by tracking the internal states of the\nLLMs during generation, considering both the question and the generated\ncontent. Since intervening after detecting a deviation from the desired\nbehavior is often too late, we further propose the backtracking mechanism to\ncorrect the deviated tokens and steer the LLMs toward the desired behavior.\nExtensive experiments on the TruthfulQA dataset and six multiple-choice\ndatasets demonstrate that our method outperforms baselines. Our code will be\nreleased at https://github.com/gjw185/FASB."
                },
                "authors": [
                    {
                        "name": "Zifeng Cheng"
                    },
                    {
                        "name": "Jinwei Gan"
                    },
                    {
                        "name": "Zhiwei Jiang"
                    },
                    {
                        "name": "Cong Wang"
                    },
                    {
                        "name": "Yafeng Yin"
                    },
                    {
                        "name": "Xiang Luo"
                    },
                    {
                        "name": "Yuchen Fu"
                    },
                    {
                        "name": "Qing Gu"
                    }
                ],
                "author_detail": {
                    "name": "Qing Gu"
                },
                "author": "Qing Gu",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17621v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17621v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24866v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24866v2",
                "updated": "2025-10-01T14:06:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    14,
                    6,
                    17,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-29T14:50:18Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    50,
                    18,
                    0,
                    272,
                    0
                ],
                "title": "Metaphor identification using large language models: A comparison of\n  RAG, prompt engineering, and fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Metaphor identification using large language models: A comparison of\n  RAG, prompt engineering, and fine-tuning"
                },
                "summary": "Metaphor is a pervasive feature of discourse and a powerful lens for\nexamining cognition, emotion, and ideology. Large-scale analysis, however, has\nbeen constrained by the need for manual annotation due to the context-sensitive\nnature of metaphor. This study investigates the potential of large language\nmodels (LLMs) to automate metaphor identification in full texts. We compare\nthree methods: (i) retrieval-augmented generation (RAG), where the model is\nprovided with a codebook and instructed to annotate texts based on its rules\nand examples; (ii) prompt engineering, where we design task-specific verbal\ninstructions; and (iii) fine-tuning, where the model is trained on hand-coded\ntexts to optimize performance. Within prompt engineering, we test zero-shot,\nfew-shot, and chain-of-thought strategies. Our results show that\nstate-of-the-art closed-source LLMs can achieve high accuracy, with fine-tuning\nyielding a median F1 score of 0.79. A comparison of human and LLM outputs\nreveals that most discrepancies are systematic, reflecting well-known grey\nareas and conceptual challenges in metaphor theory. We propose that LLMs can be\nused to at least partly automate metaphor identification and can serve as a\ntestbed for developing and refining metaphor identification protocols and the\ntheory that underpins them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Metaphor is a pervasive feature of discourse and a powerful lens for\nexamining cognition, emotion, and ideology. Large-scale analysis, however, has\nbeen constrained by the need for manual annotation due to the context-sensitive\nnature of metaphor. This study investigates the potential of large language\nmodels (LLMs) to automate metaphor identification in full texts. We compare\nthree methods: (i) retrieval-augmented generation (RAG), where the model is\nprovided with a codebook and instructed to annotate texts based on its rules\nand examples; (ii) prompt engineering, where we design task-specific verbal\ninstructions; and (iii) fine-tuning, where the model is trained on hand-coded\ntexts to optimize performance. Within prompt engineering, we test zero-shot,\nfew-shot, and chain-of-thought strategies. Our results show that\nstate-of-the-art closed-source LLMs can achieve high accuracy, with fine-tuning\nyielding a median F1 score of 0.79. A comparison of human and LLM outputs\nreveals that most discrepancies are systematic, reflecting well-known grey\nareas and conceptual challenges in metaphor theory. We propose that LLMs can be\nused to at least partly automate metaphor identification and can serve as a\ntestbed for developing and refining metaphor identification protocols and the\ntheory that underpins them."
                },
                "authors": [
                    {
                        "name": "Matteo Fuoli"
                    },
                    {
                        "name": "Weihang Huang"
                    },
                    {
                        "name": "Jeannette Littlemore"
                    },
                    {
                        "name": "Sarah Turner"
                    },
                    {
                        "name": "Ellen Wilding"
                    }
                ],
                "author_detail": {
                    "name": "Ellen Wilding"
                },
                "author": "Ellen Wilding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24866v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24866v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25448v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25448v2",
                "updated": "2025-10-01T14:04:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    14,
                    4,
                    38,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-29T19:54:36Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    19,
                    54,
                    36,
                    0,
                    272,
                    0
                ],
                "title": "Fingerprinting LLMs via Prompt Injection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fingerprinting LLMs via Prompt Injection"
                },
                "summary": "Large language models (LLMs) are often modified after release through\npost-processing such as post-training or quantization, which makes it\nchallenging to determine whether one model is derived from another. Existing\nprovenance detection methods have two main limitations: (1) they embed signals\ninto the base model before release, which is infeasible for already published\nmodels, or (2) they compare outputs across models using hand-crafted or random\nprompts, which are not robust to post-processing. In this work, we propose\nLLMPrint, a novel detection framework that constructs fingerprints by\nexploiting LLMs' inherent vulnerability to prompt injection. Our key insight is\nthat by optimizing fingerprint prompts to enforce consistent token preferences,\nwe can obtain fingerprints that are both unique to the base model and robust to\npost-processing. We further develop a unified verification procedure that\napplies to both gray-box and black-box settings, with statistical guarantees.\nWe evaluate LLMPrint on five base models and around 700 post-trained or\nquantized variants. Our results show that LLMPrint achieves high true positive\nrates while keeping false positive rates near zero.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are often modified after release through\npost-processing such as post-training or quantization, which makes it\nchallenging to determine whether one model is derived from another. Existing\nprovenance detection methods have two main limitations: (1) they embed signals\ninto the base model before release, which is infeasible for already published\nmodels, or (2) they compare outputs across models using hand-crafted or random\nprompts, which are not robust to post-processing. In this work, we propose\nLLMPrint, a novel detection framework that constructs fingerprints by\nexploiting LLMs' inherent vulnerability to prompt injection. Our key insight is\nthat by optimizing fingerprint prompts to enforce consistent token preferences,\nwe can obtain fingerprints that are both unique to the base model and robust to\npost-processing. We further develop a unified verification procedure that\napplies to both gray-box and black-box settings, with statistical guarantees.\nWe evaluate LLMPrint on five base models and around 700 post-trained or\nquantized variants. Our results show that LLMPrint achieves high true positive\nrates while keeping false positive rates near zero."
                },
                "authors": [
                    {
                        "name": "Yuepeng Hu"
                    },
                    {
                        "name": "Zhengyuan Jiang"
                    },
                    {
                        "name": "Mengyuan Li"
                    },
                    {
                        "name": "Osama Ahmed"
                    },
                    {
                        "name": "Zhicong Huang"
                    },
                    {
                        "name": "Cheng Hong"
                    },
                    {
                        "name": "Neil Gong"
                    }
                ],
                "author_detail": {
                    "name": "Neil Gong"
                },
                "author": "Neil Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25448v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25448v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09199v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09199v2",
                "updated": "2025-10-01T14:01:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    14,
                    1,
                    30,
                    2,
                    274,
                    0
                ],
                "published": "2025-07-12T08:42:10Z",
                "published_parsed": [
                    2025,
                    7,
                    12,
                    8,
                    42,
                    10,
                    5,
                    193,
                    0
                ],
                "title": "Back to the Basics: Rethinking Issue-Commit Linking with LLM-Assisted\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Back to the Basics: Rethinking Issue-Commit Linking with LLM-Assisted\n  Retrieval"
                },
                "summary": "Issue-commit linking, which connects issues with commits that fix them, is\ncrucial for software maintenance. Existing approaches have shown promise in\nautomatically recovering these links. Evaluations of these techniques assess\ntheir ability to identify genuine links from plausible but false links.\nHowever, these evaluations overlook the fact that, in reality, when a\nrepository has more commits, the presence of more plausible yet unrelated\ncommits may interfere with the tool in differentiating the correct fix commits.\nTo address this, we propose the Realistic Distribution Setting (RDS) and use it\nto construct a more realistic evaluation dataset that includes 20 open-source\nprojects. By evaluating tools on this dataset, we observe that the performance\nof the state-of-the-art deep learning-based approach drops by more than half,\nwhile the traditional Information Retrieval method, VSM, outperforms it.\nInspired by these observations, we propose EasyLink, which utilizes a vector\ndatabase as a modern Information Retrieval technique. To address the\nlong-standing problem of the semantic gap between issues and commits, EasyLink\nleverages a large language model to rerank the commits retrieved from the\ndatabase. Under our evaluation, EasyLink achieves an average Precision@1 of\n75.03\\%, improving over the state-of-the-art by over four times. Additionally,\nthis paper provides practical guidelines for advancing research in issue-commit\nlink recovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Issue-commit linking, which connects issues with commits that fix them, is\ncrucial for software maintenance. Existing approaches have shown promise in\nautomatically recovering these links. Evaluations of these techniques assess\ntheir ability to identify genuine links from plausible but false links.\nHowever, these evaluations overlook the fact that, in reality, when a\nrepository has more commits, the presence of more plausible yet unrelated\ncommits may interfere with the tool in differentiating the correct fix commits.\nTo address this, we propose the Realistic Distribution Setting (RDS) and use it\nto construct a more realistic evaluation dataset that includes 20 open-source\nprojects. By evaluating tools on this dataset, we observe that the performance\nof the state-of-the-art deep learning-based approach drops by more than half,\nwhile the traditional Information Retrieval method, VSM, outperforms it.\nInspired by these observations, we propose EasyLink, which utilizes a vector\ndatabase as a modern Information Retrieval technique. To address the\nlong-standing problem of the semantic gap between issues and commits, EasyLink\nleverages a large language model to rerank the commits retrieved from the\ndatabase. Under our evaluation, EasyLink achieves an average Precision@1 of\n75.03\\%, improving over the state-of-the-art by over four times. Additionally,\nthis paper provides practical guidelines for advancing research in issue-commit\nlink recovery."
                },
                "authors": [
                    {
                        "name": "Huihui Huang"
                    },
                    {
                        "name": "Ratnadira Widyasari"
                    },
                    {
                        "name": "Ting Zhang"
                    },
                    {
                        "name": "Ivana Clairine Irsan"
                    },
                    {
                        "name": "Jieke Shi"
                    },
                    {
                        "name": "Han Wei Ang"
                    },
                    {
                        "name": "Frank Liauw"
                    },
                    {
                        "name": "Eng Lieh Ouh"
                    },
                    {
                        "name": "Lwin Khin Shar"
                    },
                    {
                        "name": "Hong Jin Kang"
                    },
                    {
                        "name": "David Lo"
                    }
                ],
                "author_detail": {
                    "name": "David Lo"
                },
                "author": "David Lo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09199v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09199v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22319v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22319v2",
                "updated": "2025-10-01T13:53:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    13,
                    53,
                    12,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-26T13:19:32Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    13,
                    19,
                    32,
                    4,
                    269,
                    0
                ],
                "title": "Progressive Weight Loading: Accelerating Initial Inference and Gradually\n  Boosting Performance on Resource-Constrained Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Progressive Weight Loading: Accelerating Initial Inference and Gradually\n  Boosting Performance on Resource-Constrained Environments"
                },
                "summary": "Deep learning models have become increasingly large and complex, resulting in\nhigher memory consumption and computational demands. Consequently, model\nloading times and initial inference latency have increased, posing significant\nchallenges in mobile and latency-sensitive environments where frequent model\nloading and unloading are required, which directly impacts user experience.\nWhile Knowledge Distillation (KD) offers a solution by compressing large\nteacher models into smaller student ones, it often comes at the cost of reduced\nperformance. To address this trade-off, we propose Progressive Weight Loading\n(PWL), a novel technique that enables fast initial inference by first deploying\na lightweight student model, then incrementally replacing its layers with those\nof a pre-trained teacher model. To support seamless layer substitution, we\nintroduce a training method that not only aligns intermediate feature\nrepresentations between student and teacher layers, but also improves the\noverall output performance of the student model. Our experiments on VGG,\nResNet, and ViT architectures demonstrate that models trained with PWL maintain\ncompetitive distillation performance and gradually improve accuracy as teacher\nlayers are loaded-matching the final accuracy of the full teacher model without\ncompromising initial inference speed. This makes PWL particularly suited for\ndynamic, resource-constrained deployments where both responsiveness and\nperformance are critical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning models have become increasingly large and complex, resulting in\nhigher memory consumption and computational demands. Consequently, model\nloading times and initial inference latency have increased, posing significant\nchallenges in mobile and latency-sensitive environments where frequent model\nloading and unloading are required, which directly impacts user experience.\nWhile Knowledge Distillation (KD) offers a solution by compressing large\nteacher models into smaller student ones, it often comes at the cost of reduced\nperformance. To address this trade-off, we propose Progressive Weight Loading\n(PWL), a novel technique that enables fast initial inference by first deploying\na lightweight student model, then incrementally replacing its layers with those\nof a pre-trained teacher model. To support seamless layer substitution, we\nintroduce a training method that not only aligns intermediate feature\nrepresentations between student and teacher layers, but also improves the\noverall output performance of the student model. Our experiments on VGG,\nResNet, and ViT architectures demonstrate that models trained with PWL maintain\ncompetitive distillation performance and gradually improve accuracy as teacher\nlayers are loaded-matching the final accuracy of the full teacher model without\ncompromising initial inference speed. This makes PWL particularly suited for\ndynamic, resource-constrained deployments where both responsiveness and\nperformance are critical."
                },
                "authors": [
                    {
                        "name": "Hyunwoo Kim"
                    },
                    {
                        "name": "Junha Lee"
                    },
                    {
                        "name": "Mincheol Choi"
                    },
                    {
                        "name": "Jeonghwan Lee"
                    },
                    {
                        "name": "Jaeshin Cho"
                    }
                ],
                "author_detail": {
                    "name": "Jaeshin Cho"
                },
                "author": "Jaeshin Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22319v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22319v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23097v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23097v2",
                "updated": "2025-10-01T13:30:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    13,
                    30,
                    22,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-27T04:11:53Z",
                "published_parsed": [
                    2025,
                    9,
                    27,
                    4,
                    11,
                    53,
                    5,
                    270,
                    0
                ],
                "title": "Streamline pathology foundation model by cross-magnification\n  distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streamline pathology foundation model by cross-magnification\n  distillation"
                },
                "summary": "Foundation models (FM) have transformed computational pathology but remain\ncomputationally prohibitive for clinical deployment due to their massive\nparameter counts and high-magnification processing requirements. Here, we\nintroduce XMAG, a lightweight FM developed through corss-magnification\ndistillation that transfers knowledge from state-of-the-art 20x magnification\nteacher to an efficient 5x magnification student architecture. XMAG employs a\ncompact backbone and operates entirely at 5x, requiring 11.3 times fewer\npatches per whole slide image (WSI) compared to existing approaches. Our Novel\ndistillation framework incorporates dual-level knowledge transfer, aligning\nboth global image representations and local spatial token mapping. We trained\nXMAG on 3.49 million images curated from publicly available datasets and\nevaluated performance across six clinically relevant histopathology analysis\ntasks spanning multiple cancer types. XMAG achieved diagnostic accuracy within\n1% of substantially larger foundation models while delivering 30-fold\nprocessing acceleration, reaching 8.8 WSIs per minute processing speed. Our\ncross-institutional validation confirmed robust generalization. Further, we\ndeveloped an end-to-end training strategy to further boost our model's\nperformance to approach the larger FMs' performance. These results establish\ncross-magnification distillation as a viable approach for deploying FM\ncapabilities in resource-constrained clinical environments, potentially\nenabling real-time pathology AI integration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation models (FM) have transformed computational pathology but remain\ncomputationally prohibitive for clinical deployment due to their massive\nparameter counts and high-magnification processing requirements. Here, we\nintroduce XMAG, a lightweight FM developed through corss-magnification\ndistillation that transfers knowledge from state-of-the-art 20x magnification\nteacher to an efficient 5x magnification student architecture. XMAG employs a\ncompact backbone and operates entirely at 5x, requiring 11.3 times fewer\npatches per whole slide image (WSI) compared to existing approaches. Our Novel\ndistillation framework incorporates dual-level knowledge transfer, aligning\nboth global image representations and local spatial token mapping. We trained\nXMAG on 3.49 million images curated from publicly available datasets and\nevaluated performance across six clinically relevant histopathology analysis\ntasks spanning multiple cancer types. XMAG achieved diagnostic accuracy within\n1% of substantially larger foundation models while delivering 30-fold\nprocessing acceleration, reaching 8.8 WSIs per minute processing speed. Our\ncross-institutional validation confirmed robust generalization. Further, we\ndeveloped an end-to-end training strategy to further boost our model's\nperformance to approach the larger FMs' performance. These results establish\ncross-magnification distillation as a viable approach for deploying FM\ncapabilities in resource-constrained clinical environments, potentially\nenabling real-time pathology AI integration."
                },
                "authors": [
                    {
                        "name": "Ziyu Su"
                    },
                    {
                        "name": "Abdul Rehman Akbar"
                    },
                    {
                        "name": "Usama Sajjad"
                    },
                    {
                        "name": "Anil V. Parwani"
                    },
                    {
                        "name": "Muhammad Khalid Khan Niazi"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Khalid Khan Niazi"
                },
                "author": "Muhammad Khalid Khan Niazi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23097v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23097v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18553v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18553v2",
                "updated": "2025-10-01T13:25:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    13,
                    25,
                    12,
                    2,
                    274,
                    0
                ],
                "published": "2025-07-24T16:22:18Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    22,
                    18,
                    3,
                    205,
                    0
                ],
                "title": "The Geometry of LLM Quantization: GPTQ as Babai's Nearest Plane\n  Algorithm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Geometry of LLM Quantization: GPTQ as Babai's Nearest Plane\n  Algorithm"
                },
                "summary": "Quantizing the weights of large language models (LLMs) from 16-bit to lower\nbitwidth is the de facto approach to deploy massive transformers onto more\naffordable accelerators. While GPTQ emerged as one of the standard methods for\none-shot post-training quantization at LLM scale, its inner workings are\ndescribed as a sequence of ad-hoc algebraic updates that obscure geometric\nmeaning or worst-case guarantees. In this work, we show that, when executed\nback-to-front (from the last to first dimension) for a linear layer, GPTQ is\nmathematically identical to Babai's nearest plane algorithm for the classical\nclosest vector problem (CVP) on a lattice defined by the Hessian matrix of the\nlayer's inputs. This equivalence is based on a sophisticated mathematical\nargument, and has two analytical consequences: first, the GPTQ error\npropagation step gains an intuitive geometric interpretation; second, GPTQ\ninherits the error upper bound of Babai's algorithm under the assumption that\nno weights are clipped. Leveraging this bound, we design post-training\nquantization methods that avoid clipping, and outperform the original GPTQ. In\naddition, we provide efficient GPU inference kernels for the resulting\nrepresentation. Taken together, these results place GPTQ on a firm theoretical\nfooting and open the door to importing decades of progress in lattice\nalgorithms towards the design of future quantization algorithms for\nbillion-parameter models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantizing the weights of large language models (LLMs) from 16-bit to lower\nbitwidth is the de facto approach to deploy massive transformers onto more\naffordable accelerators. While GPTQ emerged as one of the standard methods for\none-shot post-training quantization at LLM scale, its inner workings are\ndescribed as a sequence of ad-hoc algebraic updates that obscure geometric\nmeaning or worst-case guarantees. In this work, we show that, when executed\nback-to-front (from the last to first dimension) for a linear layer, GPTQ is\nmathematically identical to Babai's nearest plane algorithm for the classical\nclosest vector problem (CVP) on a lattice defined by the Hessian matrix of the\nlayer's inputs. This equivalence is based on a sophisticated mathematical\nargument, and has two analytical consequences: first, the GPTQ error\npropagation step gains an intuitive geometric interpretation; second, GPTQ\ninherits the error upper bound of Babai's algorithm under the assumption that\nno weights are clipped. Leveraging this bound, we design post-training\nquantization methods that avoid clipping, and outperform the original GPTQ. In\naddition, we provide efficient GPU inference kernels for the resulting\nrepresentation. Taken together, these results place GPTQ on a firm theoretical\nfooting and open the door to importing decades of progress in lattice\nalgorithms towards the design of future quantization algorithms for\nbillion-parameter models."
                },
                "authors": [
                    {
                        "name": "Jiale Chen"
                    },
                    {
                        "name": "Yalda Shabanzadeh"
                    },
                    {
                        "name": "Elvir Crnčević"
                    },
                    {
                        "name": "Torsten Hoefler"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18553v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18553v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05724v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05724v2",
                "updated": "2025-10-01T13:23:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    13,
                    23,
                    12,
                    2,
                    274,
                    0
                ],
                "published": "2025-02-17T19:05:55Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    19,
                    5,
                    55,
                    0,
                    48,
                    0
                ],
                "title": "Addressing Moral Uncertainty using Large Language Models for Ethical\n  Decision-Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing Moral Uncertainty using Large Language Models for Ethical\n  Decision-Making"
                },
                "summary": "We present an ethical decision-making framework that refines a pre-trained\nreinforcement learning (RL) model using a task-agnostic ethical layer.\nFollowing initial training, the RL model undergoes ethical fine-tuning, where\nhuman feedback is replaced by feedback generated from a large language model\n(LLM). The LLM embodies consequentialist, deontological, virtue, social\njustice, and care ethics as moral principles to assign belief values to\nrecommended actions during ethical decision-making. An ethical layer aggregates\nbelief scores from multiple LLM-derived moral perspectives using Belief\nJensen-Shannon Divergence and Dempster-Shafer Theory into probability scores\nthat also serve as the shaping reward, steering the agent toward choices that\nalign with a balanced ethical framework. This integrated learning framework\nhelps the RL agent navigate moral uncertainty in complex environments and\nenables it to make morally sound decisions across diverse tasks. Our approach,\ntested across different LLM variants and compared with other belief aggregation\ntechniques, demonstrates improved consistency, adaptability, and reduced\nreliance on handcrafted ethical rewards. This method is especially effective in\ndynamic scenarios where ethical challenges arise unexpectedly, making it\nwell-suited for real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an ethical decision-making framework that refines a pre-trained\nreinforcement learning (RL) model using a task-agnostic ethical layer.\nFollowing initial training, the RL model undergoes ethical fine-tuning, where\nhuman feedback is replaced by feedback generated from a large language model\n(LLM). The LLM embodies consequentialist, deontological, virtue, social\njustice, and care ethics as moral principles to assign belief values to\nrecommended actions during ethical decision-making. An ethical layer aggregates\nbelief scores from multiple LLM-derived moral perspectives using Belief\nJensen-Shannon Divergence and Dempster-Shafer Theory into probability scores\nthat also serve as the shaping reward, steering the agent toward choices that\nalign with a balanced ethical framework. This integrated learning framework\nhelps the RL agent navigate moral uncertainty in complex environments and\nenables it to make morally sound decisions across diverse tasks. Our approach,\ntested across different LLM variants and compared with other belief aggregation\ntechniques, demonstrates improved consistency, adaptability, and reduced\nreliance on handcrafted ethical rewards. This method is especially effective in\ndynamic scenarios where ethical challenges arise unexpectedly, making it\nwell-suited for real-world applications."
                },
                "authors": [
                    {
                        "name": "Rohit K. Dubey"
                    },
                    {
                        "name": "Damian Dailisan"
                    },
                    {
                        "name": "Sachit Mahajan"
                    }
                ],
                "author_detail": {
                    "name": "Sachit Mahajan"
                },
                "author": "Sachit Mahajan",
                "arxiv_comment": "13 pages, 5 figures. All authors contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05724v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05724v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26184v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26184v2",
                "updated": "2025-10-01T13:05:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    13,
                    5,
                    17,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-30T12:41:11Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    12,
                    41,
                    11,
                    1,
                    273,
                    0
                ],
                "title": "Auto-ARGUE: LLM-Based Report Generation Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-ARGUE: LLM-Based Report Generation Evaluation"
                },
                "summary": "Generation of long-form, citation-backed reports is a primary use case for\nretrieval augmented generation (RAG) systems. While open-source evaluation\ntools exist for various RAG tasks, ones tailored to report generation are\nlacking. Accordingly, we introduce Auto-ARGUE, a robust LLM-based\nimplementation of the recent ARGUE framework for report generation evaluation.\nWe present analysis of Auto-ARGUE on the report generation pilot task from the\nTREC 2024 NeuCLIR track, showing good system-level correlations with human\njudgments. We further release a web app for visualization of Auto-ARGUE\noutputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generation of long-form, citation-backed reports is a primary use case for\nretrieval augmented generation (RAG) systems. While open-source evaluation\ntools exist for various RAG tasks, ones tailored to report generation are\nlacking. Accordingly, we introduce Auto-ARGUE, a robust LLM-based\nimplementation of the recent ARGUE framework for report generation evaluation.\nWe present analysis of Auto-ARGUE on the report generation pilot task from the\nTREC 2024 NeuCLIR track, showing good system-level correlations with human\njudgments. We further release a web app for visualization of Auto-ARGUE\noutputs."
                },
                "authors": [
                    {
                        "name": "William Walden"
                    },
                    {
                        "name": "Marc Mason"
                    },
                    {
                        "name": "Orion Weller"
                    },
                    {
                        "name": "Laura Dietz"
                    },
                    {
                        "name": "Hannah Recknor"
                    },
                    {
                        "name": "Bryan Li"
                    },
                    {
                        "name": "Gabrielle Kaili-May Liu"
                    },
                    {
                        "name": "Yu Hou"
                    },
                    {
                        "name": "James Mayfield"
                    },
                    {
                        "name": "Eugene Yang"
                    }
                ],
                "author_detail": {
                    "name": "Eugene Yang"
                },
                "author": "Eugene Yang",
                "arxiv_comment": "ECIR 2025 demo format",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26184v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26184v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26368v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26368v2",
                "updated": "2025-10-01T13:02:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    13,
                    2,
                    51,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-30T15:04:24Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    4,
                    24,
                    1,
                    273,
                    0
                ],
                "title": "Introducing Large Language Models into the Design Flow of Time Sensitive\n  Networking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Introducing Large Language Models into the Design Flow of Time Sensitive\n  Networking"
                },
                "summary": "The growing demand for real-time, safety-critical systems has significantly\nincreased both the adoption and complexity of Time Sensitive Networking (TSN).\nConfiguring an optimized TSN network is highly challenging, requiring careful\nplanning, design, verification, validation, and deployment. Large Language\nModels (LLMs) have recently demonstrated strong capabilities in solving complex\ntasks, positioning them as promising candidates for automating end-to-end TSN\ndeployment, referred to as TSN orchestration. This paper outlines the steps\ninvolved in TSN orchestration and the associated challenges. To assess the\ncapabilities of existing LLM models, we conduct an initial proof-of-concept\ncase study focused on TSN configuration across multiple models. Building on\nthese insights, we propose an LLM-assisted orchestration framework. Unlike\nprior research on LLMs in computer networks, which has concentrated on general\nconfiguration and management, TSN-specific orchestration has not yet been\ninvestigated. We present the building blocks for automating TSN using LLMs,\ndescribe the proposed pipeline, and analyze opportunities and limitations for\nreal-world deployment. Finally, we highlight key challenges and research\ndirections, including the development of TSN-focused datasets, standardized\nbenchmark suites, and the integration of external tools such as Network\nCalculus (NC) engines and simulators. This work provides the first roadmap\ntoward assessing the feasibility of LLM-assisted TSN orchestration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing demand for real-time, safety-critical systems has significantly\nincreased both the adoption and complexity of Time Sensitive Networking (TSN).\nConfiguring an optimized TSN network is highly challenging, requiring careful\nplanning, design, verification, validation, and deployment. Large Language\nModels (LLMs) have recently demonstrated strong capabilities in solving complex\ntasks, positioning them as promising candidates for automating end-to-end TSN\ndeployment, referred to as TSN orchestration. This paper outlines the steps\ninvolved in TSN orchestration and the associated challenges. To assess the\ncapabilities of existing LLM models, we conduct an initial proof-of-concept\ncase study focused on TSN configuration across multiple models. Building on\nthese insights, we propose an LLM-assisted orchestration framework. Unlike\nprior research on LLMs in computer networks, which has concentrated on general\nconfiguration and management, TSN-specific orchestration has not yet been\ninvestigated. We present the building blocks for automating TSN using LLMs,\ndescribe the proposed pipeline, and analyze opportunities and limitations for\nreal-world deployment. Finally, we highlight key challenges and research\ndirections, including the development of TSN-focused datasets, standardized\nbenchmark suites, and the integration of external tools such as Network\nCalculus (NC) engines and simulators. This work provides the first roadmap\ntoward assessing the feasibility of LLM-assisted TSN orchestration."
                },
                "authors": [
                    {
                        "name": "Rubi Debnath"
                    },
                    {
                        "name": "Luxi Zhao"
                    },
                    {
                        "name": "Mohammadreza Barzegaran"
                    },
                    {
                        "name": "Sebastian Steinhorst"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Steinhorst"
                },
                "author": "Sebastian Steinhorst",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26368v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26368v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17052v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17052v4",
                "updated": "2025-10-01T12:49:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    12,
                    49,
                    54,
                    2,
                    274,
                    0
                ],
                "published": "2024-12-22T15:05:30Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    15,
                    5,
                    30,
                    6,
                    357,
                    0
                ],
                "title": "ViLBias: Detecting and Reasoning about Bias in Multimodal Content",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ViLBias: Detecting and Reasoning about Bias in Multimodal Content"
                },
                "summary": "Detecting bias in multimodal news requires models that reason over\ntext--image pairs, not just classify text. In response, we present ViLBias, a\nVQA-style benchmark and framework for detecting and reasoning about bias in\nmultimodal news. The dataset comprises 40,945 text--image pairs from diverse\noutlets, each annotated with a bias label and concise rationale using a\ntwo-stage LLM-as-annotator pipeline with hierarchical majority voting and\nhuman-in-the-loop validation. We evaluate Small Language Models (SLMs), Large\nLanguage Models (LLMs), and Vision--Language Models (VLMs) across closed-ended\nclassification and open-ended reasoning (oVQA), and compare parameter-efficient\ntuning strategies. Results show that incorporating images alongside text\nimproves detection accuracy by 3--5\\%, and that LLMs/VLMs better capture subtle\nframing and text--image inconsistencies than SLMs. Parameter-efficient methods\n(LoRA/QLoRA/Adapters) recover 97--99\\% of full fine-tuning performance with\n$<5\\%$ trainable parameters. For oVQA, reasoning accuracy spans 52--79\\% and\nfaithfulness 68--89\\%, both improved by instruction tuning; closed accuracy\ncorrelates strongly with reasoning ($r = 0.91$). ViLBias offers a scalable\nbenchmark and strong baselines for multimodal bias detection and rationale\nquality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting bias in multimodal news requires models that reason over\ntext--image pairs, not just classify text. In response, we present ViLBias, a\nVQA-style benchmark and framework for detecting and reasoning about bias in\nmultimodal news. The dataset comprises 40,945 text--image pairs from diverse\noutlets, each annotated with a bias label and concise rationale using a\ntwo-stage LLM-as-annotator pipeline with hierarchical majority voting and\nhuman-in-the-loop validation. We evaluate Small Language Models (SLMs), Large\nLanguage Models (LLMs), and Vision--Language Models (VLMs) across closed-ended\nclassification and open-ended reasoning (oVQA), and compare parameter-efficient\ntuning strategies. Results show that incorporating images alongside text\nimproves detection accuracy by 3--5\\%, and that LLMs/VLMs better capture subtle\nframing and text--image inconsistencies than SLMs. Parameter-efficient methods\n(LoRA/QLoRA/Adapters) recover 97--99\\% of full fine-tuning performance with\n$<5\\%$ trainable parameters. For oVQA, reasoning accuracy spans 52--79\\% and\nfaithfulness 68--89\\%, both improved by instruction tuning; closed accuracy\ncorrelates strongly with reasoning ($r = 0.91$). ViLBias offers a scalable\nbenchmark and strong baselines for multimodal bias detection and rationale\nquality."
                },
                "authors": [
                    {
                        "name": "Shaina Raza"
                    },
                    {
                        "name": "Caesar Saleh"
                    },
                    {
                        "name": "Azib Farooq"
                    },
                    {
                        "name": "Emrul Hasan"
                    },
                    {
                        "name": "Franklin Ogidi"
                    },
                    {
                        "name": "Maximus Powers"
                    },
                    {
                        "name": "Veronica Chatrath"
                    },
                    {
                        "name": "Marcelo Lotif"
                    },
                    {
                        "name": "Karanpal Sekhon"
                    },
                    {
                        "name": "Roya Javadi"
                    },
                    {
                        "name": "Haad Zahid"
                    },
                    {
                        "name": "Anam Zahid"
                    },
                    {
                        "name": "Vahid Reza Khazaie"
                    },
                    {
                        "name": "Zhenyu Yu"
                    }
                ],
                "author_detail": {
                    "name": "Zhenyu Yu"
                },
                "author": "Zhenyu Yu",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17052v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17052v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17324v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17324v2",
                "updated": "2025-10-01T12:29:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    12,
                    29,
                    27,
                    2,
                    274,
                    0
                ],
                "published": "2025-08-24T12:11:21Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    12,
                    11,
                    21,
                    6,
                    236,
                    0
                ],
                "title": "CultranAI at PalmX 2025: Data Augmentation for Cultural Knowledge\n  Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CultranAI at PalmX 2025: Data Augmentation for Cultural Knowledge\n  Representation"
                },
                "summary": "In this paper, we report our participation to the PalmX cultural evaluation\nshared task. Our system, CultranAI, focused on data augmentation and LoRA\nfine-tuning of large language models (LLMs) for Arabic cultural knowledge\nrepresentation. We benchmarked several LLMs to identify the best-performing\nmodel for the task. In addition to utilizing the PalmX dataset, we augmented it\nby incorporating the Palm dataset and curated a new dataset of over 22K\nculturally grounded multiple-choice questions (MCQs). Our experiments showed\nthat the Fanar-1-9B-Instruct model achieved the highest performance. We\nfine-tuned this model on the combined augmented dataset of 22K+ MCQs. On the\nblind test set, our submitted system ranked 5th with an accuracy of 70.50%,\nwhile on the PalmX development set, it achieved an accuracy of 84.1%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we report our participation to the PalmX cultural evaluation\nshared task. Our system, CultranAI, focused on data augmentation and LoRA\nfine-tuning of large language models (LLMs) for Arabic cultural knowledge\nrepresentation. We benchmarked several LLMs to identify the best-performing\nmodel for the task. In addition to utilizing the PalmX dataset, we augmented it\nby incorporating the Palm dataset and curated a new dataset of over 22K\nculturally grounded multiple-choice questions (MCQs). Our experiments showed\nthat the Fanar-1-9B-Instruct model achieved the highest performance. We\nfine-tuned this model on the combined augmented dataset of 22K+ MCQs. On the\nblind test set, our submitted system ranked 5th with an accuracy of 70.50%,\nwhile on the PalmX development set, it achieved an accuracy of 84.1%."
                },
                "authors": [
                    {
                        "name": "Hunzalah Hassan Bhatti"
                    },
                    {
                        "name": "Youssef Ahmed"
                    },
                    {
                        "name": "Md Arid Hasan"
                    },
                    {
                        "name": "Firoj Alam"
                    }
                ],
                "author_detail": {
                    "name": "Firoj Alam"
                },
                "author": "Firoj Alam",
                "arxiv_comment": "LLMs, Native, Arabic LLMs, Augmentation, Multilingual, Language\n  Diversity, Contextual Understanding, Minority Languages, Culturally Informed,\n  Foundation Models, Large Language Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17324v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17324v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16495v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16495v2",
                "updated": "2025-10-01T12:27:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    12,
                    27,
                    37,
                    2,
                    274,
                    0
                ],
                "published": "2025-08-22T16:17:31Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    16,
                    17,
                    31,
                    4,
                    234,
                    0
                ],
                "title": "Post Hoc Regression Refinement via Pairwise Rankings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post Hoc Regression Refinement via Pairwise Rankings"
                },
                "summary": "Accurate prediction of continuous properties is essential to many scientific\nand engineering tasks. Although deep-learning regressors excel with abundant\nlabels, their accuracy deteriorates in data-scarce regimes. We introduce\nRankRefine, a model-agnostic, plug-and-play post hoc method that refines\nregression with expert knowledge coming from pairwise rankings. Given a query\nitem and a small reference set with known properties, RankRefine combines the\nbase regressor's output with a rank-based estimate via inverse variance\nweighting, requiring no retraining. In molecular property prediction task,\nRankRefine achieves up to 10% relative reduction in mean absolute error using\nonly 20 pairwise comparisons obtained through a general-purpose large language\nmodel (LLM) with no finetuning. As rankings provided by human experts or\ngeneral-purpose LLMs are sufficient for improving regression across diverse\ndomains, RankRefine offers practicality and broad applicability, especially in\nlow-data settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate prediction of continuous properties is essential to many scientific\nand engineering tasks. Although deep-learning regressors excel with abundant\nlabels, their accuracy deteriorates in data-scarce regimes. We introduce\nRankRefine, a model-agnostic, plug-and-play post hoc method that refines\nregression with expert knowledge coming from pairwise rankings. Given a query\nitem and a small reference set with known properties, RankRefine combines the\nbase regressor's output with a rank-based estimate via inverse variance\nweighting, requiring no retraining. In molecular property prediction task,\nRankRefine achieves up to 10% relative reduction in mean absolute error using\nonly 20 pairwise comparisons obtained through a general-purpose large language\nmodel (LLM) with no finetuning. As rankings provided by human experts or\ngeneral-purpose LLMs are sufficient for improving regression across diverse\ndomains, RankRefine offers practicality and broad applicability, especially in\nlow-data settings."
                },
                "authors": [
                    {
                        "name": "Kevin Tirta Wijaya"
                    },
                    {
                        "name": "Michael Sun"
                    },
                    {
                        "name": "Minghao Guo"
                    },
                    {
                        "name": "Hans-Peter Seidel"
                    },
                    {
                        "name": "Wojciech Matusik"
                    },
                    {
                        "name": "Vahid Babaei"
                    }
                ],
                "author_detail": {
                    "name": "Vahid Babaei"
                },
                "author": "Vahid Babaei",
                "arxiv_comment": "NeurIPS 2025 camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16495v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16495v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26272v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26272v2",
                "updated": "2025-10-01T12:27:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    12,
                    27,
                    21,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-30T13:56:05Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    13,
                    56,
                    5,
                    1,
                    273,
                    0
                ],
                "title": "PRPO: Paragraph-level Policy Optimization for Vision-Language Deepfake\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRPO: Paragraph-level Policy Optimization for Vision-Language Deepfake\n  Detection"
                },
                "summary": "The rapid rise of synthetic media has made deepfake detection a critical\nchallenge for online safety and trust. Progress remains constrained by the\nscarcity of large, high-quality datasets. Although multimodal large language\nmodels (LLMs) exhibit strong reasoning capabilities, their performance on\ndeepfake detection is poor, often producing explanations that are misaligned\nwith visual evidence or hallucinatory. To address this limitation, we introduce\na reasoning-annotated dataset for deepfake detection and propose\nParagraph-level Relative Policy Optimization (PRPO), a reinforcement learning\nalgorithm that aligns LLM reasoning with image content at the paragraph level.\nExperiments show that PRPO improves detection accuracy by a wide margin and\nachieves the highest reasoning score of 4.55/5.0. Ablation studies further\ndemonstrate that PRPO significantly outperforms GRPO under test-time\nconditions. These results underscore the importance of grounding multimodal\nreasoning in visual evidence to enable more reliable and interpretable deepfake\ndetection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid rise of synthetic media has made deepfake detection a critical\nchallenge for online safety and trust. Progress remains constrained by the\nscarcity of large, high-quality datasets. Although multimodal large language\nmodels (LLMs) exhibit strong reasoning capabilities, their performance on\ndeepfake detection is poor, often producing explanations that are misaligned\nwith visual evidence or hallucinatory. To address this limitation, we introduce\na reasoning-annotated dataset for deepfake detection and propose\nParagraph-level Relative Policy Optimization (PRPO), a reinforcement learning\nalgorithm that aligns LLM reasoning with image content at the paragraph level.\nExperiments show that PRPO improves detection accuracy by a wide margin and\nachieves the highest reasoning score of 4.55/5.0. Ablation studies further\ndemonstrate that PRPO significantly outperforms GRPO under test-time\nconditions. These results underscore the importance of grounding multimodal\nreasoning in visual evidence to enable more reliable and interpretable deepfake\ndetection."
                },
                "authors": [
                    {
                        "name": "Tuan Nguyen"
                    },
                    {
                        "name": "Naseem Khan"
                    },
                    {
                        "name": "Khang Tran"
                    },
                    {
                        "name": "NhatHai Phan"
                    },
                    {
                        "name": "Issa Khalil"
                    }
                ],
                "author_detail": {
                    "name": "Issa Khalil"
                },
                "author": "Issa Khalil",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26272v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26272v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13497v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13497v3",
                "updated": "2025-10-01T12:01:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    12,
                    1,
                    27,
                    2,
                    274,
                    0
                ],
                "published": "2025-05-15T20:23:21Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    20,
                    23,
                    21,
                    3,
                    135,
                    0
                ],
                "title": "Learning Hierarchical Domain Models Through Environment-Grounded\n  Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Hierarchical Domain Models Through Environment-Grounded\n  Interaction"
                },
                "summary": "Domain models enable autonomous agents to solve long-horizon tasks by\nproducing interpretable plans. However, in open-world environments, a single\ngeneral domain model cannot capture the variety of tasks, so agents must\ngenerate suitable task-specific models on the fly. Large Language Models\n(LLMs), with their implicit common knowledge, can generate such domains, but\nsuffer from high error rates that limit their applicability. Hence, related\nwork relies on extensive human feed-back or prior knowledge, which undermines\nautonomous, open-world deployment. In this work, we propose LODGE, a framework\nfor autonomous domain learning from LLMs and environment grounding. LODGE\nbuilds on hierarchical abstractions and automated simulations to identify and\ncorrect inconsistencies between abstraction layers and between the model and\nenvironment. Our framework is task-agnostic, as it generates predicates,\noperators, and their preconditions and effects, while only assuming access to a\nsimulator and a set of generic, executable low-level skills. Experiments on two\nInternational Planning Competition ( IPC) domains and a robotic assembly domain\nshow that LODGE yields more accurate domain models and higher task success than\nexisting methods, requiring remarkably few environment interactions and no\nhuman feedback or demonstrations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain models enable autonomous agents to solve long-horizon tasks by\nproducing interpretable plans. However, in open-world environments, a single\ngeneral domain model cannot capture the variety of tasks, so agents must\ngenerate suitable task-specific models on the fly. Large Language Models\n(LLMs), with their implicit common knowledge, can generate such domains, but\nsuffer from high error rates that limit their applicability. Hence, related\nwork relies on extensive human feed-back or prior knowledge, which undermines\nautonomous, open-world deployment. In this work, we propose LODGE, a framework\nfor autonomous domain learning from LLMs and environment grounding. LODGE\nbuilds on hierarchical abstractions and automated simulations to identify and\ncorrect inconsistencies between abstraction layers and between the model and\nenvironment. Our framework is task-agnostic, as it generates predicates,\noperators, and their preconditions and effects, while only assuming access to a\nsimulator and a set of generic, executable low-level skills. Experiments on two\nInternational Planning Competition ( IPC) domains and a robotic assembly domain\nshow that LODGE yields more accurate domain models and higher task success than\nexisting methods, requiring remarkably few environment interactions and no\nhuman feedback or demonstrations."
                },
                "authors": [
                    {
                        "name": "Claudius Kienle"
                    },
                    {
                        "name": "Benjamin Alt"
                    },
                    {
                        "name": "Oleg Arenz"
                    },
                    {
                        "name": "Jan Peters"
                    }
                ],
                "author_detail": {
                    "name": "Jan Peters"
                },
                "author": "Jan Peters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13497v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13497v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19526v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19526v3",
                "updated": "2025-10-01T11:54:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    11,
                    54,
                    28,
                    2,
                    274,
                    0
                ],
                "published": "2025-04-28T07:05:38Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    7,
                    5,
                    38,
                    0,
                    118,
                    0
                ],
                "title": "Automated flood detection from Sentinel-1 GRD time series using Bayesian\n  analysis for change point problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated flood detection from Sentinel-1 GRD time series using Bayesian\n  analysis for change point problems"
                },
                "summary": "Current Synthetic Aperture Radar (SAR)-based flood detection methods face\ncritical limitations that hinder operational deployment. Supervised learning\napproaches require extensive labeled training data, exhibit poor geographical\ntransferability, and may fail to adapt to new regions without additional\ntraining examples. Existing approaches do not fully exploit the rich temporal\ninformation available in SAR time series, instead relying on simple change\ndetection between pre- and post-flood images or supplementary datasets that\noften introduce error propagation. These limitations prevent effective\nautomated flood monitoring in data-scarce regions where disaster response is\nmost needed. To address these limitations, we develop a novel training-free\napproach by adapting Bayesian analysis for change point problems, specifically\nfor automated flood detection from Sentinel-1 Ground Range Detected time series\ndata. Our method statistically models the temporal behavior of SAR backscatter\nintensity over a one-year baseline period, then computes the posterior\nprobability of change points at flood observation dates. This approach\neliminates supervised learning dependencies by using Bayesian inference to\nidentify when backscatter deviations exceed expected normal variations,\nleveraging inherent statistical properties of time series data. Validation\nacross three diverse geographical contexts using the UrbanSARFloods benchmark\ndataset demonstrates superior performance compared to conventional thresholding\nand deep learning approaches, achieving F1 scores up to 0.76. This enables\nimmediate deployment to any region with SAR coverage, providing critical\nadvantages for disaster response.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current Synthetic Aperture Radar (SAR)-based flood detection methods face\ncritical limitations that hinder operational deployment. Supervised learning\napproaches require extensive labeled training data, exhibit poor geographical\ntransferability, and may fail to adapt to new regions without additional\ntraining examples. Existing approaches do not fully exploit the rich temporal\ninformation available in SAR time series, instead relying on simple change\ndetection between pre- and post-flood images or supplementary datasets that\noften introduce error propagation. These limitations prevent effective\nautomated flood monitoring in data-scarce regions where disaster response is\nmost needed. To address these limitations, we develop a novel training-free\napproach by adapting Bayesian analysis for change point problems, specifically\nfor automated flood detection from Sentinel-1 Ground Range Detected time series\ndata. Our method statistically models the temporal behavior of SAR backscatter\nintensity over a one-year baseline period, then computes the posterior\nprobability of change points at flood observation dates. This approach\neliminates supervised learning dependencies by using Bayesian inference to\nidentify when backscatter deviations exceed expected normal variations,\nleveraging inherent statistical properties of time series data. Validation\nacross three diverse geographical contexts using the UrbanSARFloods benchmark\ndataset demonstrates superior performance compared to conventional thresholding\nand deep learning approaches, achieving F1 scores up to 0.76. This enables\nimmediate deployment to any region with SAR coverage, providing critical\nadvantages for disaster response."
                },
                "authors": [
                    {
                        "name": "Narumasa Tsutsumida"
                    },
                    {
                        "name": "Tomohiro Tanaka"
                    },
                    {
                        "name": "Nifat Sultana"
                    }
                ],
                "author_detail": {
                    "name": "Nifat Sultana"
                },
                "author": "Nifat Sultana",
                "arxiv_comment": "17 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19526v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19526v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26432v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26432v2",
                "updated": "2025-10-01T11:26:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    11,
                    26,
                    36,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-30T15:53:56Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    53,
                    56,
                    1,
                    273,
                    0
                ],
                "title": "AdaBlock-dLLM: Semantic-Aware Diffusion LLM Inference via Adaptive Block\n  Size",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaBlock-dLLM: Semantic-Aware Diffusion LLM Inference via Adaptive Block\n  Size"
                },
                "summary": "Diffusion-based large language models (dLLMs) are gaining attention for their\ninherent capacity for parallel decoding, offering a compelling alternative to\nautoregressive LLMs. Among various decoding strategies, blockwise\nsemi-autoregressive (semi-AR) approaches are widely adopted due to their\nnatural support for KV caching and their favorable accuracy-speed trade-off.\nHowever, this paper identifies two fundamental limitations in the conventional\nsemi-AR decoding approach that applies a fixed block size: i) late decoding\noverhead, where the unmasking of high-confidence tokens outside the current\nblock is unnecessarily delayed, and ii) premature decoding error, where\nlow-confidence tokens inside the current block are committed too early, leading\nto incorrect tokens. This paper presents the first systematic investigation\nchallenging the fixed block size assumption in semi-AR decoding. Through a\nstatistical analysis of confidence dynamics during the denoising process, we\nidentify a volatility band (VB) region during dLLM decoding, which encodes\nlocal semantic structure and can be used to guide adaptive block sizing.\nLeveraging these insights, we introduce AdaBlock-dLLM, a training-free,\nplug-and-play scheduler that adaptively aligns block boundaries with semantic\nsteps by adjusting block size during runtime. Extensive experiments across\ndiverse benchmarks show that AdaBlock-dLLM achieves up to 5.3% accuracy\nimprovement under the same throughput budget. Beyond inference-time\noptimization, we hope our semantics-aware adaptive scheduling approach and\nconfidence-based analysis will inspire future training strategies for dLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based large language models (dLLMs) are gaining attention for their\ninherent capacity for parallel decoding, offering a compelling alternative to\nautoregressive LLMs. Among various decoding strategies, blockwise\nsemi-autoregressive (semi-AR) approaches are widely adopted due to their\nnatural support for KV caching and their favorable accuracy-speed trade-off.\nHowever, this paper identifies two fundamental limitations in the conventional\nsemi-AR decoding approach that applies a fixed block size: i) late decoding\noverhead, where the unmasking of high-confidence tokens outside the current\nblock is unnecessarily delayed, and ii) premature decoding error, where\nlow-confidence tokens inside the current block are committed too early, leading\nto incorrect tokens. This paper presents the first systematic investigation\nchallenging the fixed block size assumption in semi-AR decoding. Through a\nstatistical analysis of confidence dynamics during the denoising process, we\nidentify a volatility band (VB) region during dLLM decoding, which encodes\nlocal semantic structure and can be used to guide adaptive block sizing.\nLeveraging these insights, we introduce AdaBlock-dLLM, a training-free,\nplug-and-play scheduler that adaptively aligns block boundaries with semantic\nsteps by adjusting block size during runtime. Extensive experiments across\ndiverse benchmarks show that AdaBlock-dLLM achieves up to 5.3% accuracy\nimprovement under the same throughput budget. Beyond inference-time\noptimization, we hope our semantics-aware adaptive scheduling approach and\nconfidence-based analysis will inspire future training strategies for dLLMs."
                },
                "authors": [
                    {
                        "name": "Guanxi Lu"
                    },
                    {
                        "name": "Hao Mark Chen"
                    },
                    {
                        "name": "Yuto Karashima"
                    },
                    {
                        "name": "Zhican Wang"
                    },
                    {
                        "name": "Daichi Fujiki"
                    },
                    {
                        "name": "Hongxiang Fan"
                    }
                ],
                "author_detail": {
                    "name": "Hongxiang Fan"
                },
                "author": "Hongxiang Fan",
                "arxiv_comment": "Preprint. Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26432v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26432v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22678v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22678v2",
                "updated": "2025-10-01T11:09:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    11,
                    9,
                    34,
                    2,
                    274,
                    0
                ],
                "published": "2025-03-28T17:59:53Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    17,
                    59,
                    53,
                    4,
                    87,
                    0
                ],
                "title": "Self-Evolving Multi-Agent Simulations for Realistic Clinical\n  Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Evolving Multi-Agent Simulations for Realistic Clinical\n  Interactions"
                },
                "summary": "In this work, we introduce MedAgentSim, an open-source simulated clinical\nenvironment with doctor, patient, and measurement agents designed to evaluate\nand enhance LLM performance in dynamic diagnostic settings. Unlike prior\napproaches, our framework requires doctor agents to actively engage with\npatients through multi-turn conversations, requesting relevant medical\nexaminations (e.g., temperature, blood pressure, ECG) and imaging results\n(e.g., MRI, X-ray) from a measurement agent to mimic the real-world diagnostic\nprocess. Additionally, we incorporate self improvement mechanisms that allow\nmodels to iteratively refine their diagnostic strategies. We enhance LLM\nperformance in our simulated setting by integrating multi-agent discussions,\nchain-of-thought reasoning, and experience-based knowledge retrieval,\nfacilitating progressive learning as doctor agents interact with more patients.\nWe also introduce an evaluation benchmark for assessing the LLM's ability to\nengage in dynamic, context-aware diagnostic interactions. While MedAgentSim is\nfully automated, it also supports a user-controlled mode, enabling human\ninteraction with either the doctor or patient agent. Comprehensive evaluations\nin various simulated diagnostic scenarios demonstrate the effectiveness of our\napproach. Our code, simulation tool, and benchmark are available at\n\\href{https://medagentsim.netlify.app/}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we introduce MedAgentSim, an open-source simulated clinical\nenvironment with doctor, patient, and measurement agents designed to evaluate\nand enhance LLM performance in dynamic diagnostic settings. Unlike prior\napproaches, our framework requires doctor agents to actively engage with\npatients through multi-turn conversations, requesting relevant medical\nexaminations (e.g., temperature, blood pressure, ECG) and imaging results\n(e.g., MRI, X-ray) from a measurement agent to mimic the real-world diagnostic\nprocess. Additionally, we incorporate self improvement mechanisms that allow\nmodels to iteratively refine their diagnostic strategies. We enhance LLM\nperformance in our simulated setting by integrating multi-agent discussions,\nchain-of-thought reasoning, and experience-based knowledge retrieval,\nfacilitating progressive learning as doctor agents interact with more patients.\nWe also introduce an evaluation benchmark for assessing the LLM's ability to\nengage in dynamic, context-aware diagnostic interactions. While MedAgentSim is\nfully automated, it also supports a user-controlled mode, enabling human\ninteraction with either the doctor or patient agent. Comprehensive evaluations\nin various simulated diagnostic scenarios demonstrate the effectiveness of our\napproach. Our code, simulation tool, and benchmark are available at\n\\href{https://medagentsim.netlify.app/}."
                },
                "authors": [
                    {
                        "name": "Mohammad Almansoori"
                    },
                    {
                        "name": "Komal Kumar"
                    },
                    {
                        "name": "Hisham Cholakkal"
                    }
                ],
                "author_detail": {
                    "name": "Hisham Cholakkal"
                },
                "author": "Hisham Cholakkal",
                "arxiv_comment": "14 page, 4 figures, 61 references, presented in MICCAI (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22678v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22678v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14824v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14824v2",
                "updated": "2025-10-01T11:06:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    11,
                    6,
                    32,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-18T10:32:52Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    10,
                    32,
                    52,
                    3,
                    261,
                    0
                ],
                "title": "Confirmation Bias as a Cognitive Resource in LLM-Supported Deliberation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confirmation Bias as a Cognitive Resource in LLM-Supported Deliberation"
                },
                "summary": "Large language models (LLMs) are increasingly used in group decision-making,\nbut their influence risks fostering conformity and reducing epistemic\nvigilance. Drawing on the Argumentative Theory of Reasoning, we argue that\nconfirmation bias, often seen as detrimental, can be harnessed as a resource\nwhen paired with critical evaluation. We propose a three-step process in which\nindividuals first generate ideas independently, then use LLMs to refine and\narticulate them, and finally engage with LLMs as epistemic provocateurs to\nanticipate group critique. This framing positions LLMs as tools for scaffolding\ndisagreement, helping individuals prepare for more productive group\ndiscussions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly used in group decision-making,\nbut their influence risks fostering conformity and reducing epistemic\nvigilance. Drawing on the Argumentative Theory of Reasoning, we argue that\nconfirmation bias, often seen as detrimental, can be harnessed as a resource\nwhen paired with critical evaluation. We propose a three-step process in which\nindividuals first generate ideas independently, then use LLMs to refine and\narticulate them, and finally engage with LLMs as epistemic provocateurs to\nanticipate group critique. This framing positions LLMs as tools for scaffolding\ndisagreement, helping individuals prepare for more productive group\ndiscussions."
                },
                "authors": [
                    {
                        "name": "Sander de Jong"
                    },
                    {
                        "name": "Rune Møberg Jacobsen"
                    },
                    {
                        "name": "Niels van Berkel"
                    }
                ],
                "author_detail": {
                    "name": "Niels van Berkel"
                },
                "author": "Niels van Berkel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14824v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14824v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12310v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12310v2",
                "updated": "2025-10-01T10:59:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    10,
                    59,
                    9,
                    2,
                    274,
                    0
                ],
                "published": "2024-12-16T19:29:06Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    19,
                    29,
                    6,
                    0,
                    351,
                    0
                ],
                "title": "Second Language (Arabic) Acquisition of LLMs via Progressive Vocabulary\n  Expansion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Second Language (Arabic) Acquisition of LLMs via Progressive Vocabulary\n  Expansion"
                },
                "summary": "This paper addresses the critical need for democratizing large language\nmodels (LLM) in the Arab world, a region that has seen slower progress in\ndeveloping models comparable to state-of-the-art offerings like GPT-4 or\nChatGPT 3.5, due to a predominant focus on mainstream languages (e.g., English\nand Chinese). One practical objective for an Arabic LLM is to utilize an\nArabic-specific vocabulary for the tokenizer that could speed up decoding.\nHowever, using a different vocabulary often leads to a degradation of learned\nknowledge since many words are initially out-of-vocabulary (OOV) when training\nstarts. Inspired by the vocabulary learning during Second Language (Arabic)\nAcquisition for humans, the released AraLLaMA employs progressive vocabulary\nexpansion, which is implemented by a modified BPE algorithm that progressively\nextends the Arabic subwords in its dynamic vocabulary during training, thereby\nbalancing the OOV ratio at every stage. The ablation study demonstrated the\neffectiveness of Progressive Vocabulary Expansion. Moreover, AraLLaMA achieves\ndecent performance comparable to the best Arabic LLMs across a variety of\nArabic benchmarks. Models, training data, benchmarks, and codes will be all\nopen-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the critical need for democratizing large language\nmodels (LLM) in the Arab world, a region that has seen slower progress in\ndeveloping models comparable to state-of-the-art offerings like GPT-4 or\nChatGPT 3.5, due to a predominant focus on mainstream languages (e.g., English\nand Chinese). One practical objective for an Arabic LLM is to utilize an\nArabic-specific vocabulary for the tokenizer that could speed up decoding.\nHowever, using a different vocabulary often leads to a degradation of learned\nknowledge since many words are initially out-of-vocabulary (OOV) when training\nstarts. Inspired by the vocabulary learning during Second Language (Arabic)\nAcquisition for humans, the released AraLLaMA employs progressive vocabulary\nexpansion, which is implemented by a modified BPE algorithm that progressively\nextends the Arabic subwords in its dynamic vocabulary during training, thereby\nbalancing the OOV ratio at every stage. The ablation study demonstrated the\neffectiveness of Progressive Vocabulary Expansion. Moreover, AraLLaMA achieves\ndecent performance comparable to the best Arabic LLMs across a variety of\nArabic benchmarks. Models, training data, benchmarks, and codes will be all\nopen-sourced."
                },
                "authors": [
                    {
                        "name": "Jianqing Zhu"
                    },
                    {
                        "name": "Huang Huang"
                    },
                    {
                        "name": "Zhihang Lin"
                    },
                    {
                        "name": "Juhao Liang"
                    },
                    {
                        "name": "Zhengyang Tang"
                    },
                    {
                        "name": "Khalid Almubarak"
                    },
                    {
                        "name": "Abdulmohsen Alharthik"
                    },
                    {
                        "name": "Bang An"
                    },
                    {
                        "name": "Juncai He"
                    },
                    {
                        "name": "Xiangbo Wu"
                    },
                    {
                        "name": "Fei Yu"
                    },
                    {
                        "name": "Junying Chen"
                    },
                    {
                        "name": "Zhuoheng Ma"
                    },
                    {
                        "name": "Yuhao Du"
                    },
                    {
                        "name": "He Zhang"
                    },
                    {
                        "name": "Emad A. Alghamdi"
                    },
                    {
                        "name": "Lian Zhang"
                    },
                    {
                        "name": "Ruoyu Sun"
                    },
                    {
                        "name": "Haizhou Li"
                    },
                    {
                        "name": "Benyou Wang"
                    },
                    {
                        "name": "Jinchao Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jinchao Xu"
                },
                "author": "Jinchao Xu",
                "arxiv_journal_ref": "Proceedings of the 63rd Annual Meeting of the Association for\n  Computational Linguistics (Volume 1: Long Papers), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12310v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12310v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26181v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26181v2",
                "updated": "2025-10-01T10:03:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    10,
                    3,
                    53,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-30T12:40:36Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    12,
                    40,
                    36,
                    1,
                    273,
                    0
                ],
                "title": "Explaining novel senses using definition generation with open language\n  models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explaining novel senses using definition generation with open language\n  models"
                },
                "summary": "We apply definition generators based on open-weights large language models to\nthe task of creating explanations of novel senses, taking target word usages as\nan input. To this end, we employ the datasets from the AXOLOTL'24 shared task\non explainable semantic change modeling, which features Finnish, Russian and\nGerman languages. We fine-tune and provide publicly the open-source models\nperforming higher than the best submissions of the aforementioned shared task,\nwhich employed closed proprietary LLMs. In addition, we find that\nencoder-decoder definition generators perform on par with their decoder-only\ncounterparts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We apply definition generators based on open-weights large language models to\nthe task of creating explanations of novel senses, taking target word usages as\nan input. To this end, we employ the datasets from the AXOLOTL'24 shared task\non explainable semantic change modeling, which features Finnish, Russian and\nGerman languages. We fine-tune and provide publicly the open-source models\nperforming higher than the best submissions of the aforementioned shared task,\nwhich employed closed proprietary LLMs. In addition, we find that\nencoder-decoder definition generators perform on par with their decoder-only\ncounterparts."
                },
                "authors": [
                    {
                        "name": "Mariia Fedorova"
                    },
                    {
                        "name": "Andrey Kutuzov"
                    },
                    {
                        "name": "Francesco Periti"
                    },
                    {
                        "name": "Yves Scherrer"
                    }
                ],
                "author_detail": {
                    "name": "Yves Scherrer"
                },
                "author": "Yves Scherrer",
                "arxiv_comment": "EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26181v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26181v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06066v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06066v4",
                "updated": "2025-10-01T09:57:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    9,
                    57,
                    28,
                    2,
                    274,
                    0
                ],
                "published": "2025-01-10T15:57:23Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    15,
                    57,
                    23,
                    4,
                    10,
                    0
                ],
                "title": "Distilling Calibration via Conformalized Credal Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilling Calibration via Conformalized Credal Inference"
                },
                "summary": "Deploying artificial intelligence (AI) models on edge devices involves a\ndelicate balance between meeting stringent complexity constraints, such as\nlimited memory and energy resources, and ensuring reliable performance in\nsensitive decision-making tasks. One way to enhance reliability is through\nuncertainty quantification via Bayesian inference. This approach, however,\ntypically necessitates maintaining and running multiple models in an ensemble,\nwhich may exceed the computational limits of edge devices. This paper\nintroduces a low-complexity methodology to address this challenge by distilling\ncalibration information from a more complex model. In an offline phase,\npredictive probabilities generated by a high-complexity cloud-based model are\nleveraged to determine a threshold based on the typical divergence between the\ncloud and edge models. At run time, this threshold is used to construct credal\nsets -- ranges of predictive probabilities that are guaranteed, with a\nuser-selected confidence level, to include the predictions of the cloud model.\nThe credal sets are obtained through thresholding of a divergence measure in\nthe simplex of predictive probabilities. Experiments on visual and language\ntasks demonstrate that the proposed approach, termed Conformalized Distillation\nfor Credal Inference (CD-CI), significantly improves calibration performance\ncompared to low-complexity Bayesian methods, such as Laplace approximation,\nmaking it a practical and efficient solution for edge AI deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying artificial intelligence (AI) models on edge devices involves a\ndelicate balance between meeting stringent complexity constraints, such as\nlimited memory and energy resources, and ensuring reliable performance in\nsensitive decision-making tasks. One way to enhance reliability is through\nuncertainty quantification via Bayesian inference. This approach, however,\ntypically necessitates maintaining and running multiple models in an ensemble,\nwhich may exceed the computational limits of edge devices. This paper\nintroduces a low-complexity methodology to address this challenge by distilling\ncalibration information from a more complex model. In an offline phase,\npredictive probabilities generated by a high-complexity cloud-based model are\nleveraged to determine a threshold based on the typical divergence between the\ncloud and edge models. At run time, this threshold is used to construct credal\nsets -- ranges of predictive probabilities that are guaranteed, with a\nuser-selected confidence level, to include the predictions of the cloud model.\nThe credal sets are obtained through thresholding of a divergence measure in\nthe simplex of predictive probabilities. Experiments on visual and language\ntasks demonstrate that the proposed approach, termed Conformalized Distillation\nfor Credal Inference (CD-CI), significantly improves calibration performance\ncompared to low-complexity Bayesian methods, such as Laplace approximation,\nmaking it a practical and efficient solution for edge AI deployments."
                },
                "authors": [
                    {
                        "name": "Jiayi Huang"
                    },
                    {
                        "name": "Sangwoo Park"
                    },
                    {
                        "name": "Nicola Paoletti"
                    },
                    {
                        "name": "Osvaldo Simeone"
                    }
                ],
                "author_detail": {
                    "name": "Osvaldo Simeone"
                },
                "author": "Osvaldo Simeone",
                "arxiv_comment": "IJCNN 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06066v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06066v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23274v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23274v2",
                "updated": "2025-10-01T09:46:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    9,
                    46,
                    53,
                    2,
                    274,
                    0
                ],
                "published": "2025-06-29T15:01:01Z",
                "published_parsed": [
                    2025,
                    6,
                    29,
                    15,
                    1,
                    1,
                    6,
                    180,
                    0
                ],
                "title": "Towards a Progress Bar for Reasoning: Progress Prediction in Large\n  Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a Progress Bar for Reasoning: Progress Prediction in Large\n  Reasoning Models"
                },
                "summary": "Reasoning models that produce long, hidden chains of thought, have emerged as\npowerful tools for reasoning-intensive and agentic tasks. However, as the time\nhorizons at which these models can operate grow exponentially, it becomes\nincreasingly difficult to know how much progress the model is making on a task,\nmaking it challenging for users to set appropriate expectations about\ncompletion time. By probing the internal representations of Large Language\nModels (LLMs), we find evidence that their reasoning progress can be\nquantified, with simple linear probes achieving 30\\% accuracy over 10 progress\nclasses and Mean Absolute Error (MAE) of 1.75. Rooted in this insight, we\npropose a two-stage fine-tuning method that trains existing reasoning models to\nexplicitly generate progress estimates (0-100\\%) during their reasoning\nprocess. We find that the predictions of our best fine-tuned language model for\nsequences below 16K tokens are on average 10\\% from the true label.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning models that produce long, hidden chains of thought, have emerged as\npowerful tools for reasoning-intensive and agentic tasks. However, as the time\nhorizons at which these models can operate grow exponentially, it becomes\nincreasingly difficult to know how much progress the model is making on a task,\nmaking it challenging for users to set appropriate expectations about\ncompletion time. By probing the internal representations of Large Language\nModels (LLMs), we find evidence that their reasoning progress can be\nquantified, with simple linear probes achieving 30\\% accuracy over 10 progress\nclasses and Mean Absolute Error (MAE) of 1.75. Rooted in this insight, we\npropose a two-stage fine-tuning method that trains existing reasoning models to\nexplicitly generate progress estimates (0-100\\%) during their reasoning\nprocess. We find that the predictions of our best fine-tuned language model for\nsequences below 16K tokens are on average 10\\% from the true label."
                },
                "authors": [
                    {
                        "name": "Hans Peter Lynsgøe Raaschou-jensen"
                    },
                    {
                        "name": "Constanza Fierro"
                    },
                    {
                        "name": "Anders Søgaard"
                    }
                ],
                "author_detail": {
                    "name": "Anders Søgaard"
                },
                "author": "Anders Søgaard",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23274v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23274v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02670v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02670v6",
                "updated": "2025-10-01T09:38:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    9,
                    38,
                    58,
                    2,
                    274,
                    0
                ],
                "published": "2025-04-03T15:11:55Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    15,
                    11,
                    55,
                    3,
                    93,
                    0
                ],
                "title": "Affordable AI Assistants with Knowledge Graph of Thoughts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Affordable AI Assistants with Knowledge Graph of Thoughts"
                },
                "summary": "Large Language Models (LLMs) are revolutionizing the development of AI\nassistants capable of performing diverse tasks across domains. However, current\nstate-of-the-art LLM-driven agents face significant challenges, including high\noperational costs and limited success rates on complex benchmarks like GAIA. To\naddress these issues, we propose Knowledge Graph of Thoughts (KGoT), an\ninnovative AI assistant architecture that integrates LLM reasoning with\ndynamically constructed knowledge graphs (KGs). KGoT extracts and structures\ntask-relevant knowledge into a dynamic KG representation, iteratively enhanced\nthrough external tools such as math solvers, web crawlers, and Python scripts.\nSuch structured representation of task-relevant knowledge enables low-cost\nmodels to solve complex tasks effectively while also minimizing bias and noise.\nFor example, KGoT achieves a 29% improvement in task success rates on the GAIA\nbenchmark compared to Hugging Face Agents with GPT-4o mini. Moreover,\nharnessing a smaller model dramatically reduces operational costs by over 36x\ncompared to GPT-4o. Improvements for other models (e.g., Qwen2.5-32B and\nDeepseek-R1-70B) and benchmarks (e.g., SimpleQA) are similar. KGoT offers a\nscalable, affordable, versatile, and high-performing solution for AI\nassistants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are revolutionizing the development of AI\nassistants capable of performing diverse tasks across domains. However, current\nstate-of-the-art LLM-driven agents face significant challenges, including high\noperational costs and limited success rates on complex benchmarks like GAIA. To\naddress these issues, we propose Knowledge Graph of Thoughts (KGoT), an\ninnovative AI assistant architecture that integrates LLM reasoning with\ndynamically constructed knowledge graphs (KGs). KGoT extracts and structures\ntask-relevant knowledge into a dynamic KG representation, iteratively enhanced\nthrough external tools such as math solvers, web crawlers, and Python scripts.\nSuch structured representation of task-relevant knowledge enables low-cost\nmodels to solve complex tasks effectively while also minimizing bias and noise.\nFor example, KGoT achieves a 29% improvement in task success rates on the GAIA\nbenchmark compared to Hugging Face Agents with GPT-4o mini. Moreover,\nharnessing a smaller model dramatically reduces operational costs by over 36x\ncompared to GPT-4o. Improvements for other models (e.g., Qwen2.5-32B and\nDeepseek-R1-70B) and benchmarks (e.g., SimpleQA) are similar. KGoT offers a\nscalable, affordable, versatile, and high-performing solution for AI\nassistants."
                },
                "authors": [
                    {
                        "name": "Maciej Besta"
                    },
                    {
                        "name": "Lorenzo Paleari"
                    },
                    {
                        "name": "Jia Hao Andrea Jiang"
                    },
                    {
                        "name": "Robert Gerstenberger"
                    },
                    {
                        "name": "You Wu"
                    },
                    {
                        "name": "Jón Gunnar Hannesson"
                    },
                    {
                        "name": "Patrick Iff"
                    },
                    {
                        "name": "Ales Kubicek"
                    },
                    {
                        "name": "Piotr Nyczyk"
                    },
                    {
                        "name": "Diana Khimey"
                    },
                    {
                        "name": "Nils Blach"
                    },
                    {
                        "name": "Haiqiang Zhang"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Peiran Ma"
                    },
                    {
                        "name": "Grzegorz Kwaśniewski"
                    },
                    {
                        "name": "Marcin Copik"
                    },
                    {
                        "name": "Hubert Niewiadomski"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02670v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02670v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04325v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04325v3",
                "updated": "2025-10-01T09:38:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    9,
                    38,
                    55,
                    2,
                    274,
                    0
                ],
                "published": "2024-05-07T13:55:11Z",
                "published_parsed": [
                    2024,
                    5,
                    7,
                    13,
                    55,
                    11,
                    1,
                    128,
                    0
                ],
                "title": "Language Models can Subtly Deceive Without Lying: A Case Study on\n  Strategic Phrasing in Legislation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models can Subtly Deceive Without Lying: A Case Study on\n  Strategic Phrasing in Legislation"
                },
                "summary": "We explore the ability of large language models (LLMs) to engage in subtle\ndeception through strategically phrasing and intentionally manipulating\ninformation. This harmful behavior can be hard to detect, unlike blatant lying\nor unintentional hallucination. We build a simple testbed mimicking a\nlegislative environment where a corporate \\textit{lobbyist} module is proposing\namendments to bills that benefit a specific company while evading\nidentification of this benefactor. We use real-world legislative bills matched\nwith potentially affected companies to ground these interactions. Our results\nshow that LLM lobbyists can draft subtle phrasing to avoid such identification\nby strong LLM-based detectors. Further optimization of the phrasing using\nLLM-based re-planning and re-sampling increases deception rates by up to 40\npercentage points. Our human evaluations to verify the quality of deceptive\ngenerations and their retention of self-serving intent show significant\ncoherence with our automated metrics and also help in identifying certain\nstrategies of deceptive phrasing. This study highlights the risk of LLMs'\ncapabilities for strategic phrasing through seemingly neutral language to\nattain self-serving goals. This calls for future research to uncover and\nprotect against such subtle deception.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore the ability of large language models (LLMs) to engage in subtle\ndeception through strategically phrasing and intentionally manipulating\ninformation. This harmful behavior can be hard to detect, unlike blatant lying\nor unintentional hallucination. We build a simple testbed mimicking a\nlegislative environment where a corporate \\textit{lobbyist} module is proposing\namendments to bills that benefit a specific company while evading\nidentification of this benefactor. We use real-world legislative bills matched\nwith potentially affected companies to ground these interactions. Our results\nshow that LLM lobbyists can draft subtle phrasing to avoid such identification\nby strong LLM-based detectors. Further optimization of the phrasing using\nLLM-based re-planning and re-sampling increases deception rates by up to 40\npercentage points. Our human evaluations to verify the quality of deceptive\ngenerations and their retention of self-serving intent show significant\ncoherence with our automated metrics and also help in identifying certain\nstrategies of deceptive phrasing. This study highlights the risk of LLMs'\ncapabilities for strategic phrasing through seemingly neutral language to\nattain self-serving goals. This calls for future research to uncover and\nprotect against such subtle deception."
                },
                "authors": [
                    {
                        "name": "Atharvan Dogra"
                    },
                    {
                        "name": "Krishna Pillutla"
                    },
                    {
                        "name": "Ameet Deshpande"
                    },
                    {
                        "name": "Ananya B Sai"
                    },
                    {
                        "name": "John Nay"
                    },
                    {
                        "name": "Tanmay Rajpurohit"
                    },
                    {
                        "name": "Ashwin Kalyan"
                    },
                    {
                        "name": "Balaraman Ravindran"
                    }
                ],
                "author_detail": {
                    "name": "Balaraman Ravindran"
                },
                "author": "Balaraman Ravindran",
                "arxiv_doi": "10.18653/v1/2025.acl-long.1600",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.18653/v1/2025.acl-long.1600",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.04325v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04325v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "24 pages, 7 figures; published in Proceedings of the 63rd Annual\n  Meeting of the Association for Computational Linguistics (ACL 2025), Volume\n  1: Long Papers; Anthology ID 2025.acl-long.1600",
                "arxiv_journal_ref": "Proceedings of the 63rd Annual Meeting of the Association for\n  Computational Linguistics (Volume 1: Long Papers), Vienna, Austria, July\n  2025, pages 33367-33390",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23384v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23384v3",
                "updated": "2025-10-01T09:38:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    9,
                    38,
                    38,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-27T16:09:03Z",
                "published_parsed": [
                    2025,
                    9,
                    27,
                    16,
                    9,
                    3,
                    5,
                    270,
                    0
                ],
                "title": "A Predictive and Synergistic Two-Layer Scheduling Framework for LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Predictive and Synergistic Two-Layer Scheduling Framework for LLM\n  Serving"
                },
                "summary": "LLM inference serving typically scales out with a two-tier architecture: a\ncluster router distributes requests to multiple inference engines, each of\nwhich then in turn performs its own internal scheduling. However, this commonly\nused paradigm suffers from critical, systemic inefficiency caused by the\ninformation gaps across two layers. At the cluster-layer, the router mainly\nrelies on lagging, coarse-grained metrics, such as average latency and queue\nlength to make decisions, resulting in \"decision lag\" that leads to suboptimal\nrequest routing. At the engine-layer, static heuristic scheduling policies\ncannot effectively handle the dynamic workloads, leading a poor balance between\nlatency and throughput. Besides, these gaps may cause SLO violations and\nresource waste, especially in heterogeneous cloud environments.\n  To bridge such gaps, we propose NexusSched, a cross-layer framework that\nshifts LLM serving system from reactive load balancing to predictive\norchestration. The core of NexusSched lies in a structurally-informed online\nperformance model that provides accurate, forward-looking per-step latency and\ncapacity estimations. This model empowers two key components. At the\nengine-layer, LENS performs SLO-aware, adaptive scheduling, dynamically\noptimizing batching to meet SLOs under real-time loads. At the cluster-layer,\nPRISM uses predictive signals to perform state-driven routing, maximizing\ncluster-wide performance and SLO attainment. Performance evaluations show that\nNexusSched improves SLO attainment by 43% on average and achieves up to 3x\nthroughput speedup in long-context and heterogeneous scenarios. Besides, we\nalso deploy NexusSched on FlowGPT's clusters to demonstrate its advantages in\nproduction environment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM inference serving typically scales out with a two-tier architecture: a\ncluster router distributes requests to multiple inference engines, each of\nwhich then in turn performs its own internal scheduling. However, this commonly\nused paradigm suffers from critical, systemic inefficiency caused by the\ninformation gaps across two layers. At the cluster-layer, the router mainly\nrelies on lagging, coarse-grained metrics, such as average latency and queue\nlength to make decisions, resulting in \"decision lag\" that leads to suboptimal\nrequest routing. At the engine-layer, static heuristic scheduling policies\ncannot effectively handle the dynamic workloads, leading a poor balance between\nlatency and throughput. Besides, these gaps may cause SLO violations and\nresource waste, especially in heterogeneous cloud environments.\n  To bridge such gaps, we propose NexusSched, a cross-layer framework that\nshifts LLM serving system from reactive load balancing to predictive\norchestration. The core of NexusSched lies in a structurally-informed online\nperformance model that provides accurate, forward-looking per-step latency and\ncapacity estimations. This model empowers two key components. At the\nengine-layer, LENS performs SLO-aware, adaptive scheduling, dynamically\noptimizing batching to meet SLOs under real-time loads. At the cluster-layer,\nPRISM uses predictive signals to perform state-driven routing, maximizing\ncluster-wide performance and SLO attainment. Performance evaluations show that\nNexusSched improves SLO attainment by 43% on average and achieves up to 3x\nthroughput speedup in long-context and heterogeneous scenarios. Besides, we\nalso deploy NexusSched on FlowGPT's clusters to demonstrate its advantages in\nproduction environment."
                },
                "authors": [
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Yuansheng Chen"
                    },
                    {
                        "name": "Xuan Mo"
                    },
                    {
                        "name": "Alex Xi"
                    },
                    {
                        "name": "Jialun Li"
                    },
                    {
                        "name": "WeiGang Wu"
                    }
                ],
                "author_detail": {
                    "name": "WeiGang Wu"
                },
                "author": "WeiGang Wu",
                "arxiv_comment": "Update the system name in the summary",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23384v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23384v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19136v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19136v2",
                "updated": "2025-10-01T09:32:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    9,
                    32,
                    15,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-23T15:20:40Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    20,
                    40,
                    1,
                    266,
                    0
                ],
                "title": "On the Soundness and Consistency of LLM Agents for Executing Test Cases\n  Written in Natural Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Soundness and Consistency of LLM Agents for Executing Test Cases\n  Written in Natural Language"
                },
                "summary": "The use of natural language (NL) test cases for validating graphical user\ninterface (GUI) applications is emerging as a promising direction to manually\nwritten executable test scripts, which are costly to develop and difficult to\nmaintain. Recent advances in large language models (LLMs) have opened the\npossibility of the direct execution of NL test cases by LLM agents. This paper\ninvestigates this direction, focusing on the impact on NL test case unsoundness\nand on test case execution consistency. NL test cases are inherently unsound,\nas they may yield false failures due to ambiguous instructions or unpredictable\nagent behaviour. Furthermore, repeated executions of the same NL test case may\nlead to inconsistent outcomes, undermining test reliability. To address these\nchallenges, we propose an algorithm for executing NL test cases with guardrail\nmechanisms and specialised agents that dynamically verify the correct execution\nof each test step. We introduce measures to evaluate the capabilities of LLMs\nin test execution and one measure to quantify execution consistency. We propose\na definition of weak unsoundness to characterise contexts in which NL test case\nexecution remains acceptable, with respect to the industrial quality levels Six\nSigma. Our experimental evaluation with eight publicly available LLMs, ranging\nfrom 3B to 70B parameters, demonstrates both the potential and current\nlimitations of current LLM agents for GUI testing. Our experiments show that\nMeta Llama 3.1 70B demonstrates acceptable capabilities in NL test case\nexecution with high execution consistency (above the level 3-sigma). We provide\nprototype tools, test suites, and results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of natural language (NL) test cases for validating graphical user\ninterface (GUI) applications is emerging as a promising direction to manually\nwritten executable test scripts, which are costly to develop and difficult to\nmaintain. Recent advances in large language models (LLMs) have opened the\npossibility of the direct execution of NL test cases by LLM agents. This paper\ninvestigates this direction, focusing on the impact on NL test case unsoundness\nand on test case execution consistency. NL test cases are inherently unsound,\nas they may yield false failures due to ambiguous instructions or unpredictable\nagent behaviour. Furthermore, repeated executions of the same NL test case may\nlead to inconsistent outcomes, undermining test reliability. To address these\nchallenges, we propose an algorithm for executing NL test cases with guardrail\nmechanisms and specialised agents that dynamically verify the correct execution\nof each test step. We introduce measures to evaluate the capabilities of LLMs\nin test execution and one measure to quantify execution consistency. We propose\na definition of weak unsoundness to characterise contexts in which NL test case\nexecution remains acceptable, with respect to the industrial quality levels Six\nSigma. Our experimental evaluation with eight publicly available LLMs, ranging\nfrom 3B to 70B parameters, demonstrates both the potential and current\nlimitations of current LLM agents for GUI testing. Our experiments show that\nMeta Llama 3.1 70B demonstrates acceptable capabilities in NL test case\nexecution with high execution consistency (above the level 3-sigma). We provide\nprototype tools, test suites, and results."
                },
                "authors": [
                    {
                        "name": "Sébastien Salva"
                    },
                    {
                        "name": "Redha Taguelmimt"
                    }
                ],
                "author_detail": {
                    "name": "Redha Taguelmimt"
                },
                "author": "Redha Taguelmimt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19136v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19136v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.4; D.2.5; F.3.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18706v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18706v3",
                "updated": "2025-10-01T09:23:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    9,
                    23,
                    7,
                    2,
                    274,
                    0
                ],
                "published": "2025-05-24T13:55:38Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    13,
                    55,
                    38,
                    5,
                    144,
                    0
                ],
                "title": "Steering LLM Reasoning Through Bias-Only Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steering LLM Reasoning Through Bias-Only Adaptation"
                },
                "summary": "We show that training a single $d$-dimensional steering vector per layer with\nreinforcement learning, while freezing all base weights, matches the accuracy\nof fully RL-tuned reasoning models on mathematical-reasoning tasks. On an 8\nbillion-parameter model this adds only $\\approx 0.0016\\%$ additional parameters\nand reproduces performance across a range of base models and\nmathematical-reasoning benchmarks. These results tighten the upper bound on the\nparameter budget required for high-level chain-of-thought reasoning, indicating\nthat millions of adapter weights are unnecessary. The minimal trainable\nfootprint reduces optimizer memory and inter-GPU communication, lowering the\noverall cost of fine-tuning. Moreover, a logit-lens analysis shows that the\nlearned vectors amplify coherent token directions, providing clearer insight\ninto the model's internal computations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We show that training a single $d$-dimensional steering vector per layer with\nreinforcement learning, while freezing all base weights, matches the accuracy\nof fully RL-tuned reasoning models on mathematical-reasoning tasks. On an 8\nbillion-parameter model this adds only $\\approx 0.0016\\%$ additional parameters\nand reproduces performance across a range of base models and\nmathematical-reasoning benchmarks. These results tighten the upper bound on the\nparameter budget required for high-level chain-of-thought reasoning, indicating\nthat millions of adapter weights are unnecessary. The minimal trainable\nfootprint reduces optimizer memory and inter-GPU communication, lowering the\noverall cost of fine-tuning. Moreover, a logit-lens analysis shows that the\nlearned vectors amplify coherent token directions, providing clearer insight\ninto the model's internal computations."
                },
                "authors": [
                    {
                        "name": "Viacheslav Sinii"
                    },
                    {
                        "name": "Alexey Gorbatovski"
                    },
                    {
                        "name": "Artem Cherepanov"
                    },
                    {
                        "name": "Boris Shaposhnikov"
                    },
                    {
                        "name": "Nikita Balagansky"
                    },
                    {
                        "name": "Daniil Gavrilov"
                    }
                ],
                "author_detail": {
                    "name": "Daniil Gavrilov"
                },
                "author": "Daniil Gavrilov",
                "arxiv_comment": "EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18706v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18706v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23619v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23619v2",
                "updated": "2025-10-01T08:57:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    8,
                    57,
                    37,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-28T03:49:32Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    3,
                    49,
                    32,
                    6,
                    271,
                    0
                ],
                "title": "Reasoning Scaffolding: Distilling the Flow of Thought from LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Scaffolding: Distilling the Flow of Thought from LLMs"
                },
                "summary": "The prevailing approach to distilling reasoning from Large Language Models\n(LLMs)-behavioral cloning from textual rationales-is fundamentally limited. It\nteaches Small Language Models (SLMs) to mimic surface-level patterns rather\nthan the underlying algorithmic structure of thought, resulting in a critical\nlack of logical robustness. We argue that instead of cloning text, distillation\nshould transfer this algorithmic structure directly. We introduce Reasoning\nScaffolding}, a framework that reframes reasoning as a structured generation\nprocess. Our method first abstracts the teacher's thought process into a\nsequence of discrete, interpretable semantic signals (e.g., Contrast, Addition)\nthat act as a scaffold. The student model is then trained via a multi-task\nobjective to both (1)predict the next semantic signal, anticipating the\nreasoning flow, and (2)generate the corresponding step, conditioned on that\nsignal. This multi-task scheme acts as a powerful regularizer, compelling the\nstudent to internalize the computational patterns of coherent reasoning. On a\nsuite of challenging reasoning benchmarks, our method significantly outperforms\nstate-of-the-art distillation in both accuracy and logical consistency,\nproviding a path towards creating smaller models that are genuine reasoners,\nnot just fluent mimics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The prevailing approach to distilling reasoning from Large Language Models\n(LLMs)-behavioral cloning from textual rationales-is fundamentally limited. It\nteaches Small Language Models (SLMs) to mimic surface-level patterns rather\nthan the underlying algorithmic structure of thought, resulting in a critical\nlack of logical robustness. We argue that instead of cloning text, distillation\nshould transfer this algorithmic structure directly. We introduce Reasoning\nScaffolding}, a framework that reframes reasoning as a structured generation\nprocess. Our method first abstracts the teacher's thought process into a\nsequence of discrete, interpretable semantic signals (e.g., Contrast, Addition)\nthat act as a scaffold. The student model is then trained via a multi-task\nobjective to both (1)predict the next semantic signal, anticipating the\nreasoning flow, and (2)generate the corresponding step, conditioned on that\nsignal. This multi-task scheme acts as a powerful regularizer, compelling the\nstudent to internalize the computational patterns of coherent reasoning. On a\nsuite of challenging reasoning benchmarks, our method significantly outperforms\nstate-of-the-art distillation in both accuracy and logical consistency,\nproviding a path towards creating smaller models that are genuine reasoners,\nnot just fluent mimics."
                },
                "authors": [
                    {
                        "name": "Xiangyu Wen"
                    },
                    {
                        "name": "Junhua Huang"
                    },
                    {
                        "name": "Zeju Li"
                    },
                    {
                        "name": "Min Li"
                    },
                    {
                        "name": "Jianyuan Zhong"
                    },
                    {
                        "name": "Zhijian Xu"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    },
                    {
                        "name": "Yongxiang Huang"
                    },
                    {
                        "name": "Qiang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Qiang Xu"
                },
                "author": "Qiang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23619v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23619v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06608v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06608v3",
                "updated": "2025-10-01T08:37:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    8,
                    37,
                    7,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-08T12:26:31Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    12,
                    26,
                    31,
                    0,
                    251,
                    0
                ],
                "title": "Small Vectors, Big Effects: A Mechanistic Study of RL-Induced Reasoning\n  via Steering Vectors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small Vectors, Big Effects: A Mechanistic Study of RL-Induced Reasoning\n  via Steering Vectors"
                },
                "summary": "The mechanisms by which reasoning training reshapes LLMs' internal\ncomputations remain unclear. We study lightweight steering vectors inserted\ninto the base model's residual stream and trained with a reinforcement-learning\nobjective. These vectors match full fine-tuning performance while preserving\nthe interpretability of small, additive interventions. Using logit-lens\nreadouts and path-patching analyses on two models, we find that (i) the\nlast-layer steering vector acts like a token-substitution bias concentrated on\nthe first generated token, consistently boosting tokens such as \"To\" and\n\"Step\"; (ii) the penultimate-layer vector leaves attention patterns largely\nintact and instead operates through the MLP and unembedding, preferentially\nup-weighting process words and structure symbols; and (iii) middle layers\nde-emphasize non-English tokens. Next, we show that a SAE isolates features\nassociated with correct generations. We also show that steering vectors (i)\ntransfer to other models, (ii) combine across layers when trained in isolation,\nand (iii) concentrate magnitude on meaningful prompt segments under adaptive\ntoken-wise scaling. Taken together, these results deepen understanding of how\ntrained steering vectors shape computation and should inform future work in\nactivation engineering and the study of reasoning models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The mechanisms by which reasoning training reshapes LLMs' internal\ncomputations remain unclear. We study lightweight steering vectors inserted\ninto the base model's residual stream and trained with a reinforcement-learning\nobjective. These vectors match full fine-tuning performance while preserving\nthe interpretability of small, additive interventions. Using logit-lens\nreadouts and path-patching analyses on two models, we find that (i) the\nlast-layer steering vector acts like a token-substitution bias concentrated on\nthe first generated token, consistently boosting tokens such as \"To\" and\n\"Step\"; (ii) the penultimate-layer vector leaves attention patterns largely\nintact and instead operates through the MLP and unembedding, preferentially\nup-weighting process words and structure symbols; and (iii) middle layers\nde-emphasize non-English tokens. Next, we show that a SAE isolates features\nassociated with correct generations. We also show that steering vectors (i)\ntransfer to other models, (ii) combine across layers when trained in isolation,\nand (iii) concentrate magnitude on meaningful prompt segments under adaptive\ntoken-wise scaling. Taken together, these results deepen understanding of how\ntrained steering vectors shape computation and should inform future work in\nactivation engineering and the study of reasoning models."
                },
                "authors": [
                    {
                        "name": "Viacheslav Sinii"
                    },
                    {
                        "name": "Nikita Balagansky"
                    },
                    {
                        "name": "Gleb Gerasimov"
                    },
                    {
                        "name": "Daniil Laptev"
                    },
                    {
                        "name": "Yaroslav Aksenov"
                    },
                    {
                        "name": "Vadim Kurochkin"
                    },
                    {
                        "name": "Alexey Gorbatovski"
                    },
                    {
                        "name": "Boris Shaposhnikov"
                    },
                    {
                        "name": "Daniil Gavrilov"
                    }
                ],
                "author_detail": {
                    "name": "Daniil Gavrilov"
                },
                "author": "Daniil Gavrilov",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06608v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06608v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18980v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18980v2",
                "updated": "2025-10-01T07:49:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    7,
                    49,
                    58,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-23T13:30:03Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    13,
                    30,
                    3,
                    1,
                    266,
                    0
                ],
                "title": "From latent factors to language: a user study on LLM-generated\n  explanations for an inherently interpretable matrix-based recommender system",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From latent factors to language: a user study on LLM-generated\n  explanations for an inherently interpretable matrix-based recommender system"
                },
                "summary": "We investigate whether large language models (LLMs) can generate effective,\nuser-facing explanations from a mathematically interpretable recommendation\nmodel. The model is based on constrained matrix factorization, where user types\nare explicitly represented and predicted item scores share the same scale as\nobserved ratings, making the model's internal representations and predicted\nscores directly interpretable. This structure is translated into natural\nlanguage explanations using carefully designed LLM prompts. Many works in\nexplainable AI rely on automatic evaluation metrics, which often fail to\ncapture users' actual needs and perceptions. In contrast, we adopt a\nuser-centered approach: we conduct a study with 326 participants who assessed\nthe quality of the explanations across five key dimensions-transparency,\neffectiveness, persuasion, trust, and satisfaction-as well as the\nrecommendations themselves. To evaluate how different explanation strategies\nare perceived, we generate multiple explanation types from the same underlying\nmodel, varying the input information provided to the LLM. Our analysis reveals\nthat all explanation types are generally well received, with moderate\nstatistical differences between strategies. User comments further underscore\nhow participants react to each type of explanation, offering complementary\ninsights beyond the quantitative results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate whether large language models (LLMs) can generate effective,\nuser-facing explanations from a mathematically interpretable recommendation\nmodel. The model is based on constrained matrix factorization, where user types\nare explicitly represented and predicted item scores share the same scale as\nobserved ratings, making the model's internal representations and predicted\nscores directly interpretable. This structure is translated into natural\nlanguage explanations using carefully designed LLM prompts. Many works in\nexplainable AI rely on automatic evaluation metrics, which often fail to\ncapture users' actual needs and perceptions. In contrast, we adopt a\nuser-centered approach: we conduct a study with 326 participants who assessed\nthe quality of the explanations across five key dimensions-transparency,\neffectiveness, persuasion, trust, and satisfaction-as well as the\nrecommendations themselves. To evaluate how different explanation strategies\nare perceived, we generate multiple explanation types from the same underlying\nmodel, varying the input information provided to the LLM. Our analysis reveals\nthat all explanation types are generally well received, with moderate\nstatistical differences between strategies. User comments further underscore\nhow participants react to each type of explanation, offering complementary\ninsights beyond the quantitative results."
                },
                "authors": [
                    {
                        "name": "Maxime Manderlier"
                    },
                    {
                        "name": "Fabian Lecron"
                    },
                    {
                        "name": "Olivier Vu Thanh"
                    },
                    {
                        "name": "Nicolas Gillis"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Gillis"
                },
                "author": "Nicolas Gillis",
                "arxiv_journal_ref": "In Proceedings of the 12th Joint Workshop on Interfaces and Human\n  Decision Making for Recommender Systems (IntRS 2025) co-located with 19th ACM\n  Conference on Recommender Systems (RecSys 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18980v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18980v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.3; H.5.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17630v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17630v3",
                "updated": "2025-10-01T07:18:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    7,
                    18,
                    47,
                    2,
                    274,
                    0
                ],
                "published": "2025-05-23T08:41:45Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    8,
                    41,
                    45,
                    4,
                    143,
                    0
                ],
                "title": "GIM: Improved Interpretability for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GIM: Improved Interpretability for Large Language Models"
                },
                "summary": "Ensuring faithful interpretability in large language models is imperative for\ntrustworthy and reliable AI. A key obstacle is self-repair, a phenomenon where\nnetworks compensate for reduced signal in one component by amplifying others,\nmasking the true importance of the ablated component. While prior work\nattributes self-repair to layer normalization and back-up components that\ncompensate for ablated components, we identify a novel form occurring within\nthe attention mechanism, where softmax redistribution conceals the influence of\nimportant attention scores. This leads traditional ablation and gradient-based\nmethods to underestimate the significance of all components contributing to\nthese attention scores. We introduce Gradient Interaction Modifications (GIM),\na technique that accounts for self-repair during backpropagation. Extensive\nexperiments across multiple large language models (Gemma 2B/9B, LLAMA 1B/3B/8B,\nQwen 1.5B/3B) and diverse tasks demonstrate that GIM significantly improves\nfaithfulness over existing circuit identification and feature attribution\nmethods. Our work is a significant step toward better understanding the inner\nmechanisms of LLMs, which is crucial for improving them and ensuring their\nsafety. Our code is available at https://github.com/JoakimEdin/gim.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring faithful interpretability in large language models is imperative for\ntrustworthy and reliable AI. A key obstacle is self-repair, a phenomenon where\nnetworks compensate for reduced signal in one component by amplifying others,\nmasking the true importance of the ablated component. While prior work\nattributes self-repair to layer normalization and back-up components that\ncompensate for ablated components, we identify a novel form occurring within\nthe attention mechanism, where softmax redistribution conceals the influence of\nimportant attention scores. This leads traditional ablation and gradient-based\nmethods to underestimate the significance of all components contributing to\nthese attention scores. We introduce Gradient Interaction Modifications (GIM),\na technique that accounts for self-repair during backpropagation. Extensive\nexperiments across multiple large language models (Gemma 2B/9B, LLAMA 1B/3B/8B,\nQwen 1.5B/3B) and diverse tasks demonstrate that GIM significantly improves\nfaithfulness over existing circuit identification and feature attribution\nmethods. Our work is a significant step toward better understanding the inner\nmechanisms of LLMs, which is crucial for improving them and ensuring their\nsafety. Our code is available at https://github.com/JoakimEdin/gim."
                },
                "authors": [
                    {
                        "name": "Joakim Edin"
                    },
                    {
                        "name": "Róbert Csordás"
                    },
                    {
                        "name": "Tuukka Ruotsalo"
                    },
                    {
                        "name": "Zhengxuan Wu"
                    },
                    {
                        "name": "Maria Maistro"
                    },
                    {
                        "name": "Casper L. Christensen"
                    },
                    {
                        "name": "Jing Huang"
                    },
                    {
                        "name": "Lars Maaløe"
                    }
                ],
                "author_detail": {
                    "name": "Lars Maaløe"
                },
                "author": "Lars Maaløe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17630v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17630v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04215v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04215v2",
                "updated": "2025-10-01T07:14:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    7,
                    14,
                    50,
                    2,
                    274,
                    0
                ],
                "published": "2024-05-07T11:27:13Z",
                "published_parsed": [
                    2024,
                    5,
                    7,
                    11,
                    27,
                    13,
                    1,
                    128,
                    0
                ],
                "title": "NL2Plan: Robust LLM-Driven Planning from Minimal Text Descriptions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NL2Plan: Robust LLM-Driven Planning from Minimal Text Descriptions"
                },
                "summary": "Classical planners are powerful systems, but modeling tasks in input formats\nsuch as PDDL is tedious and error-prone. In contrast, planning with Large\nLanguage Models (LLMs) allows for almost any input text, but offers no\nguarantees on plan quality or even soundness. In an attempt to merge the best\nof these two approaches, some work has begun to use LLMs to automate parts of\nthe PDDL creation process. However, these methods still require various degrees\nof expert input or domain-specific adaptations. We present NL2Plan, the first\nfully automatic system for generating complete PDDL tasks from minimal natural\nlanguage descriptions. NL2Plan uses an LLM to incrementally extract the\nnecessary information from the short text input before creating a complete PDDL\ndescription of both the domain and the problem which is finally solved by a\nclassical planner. We evaluate NL2Plan on seven planning domains, five of which\nare novel and thus not in the LLM training data, and find that NL2Plan\noutperforms directly generating the files with an LLM+validator combination. As\nsuch, NL2Plan is a powerful tool for assistive PDDL modeling and a step towards\nsolving natural language planning task with interpretability and guarantees.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Classical planners are powerful systems, but modeling tasks in input formats\nsuch as PDDL is tedious and error-prone. In contrast, planning with Large\nLanguage Models (LLMs) allows for almost any input text, but offers no\nguarantees on plan quality or even soundness. In an attempt to merge the best\nof these two approaches, some work has begun to use LLMs to automate parts of\nthe PDDL creation process. However, these methods still require various degrees\nof expert input or domain-specific adaptations. We present NL2Plan, the first\nfully automatic system for generating complete PDDL tasks from minimal natural\nlanguage descriptions. NL2Plan uses an LLM to incrementally extract the\nnecessary information from the short text input before creating a complete PDDL\ndescription of both the domain and the problem which is finally solved by a\nclassical planner. We evaluate NL2Plan on seven planning domains, five of which\nare novel and thus not in the LLM training data, and find that NL2Plan\noutperforms directly generating the files with an LLM+validator combination. As\nsuch, NL2Plan is a powerful tool for assistive PDDL modeling and a step towards\nsolving natural language planning task with interpretability and guarantees."
                },
                "authors": [
                    {
                        "name": "Elliot Gestrin"
                    },
                    {
                        "name": "Marco Kuhlmann"
                    },
                    {
                        "name": "Jendrik Seipp"
                    }
                ],
                "author_detail": {
                    "name": "Jendrik Seipp"
                },
                "author": "Jendrik Seipp",
                "arxiv_comment": "Accepted for the ICAPS 2024 Workshop on Human-Aware and Explainable\n  Planning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.04215v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04215v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19552v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19552v2",
                "updated": "2025-10-01T06:54:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    6,
                    54,
                    44,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-23T20:25:53Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    20,
                    25,
                    53,
                    1,
                    266,
                    0
                ],
                "title": "iFinder: Structured Zero-Shot Vision-Based LLM Grounding for Dash-Cam\n  Video Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "iFinder: Structured Zero-Shot Vision-Based LLM Grounding for Dash-Cam\n  Video Reasoning"
                },
                "summary": "Grounding large language models (LLMs) in domain-specific tasks like post-hoc\ndash-cam driving video analysis is challenging due to their general-purpose\ntraining and lack of structured inductive biases. As vision is often the sole\nmodality available for such analysis (i.e., no LiDAR, GPS, etc.), existing\nvideo-based vision-language models (V-VLMs) struggle with spatial reasoning,\ncausal inference, and explainability of events in the input video. To this end,\nwe introduce iFinder, a structured semantic grounding framework that decouples\nperception from reasoning by translating dash-cam videos into a hierarchical,\ninterpretable data structure for LLMs. iFinder operates as a modular,\ntraining-free pipeline that employs pretrained vision models to extract\ncritical cues -- object pose, lane positions, and object trajectories -- which\nare hierarchically organized into frame- and video-level structures. Combined\nwith a three-block prompting strategy, it enables step-wise, grounded reasoning\nfor the LLM to refine a peer V-VLM's outputs and provide accurate reasoning.\nEvaluations on four public dash-cam video benchmarks show that iFinder's\nproposed grounding with domain-specific cues, especially object orientation and\nglobal context, significantly outperforms end-to-end V-VLMs on four zero-shot\ndriving benchmarks, with up to 39% gains in accident reasoning accuracy. By\ngrounding LLMs with driving domain-specific representations, iFinder offers a\nzero-shot, interpretable, and reliable alternative to end-to-end V-VLMs for\npost-hoc driving video understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grounding large language models (LLMs) in domain-specific tasks like post-hoc\ndash-cam driving video analysis is challenging due to their general-purpose\ntraining and lack of structured inductive biases. As vision is often the sole\nmodality available for such analysis (i.e., no LiDAR, GPS, etc.), existing\nvideo-based vision-language models (V-VLMs) struggle with spatial reasoning,\ncausal inference, and explainability of events in the input video. To this end,\nwe introduce iFinder, a structured semantic grounding framework that decouples\nperception from reasoning by translating dash-cam videos into a hierarchical,\ninterpretable data structure for LLMs. iFinder operates as a modular,\ntraining-free pipeline that employs pretrained vision models to extract\ncritical cues -- object pose, lane positions, and object trajectories -- which\nare hierarchically organized into frame- and video-level structures. Combined\nwith a three-block prompting strategy, it enables step-wise, grounded reasoning\nfor the LLM to refine a peer V-VLM's outputs and provide accurate reasoning.\nEvaluations on four public dash-cam video benchmarks show that iFinder's\nproposed grounding with domain-specific cues, especially object orientation and\nglobal context, significantly outperforms end-to-end V-VLMs on four zero-shot\ndriving benchmarks, with up to 39% gains in accident reasoning accuracy. By\ngrounding LLMs with driving domain-specific representations, iFinder offers a\nzero-shot, interpretable, and reliable alternative to end-to-end V-VLMs for\npost-hoc driving video understanding."
                },
                "authors": [
                    {
                        "name": "Manyi Yao"
                    },
                    {
                        "name": "Bingbing Zhuang"
                    },
                    {
                        "name": "Sparsh Garg"
                    },
                    {
                        "name": "Amit Roy-Chowdhury"
                    },
                    {
                        "name": "Christian Shelton"
                    },
                    {
                        "name": "Manmohan Chandraker"
                    },
                    {
                        "name": "Abhishek Aich"
                    }
                ],
                "author_detail": {
                    "name": "Abhishek Aich"
                },
                "author": "Abhishek Aich",
                "arxiv_comment": "Accepted at NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19552v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19552v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10360v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10360v3",
                "updated": "2025-10-01T06:37:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    6,
                    37,
                    1,
                    2,
                    274,
                    0
                ],
                "published": "2025-08-14T05:59:21Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    5,
                    59,
                    21,
                    3,
                    226,
                    0
                ],
                "title": "A dataset and model for recognition of audiologically relevant\n  environments for hearing aids: AHEAD-DS and YAMNet+",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A dataset and model for recognition of audiologically relevant\n  environments for hearing aids: AHEAD-DS and YAMNet+"
                },
                "summary": "Scene recognition of audiologically relevant environments is important for\nhearing aids; however, it is challenging, in part because of the limitations of\nexisting datasets. Datasets often lack public accessibility, completeness, or\naudiologically relevant labels, hindering systematic comparison of machine\nlearning models. Deploying these models on resource-constrained edge devices\npresents another challenge. Our solution is two-fold: we leverage several open\nsource datasets to create AHEAD-DS, a dataset designed for scene recognition of\naudiologically relevant environments, and introduce YAMNet+, a sound\nrecognition model. AHEAD-DS aims to provide a standardised, publicly available\ndataset with consistent labels relevant to hearing aids, facilitating model\ncomparison. YAMNet+ is designed for deployment on edge devices like smartphones\nconnected to hearing devices, such as hearing aids and wireless earphones with\nhearing aid functionality; serving as a baseline model for sound-based scene\nrecognition. YAMNet+ achieved a mean average precision of 0.83 and accuracy of\n0.93 on the testing set of AHEAD-DS across fourteen categories of\naudiologically relevant environments. We found that applying transfer learning\nfrom the pretrained YAMNet model was essential. We demonstrated real-time\nsound-based scene recognition capabilities on edge devices by deploying YAMNet+\nto an Android smartphone. Even with a Google Pixel 3 (a phone with modest\nspecifications, released in 2018), the model processes audio with approximately\n50ms of latency to load the model, and an approximate linear increase of 30ms\nper 1 second of audio. Our website and code\nhttps://github.com/Australian-Future-Hearing-Initiative .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scene recognition of audiologically relevant environments is important for\nhearing aids; however, it is challenging, in part because of the limitations of\nexisting datasets. Datasets often lack public accessibility, completeness, or\naudiologically relevant labels, hindering systematic comparison of machine\nlearning models. Deploying these models on resource-constrained edge devices\npresents another challenge. Our solution is two-fold: we leverage several open\nsource datasets to create AHEAD-DS, a dataset designed for scene recognition of\naudiologically relevant environments, and introduce YAMNet+, a sound\nrecognition model. AHEAD-DS aims to provide a standardised, publicly available\ndataset with consistent labels relevant to hearing aids, facilitating model\ncomparison. YAMNet+ is designed for deployment on edge devices like smartphones\nconnected to hearing devices, such as hearing aids and wireless earphones with\nhearing aid functionality; serving as a baseline model for sound-based scene\nrecognition. YAMNet+ achieved a mean average precision of 0.83 and accuracy of\n0.93 on the testing set of AHEAD-DS across fourteen categories of\naudiologically relevant environments. We found that applying transfer learning\nfrom the pretrained YAMNet model was essential. We demonstrated real-time\nsound-based scene recognition capabilities on edge devices by deploying YAMNet+\nto an Android smartphone. Even with a Google Pixel 3 (a phone with modest\nspecifications, released in 2018), the model processes audio with approximately\n50ms of latency to load the model, and an approximate linear increase of 30ms\nper 1 second of audio. Our website and code\nhttps://github.com/Australian-Future-Hearing-Initiative ."
                },
                "authors": [
                    {
                        "name": "Henry Zhong"
                    },
                    {
                        "name": "Jörg M. Buchholz"
                    },
                    {
                        "name": "Julian Maclaren"
                    },
                    {
                        "name": "Simon Carlile"
                    },
                    {
                        "name": "Richard Lyon"
                    }
                ],
                "author_detail": {
                    "name": "Richard Lyon"
                },
                "author": "Richard Lyon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10360v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10360v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20097v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20097v2",
                "updated": "2025-10-01T06:09:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    6,
                    9,
                    25,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-24T13:20:37Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    13,
                    20,
                    37,
                    2,
                    267,
                    0
                ],
                "title": "Integrated Framework for LLM Evaluation with Answer Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrated Framework for LLM Evaluation with Answer Generation"
                },
                "summary": "Reliable evaluation of large language models is essential to ensure their\napplicability in practical scenarios. Traditional benchmark-based evaluation\nmethods often rely on fixed reference answers, limiting their ability to\ncapture important qualitative aspects of generated responses. To address these\nshortcomings, we propose an integrated evaluation framework called\n\\textit{self-refining descriptive evaluation with expert-driven diagnostics},\nSPEED, which utilizes specialized functional experts to perform comprehensive,\ndescriptive analyses of model outputs. Unlike conventional approaches, SPEED\nactively incorporates expert feedback across multiple dimensions, including\nhallucination detection, toxicity assessment, and lexical-contextual\nappropriateness. Experimental results demonstrate that SPEED achieves robust\nand consistent evaluation performance across diverse domains and datasets.\nAdditionally, by employing relatively compact expert models, SPEED demonstrates\nsuperior resource efficiency compared to larger-scale evaluators. These\nfindings illustrate that SPEED significantly enhances fairness and\ninterpretability in LLM evaluations, offering a promising alternative to\nexisting evaluation methodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable evaluation of large language models is essential to ensure their\napplicability in practical scenarios. Traditional benchmark-based evaluation\nmethods often rely on fixed reference answers, limiting their ability to\ncapture important qualitative aspects of generated responses. To address these\nshortcomings, we propose an integrated evaluation framework called\n\\textit{self-refining descriptive evaluation with expert-driven diagnostics},\nSPEED, which utilizes specialized functional experts to perform comprehensive,\ndescriptive analyses of model outputs. Unlike conventional approaches, SPEED\nactively incorporates expert feedback across multiple dimensions, including\nhallucination detection, toxicity assessment, and lexical-contextual\nappropriateness. Experimental results demonstrate that SPEED achieves robust\nand consistent evaluation performance across diverse domains and datasets.\nAdditionally, by employing relatively compact expert models, SPEED demonstrates\nsuperior resource efficiency compared to larger-scale evaluators. These\nfindings illustrate that SPEED significantly enhances fairness and\ninterpretability in LLM evaluations, offering a promising alternative to\nexisting evaluation methodologies."
                },
                "authors": [
                    {
                        "name": "Sujeong Lee"
                    },
                    {
                        "name": "Hayoung Lee"
                    },
                    {
                        "name": "Seongsoo Heo"
                    },
                    {
                        "name": "Wonik Choi"
                    }
                ],
                "author_detail": {
                    "name": "Wonik Choi"
                },
                "author": "Wonik Choi",
                "arxiv_comment": "16pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20097v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20097v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11895v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11895v3",
                "updated": "2025-10-01T05:59:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    5,
                    59,
                    16,
                    2,
                    274,
                    0
                ],
                "published": "2025-03-14T21:53:12Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    21,
                    53,
                    12,
                    4,
                    73,
                    0
                ],
                "title": "Resolving UnderEdit & OverEdit with Iterative & Neighbor-Assisted Model\n  Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resolving UnderEdit & OverEdit with Iterative & Neighbor-Assisted Model\n  Editing"
                },
                "summary": "Large Language Models (LLMs) are widely deployed in downstream tasks, but\nkeeping their knowledge up-to-date via retraining or fine-tuning is often\ncomputationally expensive. Model editing provides a more efficient alternative\nby updating a targeted subset of parameters, which often follows the\nlocate-and-edit paradigm. Despite this efficiency, existing methods are\nlimited: edits may fail to inject knowledge (UnderEdit) or unintentionally\ndisrupt unrelated neighboring knowledge (OverEdit). To address these\nchallenges, we propose two complementary methods: iterative model editing,\nwhich applies successive edits to mitigate UnderEdit, and neighbor-assisted\nmodel editing, which incorporates neighboring knowledge during editing to\nreduce OverEdit. Our extensive experiments show that these techniques improve\nediting performance across multiple LLMs, algorithms, and benchmarks, reducing\nUnderEdit by up to 38 percentage points and OverEdit by up to 6, while\nremaining broadly applicable to any locate-and-edit method. We release our code\nat https://github.com/bhimanbaghel/ResolveUnderOverEdit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely deployed in downstream tasks, but\nkeeping their knowledge up-to-date via retraining or fine-tuning is often\ncomputationally expensive. Model editing provides a more efficient alternative\nby updating a targeted subset of parameters, which often follows the\nlocate-and-edit paradigm. Despite this efficiency, existing methods are\nlimited: edits may fail to inject knowledge (UnderEdit) or unintentionally\ndisrupt unrelated neighboring knowledge (OverEdit). To address these\nchallenges, we propose two complementary methods: iterative model editing,\nwhich applies successive edits to mitigate UnderEdit, and neighbor-assisted\nmodel editing, which incorporates neighboring knowledge during editing to\nreduce OverEdit. Our extensive experiments show that these techniques improve\nediting performance across multiple LLMs, algorithms, and benchmarks, reducing\nUnderEdit by up to 38 percentage points and OverEdit by up to 6, while\nremaining broadly applicable to any locate-and-edit method. We release our code\nat https://github.com/bhimanbaghel/ResolveUnderOverEdit."
                },
                "authors": [
                    {
                        "name": "Bhiman Kumar Baghel"
                    },
                    {
                        "name": "Emma Jordan"
                    },
                    {
                        "name": "Zheyuan Ryan Shi"
                    },
                    {
                        "name": "Xiang Lorraine Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Lorraine Li"
                },
                "author": "Xiang Lorraine Li",
                "arxiv_comment": "Accepted at EMNLP 2025 as Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11895v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11895v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21102v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21102v3",
                "updated": "2025-10-01T05:50:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    5,
                    50,
                    1,
                    2,
                    274,
                    0
                ],
                "published": "2024-12-30T17:25:58Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    17,
                    25,
                    58,
                    0,
                    365,
                    0
                ],
                "title": "Exploring and Controlling Diversity in LLM-Agent Conversation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring and Controlling Diversity in LLM-Agent Conversation"
                },
                "summary": "Controlling diversity in LLM-agent simulations is essential for balancing\nstability in structured tasks with variability in open-ended interactions.\nHowever, we observe that dialogue diversity tends to degrade over long-term\nsimulations. To explore the role of prompt design in this phenomenon, we\nmodularized the utterance generation prompt and found that reducing contextual\ninformation leads to more diverse outputs. Based on this insight, we propose\nAdaptive Prompt Pruning (APP), a novel method that allows users to control\ndiversity via a single parameter, lambda. APP dynamically prunes prompt\nsegments based on attention scores and is compatible with existing diversity\ncontrol methods. We demonstrate that APP effectively modulates diversity\nthrough extensive experiments and propose a method to balance the control\ntrade-offs. Our analysis reveals that all prompt components impose constraints\non diversity, with the Memory being the most influential. Additionally,\nhigh-attention contents consistently suppress output diversity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controlling diversity in LLM-agent simulations is essential for balancing\nstability in structured tasks with variability in open-ended interactions.\nHowever, we observe that dialogue diversity tends to degrade over long-term\nsimulations. To explore the role of prompt design in this phenomenon, we\nmodularized the utterance generation prompt and found that reducing contextual\ninformation leads to more diverse outputs. Based on this insight, we propose\nAdaptive Prompt Pruning (APP), a novel method that allows users to control\ndiversity via a single parameter, lambda. APP dynamically prunes prompt\nsegments based on attention scores and is compatible with existing diversity\ncontrol methods. We demonstrate that APP effectively modulates diversity\nthrough extensive experiments and propose a method to balance the control\ntrade-offs. Our analysis reveals that all prompt components impose constraints\non diversity, with the Memory being the most influential. Additionally,\nhigh-attention contents consistently suppress output diversity."
                },
                "authors": [
                    {
                        "name": "KuanChao Chu"
                    },
                    {
                        "name": "Yi-Pei Chen"
                    },
                    {
                        "name": "Hideki Nakayama"
                    }
                ],
                "author_detail": {
                    "name": "Hideki Nakayama"
                },
                "author": "Hideki Nakayama",
                "arxiv_comment": "EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21102v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21102v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15194v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15194v2",
                "updated": "2025-10-01T05:29:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    5,
                    29,
                    42,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-18T17:50:04Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    50,
                    4,
                    3,
                    261,
                    0
                ],
                "title": "Evolving Language Models without Labels: Majority Drives Selection,\n  Novelty Promotes Variation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolving Language Models without Labels: Majority Drives Selection,\n  Novelty Promotes Variation"
                },
                "summary": "Large language models (LLMs) are increasingly trained with reinforcement\nlearning from verifiable rewards (RLVR), yet real-world deployment demands\nmodels that can self-improve without labels or external judges. Existing\nself-improvement approaches primarily rely on self-confirmation signals (e.g.,\nconfidence, entropy, or consistency) to generate rewards. This reliance drives\nmodels toward over-confident, majority-favored solutions, causing an entropy\ncollapse that degrades pass@n and reasoning complexity. To address this, we\npropose EVOL-RL, a label-free framework that mirrors the evolutionary principle\nof balancing selection with variation. Concretely, EVOL-RL retains the\nmajority-voted answer as an anchor for stability, but adds a novelty-aware\nreward that scores each sampled solution by how different its reasoning is from\nother concurrently generated responses. This majority-for-stability +\nnovelty-for-exploration rule mirrors the variation-selection principle:\nselection prevents drift, while novelty prevents collapse. Evaluation results\nshow that EVOL-RL consistently outperforms the majority-only baseline; e.g.,\ntraining on label-free AIME24 lifts Qwen3-4B-Base AIME25 pass@1 from baseline's\n4.6% to 16.4%, and pass@16 from 18.5% to 37.9%. EVOL-RL not only prevents\nin-domain diversity collapse but also improves out-of-domain generalization\n(from math reasoning to broader tasks, e.g., GPQA, MMLU-Pro, and BBEH). The\ncode is available at: https://github.com/YujunZhou/EVOL-RL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly trained with reinforcement\nlearning from verifiable rewards (RLVR), yet real-world deployment demands\nmodels that can self-improve without labels or external judges. Existing\nself-improvement approaches primarily rely on self-confirmation signals (e.g.,\nconfidence, entropy, or consistency) to generate rewards. This reliance drives\nmodels toward over-confident, majority-favored solutions, causing an entropy\ncollapse that degrades pass@n and reasoning complexity. To address this, we\npropose EVOL-RL, a label-free framework that mirrors the evolutionary principle\nof balancing selection with variation. Concretely, EVOL-RL retains the\nmajority-voted answer as an anchor for stability, but adds a novelty-aware\nreward that scores each sampled solution by how different its reasoning is from\nother concurrently generated responses. This majority-for-stability +\nnovelty-for-exploration rule mirrors the variation-selection principle:\nselection prevents drift, while novelty prevents collapse. Evaluation results\nshow that EVOL-RL consistently outperforms the majority-only baseline; e.g.,\ntraining on label-free AIME24 lifts Qwen3-4B-Base AIME25 pass@1 from baseline's\n4.6% to 16.4%, and pass@16 from 18.5% to 37.9%. EVOL-RL not only prevents\nin-domain diversity collapse but also improves out-of-domain generalization\n(from math reasoning to broader tasks, e.g., GPQA, MMLU-Pro, and BBEH). The\ncode is available at: https://github.com/YujunZhou/EVOL-RL."
                },
                "authors": [
                    {
                        "name": "Yujun Zhou"
                    },
                    {
                        "name": "Zhenwen Liang"
                    },
                    {
                        "name": "Haolin Liu"
                    },
                    {
                        "name": "Wenhao Yu"
                    },
                    {
                        "name": "Kishan Panaganti"
                    },
                    {
                        "name": "Linfeng Song"
                    },
                    {
                        "name": "Dian Yu"
                    },
                    {
                        "name": "Xiangliang Zhang"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Dong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yu"
                },
                "author": "Dong Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15194v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15194v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22200v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22200v4",
                "updated": "2025-10-01T05:28:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    5,
                    28,
                    4,
                    2,
                    274,
                    0
                ],
                "published": "2025-06-27T13:09:05Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    13,
                    9,
                    5,
                    4,
                    178,
                    0
                ],
                "title": "EFRame: Deeper Reasoning via Exploration-Filter-Replay Reinforcement\n  Learning Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EFRame: Deeper Reasoning via Exploration-Filter-Replay Reinforcement\n  Learning Framework"
                },
                "summary": "Recent advances in reinforcement learning (RL) have significantly enhanced\nthe reasoning capabilities of large language models (LLMs). Group Relative\nPolicy Optimization (GRPO), a lightweight variant of Proximal Policy\nOptimization (PPO), improves efficiency but suffers from limited exploration\nand training instability, limiting its effectiveness on complex reasoning\ntasks. To address these challenges, we introduce EFRame, an\nExploration-Filter-Replay framework that augments GRPO across three dimensions:\nadditional rollouts enable deeper and more targeted exploration, online\nfiltering removes low-quality samples to stabilize gradients and accelerate\ntraining, and experience replay amplifies rare yet informative trajectories for\nstable convergence. This unified framework establishes a principled training\ncycle that balances exploration, efficiency, and stability. Experiments on\ndiverse reasoning benchmarks demonstrate that EFRame achieves consistent gains,\nincluding a 37.9\\% relative improvement on Geometry3K over GRPO. EFRame further\nsupports fine-grained sample categorization and precise entropy control,\nhighlighting it as a robust solution for advancing deeper reasoning in LLMs.\nOur code is available at https://github.com/597358816/EFRame.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in reinforcement learning (RL) have significantly enhanced\nthe reasoning capabilities of large language models (LLMs). Group Relative\nPolicy Optimization (GRPO), a lightweight variant of Proximal Policy\nOptimization (PPO), improves efficiency but suffers from limited exploration\nand training instability, limiting its effectiveness on complex reasoning\ntasks. To address these challenges, we introduce EFRame, an\nExploration-Filter-Replay framework that augments GRPO across three dimensions:\nadditional rollouts enable deeper and more targeted exploration, online\nfiltering removes low-quality samples to stabilize gradients and accelerate\ntraining, and experience replay amplifies rare yet informative trajectories for\nstable convergence. This unified framework establishes a principled training\ncycle that balances exploration, efficiency, and stability. Experiments on\ndiverse reasoning benchmarks demonstrate that EFRame achieves consistent gains,\nincluding a 37.9\\% relative improvement on Geometry3K over GRPO. EFRame further\nsupports fine-grained sample categorization and precise entropy control,\nhighlighting it as a robust solution for advancing deeper reasoning in LLMs.\nOur code is available at https://github.com/597358816/EFRame."
                },
                "authors": [
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Lai Wei"
                    },
                    {
                        "name": "Yanzhi Zhang"
                    },
                    {
                        "name": "Chenyang Shao"
                    },
                    {
                        "name": "Zedong Dan"
                    },
                    {
                        "name": "Weiran Huang"
                    },
                    {
                        "name": "Yuzhi Zhang"
                    },
                    {
                        "name": "Yue Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yue Wang"
                },
                "author": "Yue Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22200v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22200v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22646v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22646v2",
                "updated": "2025-10-01T05:14:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    5,
                    14,
                    25,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-26T17:59:54Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    59,
                    54,
                    4,
                    269,
                    0
                ],
                "title": "Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal\n  LLMs"
                },
                "summary": "Can humans identify AI-generated (fake) videos and provide grounded reasons?\nWhile video generation models have advanced rapidly, a critical dimension --\nwhether humans can detect deepfake traces within a generated video, i.e.,\nspatiotemporal grounded visual artifacts that reveal a video as machine\ngenerated -- has been largely overlooked. We introduce DeeptraceReward, the\nfirst fine-grained, spatially- and temporally- aware benchmark that annotates\nhuman-perceived fake traces for video generation reward. The dataset comprises\n4.3K detailed annotations across 3.3K high-quality generated videos. Each\nannotation provides a natural-language explanation, pinpoints a bounding-box\nregion containing the perceived trace, and marks precise onset and offset\ntimestamps. We consolidate these annotations into 9 major categories of\ndeepfake traces that lead humans to identify a video as AI-generated, and train\nmultimodal language models (LMs) as reward models to mimic human judgments and\nlocalizations. On DeeptraceReward, our 7B reward model outperforms GPT-5 by\n34.7% on average across fake clue identification, grounding, and explanation.\nInterestingly, we observe a consistent difficulty gradient: binary fake v.s.\nreal classification is substantially easier than fine-grained deepfake trace\ndetection; within the latter, performance degrades from natural language\nexplanations (easiest), to spatial grounding, to temporal labeling (hardest).\nBy foregrounding human-perceived deepfake traces, DeeptraceReward provides a\nrigorous testbed and training signal for socially aware and trustworthy video\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can humans identify AI-generated (fake) videos and provide grounded reasons?\nWhile video generation models have advanced rapidly, a critical dimension --\nwhether humans can detect deepfake traces within a generated video, i.e.,\nspatiotemporal grounded visual artifacts that reveal a video as machine\ngenerated -- has been largely overlooked. We introduce DeeptraceReward, the\nfirst fine-grained, spatially- and temporally- aware benchmark that annotates\nhuman-perceived fake traces for video generation reward. The dataset comprises\n4.3K detailed annotations across 3.3K high-quality generated videos. Each\nannotation provides a natural-language explanation, pinpoints a bounding-box\nregion containing the perceived trace, and marks precise onset and offset\ntimestamps. We consolidate these annotations into 9 major categories of\ndeepfake traces that lead humans to identify a video as AI-generated, and train\nmultimodal language models (LMs) as reward models to mimic human judgments and\nlocalizations. On DeeptraceReward, our 7B reward model outperforms GPT-5 by\n34.7% on average across fake clue identification, grounding, and explanation.\nInterestingly, we observe a consistent difficulty gradient: binary fake v.s.\nreal classification is substantially easier than fine-grained deepfake trace\ndetection; within the latter, performance degrades from natural language\nexplanations (easiest), to spatial grounding, to temporal labeling (hardest).\nBy foregrounding human-perceived deepfake traces, DeeptraceReward provides a\nrigorous testbed and training signal for socially aware and trustworthy video\ngeneration."
                },
                "authors": [
                    {
                        "name": "Xingyu Fu"
                    },
                    {
                        "name": "Siyi Liu"
                    },
                    {
                        "name": "Yinuo Xu"
                    },
                    {
                        "name": "Pan Lu"
                    },
                    {
                        "name": "Guangqiuse Hu"
                    },
                    {
                        "name": "Tianbo Yang"
                    },
                    {
                        "name": "Taran Anantasagar"
                    },
                    {
                        "name": "Christopher Shen"
                    },
                    {
                        "name": "Yikai Mao"
                    },
                    {
                        "name": "Yuanzhe Liu"
                    },
                    {
                        "name": "Keyush Shah"
                    },
                    {
                        "name": "Chung Un Lee"
                    },
                    {
                        "name": "Yejin Choi"
                    },
                    {
                        "name": "James Zou"
                    },
                    {
                        "name": "Dan Roth"
                    },
                    {
                        "name": "Chris Callison-Burch"
                    }
                ],
                "author_detail": {
                    "name": "Chris Callison-Burch"
                },
                "author": "Chris Callison-Burch",
                "arxiv_comment": "Project Page: https://deeptracereward.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22646v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22646v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25454v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25454v2",
                "updated": "2025-10-01T05:09:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    5,
                    9,
                    42,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-29T20:00:29Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    20,
                    0,
                    29,
                    0,
                    272,
                    0
                ],
                "title": "DeepSearch: Overcome the Bottleneck of Reinforcement Learning with\n  Verifiable Rewards via Monte Carlo Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepSearch: Overcome the Bottleneck of Reinforcement Learning with\n  Verifiable Rewards via Monte Carlo Tree Search"
                },
                "summary": "Although RLVR has become an essential component for developing advanced\nreasoning skills in LLMs, contemporary studies have documented training\nplateaus that emerge following thousands of optimization steps, demonstrating\nnotable decreases in performance gains despite increased computational\ninvestment. This limitation stems from the sparse exploration patterns inherent\nin current RLVR practices, where models rely on limited rollouts that often\nmiss critical reasoning paths and fail to provide systematic coverage of the\nsolution space. We present DeepSearch, a framework that integrates Monte Carlo\nTree Search directly into RLVR training. In contrast to existing methods that\nrely on tree search only at inference, DeepSearch embeds structured search into\nthe training loop, enabling systematic exploration and fine-grained credit\nassignment across reasoning steps. Through training-time exploration,\nDeepSearch addresses the fundamental bottleneck of insufficient exploration,\nwhich leads to diminishing performance improvements over prolonged training\nsteps. Our contributions include: (1) a global frontier selection strategy that\nprioritizes promising nodes across the search tree, (2) selection with\nentropy-based guidance that identifies confident paths for supervision, and (3)\nadaptive replay buffer training with solution caching for efficiency.\nExperiments on mathematical reasoning benchmarks show that DeepSearch achieves\n62.95% average accuracy and establishes a new state-of-the-art for 1.5B\nreasoning models - using 5.7x fewer GPU hours than extended training\napproaches. These results highlight the importance of strategic exploration\nover brute-force scaling and demonstrate the promise of algorithmic innovation\nfor advancing RLVR methodologies. DeepSearch establishes a new direction for\nscaling reasoning capabilities through systematic search rather than prolonged\ncomputation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although RLVR has become an essential component for developing advanced\nreasoning skills in LLMs, contemporary studies have documented training\nplateaus that emerge following thousands of optimization steps, demonstrating\nnotable decreases in performance gains despite increased computational\ninvestment. This limitation stems from the sparse exploration patterns inherent\nin current RLVR practices, where models rely on limited rollouts that often\nmiss critical reasoning paths and fail to provide systematic coverage of the\nsolution space. We present DeepSearch, a framework that integrates Monte Carlo\nTree Search directly into RLVR training. In contrast to existing methods that\nrely on tree search only at inference, DeepSearch embeds structured search into\nthe training loop, enabling systematic exploration and fine-grained credit\nassignment across reasoning steps. Through training-time exploration,\nDeepSearch addresses the fundamental bottleneck of insufficient exploration,\nwhich leads to diminishing performance improvements over prolonged training\nsteps. Our contributions include: (1) a global frontier selection strategy that\nprioritizes promising nodes across the search tree, (2) selection with\nentropy-based guidance that identifies confident paths for supervision, and (3)\nadaptive replay buffer training with solution caching for efficiency.\nExperiments on mathematical reasoning benchmarks show that DeepSearch achieves\n62.95% average accuracy and establishes a new state-of-the-art for 1.5B\nreasoning models - using 5.7x fewer GPU hours than extended training\napproaches. These results highlight the importance of strategic exploration\nover brute-force scaling and demonstrate the promise of algorithmic innovation\nfor advancing RLVR methodologies. DeepSearch establishes a new direction for\nscaling reasoning capabilities through systematic search rather than prolonged\ncomputation."
                },
                "authors": [
                    {
                        "name": "Fang Wu"
                    },
                    {
                        "name": "Weihao Xuan"
                    },
                    {
                        "name": "Heli Qi"
                    },
                    {
                        "name": "Ximing Lu"
                    },
                    {
                        "name": "Aaron Tu"
                    },
                    {
                        "name": "Li Erran Li"
                    },
                    {
                        "name": "Yejin Choi"
                    }
                ],
                "author_detail": {
                    "name": "Yejin Choi"
                },
                "author": "Yejin Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25454v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25454v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17251v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17251v2",
                "updated": "2025-10-01T04:58:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    4,
                    58,
                    58,
                    2,
                    274,
                    0
                ],
                "published": "2025-06-08T10:02:07Z",
                "published_parsed": [
                    2025,
                    6,
                    8,
                    10,
                    2,
                    7,
                    6,
                    159,
                    0
                ],
                "title": "Training-free LLM Verification via Recycling Few-shot Examples",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-free LLM Verification via Recycling Few-shot Examples"
                },
                "summary": "Although LLMs have achieved remarkable performance, the inherent\nstochasticity of their reasoning process and varying conclusions present\nsignificant challenges. Majority voting or Best-of-N with external verification\nmodels has been explored to find the most promising solution among multiple LLM\noutputs. However, these approaches have certain limitations, such as limited\napplicability or the cost of an additional training step. To address this\nproblem, we propose a novel and effective framework that Recycles Few-shot\nexamples to verify LLM outputs (ReFeri). Our key idea is to additionally\nutilize the given few-shot examples to evaluate the candidate outputs of the\ntarget query, not only using them to generate outputs as the conventional\nfew-shot prompting setup. Specifically, ReFeri evaluates the generated outputs\nby combining two different scores, designed motivated from Bayes' rule, and\nsubsequently selects the candidate that is both confidently determined and\ncontextually coherent through a few additional LLM inferences. Experiments with\nthree different LLMs and across seven diverse tasks demonstrate that our\nframework significantly improves the accuracy of LLMs-achieving an average gain\nof 4.8%-through effective response selection, without additional training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although LLMs have achieved remarkable performance, the inherent\nstochasticity of their reasoning process and varying conclusions present\nsignificant challenges. Majority voting or Best-of-N with external verification\nmodels has been explored to find the most promising solution among multiple LLM\noutputs. However, these approaches have certain limitations, such as limited\napplicability or the cost of an additional training step. To address this\nproblem, we propose a novel and effective framework that Recycles Few-shot\nexamples to verify LLM outputs (ReFeri). Our key idea is to additionally\nutilize the given few-shot examples to evaluate the candidate outputs of the\ntarget query, not only using them to generate outputs as the conventional\nfew-shot prompting setup. Specifically, ReFeri evaluates the generated outputs\nby combining two different scores, designed motivated from Bayes' rule, and\nsubsequently selects the candidate that is both confidently determined and\ncontextually coherent through a few additional LLM inferences. Experiments with\nthree different LLMs and across seven diverse tasks demonstrate that our\nframework significantly improves the accuracy of LLMs-achieving an average gain\nof 4.8%-through effective response selection, without additional training."
                },
                "authors": [
                    {
                        "name": "Dongseok Lee"
                    },
                    {
                        "name": "Jimyung Hong"
                    },
                    {
                        "name": "Dongyoung Kim"
                    },
                    {
                        "name": "Jaehyung Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jaehyung Kim"
                },
                "author": "Jaehyung Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17251v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17251v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25835v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25835v2",
                "updated": "2025-10-01T04:57:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    4,
                    57,
                    48,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-30T06:18:44Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    6,
                    18,
                    44,
                    1,
                    273,
                    0
                ],
                "title": "Chain-in-Tree: Back to Sequential Reasoning in LLM Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-in-Tree: Back to Sequential Reasoning in LLM Tree Search"
                },
                "summary": "Test-time scaling enables large language models (LLMs) to improve performance\non long-horizon reasoning tasks by allocating additional compute at inference.\nTree-search-based approaches achieve state-of-the-art results in this setting,\nbut they are notoriously inefficient, often an order of magnitude slower than\nsimpler iterative methods. We introduce Chain-in-Tree (CiT), a plug-in\nframework that adaptively decides when to branch during search rather than\nbranching at every step. CiT relies on lightweight Branching Necessity (BN)\nevaluation methods: BN-DP (Direct Prompting), where an auxiliary LLM directly\njudges whether a step requires branching, and BN-SC (Self-Consistency), which\nclusters multiple candidate actions to estimate agreement. We integrate CiT\ninto three representative LLM-in-the-loop tree search frameworks: Tree of\nThoughts (ToT-BS), ReST-MCTS, and RAP, and evaluate across GSM8K and Math500.\nOur results show that: (1) BN-DP consistently reduces token generation, model\ninvocations, and runtime by 75-85 percent across all settings, with negligible\naccuracy loss and sometimes accuracy gains; (2) BN-SC typically yields\nsubstantial savings (up to 80 percent) but shows instability in 1-4 out of 14\nsettings, caused by a small subset of examples that produce very long reasoning\nsteps; (3) the quality of auxiliary LLMs is critical, not only the BN evaluator\nin BN-DP, but also the models used in BN-SC for clustering and equivalence\nchecking. When these roles are filled by smaller LLMs, performance degrades.\nImportantly, BN-SC does not require LLMs in domains with deterministic action\nspaces, where clustering can be done programmatically. We also provide a\ntheoretical guarantee that BN-DP never increases LLM invocations relative to\nthe baseline and release a unified implementation of CiT across ToT-BS,\nReST-MCTS, and RAP to facilitate reproducibility and extension.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling enables large language models (LLMs) to improve performance\non long-horizon reasoning tasks by allocating additional compute at inference.\nTree-search-based approaches achieve state-of-the-art results in this setting,\nbut they are notoriously inefficient, often an order of magnitude slower than\nsimpler iterative methods. We introduce Chain-in-Tree (CiT), a plug-in\nframework that adaptively decides when to branch during search rather than\nbranching at every step. CiT relies on lightweight Branching Necessity (BN)\nevaluation methods: BN-DP (Direct Prompting), where an auxiliary LLM directly\njudges whether a step requires branching, and BN-SC (Self-Consistency), which\nclusters multiple candidate actions to estimate agreement. We integrate CiT\ninto three representative LLM-in-the-loop tree search frameworks: Tree of\nThoughts (ToT-BS), ReST-MCTS, and RAP, and evaluate across GSM8K and Math500.\nOur results show that: (1) BN-DP consistently reduces token generation, model\ninvocations, and runtime by 75-85 percent across all settings, with negligible\naccuracy loss and sometimes accuracy gains; (2) BN-SC typically yields\nsubstantial savings (up to 80 percent) but shows instability in 1-4 out of 14\nsettings, caused by a small subset of examples that produce very long reasoning\nsteps; (3) the quality of auxiliary LLMs is critical, not only the BN evaluator\nin BN-DP, but also the models used in BN-SC for clustering and equivalence\nchecking. When these roles are filled by smaller LLMs, performance degrades.\nImportantly, BN-SC does not require LLMs in domains with deterministic action\nspaces, where clustering can be done programmatically. We also provide a\ntheoretical guarantee that BN-DP never increases LLM invocations relative to\nthe baseline and release a unified implementation of CiT across ToT-BS,\nReST-MCTS, and RAP to facilitate reproducibility and extension."
                },
                "authors": [
                    {
                        "name": "Xinzhe Li"
                    }
                ],
                "author_detail": {
                    "name": "Xinzhe Li"
                },
                "author": "Xinzhe Li",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25835v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25835v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25868v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25868v2",
                "updated": "2025-10-01T04:57:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    4,
                    57,
                    4,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-30T07:06:23Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    7,
                    6,
                    23,
                    1,
                    273,
                    0
                ],
                "title": "ReFACT: A Benchmark for Scientific Confabulation Detection with\n  Positional Error Annotations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReFACT: A Benchmark for Scientific Confabulation Detection with\n  Positional Error Annotations"
                },
                "summary": "Large Language Models (LLMs) frequently confabulate scientific facts,\nseverely undermining their trustworthiness. Addressing this challenge requires\nbenchmarks that go beyond binary factuality and enable fine-grained evaluation.\nWe introduce ReFACT (Reddit False And Correct Texts), a benchmark of 1,001\nexpert-annotated question-answer pairs spanning diverse scientific domains for\nthe detection of scientific confabulation. Each instance includes both a\nscientifically correct answer and a non-factual counterpart annotated with\nprecise error spans and error types. ReFACT enables multi-stage evaluation: (1)\nconfabulation detection, (2) fine-grained error localization, and (3)\ncorrection. We benchmark 9 state-of-the-art LLMs, revealing limited performance\n(about 50 percent accuracy). Even top models such as GPT-4o fail to distinguish\nfactual from confabulated scientific answers, raising concerns about the\nreliability of LLM-as-judge evaluation paradigms. Our findings highlight the\nneed for fine-grained, human-validated benchmarks to detect and correct\nscientific confabulation in domain-specific contexts. The dataset is available\nat: https://github.com/ddz5431/ReFACT",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) frequently confabulate scientific facts,\nseverely undermining their trustworthiness. Addressing this challenge requires\nbenchmarks that go beyond binary factuality and enable fine-grained evaluation.\nWe introduce ReFACT (Reddit False And Correct Texts), a benchmark of 1,001\nexpert-annotated question-answer pairs spanning diverse scientific domains for\nthe detection of scientific confabulation. Each instance includes both a\nscientifically correct answer and a non-factual counterpart annotated with\nprecise error spans and error types. ReFACT enables multi-stage evaluation: (1)\nconfabulation detection, (2) fine-grained error localization, and (3)\ncorrection. We benchmark 9 state-of-the-art LLMs, revealing limited performance\n(about 50 percent accuracy). Even top models such as GPT-4o fail to distinguish\nfactual from confabulated scientific answers, raising concerns about the\nreliability of LLM-as-judge evaluation paradigms. Our findings highlight the\nneed for fine-grained, human-validated benchmarks to detect and correct\nscientific confabulation in domain-specific contexts. The dataset is available\nat: https://github.com/ddz5431/ReFACT"
                },
                "authors": [
                    {
                        "name": "Yindong Wang"
                    },
                    {
                        "name": "Martin Preiß"
                    },
                    {
                        "name": "Margarita Bugueño"
                    },
                    {
                        "name": "Jan Vincent Hoffbauer"
                    },
                    {
                        "name": "Abdullatif Ghajar"
                    },
                    {
                        "name": "Tolga Buz"
                    },
                    {
                        "name": "Gerard de Melo"
                    }
                ],
                "author_detail": {
                    "name": "Gerard de Melo"
                },
                "author": "Gerard de Melo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25868v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25868v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17349v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17349v2",
                "updated": "2025-10-01T04:26:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    4,
                    26,
                    4,
                    2,
                    274,
                    0
                ],
                "published": "2025-03-21T17:51:14Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    17,
                    51,
                    14,
                    4,
                    80,
                    0
                ],
                "title": "Beyond Semantics: Rediscovering Spatial Awareness in Vision-Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Semantics: Rediscovering Spatial Awareness in Vision-Language\n  Models"
                },
                "summary": "Vision Language Models (VLMs) excel at identifying and describing objects but\noften fail at spatial reasoning. We study why VLMs, such as LLaVA, underutilize\nspatial cues despite having positional encodings and spatially rich vision\nencoder features. Our analysis reveals a key imbalance: vision token embeddings\nhave much larger norms than text tokens, suppressing LLM's position embedding.\nTo expose this mechanism, we developed three interpretability tools: (1) the\nPosition Sensitivity Index, which quantifies reliance on token order, (2) the\nCross Modality Balance, which reveals attention head allocation patterns, and\n(3) a RoPE Sensitivity probe, which measures dependence on rotary positional\nembeddings. These tools uncover that vision tokens and system prompts dominate\nattention. We validated our mechanistic understanding through targeted\ninterventions that predictably restore positional sensitivity. These findings\nreveal previously unknown failure modes in multimodal attention and demonstrate\nhow interpretability analysis can guide principled improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Language Models (VLMs) excel at identifying and describing objects but\noften fail at spatial reasoning. We study why VLMs, such as LLaVA, underutilize\nspatial cues despite having positional encodings and spatially rich vision\nencoder features. Our analysis reveals a key imbalance: vision token embeddings\nhave much larger norms than text tokens, suppressing LLM's position embedding.\nTo expose this mechanism, we developed three interpretability tools: (1) the\nPosition Sensitivity Index, which quantifies reliance on token order, (2) the\nCross Modality Balance, which reveals attention head allocation patterns, and\n(3) a RoPE Sensitivity probe, which measures dependence on rotary positional\nembeddings. These tools uncover that vision tokens and system prompts dominate\nattention. We validated our mechanistic understanding through targeted\ninterventions that predictably restore positional sensitivity. These findings\nreveal previously unknown failure modes in multimodal attention and demonstrate\nhow interpretability analysis can guide principled improvements."
                },
                "authors": [
                    {
                        "name": "Jianing Qi"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Hao Tang"
                    },
                    {
                        "name": "Zhigang Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Zhigang Zhu"
                },
                "author": "Zhigang Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17349v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17349v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05567v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05567v3",
                "updated": "2025-10-01T04:11:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    4,
                    11,
                    33,
                    2,
                    274,
                    0
                ],
                "published": "2025-02-08T13:28:51Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    13,
                    28,
                    51,
                    5,
                    39,
                    0
                ],
                "title": "ATLAS: Autoformalizing Theorems through Lifting, Augmentation, and\n  Synthesis of Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ATLAS: Autoformalizing Theorems through Lifting, Augmentation, and\n  Synthesis of Data"
                },
                "summary": "Autoformalization, the automatic translation of mathematical content from\nnatural language into machine-verifiable formal languages, has seen significant\nprogress driven by advances in large language models (LLMs). Nonetheless, a\nprimary barrier to further improvements is the limited availability of parallel\ncorpora that map informal mathematical text to its formal counterpart. To\naddress this limitation, we propose ATLAS (Autoformalizing Theorems through\nLifting, Augmentation, and Synthesis of Data), a novel data generation\nframework designed to produce large-scale, high-quality parallel corpora of\ntheorem statements. Distinct from prior approaches, ATLAS begins with a concept\nrepository, accelerates the improvement of the student model through expert\niteration combined with knowledge distillation, and introduces two novel\naugmentation strategies that exploit the structural characteristics of formal\nlanguages. Running the proposed ATLAS framework for 10 iterations, we construct\nan undergraduate-level dataset of 117k theorem statements and develop the ATLAS\nTranslator by fine-tuning Llama3.1-8B-Instruct with LoRA. This model\nestablishes a new state of the art, demonstrating statistically significant\nimprovements over both the Herald Translator and the Kimina-Autoformalizer\nacross all benchmarks (p<0.05, two-sided t-test). Furthermore, we demonstrate\nthat the full-parameter fine-tuning of a stronger base model on the ATLAS\ndataset leads to superior performance. The datasets, model, and code are\navailable at https://github.com/XiaoyangLiu-sjtu/ATLAS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoformalization, the automatic translation of mathematical content from\nnatural language into machine-verifiable formal languages, has seen significant\nprogress driven by advances in large language models (LLMs). Nonetheless, a\nprimary barrier to further improvements is the limited availability of parallel\ncorpora that map informal mathematical text to its formal counterpart. To\naddress this limitation, we propose ATLAS (Autoformalizing Theorems through\nLifting, Augmentation, and Synthesis of Data), a novel data generation\nframework designed to produce large-scale, high-quality parallel corpora of\ntheorem statements. Distinct from prior approaches, ATLAS begins with a concept\nrepository, accelerates the improvement of the student model through expert\niteration combined with knowledge distillation, and introduces two novel\naugmentation strategies that exploit the structural characteristics of formal\nlanguages. Running the proposed ATLAS framework for 10 iterations, we construct\nan undergraduate-level dataset of 117k theorem statements and develop the ATLAS\nTranslator by fine-tuning Llama3.1-8B-Instruct with LoRA. This model\nestablishes a new state of the art, demonstrating statistically significant\nimprovements over both the Herald Translator and the Kimina-Autoformalizer\nacross all benchmarks (p<0.05, two-sided t-test). Furthermore, we demonstrate\nthat the full-parameter fine-tuning of a stronger base model on the ATLAS\ndataset leads to superior performance. The datasets, model, and code are\navailable at https://github.com/XiaoyangLiu-sjtu/ATLAS."
                },
                "authors": [
                    {
                        "name": "Xiaoyang Liu"
                    },
                    {
                        "name": "Kangjie Bao"
                    },
                    {
                        "name": "Jiashuo Zhang"
                    },
                    {
                        "name": "Yunqi Liu"
                    },
                    {
                        "name": "Yu Chen"
                    },
                    {
                        "name": "Yuntian Liu"
                    },
                    {
                        "name": "Yang Jiao"
                    },
                    {
                        "name": "Tao Luo"
                    }
                ],
                "author_detail": {
                    "name": "Tao Luo"
                },
                "author": "Tao Luo",
                "arxiv_comment": "Accepted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05567v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05567v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24159v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24159v2",
                "updated": "2025-10-01T03:46:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    3,
                    46,
                    49,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-29T01:17:49Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    1,
                    17,
                    49,
                    0,
                    272,
                    0
                ],
                "title": "Latent Collective Preference Optimization: A General Framework for\n  Robust LLM Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent Collective Preference Optimization: A General Framework for\n  Robust LLM Alignment"
                },
                "summary": "Standard human preference-based alignment methods, such as Reinforcement\nLearning from Human Feedback (RLHF), are a cornerstone technology for aligning\nLarge Language Models (LLMs) with human values. However, these methods are all\nunderpinned by a critical, yet flawed assumption: human preferences are\nhomogeneous (representing a single, unified preference) and the collected data\nis noiseless (free from error). In reality, neither is true since human\npreference is pluralistic and annotators can make mistakes. This creates a\ndiscrepancy between the recorded data and the ground-truth preferences, which\ncan misguide the model and degrade its performance. To address this challenge,\nwe introduce Latent Collective Preference Optimization (LCPO). LCPO leverages\nan Expectation-Maximization (EM) algorithm to learn the latent collective\nconsensus from noisy data. It operates by inferring the correctness of each\npreference label and using this probability as an adaptive weight to\nre-calibrate each data point's contribution to the training loss, thereby\nmitigating noise. We generalize this approach by establishing a theoretical\nlink between arbitrary preference losses and their corresponding probabilistic\nmodels, elevating LCPO from a specific algorithm to a general framework for\nrobust preference alignment. Theoretically, we prove that under the condition\nof a perfectly calibrated model, LCPO is guaranteed to converge to the true\nnoise level of the dataset. Our experiments demonstrate LCPO's effectiveness as\na general framework, consistently enhancing four state-of-the-art alignment\nalgorithms (DPO, IPO, SimPO, and CPO). When applied to Mistral and Llama 3\nmodels, the LCPO-enhanced methods achieve substantial win rate gains on\nAlpacaEval 2 and Arena-Hard, with improvements of up to 7.0% on both\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Standard human preference-based alignment methods, such as Reinforcement\nLearning from Human Feedback (RLHF), are a cornerstone technology for aligning\nLarge Language Models (LLMs) with human values. However, these methods are all\nunderpinned by a critical, yet flawed assumption: human preferences are\nhomogeneous (representing a single, unified preference) and the collected data\nis noiseless (free from error). In reality, neither is true since human\npreference is pluralistic and annotators can make mistakes. This creates a\ndiscrepancy between the recorded data and the ground-truth preferences, which\ncan misguide the model and degrade its performance. To address this challenge,\nwe introduce Latent Collective Preference Optimization (LCPO). LCPO leverages\nan Expectation-Maximization (EM) algorithm to learn the latent collective\nconsensus from noisy data. It operates by inferring the correctness of each\npreference label and using this probability as an adaptive weight to\nre-calibrate each data point's contribution to the training loss, thereby\nmitigating noise. We generalize this approach by establishing a theoretical\nlink between arbitrary preference losses and their corresponding probabilistic\nmodels, elevating LCPO from a specific algorithm to a general framework for\nrobust preference alignment. Theoretically, we prove that under the condition\nof a perfectly calibrated model, LCPO is guaranteed to converge to the true\nnoise level of the dataset. Our experiments demonstrate LCPO's effectiveness as\na general framework, consistently enhancing four state-of-the-art alignment\nalgorithms (DPO, IPO, SimPO, and CPO). When applied to Mistral and Llama 3\nmodels, the LCPO-enhanced methods achieve substantial win rate gains on\nAlpacaEval 2 and Arena-Hard, with improvements of up to 7.0% on both\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Xiaoyang Cao"
                    },
                    {
                        "name": "Zelai Xu"
                    },
                    {
                        "name": "Mo Guang"
                    },
                    {
                        "name": "Kaiwen Long"
                    },
                    {
                        "name": "Michiel A. Bakker"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Chao Yu"
                    }
                ],
                "author_detail": {
                    "name": "Chao Yu"
                },
                "author": "Chao Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24159v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24159v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05310v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05310v3",
                "updated": "2025-10-01T03:44:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    3,
                    44,
                    52,
                    2,
                    274,
                    0
                ],
                "published": "2024-06-08T01:02:49Z",
                "published_parsed": [
                    2024,
                    6,
                    8,
                    1,
                    2,
                    49,
                    5,
                    160,
                    0
                ],
                "title": "COOKIEGUARD: Characterizing and Isolating the First-Party Cookie Jar",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COOKIEGUARD: Characterizing and Isolating the First-Party Cookie Jar"
                },
                "summary": "As third-party cookies are being phased out or restricted by major browsers,\nfirst-party cookies are increasingly repurposed for tracking. Prior work has\nshown that third-party scripts embedded in the main frame can access and\nexfiltrate first-party cookies, including those set by other third-party\nscripts. However, existing browser security mechanisms, such as the Same-Origin\nPolicy, Content Security Policy, and third-party storage partitioning, do not\nprevent this type of cross-domain interaction within the main frame. While\nrecent studies have begun to highlight this issue, there remains a lack of\ncomprehensive measurement and practical defenses.\n  In this work, we conduct the first large-scale measurement of cross-domain\naccess to first-party cookies across 20,000 websites. We find that 56 percent\nof websites include third-party scripts that exfiltrate cookies they did not\nset, and 32 percent allow unauthorized overwriting or deletion, revealing\nsignificant confidentiality and integrity risks.\n  To mitigate this, we propose CookieGuard, a browser-based runtime enforcement\nmechanism that isolates first-party cookies on a per-script-origin basis.\nCookieGuard blocks all unauthorized cross-domain cookie operations while\npreserving site functionality in most cases, with Single Sign-On disruption\nobserved on 11 percent of sites.\n  Our results expose critical flaws in current browser models and offer a\ndeployable path toward stronger cookie isolation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As third-party cookies are being phased out or restricted by major browsers,\nfirst-party cookies are increasingly repurposed for tracking. Prior work has\nshown that third-party scripts embedded in the main frame can access and\nexfiltrate first-party cookies, including those set by other third-party\nscripts. However, existing browser security mechanisms, such as the Same-Origin\nPolicy, Content Security Policy, and third-party storage partitioning, do not\nprevent this type of cross-domain interaction within the main frame. While\nrecent studies have begun to highlight this issue, there remains a lack of\ncomprehensive measurement and practical defenses.\n  In this work, we conduct the first large-scale measurement of cross-domain\naccess to first-party cookies across 20,000 websites. We find that 56 percent\nof websites include third-party scripts that exfiltrate cookies they did not\nset, and 32 percent allow unauthorized overwriting or deletion, revealing\nsignificant confidentiality and integrity risks.\n  To mitigate this, we propose CookieGuard, a browser-based runtime enforcement\nmechanism that isolates first-party cookies on a per-script-origin basis.\nCookieGuard blocks all unauthorized cross-domain cookie operations while\npreserving site functionality in most cases, with Single Sign-On disruption\nobserved on 11 percent of sites.\n  Our results expose critical flaws in current browser models and offer a\ndeployable path toward stronger cookie isolation."
                },
                "authors": [
                    {
                        "name": "Pouneh Nikkhah Bahrami"
                    },
                    {
                        "name": "Aurore Fass"
                    },
                    {
                        "name": "Zubair Shafiq"
                    }
                ],
                "author_detail": {
                    "name": "Zubair Shafiq"
                },
                "author": "Zubair Shafiq",
                "arxiv_doi": "10.1145/3730567.3764490",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3730567.3764490",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.05310v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05310v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Proceedings of the ACM Internet Measurement Conference (IMC 2025)",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26093v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26093v2",
                "updated": "2025-10-01T03:38:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    3,
                    38,
                    59,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-30T11:12:01Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    11,
                    12,
                    1,
                    1,
                    273,
                    0
                ],
                "title": "Reinforced Strategy Optimization for Conversational Recommender Systems\n  via Network-of-Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforced Strategy Optimization for Conversational Recommender Systems\n  via Network-of-Experts"
                },
                "summary": "Conversational Recommender Systems (CRSs) aim to provide personalized\nrecommendations through multi-turn natural language interactions with users.\nGiven the strong interaction and reasoning skills of Large Language Models\n(LLMs), leveraging LLMs for CRSs has recently emerged as a promising direction.\nHowever, existing LLM-based methods often lack explicit optimization of\ninteraction strategies, instead relying on unified prompts and the LLM's\ninternal knowledge to decide how to interact, which can lead to suboptimal\noutcomes. In this paper, we propose a novel Reinforced Strategy Optimization\n(RSO) method for CRS, which decomposes the process of generating\nstrategy-driven response decisions into the macro-level strategy planning and\nmicro-level strategy adaptation through a network-of-experts architecture. At\nthe macro level, a Planner expert selects macro-level interaction strategies\n(e.g., recommend, explain, encourage). At the micro level, an Actor expert\ngenerates detailed responses conditioned on the selected macro-level strategy,\nguided by auxiliary experts that provide complementary information such as user\npreferences and factual grounding. This hierarchical decomposition disentangles\nthe optimization of different sub-tasks involved in CRS response generation,\nenabling more tractable learning at each level. To address the scarcity of\nhigh-quality multi-turn training data, we formulate strategy learning as a\nreinforcement learning problem, guided by an LLM-based reward model to achieve\nautomatic strategy exploration. Extensive experiments show that RSO\nsignificantly improves interaction performance compared to state-of-the-art\nbaselines, demonstrating the effectiveness of explicit hierarchical strategy\noptimization for CRS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational Recommender Systems (CRSs) aim to provide personalized\nrecommendations through multi-turn natural language interactions with users.\nGiven the strong interaction and reasoning skills of Large Language Models\n(LLMs), leveraging LLMs for CRSs has recently emerged as a promising direction.\nHowever, existing LLM-based methods often lack explicit optimization of\ninteraction strategies, instead relying on unified prompts and the LLM's\ninternal knowledge to decide how to interact, which can lead to suboptimal\noutcomes. In this paper, we propose a novel Reinforced Strategy Optimization\n(RSO) method for CRS, which decomposes the process of generating\nstrategy-driven response decisions into the macro-level strategy planning and\nmicro-level strategy adaptation through a network-of-experts architecture. At\nthe macro level, a Planner expert selects macro-level interaction strategies\n(e.g., recommend, explain, encourage). At the micro level, an Actor expert\ngenerates detailed responses conditioned on the selected macro-level strategy,\nguided by auxiliary experts that provide complementary information such as user\npreferences and factual grounding. This hierarchical decomposition disentangles\nthe optimization of different sub-tasks involved in CRS response generation,\nenabling more tractable learning at each level. To address the scarcity of\nhigh-quality multi-turn training data, we formulate strategy learning as a\nreinforcement learning problem, guided by an LLM-based reward model to achieve\nautomatic strategy exploration. Extensive experiments show that RSO\nsignificantly improves interaction performance compared to state-of-the-art\nbaselines, demonstrating the effectiveness of explicit hierarchical strategy\noptimization for CRS."
                },
                "authors": [
                    {
                        "name": "Xiaoyan Zhao"
                    },
                    {
                        "name": "Ming Yan"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Yang Deng"
                    },
                    {
                        "name": "Jian Wang"
                    },
                    {
                        "name": "Fengbin Zhu"
                    },
                    {
                        "name": "Yilun Qiu"
                    },
                    {
                        "name": "Hong Cheng"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    }
                ],
                "author_detail": {
                    "name": "Tat-Seng Chua"
                },
                "author": "Tat-Seng Chua",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26093v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26093v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25889v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25889v2",
                "updated": "2025-10-01T03:37:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    3,
                    37,
                    48,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-30T07:30:30Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    7,
                    30,
                    30,
                    1,
                    273,
                    0
                ],
                "title": "A Multimodal LLM Approach for Visual Question Answering on\n  Multiparametric 3D Brain MRI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multimodal LLM Approach for Visual Question Answering on\n  Multiparametric 3D Brain MRI"
                },
                "summary": "We introduce mpLLM, a prompt-conditioned hierarchical mixture-of-experts\n(MoE) architecture for visual question answering over multi-parametric 3D brain\nMRI (mpMRI). mpLLM routes across modality-level and token-level projection\nexperts to fuse multiple interrelated 3D modalities, enabling efficient\ntraining without image-report pretraining. To address limited image-text paired\nsupervision, mpLLM integrates a synthetic visual question answering (VQA)\nprotocol that generates medically relevant VQA from segmentation annotations,\nand we collaborate with medical experts for clinical validation. mpLLM\noutperforms strong medical VLM baselines by 5.3% on average across multiple\nmpMRI datasets. Our study features three main contributions: (1) the first\nclinically validated VQA dataset for 3D brain mpMRI, (2) a novel multimodal LLM\nthat handles multiple interrelated 3D modalities, and (3) strong empirical\nresults that demonstrate the medical utility of our methodology. Ablations\nhighlight the importance of modality-level and token-level experts and\nprompt-conditioned routing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce mpLLM, a prompt-conditioned hierarchical mixture-of-experts\n(MoE) architecture for visual question answering over multi-parametric 3D brain\nMRI (mpMRI). mpLLM routes across modality-level and token-level projection\nexperts to fuse multiple interrelated 3D modalities, enabling efficient\ntraining without image-report pretraining. To address limited image-text paired\nsupervision, mpLLM integrates a synthetic visual question answering (VQA)\nprotocol that generates medically relevant VQA from segmentation annotations,\nand we collaborate with medical experts for clinical validation. mpLLM\noutperforms strong medical VLM baselines by 5.3% on average across multiple\nmpMRI datasets. Our study features three main contributions: (1) the first\nclinically validated VQA dataset for 3D brain mpMRI, (2) a novel multimodal LLM\nthat handles multiple interrelated 3D modalities, and (3) strong empirical\nresults that demonstrate the medical utility of our methodology. Ablations\nhighlight the importance of modality-level and token-level experts and\nprompt-conditioned routing."
                },
                "authors": [
                    {
                        "name": "Arvind Murari Vepa"
                    },
                    {
                        "name": "Yannan Yu"
                    },
                    {
                        "name": "Jingru Gan"
                    },
                    {
                        "name": "Anthony Cuturrufo"
                    },
                    {
                        "name": "Weikai Li"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Fabien Scalzo"
                    },
                    {
                        "name": "Yizhou Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yizhou Sun"
                },
                "author": "Yizhou Sun",
                "arxiv_comment": "23 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25889v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25889v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21317v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21317v2",
                "updated": "2025-10-01T03:31:57Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    3,
                    31,
                    57,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-25T15:38:27Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    15,
                    38,
                    27,
                    3,
                    268,
                    0
                ],
                "title": "Interactive Recommendation Agent with Active User Commands",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interactive Recommendation Agent with Active User Commands"
                },
                "summary": "Traditional recommender systems rely on passive feedback mechanisms that\nlimit users to simple choices such as like and dislike. However, these\ncoarse-grained signals fail to capture users' nuanced behavior motivations and\nintentions. In turn, current systems cannot also distinguish which specific\nitem attributes drive user satisfaction or dissatisfaction, resulting in\ninaccurate preference modeling. These fundamental limitations create a\npersistent gap between user intentions and system interpretations, ultimately\nundermining user satisfaction and harming system effectiveness.\n  To address these limitations, we introduce the Interactive Recommendation\nFeed (IRF), a pioneering paradigm that enables natural language commands within\nmainstream recommendation feeds. Unlike traditional systems that confine users\nto passive implicit behavioral influence, IRF empowers active explicit control\nover recommendation policies through real-time linguistic commands. To support\nthis paradigm, we develop RecBot, a dual-agent architecture where a Parser\nAgent transforms linguistic expressions into structured preferences and a\nPlanner Agent dynamically orchestrates adaptive tool chains for on-the-fly\npolicy adjustment. To enable practical deployment, we employ\nsimulation-augmented knowledge distillation to achieve efficient performance\nwhile maintaining strong reasoning capabilities. Through extensive offline and\nlong-term online experiments, RecBot shows significant improvements in both\nuser satisfaction and business outcomes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional recommender systems rely on passive feedback mechanisms that\nlimit users to simple choices such as like and dislike. However, these\ncoarse-grained signals fail to capture users' nuanced behavior motivations and\nintentions. In turn, current systems cannot also distinguish which specific\nitem attributes drive user satisfaction or dissatisfaction, resulting in\ninaccurate preference modeling. These fundamental limitations create a\npersistent gap between user intentions and system interpretations, ultimately\nundermining user satisfaction and harming system effectiveness.\n  To address these limitations, we introduce the Interactive Recommendation\nFeed (IRF), a pioneering paradigm that enables natural language commands within\nmainstream recommendation feeds. Unlike traditional systems that confine users\nto passive implicit behavioral influence, IRF empowers active explicit control\nover recommendation policies through real-time linguistic commands. To support\nthis paradigm, we develop RecBot, a dual-agent architecture where a Parser\nAgent transforms linguistic expressions into structured preferences and a\nPlanner Agent dynamically orchestrates adaptive tool chains for on-the-fly\npolicy adjustment. To enable practical deployment, we employ\nsimulation-augmented knowledge distillation to achieve efficient performance\nwhile maintaining strong reasoning capabilities. Through extensive offline and\nlong-term online experiments, RecBot shows significant improvements in both\nuser satisfaction and business outcomes."
                },
                "authors": [
                    {
                        "name": "Jiakai Tang"
                    },
                    {
                        "name": "Yujie Luo"
                    },
                    {
                        "name": "Xunke Xi"
                    },
                    {
                        "name": "Fei Sun"
                    },
                    {
                        "name": "Xueyang Feng"
                    },
                    {
                        "name": "Sunhao Dai"
                    },
                    {
                        "name": "Chao Yi"
                    },
                    {
                        "name": "Dian Chen"
                    },
                    {
                        "name": "Zhujin Gao"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Xu Chen"
                    },
                    {
                        "name": "Wen Chen"
                    },
                    {
                        "name": "Jian Wu"
                    },
                    {
                        "name": "Yuning Jiang"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21317v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21317v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12185v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12185v4",
                "updated": "2025-10-01T03:28:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    3,
                    28,
                    54,
                    2,
                    274,
                    0
                ],
                "published": "2025-05-18T01:02:33Z",
                "published_parsed": [
                    2025,
                    5,
                    18,
                    1,
                    2,
                    33,
                    6,
                    138,
                    0
                ],
                "title": "EVALOOOP: A Self-Consistency-Centered Framework for Assessing Large\n  Language Model Robustness in Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EVALOOOP: A Self-Consistency-Centered Framework for Assessing Large\n  Language Model Robustness in Programming"
                },
                "summary": "Evaluating the programming robustness of large language models (LLMs) is\nparamount for ensuring their reliability in AI-based software development.\nHowever, adversarial attacks exhibit fundamental limitations that compromise\nfair robustness assessment: they demonstrate contradictory evaluation outcomes\nwhere different attack strategies tend to favor different models, and more\ncritically, they operate solely through external perturbations, failing to\ncapture the intrinsic stability essential for autonomous coding agents where\nsubsequent inputs are endogenously generated by the model itself. We introduce\nEVALOOOP, a novel assessment framework that evaluates robustness from a\nself-consistency perspective, leveraging the natural duality inherent in\nsoftware engineering tasks (e.g., code generation and code summarization).\nEVALOOOP establishes a self-contained feedback loop where an LLM iteratively\ntransforms between code and natural language until functional failure occurs,\nwith robustness quantified by a novel Average Sustainable Loops (ASL)\nmetric-the mean number of iterations maintaining functional correctness across\nbenchmark tasks. This cyclical strategy intrinsically evaluates robustness\nwithout relying on external attack configurations, providing a unified metric\nthat reveals how effectively LLMs preserve semantic integrity through sustained\nself-referential transformations. We evaluate 96 popular LLMs, ranging from\n0.5B to 685B parameters, on EVALOOOP equipped with the MBPP Plus benchmark, and\nfound that EVALOOOP typically induces a 2.65%-47.62% absolute drop in pass@1\naccuracy within ten loops. Intriguingly, robustness does not always align with\ninitial performance (i.e., one-time query); for instance,\nQwen3-235B-A22B-Instruct-2507, despite inferior initial code generation\ncompared to OpenAI's o-series models and DeepSeek-V3, demonstrated the superior\nrobustness (ASL score).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the programming robustness of large language models (LLMs) is\nparamount for ensuring their reliability in AI-based software development.\nHowever, adversarial attacks exhibit fundamental limitations that compromise\nfair robustness assessment: they demonstrate contradictory evaluation outcomes\nwhere different attack strategies tend to favor different models, and more\ncritically, they operate solely through external perturbations, failing to\ncapture the intrinsic stability essential for autonomous coding agents where\nsubsequent inputs are endogenously generated by the model itself. We introduce\nEVALOOOP, a novel assessment framework that evaluates robustness from a\nself-consistency perspective, leveraging the natural duality inherent in\nsoftware engineering tasks (e.g., code generation and code summarization).\nEVALOOOP establishes a self-contained feedback loop where an LLM iteratively\ntransforms between code and natural language until functional failure occurs,\nwith robustness quantified by a novel Average Sustainable Loops (ASL)\nmetric-the mean number of iterations maintaining functional correctness across\nbenchmark tasks. This cyclical strategy intrinsically evaluates robustness\nwithout relying on external attack configurations, providing a unified metric\nthat reveals how effectively LLMs preserve semantic integrity through sustained\nself-referential transformations. We evaluate 96 popular LLMs, ranging from\n0.5B to 685B parameters, on EVALOOOP equipped with the MBPP Plus benchmark, and\nfound that EVALOOOP typically induces a 2.65%-47.62% absolute drop in pass@1\naccuracy within ten loops. Intriguingly, robustness does not always align with\ninitial performance (i.e., one-time query); for instance,\nQwen3-235B-A22B-Instruct-2507, despite inferior initial code generation\ncompared to OpenAI's o-series models and DeepSeek-V3, demonstrated the superior\nrobustness (ASL score)."
                },
                "authors": [
                    {
                        "name": "Sen Fang"
                    },
                    {
                        "name": "Weiyuan Ding"
                    },
                    {
                        "name": "Bowen Xu"
                    }
                ],
                "author_detail": {
                    "name": "Bowen Xu"
                },
                "author": "Bowen Xu",
                "arxiv_comment": "20 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12185v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12185v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14738v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14738v2",
                "updated": "2025-10-01T03:21:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    3,
                    21,
                    53,
                    2,
                    274,
                    0
                ],
                "published": "2025-05-20T06:07:00Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    6,
                    7,
                    0,
                    1,
                    140,
                    0
                ],
                "title": "R&D-Agent: An LLM-Agent Framework Towards Autonomous Data Science",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R&D-Agent: An LLM-Agent Framework Towards Autonomous Data Science"
                },
                "summary": "Recent advances in AI and ML have transformed data science, yet increasing\ncomplexity and expertise requirements continue to hinder progress. Although\ncrowd-sourcing platforms alleviate some challenges, high-level machine learning\nengineering (MLE) tasks remain labor-intensive and iterative. We introduce\nR&D-Agent, a comprehensive, decoupled, and extensible framework that formalizes\nthe MLE process. R&D-Agent defines the MLE workflow into two phases and six\ncomponents, turning agent design for MLE from ad-hoc craftsmanship into a\nprincipled, testable process. Although several existing agents report promising\ngains on their chosen components, they can mostly be summarized as a partial\noptimization from our framework's simple baseline. Inspired by human experts,\nwe designed efficient and effective agents within this framework that achieve\nstate-of-the-art performance. Evaluated on MLE-Bench, the agent built on\nR&D-Agent ranks as the top-performing machine learning engineering agent,\nachieving 35.1% any medal rate, demonstrating the ability of the framework to\nspeed up innovation and improve accuracy across a wide range of data science\napplications. We have open-sourced R&D-Agent on GitHub:\nhttps://github.com/microsoft/RD-Agent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in AI and ML have transformed data science, yet increasing\ncomplexity and expertise requirements continue to hinder progress. Although\ncrowd-sourcing platforms alleviate some challenges, high-level machine learning\nengineering (MLE) tasks remain labor-intensive and iterative. We introduce\nR&D-Agent, a comprehensive, decoupled, and extensible framework that formalizes\nthe MLE process. R&D-Agent defines the MLE workflow into two phases and six\ncomponents, turning agent design for MLE from ad-hoc craftsmanship into a\nprincipled, testable process. Although several existing agents report promising\ngains on their chosen components, they can mostly be summarized as a partial\noptimization from our framework's simple baseline. Inspired by human experts,\nwe designed efficient and effective agents within this framework that achieve\nstate-of-the-art performance. Evaluated on MLE-Bench, the agent built on\nR&D-Agent ranks as the top-performing machine learning engineering agent,\nachieving 35.1% any medal rate, demonstrating the ability of the framework to\nspeed up innovation and improve accuracy across a wide range of data science\napplications. We have open-sourced R&D-Agent on GitHub:\nhttps://github.com/microsoft/RD-Agent."
                },
                "authors": [
                    {
                        "name": "Xu Yang"
                    },
                    {
                        "name": "Xiao Yang"
                    },
                    {
                        "name": "Shikai Fang"
                    },
                    {
                        "name": "Yifei Zhang"
                    },
                    {
                        "name": "Jian Wang"
                    },
                    {
                        "name": "Bowen Xian"
                    },
                    {
                        "name": "Qizheng Li"
                    },
                    {
                        "name": "Jingyuan Li"
                    },
                    {
                        "name": "Minrui Xu"
                    },
                    {
                        "name": "Yuante Li"
                    },
                    {
                        "name": "Haoran Pan"
                    },
                    {
                        "name": "Yuge Zhang"
                    },
                    {
                        "name": "Weiqing Liu"
                    },
                    {
                        "name": "Yelong Shen"
                    },
                    {
                        "name": "Weizhu Chen"
                    },
                    {
                        "name": "Jiang Bian"
                    }
                ],
                "author_detail": {
                    "name": "Jiang Bian"
                },
                "author": "Jiang Bian",
                "arxiv_comment": "33 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14738v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14738v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25693v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25693v2",
                "updated": "2025-10-01T03:03:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    3,
                    3,
                    1,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-30T02:47:54Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    2,
                    47,
                    54,
                    1,
                    273,
                    0
                ],
                "title": "ScheduleMe: Multi-Agent Calendar Assistant",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ScheduleMe: Multi-Agent Calendar Assistant"
                },
                "summary": "Recent advancements in LLMs have contributed to the rise of advanced\nconversational assistants that can assist with user needs through natural\nlanguage conversation. This paper presents a ScheduleMe, a multi-agent calendar\nassistant for users to manage google calendar events in natural language. The\nsystem uses a graph-structured coordination mechanism where a central\nsupervisory agent supervises specialized task agents, allowing modularity,\nconflicts resolution, and context-aware interactions to resolve ambiguities and\nevaluate user commands. This approach sets an example of how structured\nreasoning and agent cooperation might convince operators to increase the\nusability and flexibility of personal calendar assistant tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in LLMs have contributed to the rise of advanced\nconversational assistants that can assist with user needs through natural\nlanguage conversation. This paper presents a ScheduleMe, a multi-agent calendar\nassistant for users to manage google calendar events in natural language. The\nsystem uses a graph-structured coordination mechanism where a central\nsupervisory agent supervises specialized task agents, allowing modularity,\nconflicts resolution, and context-aware interactions to resolve ambiguities and\nevaluate user commands. This approach sets an example of how structured\nreasoning and agent cooperation might convince operators to increase the\nusability and flexibility of personal calendar assistant tools."
                },
                "authors": [
                    {
                        "name": "Oshadha Wijerathne"
                    },
                    {
                        "name": "Amandi Nimasha"
                    },
                    {
                        "name": "Dushan Fernando"
                    },
                    {
                        "name": "Nisansa de Silva"
                    },
                    {
                        "name": "Srinath Perera"
                    }
                ],
                "author_detail": {
                    "name": "Srinath Perera"
                },
                "arxiv_affiliation": "WSO2 LLC",
                "author": "Srinath Perera",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25693v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25693v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13324v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13324v2",
                "updated": "2025-10-01T02:21:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    2,
                    21,
                    21,
                    2,
                    274,
                    0
                ],
                "published": "2025-08-17T21:56:06Z",
                "published_parsed": [
                    2025,
                    8,
                    17,
                    21,
                    56,
                    6,
                    6,
                    229,
                    0
                ],
                "title": "Designing Psychometric Bias Measures for ChatBots: An Application to\n  Racial Bias Measurement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing Psychometric Bias Measures for ChatBots: An Application to\n  Racial Bias Measurement"
                },
                "summary": "Artificial intelligence (AI), particularly in the form of large language\nmodels (LLMs) or chatbots, has become increasingly integrated into our daily\nlives. In the past five years, several LLMs have been introduced, including\nChatGPT by OpenAI, Claude by Anthropic, and Llama by Meta, among others. These\nmodels have the potential to be employed across a wide range of human-machine\ninteraction applications, such as chatbots for information retrieval,\nassistance in corporate hiring decisions, college admissions, financial loan\napprovals, parole determinations, and even in medical fields like psychotherapy\ndelivered through chatbots. The key question is whether these chatbots will\ninteract with humans in a bias-free manner or if they will further reinforce\nthe existing pathological biases present in human-to-human interactions. If the\nlatter is true, then how can we rigorously measure these biases?\n  We address this challenge by introducing STAMP-LLM (Standardized Test and\nAssessment Measurement Protocol for LLMs), a psychometric-based principled\ntwo-phase framework for designing psychometric measures to evaluate chatbot\nbiases: (i) a Definitional phase for construct mapping, item development, and\nexpert review; and (ii) a Data/Analysis phase for protocol control\n(prompts/decoding), automated sampling, pre-specified scoring, and basic\nreliability/validity checks. We illustrate STAMP-LLM on racial bias using one\nexplicit and two implicit measures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence (AI), particularly in the form of large language\nmodels (LLMs) or chatbots, has become increasingly integrated into our daily\nlives. In the past five years, several LLMs have been introduced, including\nChatGPT by OpenAI, Claude by Anthropic, and Llama by Meta, among others. These\nmodels have the potential to be employed across a wide range of human-machine\ninteraction applications, such as chatbots for information retrieval,\nassistance in corporate hiring decisions, college admissions, financial loan\napprovals, parole determinations, and even in medical fields like psychotherapy\ndelivered through chatbots. The key question is whether these chatbots will\ninteract with humans in a bias-free manner or if they will further reinforce\nthe existing pathological biases present in human-to-human interactions. If the\nlatter is true, then how can we rigorously measure these biases?\n  We address this challenge by introducing STAMP-LLM (Standardized Test and\nAssessment Measurement Protocol for LLMs), a psychometric-based principled\ntwo-phase framework for designing psychometric measures to evaluate chatbot\nbiases: (i) a Definitional phase for construct mapping, item development, and\nexpert review; and (ii) a Data/Analysis phase for protocol control\n(prompts/decoding), automated sampling, pre-specified scoring, and basic\nreliability/validity checks. We illustrate STAMP-LLM on racial bias using one\nexplicit and two implicit measures."
                },
                "authors": [
                    {
                        "name": "Mouhacine Benosman"
                    }
                ],
                "author_detail": {
                    "name": "Mouhacine Benosman"
                },
                "author": "Mouhacine Benosman",
                "arxiv_comment": "7 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13324v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13324v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11402v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11402v2",
                "updated": "2025-10-01T02:16:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    2,
                    16,
                    42,
                    2,
                    274,
                    0
                ],
                "published": "2025-06-13T02:02:57Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    2,
                    2,
                    57,
                    4,
                    164,
                    0
                ],
                "title": "LoRA Users Beware: A Few Spurious Tokens Can Manipulate Your Finetuned\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRA Users Beware: A Few Spurious Tokens Can Manipulate Your Finetuned\n  Model"
                },
                "summary": "Large Language Models (LLMs) are commonly finetuned for a variety of use\ncases and domains. A common approach is to leverage Low-Rank Adaptation (LoRA)\n-- known to provide strong performance at low resource costs. In this study, we\ndemonstrate that LoRA actually opens the door to short-cut vulnerabilities --\nand the more resource efficient is the LoRA setup, the more vulnerable will be\nthe finetuned model to aggressive attacks. To measure that vulnerability, we\nintroduce Seamless Spurious Token Injection (SSTI), where we find that LoRA\nexclusively focuses on even just a single token that is spuriously correlated\nwith downstream labels. In short, injection of that spurious token during\nfinetuning ensure that the model's prediction at test-time can be manipulated\non-demand. We conducted experiments across model families and datasets to\nevaluate the impact of SSTI during LoRA finetuning while providing possible\nmitigations. Our experiments conclude that none of the existing checkers and\npreprocessors can sanitize a dataset raising new concerns for data quality and\nAI safety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are commonly finetuned for a variety of use\ncases and domains. A common approach is to leverage Low-Rank Adaptation (LoRA)\n-- known to provide strong performance at low resource costs. In this study, we\ndemonstrate that LoRA actually opens the door to short-cut vulnerabilities --\nand the more resource efficient is the LoRA setup, the more vulnerable will be\nthe finetuned model to aggressive attacks. To measure that vulnerability, we\nintroduce Seamless Spurious Token Injection (SSTI), where we find that LoRA\nexclusively focuses on even just a single token that is spuriously correlated\nwith downstream labels. In short, injection of that spurious token during\nfinetuning ensure that the model's prediction at test-time can be manipulated\non-demand. We conducted experiments across model families and datasets to\nevaluate the impact of SSTI during LoRA finetuning while providing possible\nmitigations. Our experiments conclude that none of the existing checkers and\npreprocessors can sanitize a dataset raising new concerns for data quality and\nAI safety."
                },
                "authors": [
                    {
                        "name": "Marcel Mateos Salles"
                    },
                    {
                        "name": "Praney Goyal"
                    },
                    {
                        "name": "Pradyut Sekhsaria"
                    },
                    {
                        "name": "Hai Huang"
                    },
                    {
                        "name": "Randall Balestriero"
                    }
                ],
                "author_detail": {
                    "name": "Randall Balestriero"
                },
                "author": "Randall Balestriero",
                "arxiv_comment": "46 pages, 17 figures, 26 tables. Submitted for publication. for\n  associated blog post, see https://pradyut3501.github.io/lora-spur-corr/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11402v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11402v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26383v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26383v2",
                "updated": "2025-10-01T02:16:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    2,
                    16,
                    36,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-30T15:14:24Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    14,
                    24,
                    1,
                    273,
                    0
                ],
                "title": "Efficient and Transferable Agentic Knowledge Graph RAG via Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient and Transferable Agentic Knowledge Graph RAG via Reinforcement\n  Learning"
                },
                "summary": "Knowledge-graph retrieval-augmented generation (KG-RAG) couples large\nlanguage models (LLMs) with structured, verifiable knowledge graphs (KGs) to\nreduce hallucinations and expose reasoning traces. However, many KG-RAG systems\ncompose multiple LLM modules (e.g planning, reasoning, and responding),\ninflating inference cost and binding behavior to a specific target KG. To\naddress this, we introduce KG-R1, an agentic KG retrieval-augmented generation\n(KG-RAG) framework through reinforcement learning (RL). KG-R1 utilizes a single\nagent that interacts with KGs as its environment, learning to retrieve at each\nstep and incorporating the retrieved information into its reasoning and\ngeneration. The process is optimized through end-to-end RL. In controlled\nexperiments across Knowledge-Graph Question Answering (KGQA) benchmarks, our\nmethod demonstrates both efficiency and transferability: Using Qwen-2.5-3B,\nKG-R1 improves answer accuracy with fewer generation tokens than prior\nmulti-module workflow methods that use larger foundation or fine-tuned models.\nFurthermore, KG-R1 enables plug and play: after training, it maintains strong\naccuracy on new KGs without modification. These properties make KG-R1 a\npromising KG-RAG framework for real-world deployment. Our code is publicly\navailable at https://github.com/Jinyeop3110/KG-R1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-graph retrieval-augmented generation (KG-RAG) couples large\nlanguage models (LLMs) with structured, verifiable knowledge graphs (KGs) to\nreduce hallucinations and expose reasoning traces. However, many KG-RAG systems\ncompose multiple LLM modules (e.g planning, reasoning, and responding),\ninflating inference cost and binding behavior to a specific target KG. To\naddress this, we introduce KG-R1, an agentic KG retrieval-augmented generation\n(KG-RAG) framework through reinforcement learning (RL). KG-R1 utilizes a single\nagent that interacts with KGs as its environment, learning to retrieve at each\nstep and incorporating the retrieved information into its reasoning and\ngeneration. The process is optimized through end-to-end RL. In controlled\nexperiments across Knowledge-Graph Question Answering (KGQA) benchmarks, our\nmethod demonstrates both efficiency and transferability: Using Qwen-2.5-3B,\nKG-R1 improves answer accuracy with fewer generation tokens than prior\nmulti-module workflow methods that use larger foundation or fine-tuned models.\nFurthermore, KG-R1 enables plug and play: after training, it maintains strong\naccuracy on new KGs without modification. These properties make KG-R1 a\npromising KG-RAG framework for real-world deployment. Our code is publicly\navailable at https://github.com/Jinyeop3110/KG-R1."
                },
                "authors": [
                    {
                        "name": "Jinyeop Song"
                    },
                    {
                        "name": "Song Wang"
                    },
                    {
                        "name": "Julian Shun"
                    },
                    {
                        "name": "Yada Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Yada Zhu"
                },
                "author": "Yada Zhu",
                "arxiv_comment": "10 pages, 5 figures. Submitted to ICLR 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26383v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26383v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26574v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26574v2",
                "updated": "2025-10-01T02:12:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    2,
                    12,
                    55,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-30T17:34:03Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    34,
                    3,
                    1,
                    273,
                    0
                ],
                "title": "Probing the Critical Point (CritPt) of AI Reasoning: a Frontier Physics\n  Research Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing the Critical Point (CritPt) of AI Reasoning: a Frontier Physics\n  Research Benchmark"
                },
                "summary": "While large language models (LLMs) with reasoning capabilities are\nprogressing rapidly on high-school math competitions and coding, can they\nreason effectively through complex, open-ended challenges found in frontier\nphysics research? And crucially, what kinds of reasoning tasks do physicists\nwant LLMs to assist with? To address these questions, we present the CritPt\n(Complex Research using Integrated Thinking - Physics Test, pronounced\n\"critical point\"), the first benchmark designed to test LLMs on unpublished,\nresearch-level reasoning tasks that broadly covers modern physics research\nareas, including condensed matter, quantum physics, atomic, molecular & optical\nphysics, astrophysics, high energy physics, mathematical physics, statistical\nphysics, nuclear physics, nonlinear dynamics, fluid dynamics and biophysics.\nCritPt consists of 71 composite research challenges designed to simulate\nfull-scale research projects at the entry level, which are also decomposed to\n190 simpler checkpoint tasks for more fine-grained insights. All problems are\nnewly created by 50+ active physics researchers based on their own research.\nEvery problem is hand-curated to admit a guess-resistant and machine-verifiable\nanswer and is evaluated by an automated grading pipeline heavily customized for\nadvanced physics-specific output formats. We find that while current\nstate-of-the-art LLMs show early promise on isolated checkpoints, they remain\nfar from being able to reliably solve full research-scale challenges: the best\naverage accuracy among base models is only 4.0% , achieved by GPT-5 (high),\nmoderately rising to around 10% when equipped with coding tools. Through the\nrealistic yet standardized evaluation offered by CritPt, we highlight a large\ndisconnect between current model capabilities and realistic physics research\ndemands, offering a foundation to guide the development of scientifically\ngrounded AI tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) with reasoning capabilities are\nprogressing rapidly on high-school math competitions and coding, can they\nreason effectively through complex, open-ended challenges found in frontier\nphysics research? And crucially, what kinds of reasoning tasks do physicists\nwant LLMs to assist with? To address these questions, we present the CritPt\n(Complex Research using Integrated Thinking - Physics Test, pronounced\n\"critical point\"), the first benchmark designed to test LLMs on unpublished,\nresearch-level reasoning tasks that broadly covers modern physics research\nareas, including condensed matter, quantum physics, atomic, molecular & optical\nphysics, astrophysics, high energy physics, mathematical physics, statistical\nphysics, nuclear physics, nonlinear dynamics, fluid dynamics and biophysics.\nCritPt consists of 71 composite research challenges designed to simulate\nfull-scale research projects at the entry level, which are also decomposed to\n190 simpler checkpoint tasks for more fine-grained insights. All problems are\nnewly created by 50+ active physics researchers based on their own research.\nEvery problem is hand-curated to admit a guess-resistant and machine-verifiable\nanswer and is evaluated by an automated grading pipeline heavily customized for\nadvanced physics-specific output formats. We find that while current\nstate-of-the-art LLMs show early promise on isolated checkpoints, they remain\nfar from being able to reliably solve full research-scale challenges: the best\naverage accuracy among base models is only 4.0% , achieved by GPT-5 (high),\nmoderately rising to around 10% when equipped with coding tools. Through the\nrealistic yet standardized evaluation offered by CritPt, we highlight a large\ndisconnect between current model capabilities and realistic physics research\ndemands, offering a foundation to guide the development of scientifically\ngrounded AI tools."
                },
                "authors": [
                    {
                        "name": "Minhui Zhu"
                    },
                    {
                        "name": "Minyang Tian"
                    },
                    {
                        "name": "Xiaocheng Yang"
                    },
                    {
                        "name": "Tianci Zhou"
                    },
                    {
                        "name": "Penghao Zhu"
                    },
                    {
                        "name": "Eli Chertkov"
                    },
                    {
                        "name": "Shengyan Liu"
                    },
                    {
                        "name": "Yufeng Du"
                    },
                    {
                        "name": "Lifan Yuan"
                    },
                    {
                        "name": "Ziming Ji"
                    },
                    {
                        "name": "Indranil Das"
                    },
                    {
                        "name": "Junyi Cao"
                    },
                    {
                        "name": "Yufeng Du"
                    },
                    {
                        "name": "Jinchen He"
                    },
                    {
                        "name": "Yifan Su"
                    },
                    {
                        "name": "Jiabin Yu"
                    },
                    {
                        "name": "Yikun Jiang"
                    },
                    {
                        "name": "Yujie Zhang"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Ze-Min Huang"
                    },
                    {
                        "name": "Weizhen Jia"
                    },
                    {
                        "name": "Xinan Chen"
                    },
                    {
                        "name": "Peixue Wu"
                    },
                    {
                        "name": "Yunkai Wang"
                    },
                    {
                        "name": "Juntai Zhou"
                    },
                    {
                        "name": "Yong Zhao"
                    },
                    {
                        "name": "Farshid Jafarpour"
                    },
                    {
                        "name": "Jessie Shelton"
                    },
                    {
                        "name": "Aaron Young"
                    },
                    {
                        "name": "John Bartolotta"
                    },
                    {
                        "name": "Wenchao Xu"
                    },
                    {
                        "name": "Yue Sun"
                    },
                    {
                        "name": "Anjun Chu"
                    },
                    {
                        "name": "Victor Colussi"
                    },
                    {
                        "name": "Chris Akers"
                    },
                    {
                        "name": "Nathan Brooks"
                    },
                    {
                        "name": "Wenbo Fu"
                    },
                    {
                        "name": "Christopher Wilson"
                    },
                    {
                        "name": "Jinchao Zhao"
                    },
                    {
                        "name": "Marvin Qi"
                    },
                    {
                        "name": "Anqi Mu"
                    },
                    {
                        "name": "Yubo Yang"
                    },
                    {
                        "name": "Allen Zang"
                    },
                    {
                        "name": "Yang Lyu"
                    },
                    {
                        "name": "Peizhi Mai"
                    },
                    {
                        "name": "Xuefei Guo"
                    },
                    {
                        "name": "Luyu Gao"
                    },
                    {
                        "name": "Ze Yang"
                    },
                    {
                        "name": "Chi Xue"
                    },
                    {
                        "name": "Dmytro Bandak"
                    },
                    {
                        "name": "Yaïr Hein"
                    },
                    {
                        "name": "Yonatan Kahn"
                    },
                    {
                        "name": "Kevin Zhou"
                    },
                    {
                        "name": "John Drew Wilson"
                    },
                    {
                        "name": "Jarrod T. Reilly"
                    },
                    {
                        "name": "Di Luo"
                    },
                    {
                        "name": "Daniel Inafuku"
                    },
                    {
                        "name": "Hao Tong"
                    },
                    {
                        "name": "Liang Yang"
                    },
                    {
                        "name": "Ruixing Zhang"
                    },
                    {
                        "name": "Xueying Wang"
                    },
                    {
                        "name": "Ofir Press"
                    },
                    {
                        "name": "Nicolas Chia"
                    },
                    {
                        "name": "Eliu Huerta"
                    },
                    {
                        "name": "Hao Peng"
                    }
                ],
                "author_detail": {
                    "name": "Hao Peng"
                },
                "author": "Hao Peng",
                "arxiv_comment": "39 pages, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26574v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26574v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.other",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26255v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26255v2",
                "updated": "2025-10-01T01:58:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    1,
                    58,
                    1,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-30T13:44:34Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    13,
                    44,
                    34,
                    1,
                    273,
                    0
                ],
                "title": "ExoPredicator: Learning Abstract Models of Dynamic Worlds for Robot\n  Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExoPredicator: Learning Abstract Models of Dynamic Worlds for Robot\n  Planning"
                },
                "summary": "Long-horizon embodied planning is challenging because the world does not only\nchange through an agent's actions: exogenous processes (e.g., water heating,\ndominoes cascading) unfold concurrently with the agent's actions. We propose a\nframework for abstract world models that jointly learns (i) symbolic state\nrepresentations and (ii) causal processes for both endogenous actions and\nexogenous mechanisms. Each causal process models the time course of a\nstochastic cause-effect relation. We learn these world models from limited data\nvia variational Bayesian inference combined with LLM proposals. Across five\nsimulated tabletop robotics environments, the learned models enable fast\nplanning that generalizes to held-out tasks with more objects and more complex\ngoals, outperforming a range of baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-horizon embodied planning is challenging because the world does not only\nchange through an agent's actions: exogenous processes (e.g., water heating,\ndominoes cascading) unfold concurrently with the agent's actions. We propose a\nframework for abstract world models that jointly learns (i) symbolic state\nrepresentations and (ii) causal processes for both endogenous actions and\nexogenous mechanisms. Each causal process models the time course of a\nstochastic cause-effect relation. We learn these world models from limited data\nvia variational Bayesian inference combined with LLM proposals. Across five\nsimulated tabletop robotics environments, the learned models enable fast\nplanning that generalizes to held-out tasks with more objects and more complex\ngoals, outperforming a range of baselines."
                },
                "authors": [
                    {
                        "name": "Yichao Liang"
                    },
                    {
                        "name": "Dat Nguyen"
                    },
                    {
                        "name": "Cambridge Yang"
                    },
                    {
                        "name": "Tianyang Li"
                    },
                    {
                        "name": "Joshua B. Tenenbaum"
                    },
                    {
                        "name": "Carl Edward Rasmussen"
                    },
                    {
                        "name": "Adrian Weller"
                    },
                    {
                        "name": "Zenna Tavares"
                    },
                    {
                        "name": "Tom Silver"
                    },
                    {
                        "name": "Kevin Ellis"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Ellis"
                },
                "author": "Kevin Ellis",
                "arxiv_comment": "41 pages. The last two authors contributed equally in co-advising",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26255v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26255v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11595v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11595v4",
                "updated": "2025-10-01T01:55:06Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    1,
                    55,
                    6,
                    2,
                    274,
                    0
                ],
                "published": "2025-05-16T18:02:05Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    18,
                    2,
                    5,
                    4,
                    136,
                    0
                ],
                "title": "Stepwise Guided Policy Optimization: Coloring your Incorrect Reasoning\n  in GRPO",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stepwise Guided Policy Optimization: Coloring your Incorrect Reasoning\n  in GRPO"
                },
                "summary": "Reinforcement learning (RL) has proven effective in strengthening the\nreasoning capabilities of large language models (LLMs). A widely adopted\nmethod, Group Relative Policy Optimization (GRPO), has shown strong empirical\nresults in training DeepSeek-R1. However, GRPO fails to update the policy when\nall responses within a group are incorrect (i.e., \\emph{all-negative-sample}\ngroups). This limitation underscores a key gap between artificial and human\nintelligence: unlike humans, who can learn from mistakes, GRPO discards these\nsignals. Our first contribution is to introduce a simple framework that\nmitigates the all-negative-sample issue by incorporating response diversity\nwithin groups using a \\textit{step-wise} judge model, which can be either\ndirectly trained or adapted from existing LLMs. We prove that this\ndiversification can accelerate GRPO's learning dynamics in a simplified\nsetting. We also empirically validate the proposed stepwise guided policy\noptimization (SGPO) method, demonstrating consistent gains across model sizes\n(7B, 14B, 32B) in offline and online training on 9 benchmarks, including base\nand distilled variants. Our results highlight two advantages: (i) SGPO\nsurpasses GRPO, especially in the early and mid-training stages where\nall-negative-sample groups are prevalent; and (ii) SGPO does not require judge\nmodels to generate correct answers, differentiating it from knowledge\ndistillation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has proven effective in strengthening the\nreasoning capabilities of large language models (LLMs). A widely adopted\nmethod, Group Relative Policy Optimization (GRPO), has shown strong empirical\nresults in training DeepSeek-R1. However, GRPO fails to update the policy when\nall responses within a group are incorrect (i.e., \\emph{all-negative-sample}\ngroups). This limitation underscores a key gap between artificial and human\nintelligence: unlike humans, who can learn from mistakes, GRPO discards these\nsignals. Our first contribution is to introduce a simple framework that\nmitigates the all-negative-sample issue by incorporating response diversity\nwithin groups using a \\textit{step-wise} judge model, which can be either\ndirectly trained or adapted from existing LLMs. We prove that this\ndiversification can accelerate GRPO's learning dynamics in a simplified\nsetting. We also empirically validate the proposed stepwise guided policy\noptimization (SGPO) method, demonstrating consistent gains across model sizes\n(7B, 14B, 32B) in offline and online training on 9 benchmarks, including base\nand distilled variants. Our results highlight two advantages: (i) SGPO\nsurpasses GRPO, especially in the early and mid-training stages where\nall-negative-sample groups are prevalent; and (ii) SGPO does not require judge\nmodels to generate correct answers, differentiating it from knowledge\ndistillation methods."
                },
                "authors": [
                    {
                        "name": "Peter Chen"
                    },
                    {
                        "name": "Xiaopeng Li"
                    },
                    {
                        "name": "Ziniu Li"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Tianyi Lin"
                    }
                ],
                "author_detail": {
                    "name": "Tianyi Lin"
                },
                "author": "Tianyi Lin",
                "arxiv_comment": "42 pages; correct some typos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11595v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11595v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20485v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20485v3",
                "updated": "2025-10-01T01:51:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    1,
                    51,
                    45,
                    2,
                    274,
                    0
                ],
                "published": "2024-05-30T21:19:24Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    21,
                    19,
                    24,
                    3,
                    151,
                    0
                ],
                "title": "Phantom: General Backdoor Attacks on Retrieval Augmented Language\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phantom: General Backdoor Attacks on Retrieval Augmented Language\n  Generation"
                },
                "summary": "Retrieval Augmented Generation (RAG) expands the capabilities of modern large\nlanguage models (LLMs), by anchoring, adapting, and personalizing their\nresponses to the most relevant knowledge sources. It is particularly useful in\nchatbot applications, allowing developers to customize LLM output without\nexpensive retraining. Despite their significant utility in various\napplications, RAG systems present new security risks. In this work, we propose\na novel attack that allows an adversary to inject a single malicious document\ninto a RAG system's knowledge base, and mount a backdoor poisoning attack. We\ndesign Phantom, a general two-stage optimization framework against RAG systems,\nthat crafts a malicious poisoned document leading to an integrity violation in\nthe model's output. First, the document is constructed to be retrieved only\nwhen a specific naturally occurring trigger sequence of tokens appears in the\nvictim's queries. Second, the document is further optimized with crafted\nadversarial text that induces various adversarial objectives on the LLM output,\nincluding refusal to answer, reputation damage, privacy violations, and harmful\nbehaviors.We demonstrate our attacks on multiple open-source LLM architectures,\nincluding Gemma, Vicuna, and Llama, and show that they transfer to\nclosed-source models such as GPT-3.5 Turbo and GPT-4. Finally, we successfully\ndemonstrate our attack on an end-to-end black-box production RAG system:\nNVIDIA's \"Chat with RTX''.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation (RAG) expands the capabilities of modern large\nlanguage models (LLMs), by anchoring, adapting, and personalizing their\nresponses to the most relevant knowledge sources. It is particularly useful in\nchatbot applications, allowing developers to customize LLM output without\nexpensive retraining. Despite their significant utility in various\napplications, RAG systems present new security risks. In this work, we propose\na novel attack that allows an adversary to inject a single malicious document\ninto a RAG system's knowledge base, and mount a backdoor poisoning attack. We\ndesign Phantom, a general two-stage optimization framework against RAG systems,\nthat crafts a malicious poisoned document leading to an integrity violation in\nthe model's output. First, the document is constructed to be retrieved only\nwhen a specific naturally occurring trigger sequence of tokens appears in the\nvictim's queries. Second, the document is further optimized with crafted\nadversarial text that induces various adversarial objectives on the LLM output,\nincluding refusal to answer, reputation damage, privacy violations, and harmful\nbehaviors.We demonstrate our attacks on multiple open-source LLM architectures,\nincluding Gemma, Vicuna, and Llama, and show that they transfer to\nclosed-source models such as GPT-3.5 Turbo and GPT-4. Finally, we successfully\ndemonstrate our attack on an end-to-end black-box production RAG system:\nNVIDIA's \"Chat with RTX''."
                },
                "authors": [
                    {
                        "name": "Harsh Chaudhari"
                    },
                    {
                        "name": "Giorgio Severi"
                    },
                    {
                        "name": "John Abascal"
                    },
                    {
                        "name": "Anshuman Suri"
                    },
                    {
                        "name": "Matthew Jagielski"
                    },
                    {
                        "name": "Christopher A. Choquette-Choo"
                    },
                    {
                        "name": "Milad Nasr"
                    },
                    {
                        "name": "Cristina Nita-Rotaru"
                    },
                    {
                        "name": "Alina Oprea"
                    }
                ],
                "author_detail": {
                    "name": "Alina Oprea"
                },
                "author": "Alina Oprea",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20485v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20485v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16656v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16656v2",
                "updated": "2025-10-01T01:50:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    1,
                    50,
                    33,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-20T12:05:47Z",
                "published_parsed": [
                    2025,
                    9,
                    20,
                    12,
                    5,
                    47,
                    5,
                    263,
                    0
                ],
                "title": "NUMINA: A Natural Understanding Benchmark for Multi-dimensional\n  Intelligence and Numerical Reasoning Abilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NUMINA: A Natural Understanding Benchmark for Multi-dimensional\n  Intelligence and Numerical Reasoning Abilities"
                },
                "summary": "Recent advancements in 2D multimodal large language models (MLLMs) have\nsignificantly improved performance in vision-language tasks. However, extending\nthese capabilities to 3D environments remains a distinct challenge due to the\ncomplexity of spatial reasoning. Nevertheless, existing 3D benchmarks often\nlack fine-grained numerical reasoning task annotations, limiting MLLMs' ability\nto perform precise spatial measurements and complex numerical reasoning. To\naddress this gap, we introduce NUMINA, the first Natural Understanding\nbenchmark for Multi-dimensional Intelligence and Numerical reasoning Abilities\nto enhance multimodal indoor perceptual understanding. NUMINA features\nmulti-scale annotations and various question-answer pairs, generated using\nNUMINA-Flow, an automated annotation pipeline that integrates LLM rewriting and\nrule-based self-verification. We evaluate the performance of various\nstate-of-the-art LLMs on NUMINA following the Chat-Scene framework,\ndemonstrating that current LLMs struggle with multimodal numerical reasoning,\nparticularly in performing precise computations such as distance and volume\nestimation, highlighting the need for further advancements in 3D models. The\ndataset and source codes can be obtained from\nhttps://github.com/fengshun124/NUMINA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in 2D multimodal large language models (MLLMs) have\nsignificantly improved performance in vision-language tasks. However, extending\nthese capabilities to 3D environments remains a distinct challenge due to the\ncomplexity of spatial reasoning. Nevertheless, existing 3D benchmarks often\nlack fine-grained numerical reasoning task annotations, limiting MLLMs' ability\nto perform precise spatial measurements and complex numerical reasoning. To\naddress this gap, we introduce NUMINA, the first Natural Understanding\nbenchmark for Multi-dimensional Intelligence and Numerical reasoning Abilities\nto enhance multimodal indoor perceptual understanding. NUMINA features\nmulti-scale annotations and various question-answer pairs, generated using\nNUMINA-Flow, an automated annotation pipeline that integrates LLM rewriting and\nrule-based self-verification. We evaluate the performance of various\nstate-of-the-art LLMs on NUMINA following the Chat-Scene framework,\ndemonstrating that current LLMs struggle with multimodal numerical reasoning,\nparticularly in performing precise computations such as distance and volume\nestimation, highlighting the need for further advancements in 3D models. The\ndataset and source codes can be obtained from\nhttps://github.com/fengshun124/NUMINA."
                },
                "authors": [
                    {
                        "name": "Changyu Zeng"
                    },
                    {
                        "name": "Yifan Wang"
                    },
                    {
                        "name": "Zimu Wang"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Zhengni Yang"
                    },
                    {
                        "name": "Muyi Bao"
                    },
                    {
                        "name": "Jiming Xiao"
                    },
                    {
                        "name": "Anh Nguyen"
                    },
                    {
                        "name": "Yutao Yue"
                    }
                ],
                "author_detail": {
                    "name": "Yutao Yue"
                },
                "author": "Yutao Yue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16656v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16656v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23694v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23694v2",
                "updated": "2025-10-01T01:48:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    1,
                    48,
                    11,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-28T07:05:17Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    7,
                    5,
                    17,
                    6,
                    271,
                    0
                ],
                "title": "SafeSearch: Automated Red-Teaming for the Safety of LLM-Based Search\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SafeSearch: Automated Red-Teaming for the Safety of LLM-Based Search\n  Agents"
                },
                "summary": "Search agents connect LLMs to the Internet, enabling access to broader and\nmore up-to-date information. However, unreliable search results may also pose\nsafety threats to end users, establishing a new threat surface. In this work,\nwe conduct two in-the-wild experiments to demonstrate both the prevalence of\nlow-quality search results and their potential to misguide agent behaviors. To\ncounter this threat, we introduce an automated red-teaming framework that is\nsystematic, scalable, and cost-efficient, enabling lightweight and harmless\nsafety assessments of search agents. Building on this framework, we construct\nthe SafeSearch benchmark, which includes 300 test cases covering five\ncategories of risks (e.g., misinformation and indirect prompt injection). Using\nthis benchmark, we evaluate three representative search agent scaffolds,\ncovering search workflow, tool-calling, and deep research, across 7 proprietary\nand 8 open-source backend LLMs. Our results reveal substantial vulnerabilities\nof LLM-based search agents: when exposed to unreliable websites, the highest\nASR reached 90.5% for GPT-4.1-mini under a search workflow setting. Moreover,\nour analysis highlights the limited effectiveness of common defense practices,\nsuch as reminder prompting. This emphasizes the value of our framework in\npromoting transparency for safer agent development. Our codebase and test cases\nare publicly available: https://github.com/jianshuod/SafeSearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search agents connect LLMs to the Internet, enabling access to broader and\nmore up-to-date information. However, unreliable search results may also pose\nsafety threats to end users, establishing a new threat surface. In this work,\nwe conduct two in-the-wild experiments to demonstrate both the prevalence of\nlow-quality search results and their potential to misguide agent behaviors. To\ncounter this threat, we introduce an automated red-teaming framework that is\nsystematic, scalable, and cost-efficient, enabling lightweight and harmless\nsafety assessments of search agents. Building on this framework, we construct\nthe SafeSearch benchmark, which includes 300 test cases covering five\ncategories of risks (e.g., misinformation and indirect prompt injection). Using\nthis benchmark, we evaluate three representative search agent scaffolds,\ncovering search workflow, tool-calling, and deep research, across 7 proprietary\nand 8 open-source backend LLMs. Our results reveal substantial vulnerabilities\nof LLM-based search agents: when exposed to unreliable websites, the highest\nASR reached 90.5% for GPT-4.1-mini under a search workflow setting. Moreover,\nour analysis highlights the limited effectiveness of common defense practices,\nsuch as reminder prompting. This emphasizes the value of our framework in\npromoting transparency for safer agent development. Our codebase and test cases\nare publicly available: https://github.com/jianshuod/SafeSearch."
                },
                "authors": [
                    {
                        "name": "Jianshuo Dong"
                    },
                    {
                        "name": "Sheng Guo"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Zhuotao Liu"
                    },
                    {
                        "name": "Tianwei Zhang"
                    },
                    {
                        "name": "Ke Xu"
                    },
                    {
                        "name": "Minlie Huang"
                    },
                    {
                        "name": "Han Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Han Qiu"
                },
                "author": "Han Qiu",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23694v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23694v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26306v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26306v3",
                "updated": "2025-10-02T04:13:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    4,
                    13,
                    53,
                    3,
                    275,
                    0
                ],
                "published": "2025-09-30T14:21:31Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    21,
                    31,
                    1,
                    273,
                    0
                ],
                "title": "Interactive Learning for LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interactive Learning for LLM Reasoning"
                },
                "summary": "Existing multi-agent learning approaches have developed interactive training\nenvironments to explicitly promote collaboration among multiple Large Language\nModels (LLMs), thereby constructing stronger multi-agent systems (MAS).\nHowever, during inference, they require re-executing the MAS to obtain final\nsolutions, which diverges from human cognition that individuals can enhance\ntheir reasoning capabilities through interactions with others and resolve\nquestions independently in the future. To investigate whether multi-agent\ninteraction can enhance LLMs' independent problem-solving ability, we introduce\nILR, a novel co-learning framework for MAS that integrates two key components:\nDynamic Interaction and Perception Calibration. Specifically, Dynamic\nInteraction first adaptively selects either cooperative or competitive\nstrategies depending on question difficulty and model ability. LLMs then\nexchange information through Idea3 (Idea Sharing, Idea Analysis, and Idea\nFusion), an innovative interaction paradigm designed to mimic human discussion,\nbefore deriving their respective final answers. In Perception Calibration, ILR\nemploys Group Relative Policy Optimization (GRPO) to train LLMs while\nintegrating one LLM's reward distribution characteristics into another's reward\nfunction, thereby enhancing the cohesion of multi-agent interactions. We\nvalidate ILR on three LLMs across two model families of varying scales,\nevaluating performance on five mathematical benchmarks and one coding\nbenchmark. Experimental results show that ILR consistently outperforms\nsingle-agent learning, yielding an improvement of up to 5% over the strongest\nbaseline. We further discover that Idea3 can enhance the robustness of stronger\nLLMs during multi-agent inference, and dynamic interaction types can boost\nmulti-agent learning compared to pure cooperative or competitive strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing multi-agent learning approaches have developed interactive training\nenvironments to explicitly promote collaboration among multiple Large Language\nModels (LLMs), thereby constructing stronger multi-agent systems (MAS).\nHowever, during inference, they require re-executing the MAS to obtain final\nsolutions, which diverges from human cognition that individuals can enhance\ntheir reasoning capabilities through interactions with others and resolve\nquestions independently in the future. To investigate whether multi-agent\ninteraction can enhance LLMs' independent problem-solving ability, we introduce\nILR, a novel co-learning framework for MAS that integrates two key components:\nDynamic Interaction and Perception Calibration. Specifically, Dynamic\nInteraction first adaptively selects either cooperative or competitive\nstrategies depending on question difficulty and model ability. LLMs then\nexchange information through Idea3 (Idea Sharing, Idea Analysis, and Idea\nFusion), an innovative interaction paradigm designed to mimic human discussion,\nbefore deriving their respective final answers. In Perception Calibration, ILR\nemploys Group Relative Policy Optimization (GRPO) to train LLMs while\nintegrating one LLM's reward distribution characteristics into another's reward\nfunction, thereby enhancing the cohesion of multi-agent interactions. We\nvalidate ILR on three LLMs across two model families of varying scales,\nevaluating performance on five mathematical benchmarks and one coding\nbenchmark. Experimental results show that ILR consistently outperforms\nsingle-agent learning, yielding an improvement of up to 5% over the strongest\nbaseline. We further discover that Idea3 can enhance the robustness of stronger\nLLMs during multi-agent inference, and dynamic interaction types can boost\nmulti-agent learning compared to pure cooperative or competitive strategies."
                },
                "authors": [
                    {
                        "name": "Hehai Lin"
                    },
                    {
                        "name": "Shilei Cao"
                    },
                    {
                        "name": "Sudong Wang"
                    },
                    {
                        "name": "Haotian Wu"
                    },
                    {
                        "name": "Minzhi Li"
                    },
                    {
                        "name": "Linyi Yang"
                    },
                    {
                        "name": "Juepeng Zheng"
                    },
                    {
                        "name": "Chengwei Qin"
                    }
                ],
                "author_detail": {
                    "name": "Chengwei Qin"
                },
                "author": "Chengwei Qin",
                "arxiv_comment": "The code is available at\n  https://github.com/linhh29/Interactive-Learning-for-LLM-Reasoning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26306v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26306v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17333v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17333v2",
                "updated": "2025-10-01T01:07:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    1,
                    7,
                    44,
                    2,
                    274,
                    0
                ],
                "published": "2024-10-22T18:08:25Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    18,
                    8,
                    25,
                    1,
                    296,
                    0
                ],
                "title": "Whose Journey Matters? Investigating Identity Biases in Large Language\n  Models (LLMs) for Travel Planning Assistance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Whose Journey Matters? Investigating Identity Biases in Large Language\n  Models (LLMs) for Travel Planning Assistance"
                },
                "summary": "As large language models (LLMs) become increasingly integral to the\nhospitality and tourism industry, concerns about their fairness in serving\ndiverse identity groups persist. Grounded in social identity theory and\nsociotechnical systems theory, this study examines ethnic and gender biases in\ntravel recommendations generated by LLMs. Using fairness probing, we analyze\noutputs from three leading open-source LLMs. The results show that test\naccuracy for both ethnicity and gender classifiers exceed random chance.\nAnalysis of the most influential features reveals the presence of stereotype\nbias in LLM-generated recommendations. We also found hallucinations among these\nfeatures, occurring more frequently in recommendations for minority groups.\nThese findings indicate that LLMs exhibit ethnic and gender bias when\nfunctioning as travel planning assistants. This study underscores the need for\nbias mitigation strategies to improve the inclusivity and reliability of\ngenerative AI-driven travel planning assistance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become increasingly integral to the\nhospitality and tourism industry, concerns about their fairness in serving\ndiverse identity groups persist. Grounded in social identity theory and\nsociotechnical systems theory, this study examines ethnic and gender biases in\ntravel recommendations generated by LLMs. Using fairness probing, we analyze\noutputs from three leading open-source LLMs. The results show that test\naccuracy for both ethnicity and gender classifiers exceed random chance.\nAnalysis of the most influential features reveals the presence of stereotype\nbias in LLM-generated recommendations. We also found hallucinations among these\nfeatures, occurring more frequently in recommendations for minority groups.\nThese findings indicate that LLMs exhibit ethnic and gender bias when\nfunctioning as travel planning assistants. This study underscores the need for\nbias mitigation strategies to improve the inclusivity and reliability of\ngenerative AI-driven travel planning assistance."
                },
                "authors": [
                    {
                        "name": "Ruiping Ren"
                    },
                    {
                        "name": "Xing Yao"
                    },
                    {
                        "name": "Shu Cole"
                    },
                    {
                        "name": "Haining Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haining Wang"
                },
                "author": "Haining Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17333v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17333v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25179v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25179v2",
                "updated": "2025-10-01T01:02:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    1,
                    2,
                    7,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-29T17:59:23Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    59,
                    23,
                    0,
                    272,
                    0
                ],
                "title": "NAIPv2: Debiased Pairwise Learning for Efficient Paper Quality\n  Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NAIPv2: Debiased Pairwise Learning for Efficient Paper Quality\n  Estimation"
                },
                "summary": "The ability to estimate the quality of scientific papers is central to how\nboth humans and AI systems will advance scientific knowledge in the future.\nHowever, existing LLM-based estimation methods suffer from high inference cost,\nwhereas the faster direct score regression approach is limited by scale\ninconsistencies. We present NAIPv2, a debiased and efficient framework for\npaper quality estimation. NAIPv2 employs pairwise learning within domain-year\ngroups to reduce inconsistencies in reviewer ratings and introduces the Review\nTendency Signal (RTS) as a probabilistic integration of reviewer scores and\nconfidences. To support training and evaluation, we further construct NAIDv2, a\nlarge-scale dataset of 24,276 ICLR submissions enriched with metadata and\ndetailed structured content. Trained on pairwise comparisons but enabling\nefficient pointwise prediction at deployment, NAIPv2 achieves state-of-the-art\nperformance (78.2% AUC, 0.432 Spearman), while maintaining scalable,\nlinear-time efficiency at inference. Notably, on unseen NeurIPS submissions, it\nfurther demonstrates strong generalization, with predicted scores increasing\nconsistently across decision categories from Rejected to Oral. These findings\nestablish NAIPv2 as a debiased and scalable framework for automated paper\nquality estimation, marking a step toward future scientific intelligence\nsystems. Code and dataset are released at\nsway.cloud.microsoft/Pr42npP80MfPhvj8.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability to estimate the quality of scientific papers is central to how\nboth humans and AI systems will advance scientific knowledge in the future.\nHowever, existing LLM-based estimation methods suffer from high inference cost,\nwhereas the faster direct score regression approach is limited by scale\ninconsistencies. We present NAIPv2, a debiased and efficient framework for\npaper quality estimation. NAIPv2 employs pairwise learning within domain-year\ngroups to reduce inconsistencies in reviewer ratings and introduces the Review\nTendency Signal (RTS) as a probabilistic integration of reviewer scores and\nconfidences. To support training and evaluation, we further construct NAIDv2, a\nlarge-scale dataset of 24,276 ICLR submissions enriched with metadata and\ndetailed structured content. Trained on pairwise comparisons but enabling\nefficient pointwise prediction at deployment, NAIPv2 achieves state-of-the-art\nperformance (78.2% AUC, 0.432 Spearman), while maintaining scalable,\nlinear-time efficiency at inference. Notably, on unseen NeurIPS submissions, it\nfurther demonstrates strong generalization, with predicted scores increasing\nconsistently across decision categories from Rejected to Oral. These findings\nestablish NAIPv2 as a debiased and scalable framework for automated paper\nquality estimation, marking a step toward future scientific intelligence\nsystems. Code and dataset are released at\nsway.cloud.microsoft/Pr42npP80MfPhvj8."
                },
                "authors": [
                    {
                        "name": "Penghai Zhao"
                    },
                    {
                        "name": "Jinyu Tian"
                    },
                    {
                        "name": "Qinghua Xing"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Jianjun Qian"
                    },
                    {
                        "name": "Ming-Ming Cheng"
                    },
                    {
                        "name": "Xiang Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Li"
                },
                "author": "Xiang Li",
                "arxiv_comment": "NAIPv2 complements our earlier work NAIPv1 (arXiv:2408.03934).\n  Whereas NAIPv1 addressed citation count-based impact prediction, NAIPv2\n  estimates research quality using peer review data",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25179v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25179v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13082v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13082v2",
                "updated": "2025-10-01T00:40:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    0,
                    40,
                    32,
                    2,
                    274,
                    0
                ],
                "published": "2025-06-16T03:59:38Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    3,
                    59,
                    38,
                    0,
                    167,
                    0
                ],
                "title": "Discerning What Matters: A Multi-Dimensional Assessment of Moral\n  Competence in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discerning What Matters: A Multi-Dimensional Assessment of Moral\n  Competence in LLMs"
                },
                "summary": "Moral competence is the ability to act in accordance with moral principles.\nAs large language models (LLMs) are increasingly deployed in situations\ndemanding moral competence, there is increasing interest in evaluating this\nability empirically. We review existing literature and identify three\nsignificant shortcoming: (i) Over-reliance on prepackaged moral scenarios with\nexplicitly highlighted moral features; (ii) Focus on verdict prediction rather\nthan moral reasoning; and (iii) Inadequate testing of models' (in)ability to\nrecognize when additional information is needed. Grounded in philosophical\nresearch on moral skill, we then introduce a novel method for assessing moral\ncompetence in LLMs. Our approach moves beyond simple verdict comparisons to\nevaluate five dimensions of moral competence: identifying morally relevant\nfeatures, weighting their importance, assigning moral reasons to these\nfeatures, synthesizing coherent moral judgments, and recognizing information\ngaps. We conduct two experiments comparing six leading LLMs against non-expert\nhumans and professional philosophers. In our first experiment using ethical\nvignettes standard to existing work, LLMs generally outperformed non-expert\nhumans across multiple dimensions of moral reasoning. However, our second\nexperiment, featuring novel scenarios designed to test moral sensitivity by\nembedding relevant features among irrelevant details, revealed a striking\nreversal: several LLMs performed significantly worse than humans. Our findings\nsuggest that current evaluations may substantially overestimate LLMs' moral\nreasoning capabilities by eliminating the task of discerning moral relevance\nfrom noisy information, which we take to be a prerequisite for genuine moral\nskill. This work provides a more nuanced framework for assessing AI moral\ncompetence and highlights important directions for improving moral competence\nin advanced AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Moral competence is the ability to act in accordance with moral principles.\nAs large language models (LLMs) are increasingly deployed in situations\ndemanding moral competence, there is increasing interest in evaluating this\nability empirically. We review existing literature and identify three\nsignificant shortcoming: (i) Over-reliance on prepackaged moral scenarios with\nexplicitly highlighted moral features; (ii) Focus on verdict prediction rather\nthan moral reasoning; and (iii) Inadequate testing of models' (in)ability to\nrecognize when additional information is needed. Grounded in philosophical\nresearch on moral skill, we then introduce a novel method for assessing moral\ncompetence in LLMs. Our approach moves beyond simple verdict comparisons to\nevaluate five dimensions of moral competence: identifying morally relevant\nfeatures, weighting their importance, assigning moral reasons to these\nfeatures, synthesizing coherent moral judgments, and recognizing information\ngaps. We conduct two experiments comparing six leading LLMs against non-expert\nhumans and professional philosophers. In our first experiment using ethical\nvignettes standard to existing work, LLMs generally outperformed non-expert\nhumans across multiple dimensions of moral reasoning. However, our second\nexperiment, featuring novel scenarios designed to test moral sensitivity by\nembedding relevant features among irrelevant details, revealed a striking\nreversal: several LLMs performed significantly worse than humans. Our findings\nsuggest that current evaluations may substantially overestimate LLMs' moral\nreasoning capabilities by eliminating the task of discerning moral relevance\nfrom noisy information, which we take to be a prerequisite for genuine moral\nskill. This work provides a more nuanced framework for assessing AI moral\ncompetence and highlights important directions for improving moral competence\nin advanced AI systems."
                },
                "authors": [
                    {
                        "name": "Daniel Kilov"
                    },
                    {
                        "name": "Caroline Hendy"
                    },
                    {
                        "name": "Secil Yanik Guyot"
                    },
                    {
                        "name": "Aaron J. Snoswell"
                    },
                    {
                        "name": "Seth Lazar"
                    }
                ],
                "author_detail": {
                    "name": "Seth Lazar"
                },
                "author": "Seth Lazar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13082v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13082v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25426v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25426v2",
                "updated": "2025-10-01T00:34:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    0,
                    34,
                    10,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-29T19:33:44Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    19,
                    33,
                    44,
                    0,
                    272,
                    0
                ],
                "title": "RADAR: Reasoning-Ability and Difficulty-Aware Routing for Reasoning LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RADAR: Reasoning-Ability and Difficulty-Aware Routing for Reasoning LLMs"
                },
                "summary": "Reasoning language models have demonstrated remarkable performance on many\nchallenging tasks in math, science, and coding. Choosing the right reasoning\nmodel for practical deployment involves a performance and cost tradeoff at two\nkey levels: model size and reasoning budget, where larger models and higher\nreasoning budget lead to better performance but with increased cost and\nlatency. In this work, we tackle this tradeoff from the angle of model\nconfiguration routing for different queries, and present RADAR\n(Reasoning-Ability and Difficulty-Aware Routing), a lightweight, interpretable,\nand scalable routing framework. Inspired by psychometrics, RADAR learns an item\nresponse model from model responses with different budgets to different\nqueries, with interpretable parameters including query difficulties and\nmodel-budget abilities. RADAR then routes queries with higher difficulty to\nmodel-budget pairs with higher ability, and vice versa. We conduct extensive\nexperiments on 8 widely used challenging reasoning benchmarks, demonstrating\nthe superior performance of RADAR compared to state-of-the-art model routing\nmethods. RADAR also exhibits query generalization capabilities, showing strong\nperformance on out-of-distribution queries in all benchmarks. RADAR is also\nscalable and can efficiently integrate additional models by dynamically\nselecting a small set of evaluation queries to estimate their abilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning language models have demonstrated remarkable performance on many\nchallenging tasks in math, science, and coding. Choosing the right reasoning\nmodel for practical deployment involves a performance and cost tradeoff at two\nkey levels: model size and reasoning budget, where larger models and higher\nreasoning budget lead to better performance but with increased cost and\nlatency. In this work, we tackle this tradeoff from the angle of model\nconfiguration routing for different queries, and present RADAR\n(Reasoning-Ability and Difficulty-Aware Routing), a lightweight, interpretable,\nand scalable routing framework. Inspired by psychometrics, RADAR learns an item\nresponse model from model responses with different budgets to different\nqueries, with interpretable parameters including query difficulties and\nmodel-budget abilities. RADAR then routes queries with higher difficulty to\nmodel-budget pairs with higher ability, and vice versa. We conduct extensive\nexperiments on 8 widely used challenging reasoning benchmarks, demonstrating\nthe superior performance of RADAR compared to state-of-the-art model routing\nmethods. RADAR also exhibits query generalization capabilities, showing strong\nperformance on out-of-distribution queries in all benchmarks. RADAR is also\nscalable and can efficiently integrate additional models by dynamically\nselecting a small set of evaluation queries to estimate their abilities."
                },
                "authors": [
                    {
                        "name": "Nigel Fernandez"
                    },
                    {
                        "name": "Branislav Kveton"
                    },
                    {
                        "name": "Ryan A. Rossi"
                    },
                    {
                        "name": "Andrew S. Lan"
                    },
                    {
                        "name": "Zichao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zichao Wang"
                },
                "author": "Zichao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25426v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25426v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23488v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23488v2",
                "updated": "2025-10-01T00:19:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    0,
                    19,
                    41,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-27T20:23:13Z",
                "published_parsed": [
                    2025,
                    9,
                    27,
                    20,
                    23,
                    13,
                    5,
                    270,
                    0
                ],
                "title": "Mapping Overlaps in Benchmarks through Perplexity in the Wild",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping Overlaps in Benchmarks through Perplexity in the Wild"
                },
                "summary": "We develop signatures of capacity familiarity to characterize large language\nmodel (LLM) benchmarks and their meaningful overlaps. Benchmark signatures\nprobe the capacity required for benchmark performance. We formally define them\nas a set of salient tokens drawn from in-the-wild, naturally authored corpora,\nwhere LLM token perplexity, reflecting more or less pre-training exposure,\nbecomes highly predictive of LLM benchmark performance. Through a large-scale\nmeta-evaluation, we extract benchmark signatures via stepwise forward selection\nwith linear regressions across 32 LLMs and 88 benchmarks spanning diverse\nknowledge, coding, logic, instruction following, math, language, reasoning, and\nworld modeling. Our analysis situates signatures in relation to both the\nsemantic similarity of benchmark questions and the correlation of model\nperformance. While performance overlaps are universally high and semantic\noverlaps remain confined to a narrow mid-range, benchmark signatures prove\nhighly informative in capturing variation, overlap, and divergence. We observe\noverlap in knowledge and reasoning subtasks, whereas multilingual and cultural\nbenchmarks exhibit less similarity, even compared to cross-task overlap.\nNotably, performance-level results are strongly influenced by\nbenchmark-orthogonal factors such as question format, highlighting limitations\nin LLM generalization, the conflation of performance with ability, and issues\ninherent in current mainstream benchmark agreement studies. Benchmark\nsignatures, however, remain robust to such effects. Ultimately, we identify\ncross-functional overlaps across logic, math, language, instruction following,\nand world modeling, with coding emerging as the least overlapping domain.\nTogether, these findings provide mechanistic insights into benchmark validity\nand LLM sensitivities, and sketch the underlying landscape of interconnected\nLLM capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop signatures of capacity familiarity to characterize large language\nmodel (LLM) benchmarks and their meaningful overlaps. Benchmark signatures\nprobe the capacity required for benchmark performance. We formally define them\nas a set of salient tokens drawn from in-the-wild, naturally authored corpora,\nwhere LLM token perplexity, reflecting more or less pre-training exposure,\nbecomes highly predictive of LLM benchmark performance. Through a large-scale\nmeta-evaluation, we extract benchmark signatures via stepwise forward selection\nwith linear regressions across 32 LLMs and 88 benchmarks spanning diverse\nknowledge, coding, logic, instruction following, math, language, reasoning, and\nworld modeling. Our analysis situates signatures in relation to both the\nsemantic similarity of benchmark questions and the correlation of model\nperformance. While performance overlaps are universally high and semantic\noverlaps remain confined to a narrow mid-range, benchmark signatures prove\nhighly informative in capturing variation, overlap, and divergence. We observe\noverlap in knowledge and reasoning subtasks, whereas multilingual and cultural\nbenchmarks exhibit less similarity, even compared to cross-task overlap.\nNotably, performance-level results are strongly influenced by\nbenchmark-orthogonal factors such as question format, highlighting limitations\nin LLM generalization, the conflation of performance with ability, and issues\ninherent in current mainstream benchmark agreement studies. Benchmark\nsignatures, however, remain robust to such effects. Ultimately, we identify\ncross-functional overlaps across logic, math, language, instruction following,\nand world modeling, with coding emerging as the least overlapping domain.\nTogether, these findings provide mechanistic insights into benchmark validity\nand LLM sensitivities, and sketch the underlying landscape of interconnected\nLLM capabilities."
                },
                "authors": [
                    {
                        "name": "Siyang Wu"
                    },
                    {
                        "name": "Honglin Bao"
                    },
                    {
                        "name": "Sida Li"
                    },
                    {
                        "name": "Ari Holtzman"
                    },
                    {
                        "name": "James A. Evans"
                    }
                ],
                "author_detail": {
                    "name": "James A. Evans"
                },
                "author": "James A. Evans",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23488v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23488v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15427v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15427v3",
                "updated": "2025-09-30T23:52:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    23,
                    52,
                    2,
                    1,
                    273,
                    0
                ],
                "published": "2025-04-21T20:37:23Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    20,
                    37,
                    23,
                    0,
                    111,
                    0
                ],
                "title": "TVR: Automotive System Requirement Traceability Validation and Recovery\n  Through Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TVR: Automotive System Requirement Traceability Validation and Recovery\n  Through Retrieval-Augmented Generation"
                },
                "summary": "In automotive software development, as well as other domains, traceability\nbetween stakeholder requirements and system requirements is crucial to ensure\nconsistency, correctness, and regulatory compliance. However, erroneous or\nmissing traceability relationships often arise due to improper propagation of\nrequirement changes or human errors in requirement mapping, leading to\ninconsistencies and increased maintenance costs. Existing approaches do not\naddress traceability between stakeholder and system requirements, and are not\nvalidated on industrial data, where the links between requirements are\nestablished manually by engineers. Additionally, automotive requirements often\nexhibit variations in the way they are expressed, posing challenges for\ntraining-based approaches. Recent advancements in large language models (LLMs)\nprovide new opportunities to address these challenges. In this paper, we\nintroduce TVR, a requirement Traceability Validation and Recovery approach\nprimarily targeting automotive systems, leveraging LLMs enhanced with\nretrieval-augmented generation (RAG). TVR is designed to validate existing\ntraceability links and recover missing ones with high accuracy. The\nexperimental results highlight the practical effectiveness of TVR in industrial\nsettings, offering a promising solution for improving requirements traceability\nin complex automotive systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In automotive software development, as well as other domains, traceability\nbetween stakeholder requirements and system requirements is crucial to ensure\nconsistency, correctness, and regulatory compliance. However, erroneous or\nmissing traceability relationships often arise due to improper propagation of\nrequirement changes or human errors in requirement mapping, leading to\ninconsistencies and increased maintenance costs. Existing approaches do not\naddress traceability between stakeholder and system requirements, and are not\nvalidated on industrial data, where the links between requirements are\nestablished manually by engineers. Additionally, automotive requirements often\nexhibit variations in the way they are expressed, posing challenges for\ntraining-based approaches. Recent advancements in large language models (LLMs)\nprovide new opportunities to address these challenges. In this paper, we\nintroduce TVR, a requirement Traceability Validation and Recovery approach\nprimarily targeting automotive systems, leveraging LLMs enhanced with\nretrieval-augmented generation (RAG). TVR is designed to validate existing\ntraceability links and recover missing ones with high accuracy. The\nexperimental results highlight the practical effectiveness of TVR in industrial\nsettings, offering a promising solution for improving requirements traceability\nin complex automotive systems."
                },
                "authors": [
                    {
                        "name": "Feifei Niu"
                    },
                    {
                        "name": "Rongqi Pan"
                    },
                    {
                        "name": "Lionel C. Briand"
                    },
                    {
                        "name": "Hanyang Hu"
                    }
                ],
                "author_detail": {
                    "name": "Hanyang Hu"
                },
                "author": "Hanyang Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15427v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15427v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14746v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14746v2",
                "updated": "2025-09-30T23:51:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    23,
                    51,
                    5,
                    1,
                    273,
                    0
                ],
                "published": "2025-08-20T14:43:04Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    14,
                    43,
                    4,
                    2,
                    232,
                    0
                ],
                "title": "MissionHD: Hyperdimensional Refinement of Distribution-Deficient\n  Reasoning Graphs for Video Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MissionHD: Hyperdimensional Refinement of Distribution-Deficient\n  Reasoning Graphs for Video Anomaly Detection"
                },
                "summary": "LLM-generated reasoning graphs, referred to as mission-specific graphs\n(MSGs), are increasingly used for video anomaly detection (VAD) and recognition\n(VAR). These MSGs are novel artifacts: they often exhibit skewed connectivity\nand lack large-scale datasets for pre-training, which makes existing graph\nstructure refinement (GSR) methods ineffective. To address this challenge, we\npropose HDC-constrained Graph Structure Refinement (HDC-GSR), a paradigm that\nleverages hyperdimensional computing (HDC) to optimize decodable graph\nrepresentations without relying on structural-distribution learning. Building\non this paradigm, we introduce MissionHD, an HDC framework that encodes graphs\nwith constrained graph-neural operations, aligns them directly with downstream\ntask loss, and decodes refined structures. Experiments on VAD/VAR benchmarks\ndemonstrate that MissionHD-refined graphs consistently improve performance,\nestablishing HDC-GSR as an effective pre-processing step for structured\nreasoning in video anomaly tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-generated reasoning graphs, referred to as mission-specific graphs\n(MSGs), are increasingly used for video anomaly detection (VAD) and recognition\n(VAR). These MSGs are novel artifacts: they often exhibit skewed connectivity\nand lack large-scale datasets for pre-training, which makes existing graph\nstructure refinement (GSR) methods ineffective. To address this challenge, we\npropose HDC-constrained Graph Structure Refinement (HDC-GSR), a paradigm that\nleverages hyperdimensional computing (HDC) to optimize decodable graph\nrepresentations without relying on structural-distribution learning. Building\non this paradigm, we introduce MissionHD, an HDC framework that encodes graphs\nwith constrained graph-neural operations, aligns them directly with downstream\ntask loss, and decodes refined structures. Experiments on VAD/VAR benchmarks\ndemonstrate that MissionHD-refined graphs consistently improve performance,\nestablishing HDC-GSR as an effective pre-processing step for structured\nreasoning in video anomaly tasks."
                },
                "authors": [
                    {
                        "name": "Sanggeon Yun"
                    },
                    {
                        "name": "Raheeb Hassan"
                    },
                    {
                        "name": "Ryozo Masukawa"
                    },
                    {
                        "name": "Nathaniel D. Bastian"
                    },
                    {
                        "name": "Mohsen Imani"
                    }
                ],
                "author_detail": {
                    "name": "Mohsen Imani"
                },
                "author": "Mohsen Imani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14746v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14746v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00245v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00245v2",
                "updated": "2025-09-30T23:48:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    23,
                    48,
                    39,
                    1,
                    273,
                    0
                ],
                "published": "2025-08-29T21:23:48Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    21,
                    23,
                    48,
                    4,
                    241,
                    0
                ],
                "title": "The Rarity Blind Spot: A Framework for Evaluating Statistical Reasoning\n  in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Rarity Blind Spot: A Framework for Evaluating Statistical Reasoning\n  in LLMs"
                },
                "summary": "Effective decision-making often relies on identifying what makes each\ncandidate distinctive. While existing benchmarks for LLMs emphasize retrieving\nor summarizing information relevant to a given query, they do not evaluate a\nmodel's ability to identify globally distinctive features across a set of\ndocuments. We introduce Distinctive Feature Mining (DFM), a new task that\nchallenges models to analyze a small-to-medium collection (10-40 documents) and\nsurface features that are rare in the global context (e.g., appearing in less\nthan 10% of documents). This setting mirrors real-world scenarios such as\ncandidate selection or product differentiation, where statistical reasoning,\nnot retrieval, is key. To enable systematic evaluation of this capability, we\npresent DiFBench, a configurable benchmark creation framework with controllable\nparameters such as document set size and distinctiveness thresholds. Using\nDiFBench, we perform a large-scale assessment of distinctive feature mining\nacross ten state-of-the-art LLMs. Our findings reveal a significant performance\ngap between general-purpose and reasoning-enhanced models. All models, however,\nsubstantially degrade as the task complexity and document count increase. We\nalso find that a common failure mode is misidentifying frequent features as\ndistinctive. These insights reveal core limitations in contemporary LLMs'\nabilities to perform fine-grained, statistical reasoning and rarity detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective decision-making often relies on identifying what makes each\ncandidate distinctive. While existing benchmarks for LLMs emphasize retrieving\nor summarizing information relevant to a given query, they do not evaluate a\nmodel's ability to identify globally distinctive features across a set of\ndocuments. We introduce Distinctive Feature Mining (DFM), a new task that\nchallenges models to analyze a small-to-medium collection (10-40 documents) and\nsurface features that are rare in the global context (e.g., appearing in less\nthan 10% of documents). This setting mirrors real-world scenarios such as\ncandidate selection or product differentiation, where statistical reasoning,\nnot retrieval, is key. To enable systematic evaluation of this capability, we\npresent DiFBench, a configurable benchmark creation framework with controllable\nparameters such as document set size and distinctiveness thresholds. Using\nDiFBench, we perform a large-scale assessment of distinctive feature mining\nacross ten state-of-the-art LLMs. Our findings reveal a significant performance\ngap between general-purpose and reasoning-enhanced models. All models, however,\nsubstantially degrade as the task complexity and document count increase. We\nalso find that a common failure mode is misidentifying frequent features as\ndistinctive. These insights reveal core limitations in contemporary LLMs'\nabilities to perform fine-grained, statistical reasoning and rarity detection."
                },
                "authors": [
                    {
                        "name": "Seiji Maekawa"
                    },
                    {
                        "name": "Hayate Iso"
                    },
                    {
                        "name": "Nikita Bhutani"
                    }
                ],
                "author_detail": {
                    "name": "Nikita Bhutani"
                },
                "author": "Nikita Bhutani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00245v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00245v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23234v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23234v2",
                "updated": "2025-09-30T21:36:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    21,
                    36,
                    20,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-27T10:33:41Z",
                "published_parsed": [
                    2025,
                    9,
                    27,
                    10,
                    33,
                    41,
                    5,
                    270,
                    0
                ],
                "title": "$p$-less Sampling: A Robust Hyperparameter-Free Approach for LLM\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$p$-less Sampling: A Robust Hyperparameter-Free Approach for LLM\n  Decoding"
                },
                "summary": "Obtaining high-quality outputs from Large Language Models (LLMs) often\ndepends upon the choice of a sampling-based decoding strategy to\nprobabilistically choose the next token at each generation step. While a\nvariety of such sampling methods have been proposed, their performance can be\nsensitive to the selection of hyperparameters which may require different\nsettings depending upon the generation task and temperature configuration. In\nthis work, we introduce $p$-less sampling: an information-theoretic approach to\nsampling which dynamically sets a truncation threshold at each decoding step\nbased on the entire token probability distribution. Unlike existing methods,\n$p$-less sampling has no hyperparameters and consistently produces high-quality\noutputs as temperature increases. We provide theoretical perspectives on\n$p$-less sampling to ground our proposed method and conduct experiments to\nempirically validate its effectiveness across a range of math, logical\nreasoning, and creative writing tasks. Our results demonstrate how $p$-less\nsampling consistently outperforms existing sampling approaches while exhibiting\nmuch less degradation in text quality at higher temperature values. We further\nshow how $p$-less achieves greater inference-time efficiency than alternative\nmethods through lower average token sampling times and shorter generation\nlengths, without sacrificing accuracy. Finally, we provide analyses to\nhighlight the benefits of $p$-less through qualitative examples, case studies,\nand diversity assessments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Obtaining high-quality outputs from Large Language Models (LLMs) often\ndepends upon the choice of a sampling-based decoding strategy to\nprobabilistically choose the next token at each generation step. While a\nvariety of such sampling methods have been proposed, their performance can be\nsensitive to the selection of hyperparameters which may require different\nsettings depending upon the generation task and temperature configuration. In\nthis work, we introduce $p$-less sampling: an information-theoretic approach to\nsampling which dynamically sets a truncation threshold at each decoding step\nbased on the entire token probability distribution. Unlike existing methods,\n$p$-less sampling has no hyperparameters and consistently produces high-quality\noutputs as temperature increases. We provide theoretical perspectives on\n$p$-less sampling to ground our proposed method and conduct experiments to\nempirically validate its effectiveness across a range of math, logical\nreasoning, and creative writing tasks. Our results demonstrate how $p$-less\nsampling consistently outperforms existing sampling approaches while exhibiting\nmuch less degradation in text quality at higher temperature values. We further\nshow how $p$-less achieves greater inference-time efficiency than alternative\nmethods through lower average token sampling times and shorter generation\nlengths, without sacrificing accuracy. Finally, we provide analyses to\nhighlight the benefits of $p$-less through qualitative examples, case studies,\nand diversity assessments."
                },
                "authors": [
                    {
                        "name": "Runyan Tan"
                    },
                    {
                        "name": "Shuang Wu"
                    },
                    {
                        "name": "Phillip Howard"
                    }
                ],
                "author_detail": {
                    "name": "Phillip Howard"
                },
                "author": "Phillip Howard",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23234v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23234v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]