[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2503.13275v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13275v2",
                "updated": "2025-04-01T14:21:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    14,
                    21,
                    15,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-17T15:27:02Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    27,
                    2,
                    0,
                    76,
                    0
                ],
                "title": "Knowledge-Aware Iterative Retrieval for Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-Aware Iterative Retrieval for Multi-Agent Systems"
                },
                "summary": "We introduce a novel large language model (LLM)-driven agent framework, which\niteratively refines queries and filters contextual evidence by leveraging\ndynamically evolving knowledge. A defining feature of the system is its\ndecoupling of external sources from an internal knowledge cache that is\nprogressively updated to guide both query generation and evidence selection.\nThis design mitigates bias-reinforcement loops and enables dynamic, trackable\nsearch exploration paths, thereby optimizing the trade-off between exploring\ndiverse information and maintaining accuracy through autonomous agent\ndecision-making. Our approach is evaluated on a broad range of open-domain\nquestion answering benchmarks, including multi-step tasks that mirror\nreal-world scenarios where integrating information from multiple sources is\ncritical, especially given the vulnerabilities of LLMs that lack explicit\nreasoning or planning capabilities. The results show that the proposed system\nnot only outperforms single-step baselines regardless of task difficulty but\nalso, compared to conventional iterative retrieval methods, demonstrates\npronounced advantages in complex tasks through precise evidence-based reasoning\nand enhanced efficiency. The proposed system supports both competitive and\ncollaborative sharing of updated context, enabling multi-agent extension. The\nbenefits of multi-agent configurations become especially prominent as task\ndifficulty increases. The number of convergence steps scales with task\ndifficulty, suggesting cost-effective scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel large language model (LLM)-driven agent framework, which\niteratively refines queries and filters contextual evidence by leveraging\ndynamically evolving knowledge. A defining feature of the system is its\ndecoupling of external sources from an internal knowledge cache that is\nprogressively updated to guide both query generation and evidence selection.\nThis design mitigates bias-reinforcement loops and enables dynamic, trackable\nsearch exploration paths, thereby optimizing the trade-off between exploring\ndiverse information and maintaining accuracy through autonomous agent\ndecision-making. Our approach is evaluated on a broad range of open-domain\nquestion answering benchmarks, including multi-step tasks that mirror\nreal-world scenarios where integrating information from multiple sources is\ncritical, especially given the vulnerabilities of LLMs that lack explicit\nreasoning or planning capabilities. The results show that the proposed system\nnot only outperforms single-step baselines regardless of task difficulty but\nalso, compared to conventional iterative retrieval methods, demonstrates\npronounced advantages in complex tasks through precise evidence-based reasoning\nand enhanced efficiency. The proposed system supports both competitive and\ncollaborative sharing of updated context, enabling multi-agent extension. The\nbenefits of multi-agent configurations become especially prominent as task\ndifficulty increases. The number of convergence steps scales with task\ndifficulty, suggesting cost-effective scalability."
                },
                "authors": [
                    {
                        "name": "Seyoung Song"
                    }
                ],
                "author_detail": {
                    "name": "Seyoung Song"
                },
                "author": "Seyoung Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13275v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13275v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.7; I.2.11; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24358v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24358v1",
                "updated": "2025-03-31T17:37:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    37,
                    32,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T17:37:32Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    37,
                    32,
                    0,
                    90,
                    0
                ],
                "title": "SQuat: Subspace-orthogonal KV Cache Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SQuat: Subspace-orthogonal KV Cache Quantization"
                },
                "summary": "The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from\npreviously generated tokens. It reduces redundant computation at the cost of\nincreased memory usage. To mitigate this overhead, existing approaches compress\nKV tensors into lower-bit representations; however, quantization errors can\naccumulate as more tokens are generated, potentially resulting in undesired\noutputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache\nquantization). It first constructs a subspace spanned by query tensors to\ncapture the most critical task-related information. During key tensor\nquantization, it enforces that the difference between the (de)quantized and\noriginal keys remains orthogonal to this subspace, minimizing the impact of\nquantization errors on the attention mechanism's outputs. SQuat requires no\nmodel fine-tuning, no additional calibration dataset for offline learning, and\nis grounded in a theoretical framework we develop. Through numerical\nexperiments, we show that our method reduces peak memory by 2.17 to 2.82,\nimproves throughput by 2.45 to 3.60, and achieves more favorable benchmark\nscores than existing KV cache quantization algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from\npreviously generated tokens. It reduces redundant computation at the cost of\nincreased memory usage. To mitigate this overhead, existing approaches compress\nKV tensors into lower-bit representations; however, quantization errors can\naccumulate as more tokens are generated, potentially resulting in undesired\noutputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache\nquantization). It first constructs a subspace spanned by query tensors to\ncapture the most critical task-related information. During key tensor\nquantization, it enforces that the difference between the (de)quantized and\noriginal keys remains orthogonal to this subspace, minimizing the impact of\nquantization errors on the attention mechanism's outputs. SQuat requires no\nmodel fine-tuning, no additional calibration dataset for offline learning, and\nis grounded in a theoretical framework we develop. Through numerical\nexperiments, we show that our method reduces peak memory by 2.17 to 2.82,\nimproves throughput by 2.45 to 3.60, and achieves more favorable benchmark\nscores than existing KV cache quantization algorithms."
                },
                "authors": [
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Ligong Han"
                    },
                    {
                        "name": "Kai Xu"
                    },
                    {
                        "name": "Akash Srivastava"
                    }
                ],
                "author_detail": {
                    "name": "Akash Srivastava"
                },
                "author": "Akash Srivastava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24358v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24358v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24007v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24007v1",
                "updated": "2025-03-31T12:32:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    32,
                    23,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T12:32:23Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    32,
                    23,
                    0,
                    90,
                    0
                ],
                "title": "CITRAS: Covariate-Informed Transformer for Time Series Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CITRAS: Covariate-Informed Transformer for Time Series Forecasting"
                },
                "summary": "Covariates play an indispensable role in practical time series forecasting,\noffering rich context from the past and sometimes extending into the future.\nHowever, their availability varies depending on the scenario, and situations\noften involve multiple target variables simultaneously. Moreover, the\ncross-variate dependencies between them are multi-granular, with some\ncovariates having a short-term impact on target variables and others showing\nlong-term correlations. This heterogeneity and the intricate dependencies\narising in covariate-informed forecasting present significant challenges to\nexisting deep models. To address these issues, we propose CITRAS, a patch-based\nTransformer that flexibly leverages multiple targets and covariates covering\nboth the past and the future forecasting horizon. While preserving the strong\nautoregressive capabilities of the canonical Transformer, CITRAS introduces two\nnovel mechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift\nand Attention Score Smoothing. KV Shift seamlessly incorporates future known\ncovariates into the forecasting of target variables based on their concurrent\ndependencies. Additionally, Attention Score Smoothing transforms locally\naccurate patch-wise cross-variate dependencies into global variate-level\ndependencies by smoothing the past series of attention scores. Experimentally,\nCITRAS achieves state-of-the-art performance in both covariate-informed and\nmultivariate forecasting, demonstrating its versatile ability to leverage\ncross-variate dependency for improved forecasting accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Covariates play an indispensable role in practical time series forecasting,\noffering rich context from the past and sometimes extending into the future.\nHowever, their availability varies depending on the scenario, and situations\noften involve multiple target variables simultaneously. Moreover, the\ncross-variate dependencies between them are multi-granular, with some\ncovariates having a short-term impact on target variables and others showing\nlong-term correlations. This heterogeneity and the intricate dependencies\narising in covariate-informed forecasting present significant challenges to\nexisting deep models. To address these issues, we propose CITRAS, a patch-based\nTransformer that flexibly leverages multiple targets and covariates covering\nboth the past and the future forecasting horizon. While preserving the strong\nautoregressive capabilities of the canonical Transformer, CITRAS introduces two\nnovel mechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift\nand Attention Score Smoothing. KV Shift seamlessly incorporates future known\ncovariates into the forecasting of target variables based on their concurrent\ndependencies. Additionally, Attention Score Smoothing transforms locally\naccurate patch-wise cross-variate dependencies into global variate-level\ndependencies by smoothing the past series of attention scores. Experimentally,\nCITRAS achieves state-of-the-art performance in both covariate-informed and\nmultivariate forecasting, demonstrating its versatile ability to leverage\ncross-variate dependency for improved forecasting accuracy."
                },
                "authors": [
                    {
                        "name": "Yosuke Yamaguchi"
                    },
                    {
                        "name": "Issei Suemitsu"
                    },
                    {
                        "name": "Wenpeng Wei"
                    }
                ],
                "author_detail": {
                    "name": "Wenpeng Wei"
                },
                "author": "Wenpeng Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24007v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24007v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24000v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24000v1",
                "updated": "2025-03-31T12:23:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    23,
                    31,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T12:23:31Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    23,
                    31,
                    0,
                    90,
                    0
                ],
                "title": "Rethinking Key-Value Cache Compression Techniques for Large Language\n  Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Key-Value Cache Compression Techniques for Large Language\n  Model Serving"
                },
                "summary": "Key-Value cache (\\texttt{KV} \\texttt{cache}) compression has emerged as a\npromising technique to optimize Large Language Model (LLM) serving. It\nprimarily decreases the memory consumption of \\texttt{KV} \\texttt{cache} to\nreduce the computation cost. Despite the development of many compression\nalgorithms, their applications in production environments are still not\nprevalent. In this paper, we revisit mainstream \\texttt{KV} \\texttt{cache}\ncompression solutions from a practical perspective. Our contributions are\nthree-fold. First, we comprehensively review existing algorithmic designs and\nbenchmark studies for \\texttt{KV} \\texttt{cache} compression and identify\nmissing pieces in their performance measurement, which could hinder their\nadoption in practice. Second, we empirically evaluate representative\n\\texttt{KV} \\texttt{cache} compression methods to uncover two key issues that\naffect the computational efficiency: (1) while compressing \\texttt{KV}\n\\texttt{cache} can reduce memory consumption, current implementations (e.g.,\nFlashAttention, PagedAttention) do not optimize for production-level LLM\nserving, resulting in suboptimal throughput performance; (2) compressing\n\\texttt{KV} \\texttt{cache} may lead to longer outputs, resulting in increased\nend-to-end latency. We further investigate the accuracy performance of\nindividual samples rather than the overall performance, revealing the intrinsic\nlimitations in \\texttt{KV} \\texttt{cache} compression when handling specific\nLLM tasks. Third, we provide tools to shed light on future \\texttt{KV}\n\\texttt{cache} compression studies and facilitate their practical deployment in\nproduction. They are open-sourced in\n\\href{https://github.com/LLMkvsys/rethink-kv-compression}{https://github.com/LLMkvsys/rethink-kv-compression}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value cache (\\texttt{KV} \\texttt{cache}) compression has emerged as a\npromising technique to optimize Large Language Model (LLM) serving. It\nprimarily decreases the memory consumption of \\texttt{KV} \\texttt{cache} to\nreduce the computation cost. Despite the development of many compression\nalgorithms, their applications in production environments are still not\nprevalent. In this paper, we revisit mainstream \\texttt{KV} \\texttt{cache}\ncompression solutions from a practical perspective. Our contributions are\nthree-fold. First, we comprehensively review existing algorithmic designs and\nbenchmark studies for \\texttt{KV} \\texttt{cache} compression and identify\nmissing pieces in their performance measurement, which could hinder their\nadoption in practice. Second, we empirically evaluate representative\n\\texttt{KV} \\texttt{cache} compression methods to uncover two key issues that\naffect the computational efficiency: (1) while compressing \\texttt{KV}\n\\texttt{cache} can reduce memory consumption, current implementations (e.g.,\nFlashAttention, PagedAttention) do not optimize for production-level LLM\nserving, resulting in suboptimal throughput performance; (2) compressing\n\\texttt{KV} \\texttt{cache} may lead to longer outputs, resulting in increased\nend-to-end latency. We further investigate the accuracy performance of\nindividual samples rather than the overall performance, revealing the intrinsic\nlimitations in \\texttt{KV} \\texttt{cache} compression when handling specific\nLLM tasks. Third, we provide tools to shed light on future \\texttt{KV}\n\\texttt{cache} compression studies and facilitate their practical deployment in\nproduction. They are open-sourced in\n\\href{https://github.com/LLMkvsys/rethink-kv-compression}{https://github.com/LLMkvsys/rethink-kv-compression}."
                },
                "authors": [
                    {
                        "name": "Wei Gao"
                    },
                    {
                        "name": "Xinyu Zhou"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Tianwei Zhang"
                    },
                    {
                        "name": "Yonggang Wen"
                    }
                ],
                "author_detail": {
                    "name": "Yonggang Wen"
                },
                "author": "Yonggang Wen",
                "arxiv_comment": "21 pages, 18 figures, published to MLSys2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24000v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24000v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23988v1",
                "updated": "2025-03-31T11:58:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    58,
                    37,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T11:58:37Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    58,
                    37,
                    0,
                    90,
                    0
                ],
                "title": "Deep Learning Model Deployment in Multiple Cloud Providers: an\n  Exploratory Study Using Low Computing Power Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning Model Deployment in Multiple Cloud Providers: an\n  Exploratory Study Using Low Computing Power Environments"
                },
                "summary": "The deployment of Machine Learning models at cloud have grown by tech\ncompanies. Hardware requirements are higher when these models involve Deep\nLearning (DL) techniques and the cloud providers' costs may be a barrier. We\nexplore deploying DL models using for experiments the GECToR model, a DL\nsolution for Grammatical Error Correction, across three of the major cloud\nplatforms (AWS, Google Cloud, Azure). We evaluate real-time latency, hardware\nusage and cost at each cloud provider by 7 execution environments with 10\nexperiments reproduced. We found that while GPUs excel in performance, they had\nan average cost 300% higher than solutions without GPU. Our analysis also\nidentifies that processor cache size is crucial for cost-effective CPU\ndeployments, enabling over 50% of cost reduction compared to GPUs. This study\ndemonstrates the feasibility and affordability of cloud-based DL inference\nsolutions without GPUs, benefiting resource-constrained users like startups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of Machine Learning models at cloud have grown by tech\ncompanies. Hardware requirements are higher when these models involve Deep\nLearning (DL) techniques and the cloud providers' costs may be a barrier. We\nexplore deploying DL models using for experiments the GECToR model, a DL\nsolution for Grammatical Error Correction, across three of the major cloud\nplatforms (AWS, Google Cloud, Azure). We evaluate real-time latency, hardware\nusage and cost at each cloud provider by 7 execution environments with 10\nexperiments reproduced. We found that while GPUs excel in performance, they had\nan average cost 300% higher than solutions without GPU. Our analysis also\nidentifies that processor cache size is crucial for cost-effective CPU\ndeployments, enabling over 50% of cost reduction compared to GPUs. This study\ndemonstrates the feasibility and affordability of cloud-based DL inference\nsolutions without GPUs, benefiting resource-constrained users like startups."
                },
                "authors": [
                    {
                        "name": "Elayne Lemos"
                    },
                    {
                        "name": "Rodrigo Oliveira"
                    },
                    {
                        "name": "Jairson Rodrigues"
                    },
                    {
                        "name": "Rosalvo F. Oliveira Neto"
                    }
                ],
                "author_detail": {
                    "name": "Rosalvo F. Oliveira Neto"
                },
                "author": "Rosalvo F. Oliveira Neto",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07, 68U01",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.4; I.2.0; B.8.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23956v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23956v1",
                "updated": "2025-03-31T11:13:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    13,
                    18,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T11:13:18Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    13,
                    18,
                    0,
                    90,
                    0
                ],
                "title": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference"
                },
                "summary": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches."
                },
                "authors": [
                    {
                        "name": "Kai Huang"
                    },
                    {
                        "name": "Hao Zou"
                    },
                    {
                        "name": "Bochen Wang"
                    },
                    {
                        "name": "Ye Xi"
                    },
                    {
                        "name": "Zhen Xie"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23956v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23956v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18334v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18334v2",
                "updated": "2025-03-31T10:28:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    10,
                    28,
                    4,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-24T04:32:35Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    4,
                    32,
                    35,
                    0,
                    83,
                    0
                ],
                "title": "Mitigating Cache Noise in Test-Time Adaptation for Large Vision-Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Cache Noise in Test-Time Adaptation for Large Vision-Language\n  Models"
                },
                "summary": "Test-time adaptation (TTA) of visual language models has recently attracted\nsignificant attention as a solution to the performance degradation caused by\ndistribution shifts in downstream tasks. However, existing cache-based TTA\nmethods have certain limitations. They mainly rely on the accuracy of cached\nfeature labels, and the presence of noisy pseudo-labels can cause these\nfeatures to deviate from their true distribution. This makes cache retrieval\nmethods based on similarity matching highly sensitive to outliers or extreme\nsamples. Moreover, current methods lack effective mechanisms to model class\ndistributions, which limits their ability to fully exploit the potential of\ncached information. To address these challenges, we introduce a comprehensive\nand reliable caching mechanism and propose a novel zero-shot TTA method called\n\"Cache, Residual, Gaussian\" (CRG). This method not only employs learnable\nresidual parameters to better align positive and negative visual prototypes\nwith text prototypes, thereby optimizing the quality of cached features, but\nalso incorporates Gaussian Discriminant Analysis (GDA) to dynamically model\nintra-class feature distributions, further mitigating the impact of noisy\nfeatures. Experimental results on 13 benchmarks demonstrate that CRG\noutperforms state-of-the-art TTA methods, showcasing exceptional robustness and\nadaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time adaptation (TTA) of visual language models has recently attracted\nsignificant attention as a solution to the performance degradation caused by\ndistribution shifts in downstream tasks. However, existing cache-based TTA\nmethods have certain limitations. They mainly rely on the accuracy of cached\nfeature labels, and the presence of noisy pseudo-labels can cause these\nfeatures to deviate from their true distribution. This makes cache retrieval\nmethods based on similarity matching highly sensitive to outliers or extreme\nsamples. Moreover, current methods lack effective mechanisms to model class\ndistributions, which limits their ability to fully exploit the potential of\ncached information. To address these challenges, we introduce a comprehensive\nand reliable caching mechanism and propose a novel zero-shot TTA method called\n\"Cache, Residual, Gaussian\" (CRG). This method not only employs learnable\nresidual parameters to better align positive and negative visual prototypes\nwith text prototypes, thereby optimizing the quality of cached features, but\nalso incorporates Gaussian Discriminant Analysis (GDA) to dynamically model\nintra-class feature distributions, further mitigating the impact of noisy\nfeatures. Experimental results on 13 benchmarks demonstrate that CRG\noutperforms state-of-the-art TTA methods, showcasing exceptional robustness and\nadaptability."
                },
                "authors": [
                    {
                        "name": "Haotian Zhai"
                    },
                    {
                        "name": "Xinyu Chen"
                    },
                    {
                        "name": "Can Zhang"
                    },
                    {
                        "name": "Tianming Sha"
                    },
                    {
                        "name": "Ruirui Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruirui Li"
                },
                "author": "Ruirui Li",
                "arxiv_comment": "Accepted by ICME 2025 and ICLR 2025 Workshop on Foundation Models in\n  the Wild",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18334v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18334v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23897v1",
                "updated": "2025-03-31T09:46:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    9,
                    46,
                    56,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T09:46:56Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    9,
                    46,
                    56,
                    0,
                    90,
                    0
                ],
                "title": "Training-Free Text-Guided Image Editing with Visual Autoregressive Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Text-Guided Image Editing with Visual Autoregressive Model"
                },
                "summary": "Text-guided image editing is an essential task that enables users to modify\nimages through natural language descriptions. Recent advances in diffusion\nmodels and rectified flows have significantly improved editing quality,\nprimarily relying on inversion techniques to extract structured noise from\ninput images. However, inaccuracies in inversion can propagate errors, leading\nto unintended modifications and compromising fidelity. Moreover, even with\nperfect inversion, the entanglement between textual prompts and image features\noften results in global changes when only local edits are intended. To address\nthese challenges, we propose a novel text-guided image editing framework based\non VAR (Visual AutoRegressive modeling), which eliminates the need for explicit\ninversion while ensuring precise and controlled modifications. Our method\nintroduces a caching mechanism that stores token indices and probability\ndistributions from the original image, capturing the relationship between the\nsource prompt and the image. Using this cache, we design an adaptive\nfine-grained masking strategy that dynamically identifies and constrains\nmodifications to relevant regions, preventing unintended changes. A token\nreassembling approach further refines the editing process, enhancing diversity,\nfidelity, and control. Our framework operates in a training-free manner and\nachieves high-fidelity editing with faster inference speeds, processing a 1K\nresolution image in as fast as 1.2 seconds. Extensive experiments demonstrate\nthat our method achieves performance comparable to, or even surpassing,\nexisting diffusion- and rectified flow-based approaches in both quantitative\nmetrics and visual quality. The code will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-guided image editing is an essential task that enables users to modify\nimages through natural language descriptions. Recent advances in diffusion\nmodels and rectified flows have significantly improved editing quality,\nprimarily relying on inversion techniques to extract structured noise from\ninput images. However, inaccuracies in inversion can propagate errors, leading\nto unintended modifications and compromising fidelity. Moreover, even with\nperfect inversion, the entanglement between textual prompts and image features\noften results in global changes when only local edits are intended. To address\nthese challenges, we propose a novel text-guided image editing framework based\non VAR (Visual AutoRegressive modeling), which eliminates the need for explicit\ninversion while ensuring precise and controlled modifications. Our method\nintroduces a caching mechanism that stores token indices and probability\ndistributions from the original image, capturing the relationship between the\nsource prompt and the image. Using this cache, we design an adaptive\nfine-grained masking strategy that dynamically identifies and constrains\nmodifications to relevant regions, preventing unintended changes. A token\nreassembling approach further refines the editing process, enhancing diversity,\nfidelity, and control. Our framework operates in a training-free manner and\nachieves high-fidelity editing with faster inference speeds, processing a 1K\nresolution image in as fast as 1.2 seconds. Extensive experiments demonstrate\nthat our method achieves performance comparable to, or even surpassing,\nexisting diffusion- and rectified flow-based approaches in both quantitative\nmetrics and visual quality. The code will be released."
                },
                "authors": [
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Lanqing Guo"
                    },
                    {
                        "name": "Zhihao Li"
                    },
                    {
                        "name": "Jiaxing Huang"
                    },
                    {
                        "name": "Pichao Wang"
                    },
                    {
                        "name": "Bihan Wen"
                    },
                    {
                        "name": "Jian Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Wang"
                },
                "author": "Jian Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17808v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17808v4",
                "updated": "2025-03-31T03:28:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    3,
                    28,
                    44,
                    0,
                    90,
                    0
                ],
                "published": "2024-06-24T03:59:17Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    3,
                    59,
                    17,
                    0,
                    176,
                    0
                ],
                "title": "Training-Free Exponential Context Extension via Cascading KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Exponential Context Extension via Cascading KV Cache"
                },
                "summary": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency."
                },
                "authors": [
                    {
                        "name": "Jeffrey Willette"
                    },
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17808v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17808v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21817v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21817v2",
                "updated": "2025-03-31T02:19:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    2,
                    19,
                    29,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-26T04:16:48Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    4,
                    16,
                    48,
                    2,
                    85,
                    0
                ],
                "title": "Skip-Vision: Efficient and Scalable Acceleration of Vision-Language\n  Models via Adaptive Token Skipping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skip-Vision: Efficient and Scalable Acceleration of Vision-Language\n  Models via Adaptive Token Skipping"
                },
                "summary": "Transformer-based models have driven significant advancements in Multimodal\nLarge Language Models (MLLMs), yet their computational costs surge drastically\nwhen scaling resolution, training data, and model parameters. A key bottleneck\nstems from the proliferation of visual tokens required for fine-grained image\nunderstanding. We propose Skip-Vision, a unified framework addressing both\ntraining and inference inefficiencies in vision-language models. On top of\nconventional token compression approaches, our method introduces two\ncomplementary acceleration strategies. For training acceleration, we observe\nthat Feed-Forward Network (FFN) computations on visual tokens induce marginal\nfeature updates. This motivates our Skip-FFN strategy, which bypasses FFN\nlayers for redundant visual tokens. For inference acceleration, we design a\nselective KV-cache removal mechanism that prunes the skipped key-value pairs\nduring decoding while preserving model performance. Experimental results\ndemonstrate that Skip-Vision reduces training time by up to 35\\%, inference\nFLOPs by 75\\%, and latency by 45\\%, while achieving comparable or superior\nperformance to existing methods. Our work provides a practical solution for\nscaling high-performance MLLMs with enhanced efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based models have driven significant advancements in Multimodal\nLarge Language Models (MLLMs), yet their computational costs surge drastically\nwhen scaling resolution, training data, and model parameters. A key bottleneck\nstems from the proliferation of visual tokens required for fine-grained image\nunderstanding. We propose Skip-Vision, a unified framework addressing both\ntraining and inference inefficiencies in vision-language models. On top of\nconventional token compression approaches, our method introduces two\ncomplementary acceleration strategies. For training acceleration, we observe\nthat Feed-Forward Network (FFN) computations on visual tokens induce marginal\nfeature updates. This motivates our Skip-FFN strategy, which bypasses FFN\nlayers for redundant visual tokens. For inference acceleration, we design a\nselective KV-cache removal mechanism that prunes the skipped key-value pairs\nduring decoding while preserving model performance. Experimental results\ndemonstrate that Skip-Vision reduces training time by up to 35\\%, inference\nFLOPs by 75\\%, and latency by 45\\%, while achieving comparable or superior\nperformance to existing methods. Our work provides a practical solution for\nscaling high-performance MLLMs with enhanced efficiency."
                },
                "authors": [
                    {
                        "name": "Weili Zeng"
                    },
                    {
                        "name": "Ziyuan Huang"
                    },
                    {
                        "name": "Kaixiang Ji"
                    },
                    {
                        "name": "Yichao Yan"
                    }
                ],
                "author_detail": {
                    "name": "Yichao Yan"
                },
                "author": "Yichao Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21817v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21817v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10270v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10270v2",
                "updated": "2025-03-30T11:14:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    11,
                    14,
                    17,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-13T11:26:45Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    11,
                    26,
                    45,
                    3,
                    72,
                    0
                ],
                "title": "EEdit: Rethinking the Spatial and Temporal Redundancy for Efficient\n  Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EEdit: Rethinking the Spatial and Temporal Redundancy for Efficient\n  Image Editing"
                },
                "summary": "Inversion-based image editing is rapidly gaining momentum while suffering\nfrom significant computation overhead, hindering its application in real-time\ninteractive scenarios. In this paper, we rethink that the redundancy in\ninversion-based image editing exists in both the spatial and temporal\ndimensions, such as the unnecessary computation in unedited regions and the\nredundancy in the inversion progress. To tackle these challenges, we propose a\npractical framework, named EEdit, to achieve efficient image editing.\nSpecifically, we introduce three techniques to solve them one by one. For\nspatial redundancy, spatial locality caching is introduced to compute the\nedited region and its neighboring regions while skipping the unedited regions,\nand token indexing preprocessing is designed to further accelerate the caching.\nFor temporal redundancy, inversion step skipping is proposed to reuse the\nlatent for efficient editing. Our experiments demonstrate an average of 2.46\n$\\times$ acceleration without performance drop in a wide range of editing tasks\nincluding prompt-guided image editing, dragging and image composition. Our\ncodes are available at https://github.com/yuriYanZeXuan/EEdit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inversion-based image editing is rapidly gaining momentum while suffering\nfrom significant computation overhead, hindering its application in real-time\ninteractive scenarios. In this paper, we rethink that the redundancy in\ninversion-based image editing exists in both the spatial and temporal\ndimensions, such as the unnecessary computation in unedited regions and the\nredundancy in the inversion progress. To tackle these challenges, we propose a\npractical framework, named EEdit, to achieve efficient image editing.\nSpecifically, we introduce three techniques to solve them one by one. For\nspatial redundancy, spatial locality caching is introduced to compute the\nedited region and its neighboring regions while skipping the unedited regions,\nand token indexing preprocessing is designed to further accelerate the caching.\nFor temporal redundancy, inversion step skipping is proposed to reuse the\nlatent for efficient editing. Our experiments demonstrate an average of 2.46\n$\\times$ acceleration without performance drop in a wide range of editing tasks\nincluding prompt-guided image editing, dragging and image composition. Our\ncodes are available at https://github.com/yuriYanZeXuan/EEdit"
                },
                "authors": [
                    {
                        "name": "Zexuan Yan"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Wenteng Chen"
                    },
                    {
                        "name": "Qifeng Chen"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "17 pages,fix figure mistake(inv/fwd skipping) in fig2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10270v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10270v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23397v1",
                "updated": "2025-03-30T11:09:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    11,
                    9,
                    6,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-30T11:09:06Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    11,
                    9,
                    6,
                    6,
                    89,
                    0
                ],
                "title": "FB$^+$-tree: A Memory-Optimized B$^+$-tree with Latch-Free Update",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FB$^+$-tree: A Memory-Optimized B$^+$-tree with Latch-Free Update"
                },
                "summary": "B$^+$-trees are prevalent in traditional database systems due to their\nversatility and balanced structure. While binary search is typically utilized\nfor branch operations, it may lead to inefficient cache utilization in\nmain-memory scenarios. In contrast, trie-based index structures drive branch\noperations through prefix matching. While these structures generally produce\nfewer cache misses and are thus increasingly popular, they may underperform in\nrange scans because of frequent pointer chasing. This paper proposes a new\nhigh-performance B$^+$-tree variant called \\textbf{Feature B$^+$-tree\n(FB$^+$-tree)}. Similar to employing bit or byte for branch operation in tries,\nFB$^+$-tree progressively considers several bytes following the common prefix\non each level of its inner nodes\\textemdash referred to as features, which\nallows FB$^+$-tree to benefit from prefix skewness. FB$^+$-tree blurs the lines\nbetween B$^+$-trees and tries, while still retaining balance. In the best case,\nFB$^+$-tree almost becomes a trie, whereas in the worst case, it continues to\nfunction as a B$^+$-tree. Meanwhile, a crafted synchronization protocol that\ncombines the link technique and optimistic lock is designed to support\nefficient concurrent index access. Distinctively, FB$^+$-tree leverages subtle\natomic operations seamlessly coordinated with optimistic lock to facilitate\nlatch-free updates, which can be easily extended to other structures. Intensive\nexperiments on multiple workload-dataset combinations demonstrate that\nFB$^+$-tree shows comparable lookup performance to state-of-the-art trie-based\nindexes and outperforms popular B$^+$-trees by 2.3x$\\ \\sim\\ $3.7x under 96\nthreads. FB$^+$-tree also exhibits significant potential on other workloads,\nespecially update workloads under contention and scan workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "B$^+$-trees are prevalent in traditional database systems due to their\nversatility and balanced structure. While binary search is typically utilized\nfor branch operations, it may lead to inefficient cache utilization in\nmain-memory scenarios. In contrast, trie-based index structures drive branch\noperations through prefix matching. While these structures generally produce\nfewer cache misses and are thus increasingly popular, they may underperform in\nrange scans because of frequent pointer chasing. This paper proposes a new\nhigh-performance B$^+$-tree variant called \\textbf{Feature B$^+$-tree\n(FB$^+$-tree)}. Similar to employing bit or byte for branch operation in tries,\nFB$^+$-tree progressively considers several bytes following the common prefix\non each level of its inner nodes\\textemdash referred to as features, which\nallows FB$^+$-tree to benefit from prefix skewness. FB$^+$-tree blurs the lines\nbetween B$^+$-trees and tries, while still retaining balance. In the best case,\nFB$^+$-tree almost becomes a trie, whereas in the worst case, it continues to\nfunction as a B$^+$-tree. Meanwhile, a crafted synchronization protocol that\ncombines the link technique and optimistic lock is designed to support\nefficient concurrent index access. Distinctively, FB$^+$-tree leverages subtle\natomic operations seamlessly coordinated with optimistic lock to facilitate\nlatch-free updates, which can be easily extended to other structures. Intensive\nexperiments on multiple workload-dataset combinations demonstrate that\nFB$^+$-tree shows comparable lookup performance to state-of-the-art trie-based\nindexes and outperforms popular B$^+$-trees by 2.3x$\\ \\sim\\ $3.7x under 96\nthreads. FB$^+$-tree also exhibits significant potential on other workloads,\nespecially update workloads under contention and scan workloads."
                },
                "authors": [
                    {
                        "name": "Yuan Chen"
                    },
                    {
                        "name": "Ao Li"
                    },
                    {
                        "name": "Wenhai Li"
                    },
                    {
                        "name": "Lingfeng Deng"
                    }
                ],
                "author_detail": {
                    "name": "Lingfeng Deng"
                },
                "author": "Lingfeng Deng",
                "arxiv_comment": "14 pages,17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23388v1",
                "updated": "2025-03-30T10:34:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    10,
                    34,
                    45,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-30T10:34:45Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    10,
                    34,
                    45,
                    6,
                    89,
                    0
                ],
                "title": "COSMIC: Clique-Oriented Semantic Multi-space Integration for Robust CLIP\n  Test-Time Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COSMIC: Clique-Oriented Semantic Multi-space Integration for Robust CLIP\n  Test-Time Adaptation"
                },
                "summary": "Recent vision-language models (VLMs) face significant challenges in test-time\nadaptation to novel domains. While cache-based methods show promise by\nleveraging historical information, they struggle with both caching unreliable\nfeature-label pairs and indiscriminately using single-class information during\nquerying, significantly compromising adaptation accuracy. To address these\nlimitations, we propose COSMIC (Clique-Oriented Semantic Multi-space\nIntegration for CLIP), a robust test-time adaptation framework that enhances\nadaptability through multi-granular, cross-modal semantic caching and\ngraph-based querying mechanisms. Our framework introduces two key innovations:\nDual Semantics Graph (DSG) and Clique Guided Hyper-class (CGH). The Dual\nSemantics Graph constructs complementary semantic spaces by incorporating\ntextual features, coarse-grained CLIP features, and fine-grained DINOv2\nfeatures to capture rich semantic relationships. Building upon these dual\ngraphs, the Clique Guided Hyper-class component leverages structured class\nrelationships to enhance prediction robustness through correlated class\nselection. Extensive experiments demonstrate COSMIC's superior performance\nacross multiple benchmarks, achieving significant improvements over\nstate-of-the-art methods: 15.81% gain on out-of-distribution tasks and 5.33% on\ncross-domain generation with CLIP RN-50. Code is available at\ngithub.com/hf618/COSMIC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent vision-language models (VLMs) face significant challenges in test-time\nadaptation to novel domains. While cache-based methods show promise by\nleveraging historical information, they struggle with both caching unreliable\nfeature-label pairs and indiscriminately using single-class information during\nquerying, significantly compromising adaptation accuracy. To address these\nlimitations, we propose COSMIC (Clique-Oriented Semantic Multi-space\nIntegration for CLIP), a robust test-time adaptation framework that enhances\nadaptability through multi-granular, cross-modal semantic caching and\ngraph-based querying mechanisms. Our framework introduces two key innovations:\nDual Semantics Graph (DSG) and Clique Guided Hyper-class (CGH). The Dual\nSemantics Graph constructs complementary semantic spaces by incorporating\ntextual features, coarse-grained CLIP features, and fine-grained DINOv2\nfeatures to capture rich semantic relationships. Building upon these dual\ngraphs, the Clique Guided Hyper-class component leverages structured class\nrelationships to enhance prediction robustness through correlated class\nselection. Extensive experiments demonstrate COSMIC's superior performance\nacross multiple benchmarks, achieving significant improvements over\nstate-of-the-art methods: 15.81% gain on out-of-distribution tasks and 5.33% on\ncross-domain generation with CLIP RN-50. Code is available at\ngithub.com/hf618/COSMIC."
                },
                "authors": [
                    {
                        "name": "Fanding Huang"
                    },
                    {
                        "name": "Jingyan Jiang"
                    },
                    {
                        "name": "Qinting Jiang"
                    },
                    {
                        "name": "Hebei Li"
                    },
                    {
                        "name": "Faisal Nadeem Khan"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "arxiv_comment": "Accepted to CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16588v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16588v2",
                "updated": "2025-03-30T09:46:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    9,
                    46,
                    34,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-20T17:37:15Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    37,
                    15,
                    3,
                    79,
                    0
                ],
                "title": "A Unified Framework for Quantitative Cache Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Framework for Quantitative Cache Analysis"
                },
                "summary": "In this work we unify two existing lines of work towards cache analysis for\nnon-LRU policies. To this end, we extend the notion of competitiveness to block\ncompetitiveness and systematically analyze the competitiveness and block\ncompetitiveness of FIFO and MRU relative to LRU for arbitrary associativities.\nWe show how competitiveness and block competitiveness can be exploited in\nstate-of-the-art WCET analysis based on the results of existing persistence\nanalyses for LRU. Unlike prior work, our approach is applicable to\nmicroarchitectures that exhibit timing anomalies. We experimentally evaluate\nthe precision and cost of our approach on benchmarks from TACLeBench. The\nexperiments demonstrate that quantitative cache analysis for FIFO and MRU comes\nclose to the precision of LRU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work we unify two existing lines of work towards cache analysis for\nnon-LRU policies. To this end, we extend the notion of competitiveness to block\ncompetitiveness and systematically analyze the competitiveness and block\ncompetitiveness of FIFO and MRU relative to LRU for arbitrary associativities.\nWe show how competitiveness and block competitiveness can be exploited in\nstate-of-the-art WCET analysis based on the results of existing persistence\nanalyses for LRU. Unlike prior work, our approach is applicable to\nmicroarchitectures that exhibit timing anomalies. We experimentally evaluate\nthe precision and cost of our approach on benchmarks from TACLeBench. The\nexperiments demonstrate that quantitative cache analysis for FIFO and MRU comes\nclose to the precision of LRU."
                },
                "authors": [
                    {
                        "name": "Sophie Kahlen"
                    },
                    {
                        "name": "Jan Reineke"
                    }
                ],
                "author_detail": {
                    "name": "Jan Reineke"
                },
                "author": "Jan Reineke",
                "arxiv_comment": "Extended version of RTAS 2025 paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16588v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16588v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16897v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16897v2",
                "updated": "2025-03-30T09:19:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    9,
                    19,
                    53,
                    6,
                    89,
                    0
                ],
                "published": "2024-12-22T07:14:45Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    7,
                    14,
                    45,
                    6,
                    357,
                    0
                ],
                "title": "MVREC: A General Few-shot Defect Classification Model Using Multi-View\n  Region-Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MVREC: A General Few-shot Defect Classification Model Using Multi-View\n  Region-Context"
                },
                "summary": "Few-shot defect multi-classification (FSDMC) is an emerging trend in quality\ncontrol within industrial manufacturing. However, current FSDMC research often\nlacks generalizability due to its focus on specific datasets. Additionally,\ndefect classification heavily relies on contextual information within images,\nand existing methods fall short of effectively extracting this information. To\naddress these challenges, we propose a general FSDMC framework called MVREC,\nwhich offers two primary advantages: (1) MVREC extracts general features for\ndefect instances by incorporating the pre-trained AlphaCLIP model. (2) It\nutilizes a region-context framework to enhance defect features by leveraging\nmask region input and multi-view context augmentation. Furthermore, Few-shot\nZip-Adapter(-F) classifiers within the model are introduced to cache the visual\nfeatures of the support set and perform few-shot classification. We also\nintroduce MVTec-FS, a new FSDMC benchmark based on MVTec AD, which includes\n1228 defect images with instance-level mask annotations and 46 defect types.\nExtensive experiments conducted on MVTec-FS and four additional datasets\ndemonstrate its effectiveness in general defect classification and its ability\nto incorporate contextual information to improve classification performance.\nCode: https://github.com/ShuaiLYU/MVREC",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-shot defect multi-classification (FSDMC) is an emerging trend in quality\ncontrol within industrial manufacturing. However, current FSDMC research often\nlacks generalizability due to its focus on specific datasets. Additionally,\ndefect classification heavily relies on contextual information within images,\nand existing methods fall short of effectively extracting this information. To\naddress these challenges, we propose a general FSDMC framework called MVREC,\nwhich offers two primary advantages: (1) MVREC extracts general features for\ndefect instances by incorporating the pre-trained AlphaCLIP model. (2) It\nutilizes a region-context framework to enhance defect features by leveraging\nmask region input and multi-view context augmentation. Furthermore, Few-shot\nZip-Adapter(-F) classifiers within the model are introduced to cache the visual\nfeatures of the support set and perform few-shot classification. We also\nintroduce MVTec-FS, a new FSDMC benchmark based on MVTec AD, which includes\n1228 defect images with instance-level mask annotations and 46 defect types.\nExtensive experiments conducted on MVTec-FS and four additional datasets\ndemonstrate its effectiveness in general defect classification and its ability\nto incorporate contextual information to improve classification performance.\nCode: https://github.com/ShuaiLYU/MVREC"
                },
                "authors": [
                    {
                        "name": "Shuai Lyu"
                    },
                    {
                        "name": "Rongchen Zhang"
                    },
                    {
                        "name": "Zeqi Ma"
                    },
                    {
                        "name": "Fangjian Liao"
                    },
                    {
                        "name": "Dongmei Mo"
                    },
                    {
                        "name": "Waikeung Wong"
                    }
                ],
                "author_detail": {
                    "name": "Waikeung Wong"
                },
                "author": "Waikeung Wong",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16897v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16897v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23367v1",
                "updated": "2025-03-30T08:51:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    8,
                    51,
                    19,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-30T08:51:19Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    8,
                    51,
                    19,
                    6,
                    89,
                    0
                ],
                "title": "FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning"
                },
                "summary": "Visual Autoregressive (VAR) modeling has gained popularity for its shift\ntowards next-scale prediction. However, existing VAR paradigms process the\nentire token map at each scale step, leading to the complexity and runtime\nscaling dramatically with image resolution. To address this challenge, we\npropose FastVAR, a post-training acceleration method for efficient resolution\nscaling with VARs. Our key finding is that the majority of latency arises from\nthe large-scale step where most tokens have already converged. Leveraging this\nobservation, we develop the cached token pruning strategy that only forwards\npivotal tokens for scale-specific modeling while using cached tokens from\nprevious scale steps to restore the pruned slots. This significantly reduces\nthe number of forwarded tokens and improves the efficiency at larger\nresolutions. Experiments show the proposed FastVAR can further speedup\nFlashAttention-accelerated VAR by 2.7$\\times$ with negligible performance drop\nof <1%. We further extend FastVAR to zero-shot generation of higher resolution\nimages. In particular, FastVAR can generate one 2K image with 15GB memory\nfootprints in 1.5s on a single NVIDIA 3090 GPU. Code is available at\nhttps://github.com/csguoh/FastVAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Autoregressive (VAR) modeling has gained popularity for its shift\ntowards next-scale prediction. However, existing VAR paradigms process the\nentire token map at each scale step, leading to the complexity and runtime\nscaling dramatically with image resolution. To address this challenge, we\npropose FastVAR, a post-training acceleration method for efficient resolution\nscaling with VARs. Our key finding is that the majority of latency arises from\nthe large-scale step where most tokens have already converged. Leveraging this\nobservation, we develop the cached token pruning strategy that only forwards\npivotal tokens for scale-specific modeling while using cached tokens from\nprevious scale steps to restore the pruned slots. This significantly reduces\nthe number of forwarded tokens and improves the efficiency at larger\nresolutions. Experiments show the proposed FastVAR can further speedup\nFlashAttention-accelerated VAR by 2.7$\\times$ with negligible performance drop\nof <1%. We further extend FastVAR to zero-shot generation of higher resolution\nimages. In particular, FastVAR can generate one 2K image with 15GB memory\nfootprints in 1.5s on a single NVIDIA 3090 GPU. Code is available at\nhttps://github.com/csguoh/FastVAR."
                },
                "authors": [
                    {
                        "name": "Hang Guo"
                    },
                    {
                        "name": "Yawei Li"
                    },
                    {
                        "name": "Taolin Zhang"
                    },
                    {
                        "name": "Jiangshan Wang"
                    },
                    {
                        "name": "Tao Dai"
                    },
                    {
                        "name": "Shu-Tao Xia"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12820v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12820v2",
                "updated": "2025-03-30T08:13:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    8,
                    13,
                    50,
                    6,
                    89,
                    0
                ],
                "published": "2024-07-01T13:05:42Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    13,
                    5,
                    42,
                    0,
                    183,
                    0
                ],
                "title": "PQCache: Product Quantization-based KVCache for Long Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PQCache: Product Quantization-based KVCache for Long Context LLM\n  Inference"
                },
                "summary": "As the field of Large Language Models (LLMs) continues to evolve, the context\nlength in inference is steadily growing. Key-Value Cache (KVCache), the\nintermediate representations of tokens within LLM inference, has now become the\nprimary memory bottleneck due to limited GPU memory. Current methods\nselectively determine suitable keys and values for self-attention computation\nin LLMs to address the issue. However, they either fall short in maintaining\nmodel quality or result in high serving latency. Drawing inspiration from\nadvanced embedding retrieval techniques prevalent in the data management\ncommunity, we consider the storage and retrieval of KVCache as a typical\nembedding retrieval problem. We propose PQCache, which employs Product\nQuantization (PQ) to manage KVCache, maintaining model quality while ensuring\nlow serving latency. During the prefilling phase, we apply PQ to tokens' keys\nfor each LLM layer and head. During the autoregressive decoding phase, we use\nPQ codes and centroids to approximately identify important preceding tokens,\nthen fetch the corresponding key-value pairs for self-attention computation.\nThrough meticulous design of overlapping and caching, we minimize any\nadditional computation and communication overhead during both phases. Extensive\nexperiments demonstrate that PQCache achieves both effectiveness and\nefficiency, with 4.60% score improvement over existing methods on InfiniteBench\nand low system latency in both prefilling and decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the field of Large Language Models (LLMs) continues to evolve, the context\nlength in inference is steadily growing. Key-Value Cache (KVCache), the\nintermediate representations of tokens within LLM inference, has now become the\nprimary memory bottleneck due to limited GPU memory. Current methods\nselectively determine suitable keys and values for self-attention computation\nin LLMs to address the issue. However, they either fall short in maintaining\nmodel quality or result in high serving latency. Drawing inspiration from\nadvanced embedding retrieval techniques prevalent in the data management\ncommunity, we consider the storage and retrieval of KVCache as a typical\nembedding retrieval problem. We propose PQCache, which employs Product\nQuantization (PQ) to manage KVCache, maintaining model quality while ensuring\nlow serving latency. During the prefilling phase, we apply PQ to tokens' keys\nfor each LLM layer and head. During the autoregressive decoding phase, we use\nPQ codes and centroids to approximately identify important preceding tokens,\nthen fetch the corresponding key-value pairs for self-attention computation.\nThrough meticulous design of overlapping and caching, we minimize any\nadditional computation and communication overhead during both phases. Extensive\nexperiments demonstrate that PQCache achieves both effectiveness and\nefficiency, with 4.60% score improvement over existing methods on InfiniteBench\nand low system latency in both prefilling and decoding."
                },
                "authors": [
                    {
                        "name": "Hailin Zhang"
                    },
                    {
                        "name": "Xiaodong Ji"
                    },
                    {
                        "name": "Yilin Chen"
                    },
                    {
                        "name": "Fangcheng Fu"
                    },
                    {
                        "name": "Xupeng Miao"
                    },
                    {
                        "name": "Xiaonan Nie"
                    },
                    {
                        "name": "Weipeng Chen"
                    },
                    {
                        "name": "Bin Cui"
                    }
                ],
                "author_detail": {
                    "name": "Bin Cui"
                },
                "author": "Bin Cui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12820v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12820v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23294v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23294v1",
                "updated": "2025-03-30T03:20:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    3,
                    20,
                    34,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-30T03:20:34Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    3,
                    20,
                    34,
                    6,
                    89,
                    0
                ],
                "title": "Cocktail: Chunk-Adaptive Mixed-Precision Quantization for Long-Context\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cocktail: Chunk-Adaptive Mixed-Precision Quantization for Long-Context\n  LLM Inference"
                },
                "summary": "Recently, large language models (LLMs) have been able to handle longer and\nlonger contexts. However, a context that is too long may cause intolerant\ninference latency and GPU memory usage. Existing methods propose\nmixed-precision quantization to the key-value (KV) cache in LLMs based on token\ngranularity, which is time-consuming in the search process and hardware\ninefficient during computation. This paper introduces a novel approach called\nCocktail, which employs chunk-adaptive mixed-precision quantization to optimize\nthe KV cache. Cocktail consists of two modules: chunk-level quantization search\nand chunk-level KV cache computation. Chunk-level quantization search\ndetermines the optimal bitwidth configuration of the KV cache chunks quickly\nbased on the similarity scores between the corresponding context chunks and the\nquery, maintaining the model accuracy. Furthermore, chunk-level KV cache\ncomputation reorders the KV cache chunks before quantization, avoiding the\nhardware inefficiency caused by mixed-precision quantization in inference\ncomputation. Extensive experiments demonstrate that Cocktail outperforms\nstate-of-the-art KV cache quantization methods on various models and datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) have been able to handle longer and\nlonger contexts. However, a context that is too long may cause intolerant\ninference latency and GPU memory usage. Existing methods propose\nmixed-precision quantization to the key-value (KV) cache in LLMs based on token\ngranularity, which is time-consuming in the search process and hardware\ninefficient during computation. This paper introduces a novel approach called\nCocktail, which employs chunk-adaptive mixed-precision quantization to optimize\nthe KV cache. Cocktail consists of two modules: chunk-level quantization search\nand chunk-level KV cache computation. Chunk-level quantization search\ndetermines the optimal bitwidth configuration of the KV cache chunks quickly\nbased on the similarity scores between the corresponding context chunks and the\nquery, maintaining the model accuracy. Furthermore, chunk-level KV cache\ncomputation reorders the KV cache chunks before quantization, avoiding the\nhardware inefficiency caused by mixed-precision quantization in inference\ncomputation. Extensive experiments demonstrate that Cocktail outperforms\nstate-of-the-art KV cache quantization methods on various models and datasets."
                },
                "authors": [
                    {
                        "name": "Wei Tao"
                    },
                    {
                        "name": "Bin Zhang"
                    },
                    {
                        "name": "Xiaoyang Qu"
                    },
                    {
                        "name": "Jiguang Wan"
                    },
                    {
                        "name": "Jianzong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jianzong Wang"
                },
                "author": "Jianzong Wang",
                "arxiv_comment": "Accepted by the Design, Automation, and Test in Europe 2025 (DATE\n  2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23294v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23294v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11816v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11816v2",
                "updated": "2025-03-30T02:45:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    2,
                    45,
                    0,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-14T19:02:16Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    19,
                    2,
                    16,
                    4,
                    73,
                    0
                ],
                "title": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques"
                },
                "summary": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations."
                },
                "authors": [
                    {
                        "name": "Neusha Javidnia"
                    },
                    {
                        "name": "Bita Darvish Rouhani"
                    },
                    {
                        "name": "Farinaz Koushanfar"
                    }
                ],
                "author_detail": {
                    "name": "Farinaz Koushanfar"
                },
                "author": "Farinaz Koushanfar",
                "arxiv_comment": "Invited paper to IEEE Custom Integrated Circuits Conference (CICC)\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11816v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11816v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18278v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18278v2",
                "updated": "2025-03-29T23:00:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    29,
                    23,
                    0,
                    27,
                    5,
                    88,
                    0
                ],
                "published": "2025-03-24T01:47:26Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    1,
                    47,
                    26,
                    0,
                    83,
                    0
                ],
                "title": "TopV: Compatible Token Pruning with Inference Time Optimization for Fast\n  and Low-Memory Multimodal Vision Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TopV: Compatible Token Pruning with Inference Time Optimization for Fast\n  and Low-Memory Multimodal Vision Language Model"
                },
                "summary": "Vision-Language Models (VLMs) demand substantial computational resources\nduring inference, largely due to the extensive visual input tokens for\nrepresenting visual information. Previous studies have noted that visual tokens\ntend to receive less attention than text tokens, suggesting their lower\nimportance during inference and potential for pruning. However, their methods\nencounter several challenges: reliance on greedy heuristic criteria for token\nimportance and incompatibility with FlashAttention and KV cache. To address\nthese issues, we introduce \\textbf{TopV}, a compatible \\textbf{TO}ken\n\\textbf{P}runing with inference Time Optimization for fast and low-memory\n\\textbf{V}LM, achieving efficient pruning without additional training or\nfine-tuning. Instead of relying on attention scores, we formulate token pruning\nas an optimization problem, accurately identifying important visual tokens\nwhile remaining compatible with FlashAttention. Additionally, since we only\nperform this pruning once during the prefilling stage, it effectively reduces\nKV cache size. Our optimization framework incorporates a visual-aware cost\nfunction considering factors such as Feature Similarity, Relative Spatial\nDistance, and Absolute Central Distance, to measure the importance of each\nsource visual token, enabling effective pruning of low-importance tokens.\nExtensive experiments demonstrate that our method outperforms previous token\npruning methods, validating the effectiveness and efficiency of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) demand substantial computational resources\nduring inference, largely due to the extensive visual input tokens for\nrepresenting visual information. Previous studies have noted that visual tokens\ntend to receive less attention than text tokens, suggesting their lower\nimportance during inference and potential for pruning. However, their methods\nencounter several challenges: reliance on greedy heuristic criteria for token\nimportance and incompatibility with FlashAttention and KV cache. To address\nthese issues, we introduce \\textbf{TopV}, a compatible \\textbf{TO}ken\n\\textbf{P}runing with inference Time Optimization for fast and low-memory\n\\textbf{V}LM, achieving efficient pruning without additional training or\nfine-tuning. Instead of relying on attention scores, we formulate token pruning\nas an optimization problem, accurately identifying important visual tokens\nwhile remaining compatible with FlashAttention. Additionally, since we only\nperform this pruning once during the prefilling stage, it effectively reduces\nKV cache size. Our optimization framework incorporates a visual-aware cost\nfunction considering factors such as Feature Similarity, Relative Spatial\nDistance, and Absolute Central Distance, to measure the importance of each\nsource visual token, enabling effective pruning of low-importance tokens.\nExtensive experiments demonstrate that our method outperforms previous token\npruning methods, validating the effectiveness and efficiency of our approach."
                },
                "authors": [
                    {
                        "name": "Cheng Yang"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Jinqi Xiao"
                    },
                    {
                        "name": "Lingyi Huang"
                    },
                    {
                        "name": "Yu Gong"
                    },
                    {
                        "name": "Chendi Li"
                    },
                    {
                        "name": "Jinghua Yan"
                    },
                    {
                        "name": "Yu Bai"
                    },
                    {
                        "name": "Ponnuswamy Sadayappan"
                    },
                    {
                        "name": "Xia Hu"
                    },
                    {
                        "name": "Bo Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Bo Yuan"
                },
                "author": "Bo Yuan",
                "arxiv_comment": "Accepted by CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18278v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18278v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11132v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11132v2",
                "updated": "2025-03-29T04:43:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    29,
                    4,
                    43,
                    11,
                    5,
                    88,
                    0
                ],
                "published": "2025-03-14T06:49:37Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    49,
                    37,
                    4,
                    73,
                    0
                ],
                "title": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression"
                },
                "summary": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1\\% average\nscore drop with 7B training tokens and 140 GPU hours.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1\\% average\nscore drop with 7B training tokens and 140 GPU hours."
                },
                "authors": [
                    {
                        "name": "Guihong Li"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Vikram Appia"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11132v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11132v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22926v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22926v1",
                "updated": "2025-03-29T01:06:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    29,
                    1,
                    6,
                    54,
                    5,
                    88,
                    0
                ],
                "published": "2025-03-29T01:06:54Z",
                "published_parsed": [
                    2025,
                    3,
                    29,
                    1,
                    6,
                    54,
                    5,
                    88,
                    0
                ],
                "title": "SR-LIO++: Efficient LiDAR-Inertial Odometry and Quantized Mapping with\n  Sweep Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SR-LIO++: Efficient LiDAR-Inertial Odometry and Quantized Mapping with\n  Sweep Reconstruction"
                },
                "summary": "Addressing the inherent low acquisition frequency limitation of 3D LiDAR to\nachieve high-frequency output has become a critical research focus in the\nLiDAR-Inertial Odometry (LIO) domain. To ensure real-time performance,\nfrequency-enhanced LIO systems must process each sweep within significantly\nreduced timeframe, which presents substantial challenges for deployment on\nlow-computational-power platforms. To address these limitations, we introduce\nSR-LIO++, an innovative LIO system capable of achieving doubled output\nfrequency relative to input frequency on resource-constrained hardware\nplatforms, including the Raspberry Pi 4B. Our system employs a sweep\nreconstruction methodology to enhance LiDAR sweep frequency, generating\nhigh-frequency reconstructed sweeps. Building upon this foundation, we propose\na caching mechanism for intermediate results (i.e., surface parameters) of the\nmost recent segments, effectively minimizing redundant processing of common\nsegments in adjacent reconstructed sweeps. This method decouples processing\ntime from the traditionally linear dependence on reconstructed sweep frequency.\nFurthermore, we present a quantized map point management based on index table\nmapping, significantly reducing memory usage by converting global 3D point\nstorage from 64-bit double precision to 8-bit char representation. This method\nalso converts the computationally intensive Euclidean distance calculations in\nnearest neighbor searches from 64-bit double precision to 16-bit short and\n32-bit integer formats, significantly reducing both memory and computational\ncost. Extensive experimental evaluations across three distinct computing\nplatforms and four public datasets demonstrate that SR-LIO++ maintains\nstate-of-the-art accuracy while substantially enhancing efficiency. Notably,\nour system successfully achieves 20Hz state output on Raspberry Pi 4B hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing the inherent low acquisition frequency limitation of 3D LiDAR to\nachieve high-frequency output has become a critical research focus in the\nLiDAR-Inertial Odometry (LIO) domain. To ensure real-time performance,\nfrequency-enhanced LIO systems must process each sweep within significantly\nreduced timeframe, which presents substantial challenges for deployment on\nlow-computational-power platforms. To address these limitations, we introduce\nSR-LIO++, an innovative LIO system capable of achieving doubled output\nfrequency relative to input frequency on resource-constrained hardware\nplatforms, including the Raspberry Pi 4B. Our system employs a sweep\nreconstruction methodology to enhance LiDAR sweep frequency, generating\nhigh-frequency reconstructed sweeps. Building upon this foundation, we propose\na caching mechanism for intermediate results (i.e., surface parameters) of the\nmost recent segments, effectively minimizing redundant processing of common\nsegments in adjacent reconstructed sweeps. This method decouples processing\ntime from the traditionally linear dependence on reconstructed sweep frequency.\nFurthermore, we present a quantized map point management based on index table\nmapping, significantly reducing memory usage by converting global 3D point\nstorage from 64-bit double precision to 8-bit char representation. This method\nalso converts the computationally intensive Euclidean distance calculations in\nnearest neighbor searches from 64-bit double precision to 16-bit short and\n32-bit integer formats, significantly reducing both memory and computational\ncost. Extensive experimental evaluations across three distinct computing\nplatforms and four public datasets demonstrate that SR-LIO++ maintains\nstate-of-the-art accuracy while substantially enhancing efficiency. Notably,\nour system successfully achieves 20Hz state output on Raspberry Pi 4B hardware."
                },
                "authors": [
                    {
                        "name": "Zikang Yuan"
                    },
                    {
                        "name": "Ruiye Ming"
                    },
                    {
                        "name": "Chengwei Zhao"
                    },
                    {
                        "name": "Yonghao Tan"
                    },
                    {
                        "name": "Pingcheng Dong"
                    },
                    {
                        "name": "Hongcheng Luo"
                    },
                    {
                        "name": "Yuzhong Jiao"
                    },
                    {
                        "name": "Xin Yang"
                    },
                    {
                        "name": "Kwang-Ting Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Kwang-Ting Cheng"
                },
                "author": "Kwang-Ting Cheng",
                "arxiv_comment": "10 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22926v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22926v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22875v1",
                "updated": "2025-03-28T21:02:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    21,
                    2,
                    32,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T21:02:32Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    21,
                    2,
                    32,
                    4,
                    87,
                    0
                ],
                "title": "A Pilot Study on Tunable Precision Emulation via Automatic BLAS\n  Offloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Pilot Study on Tunable Precision Emulation via Automatic BLAS\n  Offloading"
                },
                "summary": "This study explores the use of automatic BLAS offloading and INT8-based\nemulation for accelerating traditional HPC workloads on modern GPU\narchitectures. Through the use of low-bitwidth integer units and cache-coherent\nUnified Memory Architecture, we emulate double-precision matrix multiplications\nin the MuST application without code changes. We find that accuracy depends on\nboth arithmetic precision and the properties of the operator, which can be\ndealt with through tunable precision emulation. Unlike traditional\nmixed-precision approaches, this method preserves original algorithms while\noptimizing hardware utilization. We showcases the potential of improving\naccuracy and performance at the same time. This work highlights the potential\nof AI-driven hardware to transform HPC, advocating for adaptive precision\nstrategies in future scientific computing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores the use of automatic BLAS offloading and INT8-based\nemulation for accelerating traditional HPC workloads on modern GPU\narchitectures. Through the use of low-bitwidth integer units and cache-coherent\nUnified Memory Architecture, we emulate double-precision matrix multiplications\nin the MuST application without code changes. We find that accuracy depends on\nboth arithmetic precision and the properties of the operator, which can be\ndealt with through tunable precision emulation. Unlike traditional\nmixed-precision approaches, this method preserves original algorithms while\noptimizing hardware utilization. We showcases the potential of improving\naccuracy and performance at the same time. This work highlights the potential\nof AI-driven hardware to transform HPC, advocating for adaptive precision\nstrategies in future scientific computing."
                },
                "authors": [
                    {
                        "name": "Hang Liu"
                    },
                    {
                        "name": "Junjie Li"
                    },
                    {
                        "name": "Yinzhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yinzhi Wang"
                },
                "author": "Yinzhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22796v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22796v1",
                "updated": "2025-03-28T18:00:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    18,
                    0,
                    12,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T18:00:12Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    18,
                    0,
                    12,
                    4,
                    87,
                    0
                ],
                "title": "DiTFastAttnV2: Head-wise Attention Compression for Multi-Modality\n  Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiTFastAttnV2: Head-wise Attention Compression for Multi-Modality\n  Diffusion Transformers"
                },
                "summary": "Text-to-image generation models, especially Multimodal Diffusion Transformers\n(MMDiT), have shown remarkable progress in generating high-quality images.\nHowever, these models often face significant computational bottlenecks,\nparticularly in attention mechanisms, which hinder their scalability and\nefficiency. In this paper, we introduce DiTFastAttnV2, a post-training\ncompression method designed to accelerate attention in MMDiT. Through an\nin-depth analysis of MMDiT's attention patterns, we identify key differences\nfrom prior DiT-based methods and propose head-wise arrow attention and caching\nmechanisms to dynamically adjust attention heads, effectively bridging this\ngap. We also design an Efficient Fused Kernel for further acceleration. By\nleveraging local metric methods and optimization techniques, our approach\nsignificantly reduces the search time for optimal compression schemes to just\nminutes while maintaining generation quality. Furthermore, with the customized\nkernel, DiTFastAttnV2 achieves a 68% reduction in attention FLOPs and 1.5x\nend-to-end speedup on 2K image generation without compromising visual fidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image generation models, especially Multimodal Diffusion Transformers\n(MMDiT), have shown remarkable progress in generating high-quality images.\nHowever, these models often face significant computational bottlenecks,\nparticularly in attention mechanisms, which hinder their scalability and\nefficiency. In this paper, we introduce DiTFastAttnV2, a post-training\ncompression method designed to accelerate attention in MMDiT. Through an\nin-depth analysis of MMDiT's attention patterns, we identify key differences\nfrom prior DiT-based methods and propose head-wise arrow attention and caching\nmechanisms to dynamically adjust attention heads, effectively bridging this\ngap. We also design an Efficient Fused Kernel for further acceleration. By\nleveraging local metric methods and optimization techniques, our approach\nsignificantly reduces the search time for optimal compression schemes to just\nminutes while maintaining generation quality. Furthermore, with the customized\nkernel, DiTFastAttnV2 achieves a 68% reduction in attention FLOPs and 1.5x\nend-to-end speedup on 2K image generation without compromising visual fidelity."
                },
                "authors": [
                    {
                        "name": "Hanling Zhang"
                    },
                    {
                        "name": "Rundong Su"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Pengtao Chen"
                    },
                    {
                        "name": "Mingzhu Shen Yibo Fan"
                    },
                    {
                        "name": "Shengen Yan"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22796v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22796v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17616v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17616v3",
                "updated": "2025-03-28T16:15:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    16,
                    15,
                    19,
                    4,
                    87,
                    0
                ],
                "published": "2024-11-26T17:28:10Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    28,
                    10,
                    1,
                    331,
                    0
                ],
                "title": "Towards Stabilized and Efficient Diffusion Transformers through\n  Long-Skip-Connections with Spectral Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Stabilized and Efficient Diffusion Transformers through\n  Long-Skip-Connections with Spectral Constraints"
                },
                "summary": "Diffusion Transformers (DiT) have emerged as a powerful architecture for\nimage and video generation, offering superior quality and scalability. However,\ntheir practical application suffers from inherent dynamic feature instability,\nleading to error amplification during cached inference. Through systematic\nanalysis, we identify the absence of long-range feature preservation mechanisms\nas the root cause of unstable feature propagation and perturbation sensitivity.\nTo this end, we propose Skip-DiT, a novel DiT variant enhanced with\nLong-Skip-Connections (LSCs) - the key efficiency component in U-Nets.\nTheoretical spectral norm and visualization analysis demonstrate how LSCs\nstabilize feature dynamics. Skip-DiT architecture and its stabilized dynamic\nfeature enable an efficient statical caching mechanism that reuses deep\nfeatures across timesteps while updating shallow components. Extensive\nexperiments across image and video generation tasks demonstrate that Skip-DiT\nachieves: (1) 4.4 times training acceleration and faster convergence, (2) 1.5-2\ntimes inference acceleration without quality loss and high fidelity to original\noutput, outperforming existing DiT caching methods across various quantitative\nmetrics. Our findings establish long-skip connections as critical architectural\ncomponents for training stable and efficient diffusion transformers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have emerged as a powerful architecture for\nimage and video generation, offering superior quality and scalability. However,\ntheir practical application suffers from inherent dynamic feature instability,\nleading to error amplification during cached inference. Through systematic\nanalysis, we identify the absence of long-range feature preservation mechanisms\nas the root cause of unstable feature propagation and perturbation sensitivity.\nTo this end, we propose Skip-DiT, a novel DiT variant enhanced with\nLong-Skip-Connections (LSCs) - the key efficiency component in U-Nets.\nTheoretical spectral norm and visualization analysis demonstrate how LSCs\nstabilize feature dynamics. Skip-DiT architecture and its stabilized dynamic\nfeature enable an efficient statical caching mechanism that reuses deep\nfeatures across timesteps while updating shallow components. Extensive\nexperiments across image and video generation tasks demonstrate that Skip-DiT\nachieves: (1) 4.4 times training acceleration and faster convergence, (2) 1.5-2\ntimes inference acceleration without quality loss and high fidelity to original\noutput, outperforming existing DiT caching methods across various quantitative\nmetrics. Our findings establish long-skip connections as critical architectural\ncomponents for training stable and efficient diffusion transformers."
                },
                "authors": [
                    {
                        "name": "Guanjie Chen"
                    },
                    {
                        "name": "Xinyu Zhao"
                    },
                    {
                        "name": "Yucheng Zhou"
                    },
                    {
                        "name": "Xiaoye Qu"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Yu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Cheng"
                },
                "author": "Yu Cheng",
                "arxiv_comment": "17 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17616v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17616v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15024v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15024v3",
                "updated": "2025-03-28T14:11:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    14,
                    11,
                    37,
                    4,
                    87,
                    0
                ],
                "published": "2024-11-22T15:55:19Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    55,
                    19,
                    4,
                    327,
                    0
                ],
                "title": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models"
                },
                "summary": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "13 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15024v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15024v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22329v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22329v1",
                "updated": "2025-03-28T11:08:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    11,
                    8,
                    34,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T11:08:34Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    11,
                    8,
                    34,
                    4,
                    87,
                    0
                ],
                "title": "A Refined Analysis of Massive Activations in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Refined Analysis of Massive Activations in LLMs"
                },
                "summary": "Motivated in part by their relevance for low-precision training and\nquantization, massive activations in large language models (LLMs) have recently\nemerged as a topic of interest. However, existing analyses are limited in\nscope, and generalizability across architectures is unclear. This paper helps\naddress some of these gaps by conducting an analysis of massive activations\nacross a broad range of LLMs, including both GLU-based and non-GLU-based\narchitectures. Our findings challenge several prior assumptions, most\nimportantly: (1) not all massive activations are detrimental, i.e. suppressing\nthem does not lead to an explosion of perplexity or a collapse in downstream\ntask performance; (2) proposed mitigation strategies such as Attention KV bias\nare model-specific and ineffective in certain cases. We consequently\ninvestigate novel hybrid mitigation strategies; in particular pairing Target\nVariance Rescaling (TVR) with Attention KV bias or Dynamic Tanh (DyT)\nsuccessfully balances the mitigation of massive activations with preserved\ndownstream model performance in the scenarios we investigated. Our code is\navailable at: https://github.com/bluorion-com/refine_massive_activations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated in part by their relevance for low-precision training and\nquantization, massive activations in large language models (LLMs) have recently\nemerged as a topic of interest. However, existing analyses are limited in\nscope, and generalizability across architectures is unclear. This paper helps\naddress some of these gaps by conducting an analysis of massive activations\nacross a broad range of LLMs, including both GLU-based and non-GLU-based\narchitectures. Our findings challenge several prior assumptions, most\nimportantly: (1) not all massive activations are detrimental, i.e. suppressing\nthem does not lead to an explosion of perplexity or a collapse in downstream\ntask performance; (2) proposed mitigation strategies such as Attention KV bias\nare model-specific and ineffective in certain cases. We consequently\ninvestigate novel hybrid mitigation strategies; in particular pairing Target\nVariance Rescaling (TVR) with Attention KV bias or Dynamic Tanh (DyT)\nsuccessfully balances the mitigation of massive activations with preserved\ndownstream model performance in the scenarios we investigated. Our code is\navailable at: https://github.com/bluorion-com/refine_massive_activations."
                },
                "authors": [
                    {
                        "name": "Louis Owen"
                    },
                    {
                        "name": "Nilabhra Roy Chowdhury"
                    },
                    {
                        "name": "Abhay Kumar"
                    },
                    {
                        "name": "Fabian Güra"
                    }
                ],
                "author_detail": {
                    "name": "Fabian Güra"
                },
                "author": "Fabian Güra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22329v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22329v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22196v1",
                "updated": "2025-03-28T07:26:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    7,
                    26,
                    37,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T07:26:37Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    7,
                    26,
                    37,
                    4,
                    87,
                    0
                ],
                "title": "EdgeInfinite: A Memory-Efficient Infinite-Context Transformer for Edge\n  Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeInfinite: A Memory-Efficient Infinite-Context Transformer for Edge\n  Devices"
                },
                "summary": "Transformer-based large language models (LLMs) encounter challenges in\nprocessing long sequences on edge devices due to the quadratic complexity of\nattention mechanisms and growing memory demands from Key-Value (KV) cache.\nExisting KV cache optimizations struggle with irreversible token eviction in\nlong-output tasks, while alternative sequence modeling architectures prove\ncostly to adopt within established Transformer infrastructure. We present\nEdgeInfinite, a memory-efficient solution for infinite contexts that integrates\ncompressed memory into Transformer-based LLMs through a trainable memory-gating\nmodule. This approach maintains full compatibility with standard Transformer\narchitectures, requiring fine-tuning only a small part of parameters, and\nenables selective activation of the memory-gating module for long and short\ncontext task routing. The experimental result shows that EdgeInfinite achieves\ncomparable performance to baseline Transformer-based LLM on long context\nbenchmarks while optimizing memory consumption and time to first token.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) encounter challenges in\nprocessing long sequences on edge devices due to the quadratic complexity of\nattention mechanisms and growing memory demands from Key-Value (KV) cache.\nExisting KV cache optimizations struggle with irreversible token eviction in\nlong-output tasks, while alternative sequence modeling architectures prove\ncostly to adopt within established Transformer infrastructure. We present\nEdgeInfinite, a memory-efficient solution for infinite contexts that integrates\ncompressed memory into Transformer-based LLMs through a trainable memory-gating\nmodule. This approach maintains full compatibility with standard Transformer\narchitectures, requiring fine-tuning only a small part of parameters, and\nenables selective activation of the memory-gating module for long and short\ncontext task routing. The experimental result shows that EdgeInfinite achieves\ncomparable performance to baseline Transformer-based LLM on long context\nbenchmarks while optimizing memory consumption and time to first token."
                },
                "authors": [
                    {
                        "name": "Jiyu Chen"
                    },
                    {
                        "name": "Shuang Peng"
                    },
                    {
                        "name": "Daxiong Luo"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Renshou Wu"
                    },
                    {
                        "name": "Fangyuan Li"
                    },
                    {
                        "name": "Xiaoxin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoxin Chen"
                },
                "author": "Xiaoxin Chen",
                "arxiv_comment": "8 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22017v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22017v1",
                "updated": "2025-03-27T22:16:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    22,
                    16,
                    57,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T22:16:57Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    22,
                    16,
                    57,
                    3,
                    86,
                    0
                ],
                "title": "Performance Characterizations and Usage Guidelines of Samsung CXL Memory\n  Module Hybrid Prototype",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Characterizations and Usage Guidelines of Samsung CXL Memory\n  Module Hybrid Prototype"
                },
                "summary": "The growing prevalence of data-intensive workloads, such as artificial\nintelligence (AI), machine learning (ML), high-performance computing (HPC),\nin-memory databases, and real-time analytics, has exposed limitations in\nconventional memory technologies like DRAM. While DRAM offers low latency and\nhigh throughput, it is constrained by high costs, scalability challenges, and\nvolatility, making it less viable for capacity-bound and persistent\napplications in modern datacenters.\n  Recently, Compute Express Link (CXL) has emerged as a promising alternative,\nenabling high-speed, cacheline-granular communication between CPUs and external\ndevices. By leveraging CXL technology, NAND flash can now be used as memory\nexpansion, offering three-fold benefits: byte-addressability, scalable\ncapacity, and persistence at a low cost. Samsung's CXL Memory Module Hybrid\n(CMM-H) is the first product to deliver these benefits through a hardware-only\nsolution, i.e., it does not incur any OS and IO overheads like conventional\nblock devices. In particular, CMM-H integrates a DRAM cache with NAND flash in\na single device to deliver near-DRAM latency. This paper presents the first\npublicly available study for comprehensive characterizations of an FPGA-based\nCMM-H prototype. Through this study, we address users' concerns about whether a\nwide variety of applications can successfully run on a memory device backed by\nNAND flash medium. Additionally, based on these characterizations, we provide\nkey insights into how to best take advantage of the CMM-H device.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing prevalence of data-intensive workloads, such as artificial\nintelligence (AI), machine learning (ML), high-performance computing (HPC),\nin-memory databases, and real-time analytics, has exposed limitations in\nconventional memory technologies like DRAM. While DRAM offers low latency and\nhigh throughput, it is constrained by high costs, scalability challenges, and\nvolatility, making it less viable for capacity-bound and persistent\napplications in modern datacenters.\n  Recently, Compute Express Link (CXL) has emerged as a promising alternative,\nenabling high-speed, cacheline-granular communication between CPUs and external\ndevices. By leveraging CXL technology, NAND flash can now be used as memory\nexpansion, offering three-fold benefits: byte-addressability, scalable\ncapacity, and persistence at a low cost. Samsung's CXL Memory Module Hybrid\n(CMM-H) is the first product to deliver these benefits through a hardware-only\nsolution, i.e., it does not incur any OS and IO overheads like conventional\nblock devices. In particular, CMM-H integrates a DRAM cache with NAND flash in\na single device to deliver near-DRAM latency. This paper presents the first\npublicly available study for comprehensive characterizations of an FPGA-based\nCMM-H prototype. Through this study, we address users' concerns about whether a\nwide variety of applications can successfully run on a memory device backed by\nNAND flash medium. Additionally, based on these characterizations, we provide\nkey insights into how to best take advantage of the CMM-H device."
                },
                "authors": [
                    {
                        "name": "Jianping Zeng"
                    },
                    {
                        "name": "Shuyi Pei"
                    },
                    {
                        "name": "Da Zhang"
                    },
                    {
                        "name": "Yuchen Zhou"
                    },
                    {
                        "name": "Amir Beygi"
                    },
                    {
                        "name": "Xuebin Yao"
                    },
                    {
                        "name": "Ramdas Kachare"
                    },
                    {
                        "name": "Tong Zhang"
                    },
                    {
                        "name": "Zongwang Li"
                    },
                    {
                        "name": "Marie Nguyen"
                    },
                    {
                        "name": "Rekha Pitchumani"
                    },
                    {
                        "name": "Yang Soek Ki"
                    },
                    {
                        "name": "Changhee Jung"
                    }
                ],
                "author_detail": {
                    "name": "Changhee Jung"
                },
                "author": "Changhee Jung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22017v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22017v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18869v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18869v2",
                "updated": "2025-03-27T17:48:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    48,
                    14,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-24T16:44:32Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    44,
                    32,
                    0,
                    83,
                    0
                ],
                "title": "Reimagining Memory Access for LLM Inference: Compression-Aware Memory\n  Controller Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reimagining Memory Access for LLM Inference: Compression-Aware Memory\n  Controller Design"
                },
                "summary": "The efficiency of Large Language Model~(LLM) inference is often constrained\nby substantial memory bandwidth and capacity demands. Existing techniques, such\nas pruning, quantization, and mixture of experts/depth, reduce memory capacity\nand/or bandwidth consumption at the cost of slight degradation in inference\nquality. This paper introduces a design solution that further alleviates memory\nbottlenecks by enhancing the on-chip memory controller in AI accelerators to\nachieve two main objectives: (1) significantly reducing memory capacity and\nbandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of\nmodel weights and key-value (KV) cache without compromising inference quality,\nand (2) enabling memory bandwidth and energy consumption to scale\nproportionally with context-dependent dynamic quantization. These goals are\naccomplished by equipping the on-chip memory controller with mechanisms to\nimprove fine-grained bit-level accessibility and compressibility of weights and\nKV cache through LLM-aware configuration of in-memory placement and\nrepresentation. Experimental results on publicly available LLMs demonstrate the\neffectiveness of this approach, showing memory footprint reductions of 25.2\\%\nfor model weights and 46.9\\% for KV cache. In addition, our hardware prototype\nat 4\\,GHz and 32 lanes (7\\,nm) achieves 8\\,TB/s throughput with a modest area\noverhead (under 3.8\\,mm\\(^2\\)), which underscores the viability of LLM-aware\nmemory control as a key to efficient large-scale inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of Large Language Model~(LLM) inference is often constrained\nby substantial memory bandwidth and capacity demands. Existing techniques, such\nas pruning, quantization, and mixture of experts/depth, reduce memory capacity\nand/or bandwidth consumption at the cost of slight degradation in inference\nquality. This paper introduces a design solution that further alleviates memory\nbottlenecks by enhancing the on-chip memory controller in AI accelerators to\nachieve two main objectives: (1) significantly reducing memory capacity and\nbandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of\nmodel weights and key-value (KV) cache without compromising inference quality,\nand (2) enabling memory bandwidth and energy consumption to scale\nproportionally with context-dependent dynamic quantization. These goals are\naccomplished by equipping the on-chip memory controller with mechanisms to\nimprove fine-grained bit-level accessibility and compressibility of weights and\nKV cache through LLM-aware configuration of in-memory placement and\nrepresentation. Experimental results on publicly available LLMs demonstrate the\neffectiveness of this approach, showing memory footprint reductions of 25.2\\%\nfor model weights and 46.9\\% for KV cache. In addition, our hardware prototype\nat 4\\,GHz and 32 lanes (7\\,nm) achieves 8\\,TB/s throughput with a modest area\noverhead (under 3.8\\,mm\\(^2\\)), which underscores the viability of LLM-aware\nmemory control as a key to efficient large-scale inference."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Zirak Burzin Engineer"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "arxiv_comment": "9 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18869v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18869v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21725v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21725v1",
                "updated": "2025-03-27T17:37:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    37,
                    12,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T17:37:12Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    37,
                    12,
                    3,
                    86,
                    0
                ],
                "title": "Low-noise environment for probing fundamental symmetries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-noise environment for probing fundamental symmetries"
                },
                "summary": "We present the design and characterization of a low-noise environment for\nmeasuring the electron's electric dipole moment (EDM) with a beam of molecules.\nTo minimize magnetic Johnson noise from metals, the design features ceramic\nelectric field plates housed in a glass vacuum chamber. To suppress external\nmagnetic noise the apparatus is enclosed within a cylindrical four-layer\nmu-metal shield with a shielding factor exceeding $10^6$ in one radial\ndirection and $10^5$ in the other. Finite element modelling shows that the\ndifference between these shielding factors is due to imperfect joints between\nsections of mu-metal. Using atomic magnetometers to monitor the magnetic field\ninside the shield, we measure noise below 40 fT/$\\sqrt{{\\rm Hz}}$ at 1 Hz and\nabove, rising to 500 fT/$\\sqrt{{\\rm Hz}}$ at 0.1 Hz. Analytical and numerical\nstudies show that residual magnetic Johnson noise contributes approximately 13\nfT/$\\sqrt{{\\rm Hz}}$. The background magnetic field averaged along the beamline\nis maintained below 3 pT, with typical gradients of a few nT/m. An electric\nfield of 20 kV/cm is applied without discharges and with leakage currents below\n1 nA. Each magnetometer measures the magnetic field correlated with the\ndirection of the applied electric field with a precision of 0.11 fT in 104\nhours of data. These results demonstrate that the apparatus is suitable for\nmeasuring the electron EDM with precision at the $10^{-31}$ e cm level. The\ndesign principles and characterization techniques presented here are broadly\napplicable to precision measurements probing fundamental symmetries in\nmolecules, atoms, and neutrons.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the design and characterization of a low-noise environment for\nmeasuring the electron's electric dipole moment (EDM) with a beam of molecules.\nTo minimize magnetic Johnson noise from metals, the design features ceramic\nelectric field plates housed in a glass vacuum chamber. To suppress external\nmagnetic noise the apparatus is enclosed within a cylindrical four-layer\nmu-metal shield with a shielding factor exceeding $10^6$ in one radial\ndirection and $10^5$ in the other. Finite element modelling shows that the\ndifference between these shielding factors is due to imperfect joints between\nsections of mu-metal. Using atomic magnetometers to monitor the magnetic field\ninside the shield, we measure noise below 40 fT/$\\sqrt{{\\rm Hz}}$ at 1 Hz and\nabove, rising to 500 fT/$\\sqrt{{\\rm Hz}}$ at 0.1 Hz. Analytical and numerical\nstudies show that residual magnetic Johnson noise contributes approximately 13\nfT/$\\sqrt{{\\rm Hz}}$. The background magnetic field averaged along the beamline\nis maintained below 3 pT, with typical gradients of a few nT/m. An electric\nfield of 20 kV/cm is applied without discharges and with leakage currents below\n1 nA. Each magnetometer measures the magnetic field correlated with the\ndirection of the applied electric field with a precision of 0.11 fT in 104\nhours of data. These results demonstrate that the apparatus is suitable for\nmeasuring the electron EDM with precision at the $10^{-31}$ e cm level. The\ndesign principles and characterization techniques presented here are broadly\napplicable to precision measurements probing fundamental symmetries in\nmolecules, atoms, and neutrons."
                },
                "authors": [
                    {
                        "name": "F. J. Collings"
                    },
                    {
                        "name": "N. J. Fitch"
                    },
                    {
                        "name": "J. M. Dyne"
                    },
                    {
                        "name": "R. A. Jenkins"
                    },
                    {
                        "name": "E. Wursten"
                    },
                    {
                        "name": "M. T. Ziemba"
                    },
                    {
                        "name": "X. S. Zheng"
                    },
                    {
                        "name": "F. Castellini"
                    },
                    {
                        "name": "J. Lim"
                    },
                    {
                        "name": "B. E. Sauer"
                    },
                    {
                        "name": "M. R. Tarbutt"
                    }
                ],
                "author_detail": {
                    "name": "M. R. Tarbutt"
                },
                "author": "M. R. Tarbutt",
                "arxiv_comment": "34 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21725v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21725v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.atom-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v4",
                "updated": "2025-03-27T15:21:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    15,
                    21,
                    19,
                    3,
                    86,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17922v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17922v2",
                "updated": "2025-03-27T14:11:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    14,
                    11,
                    37,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-23T03:36:52Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    36,
                    52,
                    6,
                    82,
                    0
                ],
                "title": "WindowKV: Task-Adaptive Group-Wise KV Cache Window Selection for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WindowKV: Task-Adaptive Group-Wise KV Cache Window Selection for\n  Efficient LLM Inference"
                },
                "summary": "With the advancements in long-context inference capabilities of large\nlanguage models (LLMs), the KV cache has become one of the foundational\ncomponents. However, its substantial GPU memory consumption makes KV cache\ncompression a key technique for enabling efficient LLM inference in industrial\nscenarios. While recent studies have focused on optimizing the memory occupied\nby the KV cache, they overlook two critical factors: preserving semantic\ncoherence and considering task-specific characteristic during compression. To\naddress these limitations, we propose a novel task-adaptive KV cache window\nselection method, WindowKV. WindowKV dynamically selects local semantic windows\nconsisting of consecutive tokens, according to task-specific characteristics,\nensuring the retained KV cache captures continuous, essential context.\nAdditionally, we introduce an intra-group layer KV cache indices sharing\nstrategy to reduce computational overhead, achieving a balance between\nperformance and efficiency. We rigorously evaluate WindowKV on the LongBench\nbenchmark, and the results demonstrate that it maintains a performance\ncomparable to full KV cache retention while using only 12% of the original KV\ncache, significantly reducing memory requirements. Furthermore, our method also\nachieves state-of-the-art results in the Needle-in-a-Haystack evaluation,\nhighlighting its effectiveness and robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advancements in long-context inference capabilities of large\nlanguage models (LLMs), the KV cache has become one of the foundational\ncomponents. However, its substantial GPU memory consumption makes KV cache\ncompression a key technique for enabling efficient LLM inference in industrial\nscenarios. While recent studies have focused on optimizing the memory occupied\nby the KV cache, they overlook two critical factors: preserving semantic\ncoherence and considering task-specific characteristic during compression. To\naddress these limitations, we propose a novel task-adaptive KV cache window\nselection method, WindowKV. WindowKV dynamically selects local semantic windows\nconsisting of consecutive tokens, according to task-specific characteristics,\nensuring the retained KV cache captures continuous, essential context.\nAdditionally, we introduce an intra-group layer KV cache indices sharing\nstrategy to reduce computational overhead, achieving a balance between\nperformance and efficiency. We rigorously evaluate WindowKV on the LongBench\nbenchmark, and the results demonstrate that it maintains a performance\ncomparable to full KV cache retention while using only 12% of the original KV\ncache, significantly reducing memory requirements. Furthermore, our method also\nachieves state-of-the-art results in the Needle-in-a-Haystack evaluation,\nhighlighting its effectiveness and robustness."
                },
                "authors": [
                    {
                        "name": "Youhui Zuo"
                    },
                    {
                        "name": "Sibo Wei"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Zhuorui Liu"
                    },
                    {
                        "name": "Wenpeng Lu"
                    },
                    {
                        "name": "Dawei Song"
                    }
                ],
                "author_detail": {
                    "name": "Dawei Song"
                },
                "author": "Dawei Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17922v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17922v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17038v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17038v3",
                "updated": "2025-03-27T12:14:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    12,
                    14,
                    56,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-21T10:48:35Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    10,
                    48,
                    35,
                    4,
                    80,
                    0
                ],
                "title": "Arm DynamIQ Shared Unit and Real-Time: An Empirical Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arm DynamIQ Shared Unit and Real-Time: An Empirical Evaluation"
                },
                "summary": "The increasing complexity of embedded hardware platforms poses significant\nchallenges for real-time workloads. Architectural features such as Intel RDT,\nArm QoS, and Arm MPAM are either unavailable on commercial embedded platforms\nor designed primarily for server environments optimized for average-case\nperformance and might fail to deliver the expected real-time guarantees. Arm\nDynamIQ Shared Unit (DSU) includes isolation features-among others, hardware\nper-way cache partitioning-that can improve the real-time guarantees of complex\nembedded multicore systems and facilitate real-time analysis. However, the DSU\nalso targets average cases, and its real-time capabilities have not yet been\nevaluated. This paper presents the first comprehensive analysis of three\nreal-world deployments of the Arm DSU on Rockchip RK3568, Rockchip RK3588, and\nNVIDIA Orin platforms. We integrate support for the DSU at the operating system\nand hypervisor level and conduct a large-scale evaluation using both synthetic\nand real-world benchmarks with varying types and intensities of interference.\nOur results make extensive use of performance counters and indicate that,\nalthough effective, the quality of partitioning and isolation provided by the\nDSU depends on the type and the intensity of the interfering workloads. In\naddition, we uncover and analyze in detail the correlation between benchmarks\nand different types and intensities of interference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing complexity of embedded hardware platforms poses significant\nchallenges for real-time workloads. Architectural features such as Intel RDT,\nArm QoS, and Arm MPAM are either unavailable on commercial embedded platforms\nor designed primarily for server environments optimized for average-case\nperformance and might fail to deliver the expected real-time guarantees. Arm\nDynamIQ Shared Unit (DSU) includes isolation features-among others, hardware\nper-way cache partitioning-that can improve the real-time guarantees of complex\nembedded multicore systems and facilitate real-time analysis. However, the DSU\nalso targets average cases, and its real-time capabilities have not yet been\nevaluated. This paper presents the first comprehensive analysis of three\nreal-world deployments of the Arm DSU on Rockchip RK3568, Rockchip RK3588, and\nNVIDIA Orin platforms. We integrate support for the DSU at the operating system\nand hypervisor level and conduct a large-scale evaluation using both synthetic\nand real-world benchmarks with varying types and intensities of interference.\nOur results make extensive use of performance counters and indicate that,\nalthough effective, the quality of partitioning and isolation provided by the\nDSU depends on the type and the intensity of the interfering workloads. In\naddition, we uncover and analyze in detail the correlation between benchmarks\nand different types and intensities of interference."
                },
                "authors": [
                    {
                        "name": "Ashutosh Pradhan"
                    },
                    {
                        "name": "Daniele Ottaviano"
                    },
                    {
                        "name": "Yi Jiang"
                    },
                    {
                        "name": "Haozheng Huang"
                    },
                    {
                        "name": "Alexander Zuepke"
                    },
                    {
                        "name": "Andrea Bastoni"
                    },
                    {
                        "name": "Marco Caccamo"
                    }
                ],
                "author_detail": {
                    "name": "Marco Caccamo"
                },
                "author": "Marco Caccamo",
                "arxiv_comment": "Accepted for publication in the Proceedings of the 31st IEEE\n  Real-Time and Embedded Technology and Applications Symposium (RTAS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17038v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17038v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.3; C.4; D.4.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03708v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03708v3",
                "updated": "2025-03-27T11:46:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    11,
                    46,
                    22,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-05T17:59:19Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    59,
                    19,
                    2,
                    64,
                    0
                ],
                "title": "Rethinking Video Tokenization: A Conditioned Diffusion-based Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Video Tokenization: A Conditioned Diffusion-based Approach"
                },
                "summary": "Existing video tokenizers typically use the traditional Variational\nAutoencoder (VAE) architecture for video compression and reconstruction.\nHowever, to achieve good performance, its training process often relies on\ncomplex multi-stage training tricks that go beyond basic reconstruction loss\nand KL regularization. Among these tricks, the most challenging is the precise\ntuning of adversarial training with additional Generative Adversarial Networks\n(GANs) in the final stage, which can hinder stable convergence. In contrast to\nGANs, diffusion models offer more stable training processes and can generate\nhigher-quality results. Inspired by these advantages, we propose CDT, a novel\nConditioned Diffusion-based video Tokenizer, that replaces the GAN-based\ndecoder with a conditional causal diffusion model. The encoder compresses\nspatio-temporal information into compact latents, while the decoder\nreconstructs videos through a reverse diffusion process conditioned on these\nlatents. During inference, we incorporate a feature cache mechanism to generate\nvideos of arbitrary length while maintaining temporal continuity and adopt\nsampling acceleration technique to enhance efficiency. Trained using only a\nbasic MSE diffusion loss for reconstruction, along with KL term and LPIPS\nperceptual loss from scratch, extensive experiments demonstrate that CDT\nachieves state-of-the-art performance in video reconstruction tasks with just a\nsingle-step sampling. Even a scaled-down version of CDT (3$\\times$ inference\nspeedup) still performs comparably with top baselines. Moreover, the latent\nvideo generation model trained with CDT also exhibits superior performance. The\nsource code and pretrained weights are available at\nhttps://github.com/ali-vilab/CDT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing video tokenizers typically use the traditional Variational\nAutoencoder (VAE) architecture for video compression and reconstruction.\nHowever, to achieve good performance, its training process often relies on\ncomplex multi-stage training tricks that go beyond basic reconstruction loss\nand KL regularization. Among these tricks, the most challenging is the precise\ntuning of adversarial training with additional Generative Adversarial Networks\n(GANs) in the final stage, which can hinder stable convergence. In contrast to\nGANs, diffusion models offer more stable training processes and can generate\nhigher-quality results. Inspired by these advantages, we propose CDT, a novel\nConditioned Diffusion-based video Tokenizer, that replaces the GAN-based\ndecoder with a conditional causal diffusion model. The encoder compresses\nspatio-temporal information into compact latents, while the decoder\nreconstructs videos through a reverse diffusion process conditioned on these\nlatents. During inference, we incorporate a feature cache mechanism to generate\nvideos of arbitrary length while maintaining temporal continuity and adopt\nsampling acceleration technique to enhance efficiency. Trained using only a\nbasic MSE diffusion loss for reconstruction, along with KL term and LPIPS\nperceptual loss from scratch, extensive experiments demonstrate that CDT\nachieves state-of-the-art performance in video reconstruction tasks with just a\nsingle-step sampling. Even a scaled-down version of CDT (3$\\times$ inference\nspeedup) still performs comparably with top baselines. Moreover, the latent\nvideo generation model trained with CDT also exhibits superior performance. The\nsource code and pretrained weights are available at\nhttps://github.com/ali-vilab/CDT."
                },
                "authors": [
                    {
                        "name": "Nianzu Yang"
                    },
                    {
                        "name": "Pandeng Li"
                    },
                    {
                        "name": "Liming Zhao"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Chen-Wei Xie"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Zhihang Liu"
                    },
                    {
                        "name": "Yun Zheng"
                    },
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Junchi Yan"
                    }
                ],
                "author_detail": {
                    "name": "Junchi Yan"
                },
                "author": "Junchi Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03708v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03708v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17606v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17606v2",
                "updated": "2025-03-27T09:53:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    9,
                    53,
                    15,
                    3,
                    86,
                    0
                ],
                "published": "2024-09-26T07:44:47Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    7,
                    44,
                    47,
                    3,
                    270,
                    0
                ],
                "title": "FlooNoC: A 645 Gbps/link 0.15 pJ/B/hop Open-Source NoC with Wide\n  Physical Links and End-to-End AXI4 Parallel Multi-Stream Support",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlooNoC: A 645 Gbps/link 0.15 pJ/B/hop Open-Source NoC with Wide\n  Physical Links and End-to-End AXI4 Parallel Multi-Stream Support"
                },
                "summary": "The new generation of domain-specific AI accelerators is characterized by\nrapidly increasing demands for bulk data transfers, as opposed to small,\nlatency-critical cache line transfers typical of traditional cache-coherent\nsystems. In this paper, we address this critical need by introducing the\nFlooNoC Network-on-Chip (NoC), featuring very wide, fully Advanced eXtensible\nInterface (AXI4) compliant links designed to meet the massive bandwidth needs\nat high energy efficiency. At the transport level, non-blocking transactions\nare supported for latency tolerance. Additionally, a novel end-to-end ordering\napproach for AXI4, enabled by a multi-stream capable Direct Memory Access (DMA)\nengine simplifies network interfaces and eliminates inter-stream dependencies.\nFurthermore, dedicated physical links are instantiated for short,\nlatency-critical messages. A complete end-to-end reference implementation in\n12nm FinFET technology demonstrates the physical feasibility and power\nperformance area (PPA) benefits of our approach. Utilizing wide links on high\nlevels of metal, we achieve a bandwidth of 645 Gbps per link and a total\naggregate bandwidth of 103 Tbps for an 8x4 mesh of processors cluster tiles,\nwith a total of 288 RISC-V cores. The NoC imposes a minimal area overhead of\nonly 3.5% per compute tile and achieves a leading-edge energy efficiency of\n0.15 pJ/B/hop at 0.8 V. Compared to state-of-the-art NoCs, our system offers\nthree times the energy efficiency and more than double the link bandwidth.\nFurthermore, compared to a traditional AXI4-based multi-layer interconnect, our\nNoC achieves a 30% reduction in area, corresponding to a 47% increase in\nGFLOPSDP within the same floorplan.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The new generation of domain-specific AI accelerators is characterized by\nrapidly increasing demands for bulk data transfers, as opposed to small,\nlatency-critical cache line transfers typical of traditional cache-coherent\nsystems. In this paper, we address this critical need by introducing the\nFlooNoC Network-on-Chip (NoC), featuring very wide, fully Advanced eXtensible\nInterface (AXI4) compliant links designed to meet the massive bandwidth needs\nat high energy efficiency. At the transport level, non-blocking transactions\nare supported for latency tolerance. Additionally, a novel end-to-end ordering\napproach for AXI4, enabled by a multi-stream capable Direct Memory Access (DMA)\nengine simplifies network interfaces and eliminates inter-stream dependencies.\nFurthermore, dedicated physical links are instantiated for short,\nlatency-critical messages. A complete end-to-end reference implementation in\n12nm FinFET technology demonstrates the physical feasibility and power\nperformance area (PPA) benefits of our approach. Utilizing wide links on high\nlevels of metal, we achieve a bandwidth of 645 Gbps per link and a total\naggregate bandwidth of 103 Tbps for an 8x4 mesh of processors cluster tiles,\nwith a total of 288 RISC-V cores. The NoC imposes a minimal area overhead of\nonly 3.5% per compute tile and achieves a leading-edge energy efficiency of\n0.15 pJ/B/hop at 0.8 V. Compared to state-of-the-art NoCs, our system offers\nthree times the energy efficiency and more than double the link bandwidth.\nFurthermore, compared to a traditional AXI4-based multi-layer interconnect, our\nNoC achieves a 30% reduction in area, corresponding to a 47% increase in\nGFLOPSDP within the same floorplan."
                },
                "authors": [
                    {
                        "name": "Tim Fischer"
                    },
                    {
                        "name": "Michael Rogenmoser"
                    },
                    {
                        "name": "Thomas Benz"
                    },
                    {
                        "name": "Frank K. Gürkaynak"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "arxiv_doi": "10.1109/TVLSI.2025.3527225",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TVLSI.2025.3527225",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.17606v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17606v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Transactions on Very Large Scale Integration (VLSI) Systems (\n  Volume: 33, Issue: 4, April 2025)",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11108v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11108v2",
                "updated": "2025-03-27T07:02:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    7,
                    2,
                    19,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-14T06:01:42Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    1,
                    42,
                    4,
                    73,
                    0
                ],
                "title": "Time and Memory Trade-off of KV-Cache Compression in Tensor Transformer\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time and Memory Trade-off of KV-Cache Compression in Tensor Transformer\n  Decoding"
                },
                "summary": "The key-value (KV) cache in the tensor version of transformers presents a\nsignificant bottleneck during inference. While previous work analyzes the\nfundamental space complexity barriers in standard attention mechanisms [Haris\nand Onak, 2025], our work generalizes the space complexity barriers result to\ntensor attention version. Our theoretical contributions rely on a reduction\nfrom communication complexity and deduce the memory lower bound for\ntensor-structured attention mechanisms when $d = \\Omega(\\log n)$. Furthermore,\nwe introduce two types of tensor attention cache and present a trade-off\nbetween time and memory for two scenarios. Overall, our work provides a\ntheoretical foundation for us to understand the time-memory tradeoff of\nKV-Cache compression in tensor attention decoding and offers more perspectives\nin developing more memory-efficient tensor attention Transformer architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The key-value (KV) cache in the tensor version of transformers presents a\nsignificant bottleneck during inference. While previous work analyzes the\nfundamental space complexity barriers in standard attention mechanisms [Haris\nand Onak, 2025], our work generalizes the space complexity barriers result to\ntensor attention version. Our theoretical contributions rely on a reduction\nfrom communication complexity and deduce the memory lower bound for\ntensor-structured attention mechanisms when $d = \\Omega(\\log n)$. Furthermore,\nwe introduce two types of tensor attention cache and present a trade-off\nbetween time and memory for two scenarios. Overall, our work provides a\ntheoretical foundation for us to understand the time-memory tradeoff of\nKV-Cache compression in tensor attention decoding and offers more perspectives\nin developing more memory-efficient tensor attention Transformer architectures."
                },
                "authors": [
                    {
                        "name": "Yifang Chen"
                    },
                    {
                        "name": "Xiaoyu Li"
                    },
                    {
                        "name": "Yingyu Liang"
                    },
                    {
                        "name": "Zhenmei Shi"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Yu Tian"
                    }
                ],
                "author_detail": {
                    "name": "Yu Tian"
                },
                "author": "Yu Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11108v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11108v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11049v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11049v5",
                "updated": "2025-04-02T01:58:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    58,
                    38,
                    2,
                    92,
                    0
                ],
                "published": "2024-08-20T17:57:31Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    57,
                    31,
                    1,
                    233,
                    0
                ],
                "title": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding"
                },
                "summary": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency losslessly, but the conventional wisdom suggests\nthat its efficacy is limited to small batch sizes. In MagicDec, we show that\nsurprisingly SD can achieve speedup even for a high throughput inference regime\nfor moderate to long sequences. More interestingly, an intelligent drafting\nstrategy can achieve better speedup with increasing batch size based on our\nrigorous analysis. MagicDec first identifies the bottleneck shifts with\nincreasing batch size and sequence length, and uses these insights to deploy SD\nmore effectively for high throughput inference. We leverage draft model with\nsparse KV cache to address the KV bottleneck, which scales with both sequence\nlength and batch size. Additionally, we propose a theoretical model to select\nthe optimal drafting strategy for maximum speedup. Our work highlights the\nbroad applicability of speculative decoding in long-context serving, as it can\nenhance throughput and reduce latency without compromising accuracy. For\nmoderate to long sequences, we demonstrate up to 2.51x speedup for Llama3.1-8B\nwhen serving batch sizes ranging from 32 to 256 on various types of hardware\nand tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency losslessly, but the conventional wisdom suggests\nthat its efficacy is limited to small batch sizes. In MagicDec, we show that\nsurprisingly SD can achieve speedup even for a high throughput inference regime\nfor moderate to long sequences. More interestingly, an intelligent drafting\nstrategy can achieve better speedup with increasing batch size based on our\nrigorous analysis. MagicDec first identifies the bottleneck shifts with\nincreasing batch size and sequence length, and uses these insights to deploy SD\nmore effectively for high throughput inference. We leverage draft model with\nsparse KV cache to address the KV bottleneck, which scales with both sequence\nlength and batch size. Additionally, we propose a theoretical model to select\nthe optimal drafting strategy for maximum speedup. Our work highlights the\nbroad applicability of speculative decoding in long-context serving, as it can\nenhance throughput and reduce latency without compromising accuracy. For\nmoderate to long sequences, we demonstrate up to 2.51x speedup for Llama3.1-8B\nwhen serving batch sizes ranging from 32 to 256 on various types of hardware\nand tasks."
                },
                "authors": [
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Vashisth Tiwari"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Jinyuan Shi"
                    },
                    {
                        "name": "Ian En-Hsu Yen"
                    },
                    {
                        "name": "Avner May"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11049v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11049v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16302v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16302v2",
                "updated": "2025-03-26T15:08:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    15,
                    8,
                    12,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-20T16:23:44Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    23,
                    44,
                    3,
                    79,
                    0
                ],
                "title": "Unleashing Vecset Diffusion Model for Fast Shape Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing Vecset Diffusion Model for Fast Shape Generation"
                },
                "summary": "3D shape generation has greatly flourished through the development of\nso-called \"native\" 3D diffusion, particularly through the Vecset Diffusion\nModel (VDM). While recent advancements have shown promising results in\ngenerating high-resolution 3D shapes, VDM still struggles with high-speed\ngeneration. Challenges exist because of difficulties not only in accelerating\ndiffusion sampling but also VAE decoding in VDM, areas under-explored in\nprevious works. To address these challenges, we present FlashVDM, a systematic\nframework for accelerating both VAE and DiT in VDM. For DiT, FlashVDM enables\nflexible diffusion sampling with as few as 5 inference steps and comparable\nquality, which is made possible by stabilizing consistency distillation with\nour newly introduced Progressive Flow Distillation. For VAE, we introduce a\nlightning vecset decoder equipped with Adaptive KV Selection, Hierarchical\nVolume Decoding, and Efficient Network Design. By exploiting the locality of\nthe vecset and the sparsity of shape surface in the volume, our decoder\ndrastically lowers FLOPs, minimizing the overall decoding overhead. We apply\nFlashVDM to Hunyuan3D-2 to obtain Hunyuan3D-2 Turbo. Through systematic\nevaluation, we show that our model significantly outperforms existing fast 3D\ngeneration methods, achieving comparable performance to the state-of-the-art\nwhile reducing inference time by over 45x for reconstruction and 32x for\ngeneration. Code and models are available at\nhttps://github.com/Tencent/FlashVDM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D shape generation has greatly flourished through the development of\nso-called \"native\" 3D diffusion, particularly through the Vecset Diffusion\nModel (VDM). While recent advancements have shown promising results in\ngenerating high-resolution 3D shapes, VDM still struggles with high-speed\ngeneration. Challenges exist because of difficulties not only in accelerating\ndiffusion sampling but also VAE decoding in VDM, areas under-explored in\nprevious works. To address these challenges, we present FlashVDM, a systematic\nframework for accelerating both VAE and DiT in VDM. For DiT, FlashVDM enables\nflexible diffusion sampling with as few as 5 inference steps and comparable\nquality, which is made possible by stabilizing consistency distillation with\nour newly introduced Progressive Flow Distillation. For VAE, we introduce a\nlightning vecset decoder equipped with Adaptive KV Selection, Hierarchical\nVolume Decoding, and Efficient Network Design. By exploiting the locality of\nthe vecset and the sparsity of shape surface in the volume, our decoder\ndrastically lowers FLOPs, minimizing the overall decoding overhead. We apply\nFlashVDM to Hunyuan3D-2 to obtain Hunyuan3D-2 Turbo. Through systematic\nevaluation, we show that our model significantly outperforms existing fast 3D\ngeneration methods, achieving comparable performance to the state-of-the-art\nwhile reducing inference time by over 45x for reconstruction and 32x for\ngeneration. Code and models are available at\nhttps://github.com/Tencent/FlashVDM."
                },
                "authors": [
                    {
                        "name": "Zeqiang Lai"
                    },
                    {
                        "name": "Yunfei Zhao"
                    },
                    {
                        "name": "Zibo Zhao"
                    },
                    {
                        "name": "Haolin Liu"
                    },
                    {
                        "name": "Fuyun Wang"
                    },
                    {
                        "name": "Huiwen Shi"
                    },
                    {
                        "name": "Xianghui Yang"
                    },
                    {
                        "name": "Qingxiang Lin"
                    },
                    {
                        "name": "Jingwei Huang"
                    },
                    {
                        "name": "Yuhong Liu"
                    },
                    {
                        "name": "Jie Jiang"
                    },
                    {
                        "name": "Chunchao Guo"
                    },
                    {
                        "name": "Xiangyu Yue"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Yue"
                },
                "author": "Xiangyu Yue",
                "arxiv_comment": "Technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16302v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16302v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13996v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13996v3",
                "updated": "2025-03-26T13:59:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    13,
                    59,
                    53,
                    2,
                    85,
                    0
                ],
                "published": "2024-07-19T03:01:32Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    3,
                    1,
                    32,
                    4,
                    201,
                    0
                ],
                "title": "SGDRC: Software-Defined Dynamic Resource Control for Concurrent DNN\n  Inference on NVIDIA GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SGDRC: Software-Defined Dynamic Resource Control for Concurrent DNN\n  Inference on NVIDIA GPUs"
                },
                "summary": "Cloud service providers heavily colocate high-priority, latency-sensitive\n(LS), and low-priority, best-effort (BE) DNN inference services on the same GPU\nto improve resource utilization in data centers. Among the critical shared GPU\nresources, there has been very limited analysis on the dynamic allocation of\ncompute units and VRAM bandwidth, mainly for two reasons: (1) The native GPU\nresource management solutions are either hardware-specific, or unable to\ndynamically allocate resources to different tenants, or both; (2) NVIDIA\ndoesn't expose interfaces for VRAM bandwidth allocation, and the software stack\nand VRAM channel architectures are black-box, both of which limit the\nsoftware-level resource management. These drive prior work to design either\nconservative sharing policies detrimental to throughput, or static resource\npartitioning only applicable to a few GPU models.\n  To bridge this gap, this paper proposes SGDRC, a fully software-defined\ndynamic VRAM bandwidth and compute unit management solution for concurrent DNN\ninference services. SGDRC aims at guaranteeing service quality, maximizing the\noverall throughput, and providing general applicability to NVIDIA GPUs. SGDRC\nfirst reveals a general VRAM channel hash mapping architecture of NVIDIA GPUs\nthrough comprehensive reverse engineering and eliminates VRAM channel conflicts\nusing software-level cache coloring. SGDRC applies bimodal tensors and tidal SM\nmasking to dynamically allocate VRAM bandwidth and compute units, and guides\nthe allocation of resources based on offline profiling. We evaluate 11\nmainstream DNNs with real-world workloads on two NVIDIA GPUs. The results show\nthat compared with the state-of-the-art GPU sharing solutions, SGDRC achieves\nthe highest SLO attainment rates (99.0% on average), and improves overall\nthroughput by up to 1.47x and BE job throughput by up to 2.36x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud service providers heavily colocate high-priority, latency-sensitive\n(LS), and low-priority, best-effort (BE) DNN inference services on the same GPU\nto improve resource utilization in data centers. Among the critical shared GPU\nresources, there has been very limited analysis on the dynamic allocation of\ncompute units and VRAM bandwidth, mainly for two reasons: (1) The native GPU\nresource management solutions are either hardware-specific, or unable to\ndynamically allocate resources to different tenants, or both; (2) NVIDIA\ndoesn't expose interfaces for VRAM bandwidth allocation, and the software stack\nand VRAM channel architectures are black-box, both of which limit the\nsoftware-level resource management. These drive prior work to design either\nconservative sharing policies detrimental to throughput, or static resource\npartitioning only applicable to a few GPU models.\n  To bridge this gap, this paper proposes SGDRC, a fully software-defined\ndynamic VRAM bandwidth and compute unit management solution for concurrent DNN\ninference services. SGDRC aims at guaranteeing service quality, maximizing the\noverall throughput, and providing general applicability to NVIDIA GPUs. SGDRC\nfirst reveals a general VRAM channel hash mapping architecture of NVIDIA GPUs\nthrough comprehensive reverse engineering and eliminates VRAM channel conflicts\nusing software-level cache coloring. SGDRC applies bimodal tensors and tidal SM\nmasking to dynamically allocate VRAM bandwidth and compute units, and guides\nthe allocation of resources based on offline profiling. We evaluate 11\nmainstream DNNs with real-world workloads on two NVIDIA GPUs. The results show\nthat compared with the state-of-the-art GPU sharing solutions, SGDRC achieves\nthe highest SLO attainment rates (99.0% on average), and improves overall\nthroughput by up to 1.47x and BE job throughput by up to 2.36x."
                },
                "authors": [
                    {
                        "name": "Yongkang Zhang"
                    },
                    {
                        "name": "Haoxuan Yu"
                    },
                    {
                        "name": "Chenxia Han"
                    },
                    {
                        "name": "Cheng Wang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Yunzhe Li"
                    },
                    {
                        "name": "Zhifeng Jiang"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Huaicheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Huaicheng Li"
                },
                "author": "Huaicheng Li",
                "arxiv_doi": "10.1145/3710848.3710863",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3710848.3710863",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.13996v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13996v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 19 figures",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.9; I.2.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20481v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20481v1",
                "updated": "2025-03-26T12:10:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    12,
                    10,
                    53,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T12:10:53Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    12,
                    10,
                    53,
                    2,
                    85,
                    0
                ],
                "title": "Analyzing Modern NVIDIA GPU cores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing Modern NVIDIA GPU cores"
                },
                "summary": "GPUs are the most popular platform for accelerating HPC workloads, such as\nartificial intelligence and science simulations. However, most\nmicroarchitectural research in academia relies on GPU core pipeline designs\nbased on architectures that are more than 15 years old.\n  This paper reverse engineers modern NVIDIA GPU cores, unveiling many key\naspects of its design and explaining how GPUs leverage hardware-compiler\ntechniques where the compiler guides hardware during execution. In particular,\nit reveals how the issue logic works including the policy of the issue\nscheduler, the structure of the register file and its associated cache, and\nmultiple features of the memory pipeline. Moreover, it analyses how a simple\ninstruction prefetcher based on a stream buffer fits well with modern NVIDIA\nGPUs and is likely to be used. Furthermore, we investigate the impact of the\nregister file cache and the number of register file read ports on both\nsimulation accuracy and performance.\n  By modeling all these new discovered microarchitectural details, we achieve\n18.24% lower mean absolute percentage error (MAPE) in execution cycles than\nprevious state-of-the-art simulators, resulting in an average of 13.98% MAPE\nwith respect to real hardware (NVIDIA RTX A6000). Also, we demonstrate that\nthis new model stands for other NVIDIA architectures, such as Turing. Finally,\nwe show that the software-based dependence management mechanism included in\nmodern NVIDIA GPUs outperforms a hardware mechanism based on scoreboards in\nterms of performance and area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPUs are the most popular platform for accelerating HPC workloads, such as\nartificial intelligence and science simulations. However, most\nmicroarchitectural research in academia relies on GPU core pipeline designs\nbased on architectures that are more than 15 years old.\n  This paper reverse engineers modern NVIDIA GPU cores, unveiling many key\naspects of its design and explaining how GPUs leverage hardware-compiler\ntechniques where the compiler guides hardware during execution. In particular,\nit reveals how the issue logic works including the policy of the issue\nscheduler, the structure of the register file and its associated cache, and\nmultiple features of the memory pipeline. Moreover, it analyses how a simple\ninstruction prefetcher based on a stream buffer fits well with modern NVIDIA\nGPUs and is likely to be used. Furthermore, we investigate the impact of the\nregister file cache and the number of register file read ports on both\nsimulation accuracy and performance.\n  By modeling all these new discovered microarchitectural details, we achieve\n18.24% lower mean absolute percentage error (MAPE) in execution cycles than\nprevious state-of-the-art simulators, resulting in an average of 13.98% MAPE\nwith respect to real hardware (NVIDIA RTX A6000). Also, we demonstrate that\nthis new model stands for other NVIDIA architectures, such as Turing. Finally,\nwe show that the software-based dependence management mechanism included in\nmodern NVIDIA GPUs outperforms a hardware mechanism based on scoreboards in\nterms of performance and area."
                },
                "authors": [
                    {
                        "name": "Rodrigo Huerta"
                    },
                    {
                        "name": "Mojtaba Abaie Shoushtary"
                    },
                    {
                        "name": "José-Lorenzo Cruz"
                    },
                    {
                        "name": "Antonio González"
                    }
                ],
                "author_detail": {
                    "name": "Antonio González"
                },
                "author": "Antonio González",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20481v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20481v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12150v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12150v2",
                "updated": "2025-03-26T11:08:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    11,
                    8,
                    20,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-15T14:13:23Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    14,
                    13,
                    23,
                    5,
                    74,
                    0
                ],
                "title": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and\n  Generalizable Point Cloud Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and\n  Generalizable Point Cloud Analysis"
                },
                "summary": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data (often inaccessible during online inference) and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\n\\textbf{Point-Cache}, a hierarchical cache model that captures essential clues\nof online test samples, particularly focusing on the global structure of point\nclouds and their local-part details. Point-Cache, which serves as a rich 3D\nknowledge base, is dynamically managed to prioritize the inclusion of\nhigh-quality samples. Designed as a plug-and-play module, our method can be\nflexibly integrated into large multimodal 3D models to support open-vocabulary\npoint cloud recognition. Notably, our solution operates with efficiency\ncomparable to zero-shot inference, as it is entirely training-free. Point-Cache\ndemonstrates substantial gains across 8 challenging benchmarks and 4\nrepresentative large 3D models, highlighting its effectiveness. Code is\navailable at https://github.com/auniquesun/Point-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data (often inaccessible during online inference) and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\n\\textbf{Point-Cache}, a hierarchical cache model that captures essential clues\nof online test samples, particularly focusing on the global structure of point\nclouds and their local-part details. Point-Cache, which serves as a rich 3D\nknowledge base, is dynamically managed to prioritize the inclusion of\nhigh-quality samples. Designed as a plug-and-play module, our method can be\nflexibly integrated into large multimodal 3D models to support open-vocabulary\npoint cloud recognition. Notably, our solution operates with efficiency\ncomparable to zero-shot inference, as it is entirely training-free. Point-Cache\ndemonstrates substantial gains across 8 challenging benchmarks and 4\nrepresentative large 3D models, highlighting its effectiveness. Code is\navailable at https://github.com/auniquesun/Point-Cache."
                },
                "authors": [
                    {
                        "name": "Hongyu Sun"
                    },
                    {
                        "name": "Qiuhong Ke"
                    },
                    {
                        "name": "Ming Cheng"
                    },
                    {
                        "name": "Yongcai Wang"
                    },
                    {
                        "name": "Deying Li"
                    },
                    {
                        "name": "Chenhui Gou"
                    },
                    {
                        "name": "Jianfei Cai"
                    }
                ],
                "author_detail": {
                    "name": "Jianfei Cai"
                },
                "author": "Jianfei Cai",
                "arxiv_comment": "Accepted by CVPR 2025; 24 pages, 14 figures, 18 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12150v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12150v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20174v1",
                "updated": "2025-03-26T02:58:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    2,
                    58,
                    41,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T02:58:41Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    2,
                    58,
                    41,
                    2,
                    85,
                    0
                ],
                "title": "Devil is in the Uniformity: Exploring Diverse Learners within\n  Transformer for Image Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Devil is in the Uniformity: Exploring Diverse Learners within\n  Transformer for Image Restoration"
                },
                "summary": "Transformer-based approaches have gained significant attention in image\nrestoration, where the core component, i.e, Multi-Head Attention (MHA), plays a\ncrucial role in capturing diverse features and recovering high-quality results.\nIn MHA, heads perform attention calculation independently from uniform split\nsubspaces, and a redundancy issue is triggered to hinder the model from\nachieving satisfactory outputs. In this paper, we propose to improve MHA by\nexploring diverse learners and introducing various interactions between heads,\nwhich results in a Hierarchical multI-head atteNtion driven Transformer model,\ntermed HINT, for image restoration. HINT contains two modules, i.e., the\nHierarchical Multi-Head Attention (HMHA) and the Query-Key Cache Updating\n(QKCU) module, to address the redundancy problem that is rooted in vanilla MHA.\nSpecifically, HMHA extracts diverse contextual features by employing heads to\nlearn from subspaces of varying sizes and containing different information.\nMoreover, QKCU, comprising intra- and inter-layer schemes, further reduces the\nredundancy problem by facilitating enhanced interactions between attention\nheads within and across layers. Extensive experiments are conducted on 12\nbenchmarks across 5 image restoration tasks, including low-light enhancement,\ndehazing, desnowing, denoising, and deraining, to demonstrate the superiority\nof HINT. The source code is available in the supplementary materials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based approaches have gained significant attention in image\nrestoration, where the core component, i.e, Multi-Head Attention (MHA), plays a\ncrucial role in capturing diverse features and recovering high-quality results.\nIn MHA, heads perform attention calculation independently from uniform split\nsubspaces, and a redundancy issue is triggered to hinder the model from\nachieving satisfactory outputs. In this paper, we propose to improve MHA by\nexploring diverse learners and introducing various interactions between heads,\nwhich results in a Hierarchical multI-head atteNtion driven Transformer model,\ntermed HINT, for image restoration. HINT contains two modules, i.e., the\nHierarchical Multi-Head Attention (HMHA) and the Query-Key Cache Updating\n(QKCU) module, to address the redundancy problem that is rooted in vanilla MHA.\nSpecifically, HMHA extracts diverse contextual features by employing heads to\nlearn from subspaces of varying sizes and containing different information.\nMoreover, QKCU, comprising intra- and inter-layer schemes, further reduces the\nredundancy problem by facilitating enhanced interactions between attention\nheads within and across layers. Extensive experiments are conducted on 12\nbenchmarks across 5 image restoration tasks, including low-light enhancement,\ndehazing, desnowing, denoising, and deraining, to demonstrate the superiority\nof HINT. The source code is available in the supplementary materials."
                },
                "authors": [
                    {
                        "name": "Shihao Zhou"
                    },
                    {
                        "name": "Dayu Li"
                    },
                    {
                        "name": "Jinshan Pan"
                    },
                    {
                        "name": "Juncheng Zhou"
                    },
                    {
                        "name": "Jinglei Shi"
                    },
                    {
                        "name": "Jufeng Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jufeng Yang"
                },
                "author": "Jufeng Yang",
                "arxiv_comment": "11 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17264v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17264v2",
                "updated": "2025-03-26T01:58:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    1,
                    58,
                    40,
                    2,
                    85,
                    0
                ],
                "published": "2024-09-25T18:21:05Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    18,
                    21,
                    5,
                    2,
                    269,
                    0
                ],
                "title": "Medha: Efficiently Serving Multi-Million Context Length LLM Inference\n  Requests Without Approximations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medha: Efficiently Serving Multi-Million Context Length LLM Inference\n  Requests Without Approximations"
                },
                "summary": "As large language models (LLMs) handle increasingly longer contexts, serving\ninference requests for context lengths in the range of millions of tokens\npresents unique challenges. While existing techniques are effective for\ntraining, they fail to address the unique challenges of inference, such as\nvarying prefill and decode phases and their associated latency constraints --\nlike Time to First Token (TTFT) and Time per Output Token (TPOT). Furthermore,\nno long-context inference solutions address head-of-line blocking today.\n  We present Medha, a system for efficient long-context LLM inference that\nintroduces three key innovations: adaptive chunking with slack-aware scheduling\nto prevent head-ofline blocking, Sequence Pipeline Parallelism (SPP) to reduce\nTTFT, and KV Cache Parallelism (KVP) to minimize TPOT. By combining these into\na novel 3D parallelism serving engine, Medha achieves unprecedented scale --\nsupporting contexts up to 10M tokens with production-grade latency. Our\nevaluation shows Medha reduces median latency by up to 30x compared to\nstate-of-the-art systems when serving a mix of short and long requests, while\nimproving throughput by upwards of 5x. This enables, for the first time,\nefficient long-context LLM inference at scale without compromising on shorter\nrequest latencies or system efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) handle increasingly longer contexts, serving\ninference requests for context lengths in the range of millions of tokens\npresents unique challenges. While existing techniques are effective for\ntraining, they fail to address the unique challenges of inference, such as\nvarying prefill and decode phases and their associated latency constraints --\nlike Time to First Token (TTFT) and Time per Output Token (TPOT). Furthermore,\nno long-context inference solutions address head-of-line blocking today.\n  We present Medha, a system for efficient long-context LLM inference that\nintroduces three key innovations: adaptive chunking with slack-aware scheduling\nto prevent head-ofline blocking, Sequence Pipeline Parallelism (SPP) to reduce\nTTFT, and KV Cache Parallelism (KVP) to minimize TPOT. By combining these into\na novel 3D parallelism serving engine, Medha achieves unprecedented scale --\nsupporting contexts up to 10M tokens with production-grade latency. Our\nevaluation shows Medha reduces median latency by up to 30x compared to\nstate-of-the-art systems when serving a mix of short and long requests, while\nimproving throughput by upwards of 5x. This enables, for the first time,\nefficient long-context LLM inference at scale without compromising on shorter\nrequest latencies or system efficiency."
                },
                "authors": [
                    {
                        "name": "Amey Agrawal"
                    },
                    {
                        "name": "Haoran Qiu"
                    },
                    {
                        "name": "Junda Chen"
                    },
                    {
                        "name": "Íñigo Goiri"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    },
                    {
                        "name": "Chaojie Zhang"
                    },
                    {
                        "name": "Alexey Tumanov"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17264v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17264v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13509v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13509v2",
                "updated": "2025-03-25T17:56:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    17,
                    56,
                    1,
                    1,
                    84,
                    0
                ],
                "published": "2024-12-18T05:16:11Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    5,
                    16,
                    11,
                    2,
                    353,
                    0
                ],
                "title": "Visualizing the Invisible: A Generative AR System for Intuitive\n  Multi-Modal Sensor Data Presentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visualizing the Invisible: A Generative AR System for Intuitive\n  Multi-Modal Sensor Data Presentation"
                },
                "summary": "Understanding sensor data can be difficult for non-experts because of the\ncomplexity and different semantic meanings of sensor modalities. This leads to\na need for intuitive and effective methods to present sensor information.\nHowever, creating intuitive sensor data visualizations presents three key\nchallenges: the variability of sensor readings, gaps in domain comprehension,\nand the dynamic nature of sensor data. To address these issues, we propose\nVivar, a novel system that integrates multi-modal sensor data and presents 3D\nvolumetric content for AR visualization. In particular, we introduce a\ncross-modal embedding approach that maps sensor data into a pre-trained visual\nembedding space through barycentric interpolation. This approach accurately\nreflects value changes in multi-modal sensor information, ensuring that sensor\nvariations are properly shown in visualization outcomes. Vivar also\nincorporates sensor-aware AR scene generation using foundation models and 3D\nGaussian Splatting (3DGS) without requiring domain expertise. In addition,\nVivar leverages latent reuse and caching strategies to accelerate 2D and AR\ncontent generation, demonstrating 11x latency reduction without compromising\nquality. A user study involving over 503 participants, including domain\nexperts, demonstrates Vivar's effectiveness in accuracy, consistency, and\nreal-world applicability, paving the way for more intuitive sensor data\nvisualization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding sensor data can be difficult for non-experts because of the\ncomplexity and different semantic meanings of sensor modalities. This leads to\na need for intuitive and effective methods to present sensor information.\nHowever, creating intuitive sensor data visualizations presents three key\nchallenges: the variability of sensor readings, gaps in domain comprehension,\nand the dynamic nature of sensor data. To address these issues, we propose\nVivar, a novel system that integrates multi-modal sensor data and presents 3D\nvolumetric content for AR visualization. In particular, we introduce a\ncross-modal embedding approach that maps sensor data into a pre-trained visual\nembedding space through barycentric interpolation. This approach accurately\nreflects value changes in multi-modal sensor information, ensuring that sensor\nvariations are properly shown in visualization outcomes. Vivar also\nincorporates sensor-aware AR scene generation using foundation models and 3D\nGaussian Splatting (3DGS) without requiring domain expertise. In addition,\nVivar leverages latent reuse and caching strategies to accelerate 2D and AR\ncontent generation, demonstrating 11x latency reduction without compromising\nquality. A user study involving over 503 participants, including domain\nexperts, demonstrates Vivar's effectiveness in accuracy, consistency, and\nreal-world applicability, paving the way for more intuitive sensor data\nvisualization."
                },
                "authors": [
                    {
                        "name": "Yunqi Guo"
                    },
                    {
                        "name": "Kaiyuan Hou"
                    },
                    {
                        "name": "Heming Fu"
                    },
                    {
                        "name": "Hongkai Chen"
                    },
                    {
                        "name": "Zhenyu Yan"
                    },
                    {
                        "name": "Guoliang Xing"
                    },
                    {
                        "name": "Xiaofan Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofan Jiang"
                },
                "author": "Xiaofan Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13509v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13509v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19950v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19950v1",
                "updated": "2025-03-25T16:24:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    16,
                    24,
                    45,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T16:24:45Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    16,
                    24,
                    45,
                    1,
                    84,
                    0
                ],
                "title": "LogQuant: Log-Distributed 2-Bit Quantization of KV Cache with Superior\n  Accuracy Preservation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LogQuant: Log-Distributed 2-Bit Quantization of KV Cache with Superior\n  Accuracy Preservation"
                },
                "summary": "We introduce LogQuant, a groundbreaking 2-bit quantization technique for KV\nCache in large language model (LLM) inference, delivering substantial memory\nsavings while preserving superior performance. Previous methods either assume\nthat later tokens are more important or attempt to predict important tokens\nbased on earlier attention patterns. Both approaches, however, can result in\nperformance bottlenecks or frequent mispredictions.\n  LogQuant takes a different approach. By applying a log-based filtering\nmechanism, it selectively compresses the KV Cache across the entire context,\nachieving better performance with the same or even reduced memory footprint\ncompared to existing methods. In benchmark tests, it enhances throughput by 25%\nand boosts batch size by 60% without increasing memory consumption. For\nchallenging tasks such as Math and Code Completion, LogQuant improves accuracy\nby 40% to 200% at the same compression ratio, outperforming comparable\ntechniques.LogQuant integrates effortlessly with popular inference frameworks\nlike Python's transformers library. Implementation can be available in\nhttps://github.com/Concyclics/LogQuantKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce LogQuant, a groundbreaking 2-bit quantization technique for KV\nCache in large language model (LLM) inference, delivering substantial memory\nsavings while preserving superior performance. Previous methods either assume\nthat later tokens are more important or attempt to predict important tokens\nbased on earlier attention patterns. Both approaches, however, can result in\nperformance bottlenecks or frequent mispredictions.\n  LogQuant takes a different approach. By applying a log-based filtering\nmechanism, it selectively compresses the KV Cache across the entire context,\nachieving better performance with the same or even reduced memory footprint\ncompared to existing methods. In benchmark tests, it enhances throughput by 25%\nand boosts batch size by 60% without increasing memory consumption. For\nchallenging tasks such as Math and Code Completion, LogQuant improves accuracy\nby 40% to 200% at the same compression ratio, outperforming comparable\ntechniques.LogQuant integrates effortlessly with popular inference frameworks\nlike Python's transformers library. Implementation can be available in\nhttps://github.com/Concyclics/LogQuantKV."
                },
                "authors": [
                    {
                        "name": "Han Chen"
                    },
                    {
                        "name": "Zicong Jiang"
                    },
                    {
                        "name": "Zining Zhang"
                    },
                    {
                        "name": "Bingsheng He"
                    },
                    {
                        "name": "Pingyi Luo"
                    },
                    {
                        "name": "Mian Lu"
                    },
                    {
                        "name": "Yuqiang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yuqiang Chen"
                },
                "author": "Yuqiang Chen",
                "arxiv_comment": "Accepted by ICLR 2025 Workshop on Sparsity in LLMs (SLLM)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19950v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19950v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19786v1",
                "updated": "2025-03-25T15:52:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    15,
                    52,
                    34,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T15:52:34Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    15,
                    52,
                    34,
                    1,
                    84,
                    0
                ],
                "title": "Gemma 3 Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gemma 3 Technical Report"
                },
                "summary": "We introduce Gemma 3, a multimodal addition to the Gemma family of\nlightweight open models, ranging in scale from 1 to 27 billion parameters. This\nversion introduces vision understanding abilities, a wider coverage of\nlanguages and longer context - at least 128K tokens. We also change the\narchitecture of the model to reduce the KV-cache memory that tends to explode\nwith long context. This is achieved by increasing the ratio of local to global\nattention layers, and keeping the span on local attention short. The Gemma 3\nmodels are trained with distillation and achieve superior performance to Gemma\n2 for both pre-trained and instruction finetuned versions. In particular, our\nnovel post-training recipe significantly improves the math, chat,\ninstruction-following and multilingual abilities, making Gemma3-4B-IT\ncompetitive with Gemma2-27B-IT and Gemma3-27B-IT comparable to Gemini-1.5-Pro\nacross benchmarks. We release all our models to the community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Gemma 3, a multimodal addition to the Gemma family of\nlightweight open models, ranging in scale from 1 to 27 billion parameters. This\nversion introduces vision understanding abilities, a wider coverage of\nlanguages and longer context - at least 128K tokens. We also change the\narchitecture of the model to reduce the KV-cache memory that tends to explode\nwith long context. This is achieved by increasing the ratio of local to global\nattention layers, and keeping the span on local attention short. The Gemma 3\nmodels are trained with distillation and achieve superior performance to Gemma\n2 for both pre-trained and instruction finetuned versions. In particular, our\nnovel post-training recipe significantly improves the math, chat,\ninstruction-following and multilingual abilities, making Gemma3-4B-IT\ncompetitive with Gemma2-27B-IT and Gemma3-27B-IT comparable to Gemini-1.5-Pro\nacross benchmarks. We release all our models to the community."
                },
                "authors": [
                    {
                        "name": "Gemma Team"
                    },
                    {
                        "name": "Aishwarya Kamath"
                    },
                    {
                        "name": "Johan Ferret"
                    },
                    {
                        "name": "Shreya Pathak"
                    },
                    {
                        "name": "Nino Vieillard"
                    },
                    {
                        "name": "Ramona Merhej"
                    },
                    {
                        "name": "Sarah Perrin"
                    },
                    {
                        "name": "Tatiana Matejovicova"
                    },
                    {
                        "name": "Alexandre Ramé"
                    },
                    {
                        "name": "Morgane Rivière"
                    },
                    {
                        "name": "Louis Rouillard"
                    },
                    {
                        "name": "Thomas Mesnard"
                    },
                    {
                        "name": "Geoffrey Cideron"
                    },
                    {
                        "name": "Jean-bastien Grill"
                    },
                    {
                        "name": "Sabela Ramos"
                    },
                    {
                        "name": "Edouard Yvinec"
                    },
                    {
                        "name": "Michelle Casbon"
                    },
                    {
                        "name": "Etienne Pot"
                    },
                    {
                        "name": "Ivo Penchev"
                    },
                    {
                        "name": "Gaël Liu"
                    },
                    {
                        "name": "Francesco Visin"
                    },
                    {
                        "name": "Kathleen Kenealy"
                    },
                    {
                        "name": "Lucas Beyer"
                    },
                    {
                        "name": "Xiaohai Zhai"
                    },
                    {
                        "name": "Anton Tsitsulin"
                    },
                    {
                        "name": "Robert Busa-Fekete"
                    },
                    {
                        "name": "Alex Feng"
                    },
                    {
                        "name": "Noveen Sachdeva"
                    },
                    {
                        "name": "Benjamin Coleman"
                    },
                    {
                        "name": "Yi Gao"
                    },
                    {
                        "name": "Basil Mustafa"
                    },
                    {
                        "name": "Iain Barr"
                    },
                    {
                        "name": "Emilio Parisotto"
                    },
                    {
                        "name": "David Tian"
                    },
                    {
                        "name": "Matan Eyal"
                    },
                    {
                        "name": "Colin Cherry"
                    },
                    {
                        "name": "Jan-Thorsten Peter"
                    },
                    {
                        "name": "Danila Sinopalnikov"
                    },
                    {
                        "name": "Surya Bhupatiraju"
                    },
                    {
                        "name": "Rishabh Agarwal"
                    },
                    {
                        "name": "Mehran Kazemi"
                    },
                    {
                        "name": "Dan Malkin"
                    },
                    {
                        "name": "Ravin Kumar"
                    },
                    {
                        "name": "David Vilar"
                    },
                    {
                        "name": "Idan Brusilovsky"
                    },
                    {
                        "name": "Jiaming Luo"
                    },
                    {
                        "name": "Andreas Steiner"
                    },
                    {
                        "name": "Abe Friesen"
                    },
                    {
                        "name": "Abhanshu Sharma"
                    },
                    {
                        "name": "Abheesht Sharma"
                    },
                    {
                        "name": "Adi Mayrav Gilady"
                    },
                    {
                        "name": "Adrian Goedeckemeyer"
                    },
                    {
                        "name": "Alaa Saade"
                    },
                    {
                        "name": "Alex Feng"
                    },
                    {
                        "name": "Alexander Kolesnikov"
                    },
                    {
                        "name": "Alexei Bendebury"
                    },
                    {
                        "name": "Alvin Abdagic"
                    },
                    {
                        "name": "Amit Vadi"
                    },
                    {
                        "name": "András György"
                    },
                    {
                        "name": "André Susano Pinto"
                    },
                    {
                        "name": "Anil Das"
                    },
                    {
                        "name": "Ankur Bapna"
                    },
                    {
                        "name": "Antoine Miech"
                    },
                    {
                        "name": "Antoine Yang"
                    },
                    {
                        "name": "Antonia Paterson"
                    },
                    {
                        "name": "Ashish Shenoy"
                    },
                    {
                        "name": "Ayan Chakrabarti"
                    },
                    {
                        "name": "Bilal Piot"
                    },
                    {
                        "name": "Bo Wu"
                    },
                    {
                        "name": "Bobak Shahriari"
                    },
                    {
                        "name": "Bryce Petrini"
                    },
                    {
                        "name": "Charlie Chen"
                    },
                    {
                        "name": "Charline Le Lan"
                    },
                    {
                        "name": "Christopher A. Choquette-Choo"
                    },
                    {
                        "name": "CJ Carey"
                    },
                    {
                        "name": "Cormac Brick"
                    },
                    {
                        "name": "Daniel Deutsch"
                    },
                    {
                        "name": "Danielle Eisenbud"
                    },
                    {
                        "name": "Dee Cattle"
                    },
                    {
                        "name": "Derek Cheng"
                    },
                    {
                        "name": "Dimitris Paparas"
                    },
                    {
                        "name": "Divyashree Shivakumar Sreepathihalli"
                    },
                    {
                        "name": "Doug Reid"
                    },
                    {
                        "name": "Dustin Tran"
                    },
                    {
                        "name": "Dustin Zelle"
                    },
                    {
                        "name": "Eric Noland"
                    },
                    {
                        "name": "Erwin Huizenga"
                    },
                    {
                        "name": "Eugene Kharitonov"
                    },
                    {
                        "name": "Frederick Liu"
                    },
                    {
                        "name": "Gagik Amirkhanyan"
                    },
                    {
                        "name": "Glenn Cameron"
                    },
                    {
                        "name": "Hadi Hashemi"
                    },
                    {
                        "name": "Hanna Klimczak-Plucińska"
                    },
                    {
                        "name": "Harman Singh"
                    },
                    {
                        "name": "Harsh Mehta"
                    },
                    {
                        "name": "Harshal Tushar Lehri"
                    },
                    {
                        "name": "Hussein Hazimeh"
                    },
                    {
                        "name": "Ian Ballantyne"
                    },
                    {
                        "name": "Idan Szpektor"
                    },
                    {
                        "name": "Ivan Nardini"
                    },
                    {
                        "name": "Jean Pouget-Abadie"
                    },
                    {
                        "name": "Jetha Chan"
                    },
                    {
                        "name": "Joe Stanton"
                    },
                    {
                        "name": "John Wieting"
                    },
                    {
                        "name": "Jonathan Lai"
                    },
                    {
                        "name": "Jordi Orbay"
                    },
                    {
                        "name": "Joseph Fernandez"
                    },
                    {
                        "name": "Josh Newlan"
                    },
                    {
                        "name": "Ju-yeong Ji"
                    },
                    {
                        "name": "Jyotinder Singh"
                    },
                    {
                        "name": "Kat Black"
                    },
                    {
                        "name": "Kathy Yu"
                    },
                    {
                        "name": "Kevin Hui"
                    },
                    {
                        "name": "Kiran Vodrahalli"
                    },
                    {
                        "name": "Klaus Greff"
                    },
                    {
                        "name": "Linhai Qiu"
                    },
                    {
                        "name": "Marcella Valentine"
                    },
                    {
                        "name": "Marina Coelho"
                    },
                    {
                        "name": "Marvin Ritter"
                    },
                    {
                        "name": "Matt Hoffman"
                    },
                    {
                        "name": "Matthew Watson"
                    },
                    {
                        "name": "Mayank Chaturvedi"
                    },
                    {
                        "name": "Michael Moynihan"
                    },
                    {
                        "name": "Min Ma"
                    },
                    {
                        "name": "Nabila Babar"
                    },
                    {
                        "name": "Natasha Noy"
                    },
                    {
                        "name": "Nathan Byrd"
                    },
                    {
                        "name": "Nick Roy"
                    },
                    {
                        "name": "Nikola Momchev"
                    },
                    {
                        "name": "Nilay Chauhan"
                    },
                    {
                        "name": "Noveen Sachdeva"
                    },
                    {
                        "name": "Oskar Bunyan"
                    },
                    {
                        "name": "Pankil Botarda"
                    },
                    {
                        "name": "Paul Caron"
                    },
                    {
                        "name": "Paul Kishan Rubenstein"
                    },
                    {
                        "name": "Phil Culliton"
                    },
                    {
                        "name": "Philipp Schmid"
                    },
                    {
                        "name": "Pier Giuseppe Sessa"
                    },
                    {
                        "name": "Pingmei Xu"
                    },
                    {
                        "name": "Piotr Stanczyk"
                    },
                    {
                        "name": "Pouya Tafti"
                    },
                    {
                        "name": "Rakesh Shivanna"
                    },
                    {
                        "name": "Renjie Wu"
                    },
                    {
                        "name": "Renke Pan"
                    },
                    {
                        "name": "Reza Rokni"
                    },
                    {
                        "name": "Rob Willoughby"
                    },
                    {
                        "name": "Rohith Vallu"
                    },
                    {
                        "name": "Ryan Mullins"
                    },
                    {
                        "name": "Sammy Jerome"
                    },
                    {
                        "name": "Sara Smoot"
                    },
                    {
                        "name": "Sertan Girgin"
                    },
                    {
                        "name": "Shariq Iqbal"
                    },
                    {
                        "name": "Shashir Reddy"
                    },
                    {
                        "name": "Shruti Sheth"
                    },
                    {
                        "name": "Siim Põder"
                    },
                    {
                        "name": "Sijal Bhatnagar"
                    },
                    {
                        "name": "Sindhu Raghuram Panyam"
                    },
                    {
                        "name": "Sivan Eiger"
                    },
                    {
                        "name": "Susan Zhang"
                    },
                    {
                        "name": "Tianqi Liu"
                    },
                    {
                        "name": "Trevor Yacovone"
                    },
                    {
                        "name": "Tyler Liechty"
                    },
                    {
                        "name": "Uday Kalra"
                    },
                    {
                        "name": "Utku Evci"
                    },
                    {
                        "name": "Vedant Misra"
                    },
                    {
                        "name": "Vincent Roseberry"
                    },
                    {
                        "name": "Vlad Feinberg"
                    },
                    {
                        "name": "Vlad Kolesnikov"
                    },
                    {
                        "name": "Woohyun Han"
                    },
                    {
                        "name": "Woosuk Kwon"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Yinlam Chow"
                    },
                    {
                        "name": "Yuvein Zhu"
                    },
                    {
                        "name": "Zichuan Wei"
                    },
                    {
                        "name": "Zoltan Egyed"
                    },
                    {
                        "name": "Victor Cotruta"
                    },
                    {
                        "name": "Minh Giang"
                    },
                    {
                        "name": "Phoebe Kirk"
                    },
                    {
                        "name": "Anand Rao"
                    },
                    {
                        "name": "Kat Black"
                    },
                    {
                        "name": "Nabila Babar"
                    },
                    {
                        "name": "Jessica Lo"
                    },
                    {
                        "name": "Erica Moreira"
                    },
                    {
                        "name": "Luiz Gustavo Martins"
                    },
                    {
                        "name": "Omar Sanseviero"
                    },
                    {
                        "name": "Lucas Gonzalez"
                    },
                    {
                        "name": "Zach Gleicher"
                    },
                    {
                        "name": "Tris Warkentin"
                    },
                    {
                        "name": "Vahab Mirrokni"
                    },
                    {
                        "name": "Evan Senter"
                    },
                    {
                        "name": "Eli Collins"
                    },
                    {
                        "name": "Joelle Barral"
                    },
                    {
                        "name": "Zoubin Ghahramani"
                    },
                    {
                        "name": "Raia Hadsell"
                    },
                    {
                        "name": "Yossi Matias"
                    },
                    {
                        "name": "D. Sculley"
                    },
                    {
                        "name": "Slav Petrov"
                    },
                    {
                        "name": "Noah Fiedel"
                    },
                    {
                        "name": "Noam Shazeer"
                    },
                    {
                        "name": "Oriol Vinyals"
                    },
                    {
                        "name": "Jeff Dean"
                    },
                    {
                        "name": "Demis Hassabis"
                    },
                    {
                        "name": "Koray Kavukcuoglu"
                    },
                    {
                        "name": "Clement Farabet"
                    },
                    {
                        "name": "Elena Buchatskaya"
                    },
                    {
                        "name": "Jean-Baptiste Alayrac"
                    },
                    {
                        "name": "Rohan Anil"
                    },
                    {
                        "name": "Dmitry"
                    },
                    {
                        "name": "Lepikhin"
                    },
                    {
                        "name": "Sebastian Borgeaud"
                    },
                    {
                        "name": "Olivier Bachem"
                    },
                    {
                        "name": "Armand Joulin"
                    },
                    {
                        "name": "Alek Andreev"
                    },
                    {
                        "name": "Cassidy Hardin"
                    },
                    {
                        "name": "Robert Dadashi"
                    },
                    {
                        "name": "Léonard Hussenot"
                    }
                ],
                "author_detail": {
                    "name": "Léonard Hussenot"
                },
                "author": "Léonard Hussenot",
                "arxiv_affiliation": "Dima",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19390v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19390v1",
                "updated": "2025-03-25T06:45:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    6,
                    45,
                    13,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T06:45:13Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    6,
                    45,
                    13,
                    1,
                    84,
                    0
                ],
                "title": "Integrating Prefetcher Selection with Dynamic Request Allocation\n  Improves Prefetching Efficiency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Prefetcher Selection with Dynamic Request Allocation\n  Improves Prefetching Efficiency"
                },
                "summary": "Hardware prefetching plays a critical role in hiding the off-chip DRAM\nlatency. The complexity of applications results in a wide variety of memory\naccess patterns, prompting the development of numerous cache-prefetching\nalgorithms. Consequently, commercial processors often employ a hybrid of these\nalgorithms to enhance the overall prefetching performance. Nonetheless, since\nthese prefetchers share hardware resources, conflicts arising from competing\nprefetching requests can negate the benefits of hardware prefetching. Under\nsuch circumstances, several prefetcher selection algorithms have been proposed\nto mitigate conflicts between prefetchers. However, these prior solutions\nsuffer from two limitations. First, the input demand request allocation is\ninaccurate. Second, the prefetcher selection criteria are coarse-grained.\n  In this paper, we address both limitations by introducing an efficient and\nwidely applicable prefetcher selection algorithm--Alecto, which tailors the\ndemand requests for each prefetcher. Every demand request is first sent to\nAlecto to identify suitable prefetchers before being routed to prefetchers for\ntraining and prefetching. Our analysis shows that Alecto is adept at not only\nharmonizing prefetching accuracy, coverage, and timeliness but also\nsignificantly enhancing the utilization of the prefetcher table, which is vital\nfor temporal prefetching. Alecto outperforms the state-of-the-art RL-based\nprefetcher selection algorithm--Bandit by 2.76% in single-core, and 7.56% in\neight-core. For memory-intensive benchmarks, Alecto outperforms Bandit by\n5.25%. Alecto consistently delivers state-of-the-art performance in scheduling\nvarious types of cache prefetchers. In addition to the performance improvement,\nAlecto can reduce the energy consumption associated with accessing the\nprefetchers' table by 48%, while only adding less than 1 KB of storage\noverhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hardware prefetching plays a critical role in hiding the off-chip DRAM\nlatency. The complexity of applications results in a wide variety of memory\naccess patterns, prompting the development of numerous cache-prefetching\nalgorithms. Consequently, commercial processors often employ a hybrid of these\nalgorithms to enhance the overall prefetching performance. Nonetheless, since\nthese prefetchers share hardware resources, conflicts arising from competing\nprefetching requests can negate the benefits of hardware prefetching. Under\nsuch circumstances, several prefetcher selection algorithms have been proposed\nto mitigate conflicts between prefetchers. However, these prior solutions\nsuffer from two limitations. First, the input demand request allocation is\ninaccurate. Second, the prefetcher selection criteria are coarse-grained.\n  In this paper, we address both limitations by introducing an efficient and\nwidely applicable prefetcher selection algorithm--Alecto, which tailors the\ndemand requests for each prefetcher. Every demand request is first sent to\nAlecto to identify suitable prefetchers before being routed to prefetchers for\ntraining and prefetching. Our analysis shows that Alecto is adept at not only\nharmonizing prefetching accuracy, coverage, and timeliness but also\nsignificantly enhancing the utilization of the prefetcher table, which is vital\nfor temporal prefetching. Alecto outperforms the state-of-the-art RL-based\nprefetcher selection algorithm--Bandit by 2.76% in single-core, and 7.56% in\neight-core. For memory-intensive benchmarks, Alecto outperforms Bandit by\n5.25%. Alecto consistently delivers state-of-the-art performance in scheduling\nvarious types of cache prefetchers. In addition to the performance improvement,\nAlecto can reduce the energy consumption associated with accessing the\nprefetchers' table by 48%, while only adding less than 1 KB of storage\noverhead."
                },
                "authors": [
                    {
                        "name": "Mengming Li"
                    },
                    {
                        "name": "Qijun Zhang"
                    },
                    {
                        "name": "Yongqing Ren"
                    },
                    {
                        "name": "Zhiyao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyao Xie"
                },
                "author": "Zhiyao Xie",
                "arxiv_comment": "In 31th IEEE International Symposium on High-Performance Computer\n  Architecture (HPCA 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19390v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19390v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14882v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14882v2",
                "updated": "2025-03-24T23:47:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    23,
                    47,
                    51,
                    0,
                    83,
                    0
                ],
                "published": "2025-02-15T05:08:01Z",
                "published_parsed": [
                    2025,
                    2,
                    15,
                    5,
                    8,
                    1,
                    5,
                    46,
                    0
                ],
                "title": "CalibQuant: 1-Bit KV Cache Quantization for Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CalibQuant: 1-Bit KV Cache Quantization for Multimodal LLMs"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\nperformance across diverse applications. However, their computational overhead\nduring deployment remains a critical bottleneck. While Key-Value (KV) caching\neffectively trades memory for computation to enhance inference efficiency, the\ngrowing memory footprint from extensive KV caches significantly reduces\nthroughput and restricts prolonged deployment on memory-constrained GPU\ndevices. To address this challenge, we propose CalibQuant, a simple yet highly\neffective visual quantization strategy that drastically reduces both memory and\ncomputational overhead. Specifically, CalibQuant introduces an extreme 1-bit\nquantization scheme, complemented by novel post-scaling and calibration\ntechniques tailored to the intrinsic patterns of KV caches, thereby ensuring\nhigh efficiency without compromising model performance. Leveraging Triton for\nruntime optimization, we achieve a 10x throughput increase on InternVL models.\nOur method is designed to be plug-and-play, seamlessly integrating with various\nexisting MLLMs without requiring architectural changes. Extensive experiments\nconfirm that our approach significantly reduces memory usage while maintaining\ncomputational efficiency and preserving multimodal capabilities. Codes are\navailable at https://github.com/insuhan/calibquant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\nperformance across diverse applications. However, their computational overhead\nduring deployment remains a critical bottleneck. While Key-Value (KV) caching\neffectively trades memory for computation to enhance inference efficiency, the\ngrowing memory footprint from extensive KV caches significantly reduces\nthroughput and restricts prolonged deployment on memory-constrained GPU\ndevices. To address this challenge, we propose CalibQuant, a simple yet highly\neffective visual quantization strategy that drastically reduces both memory and\ncomputational overhead. Specifically, CalibQuant introduces an extreme 1-bit\nquantization scheme, complemented by novel post-scaling and calibration\ntechniques tailored to the intrinsic patterns of KV caches, thereby ensuring\nhigh efficiency without compromising model performance. Leveraging Triton for\nruntime optimization, we achieve a 10x throughput increase on InternVL models.\nOur method is designed to be plug-and-play, seamlessly integrating with various\nexisting MLLMs without requiring architectural changes. Extensive experiments\nconfirm that our approach significantly reduces memory usage while maintaining\ncomputational efficiency and preserving multimodal capabilities. Codes are\navailable at https://github.com/insuhan/calibquant."
                },
                "authors": [
                    {
                        "name": "Insu Han"
                    },
                    {
                        "name": "Zeliang Zhang"
                    },
                    {
                        "name": "Zhiyuan Wang"
                    },
                    {
                        "name": "Yifan Zhu"
                    },
                    {
                        "name": "Susan Liang"
                    },
                    {
                        "name": "Jiani Liu"
                    },
                    {
                        "name": "Haiting Lin"
                    },
                    {
                        "name": "Mingjie Zhao"
                    },
                    {
                        "name": "Chenliang Xu"
                    },
                    {
                        "name": "Kun Wan"
                    },
                    {
                        "name": "Wentian Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Wentian Zhao"
                },
                "author": "Wentian Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14882v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14882v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09859v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09859v2",
                "updated": "2025-03-24T21:27:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    21,
                    27,
                    53,
                    0,
                    83,
                    0
                ],
                "published": "2024-11-15T00:37:31Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    0,
                    37,
                    31,
                    4,
                    320,
                    0
                ],
                "title": "Skew-Symmetric Matrix Decompositions on Shared-Memory Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skew-Symmetric Matrix Decompositions on Shared-Memory Architectures"
                },
                "summary": "The factorization of skew-symmetric matrices is a critically understudied\narea of dense linear algebra, particularly in comparison to that of general and\nsymmetric matrices. While some algorithms can be adapted from the symmetric\ncase, the cost of algorithms can be reduced by exploiting skew-symmetry. This\nwork examines the factorization of a skew-symmetric matrix $X$ into its\n$LTL^\\mathrm{T}$ decomposition, where $L$ is unit lower triangular and $T$ is\ntridiagonal. This is also known as a triangular tridiagonalization. This\noperation is a means for computing the determinant of $X$ as the square of the\n(cheaply-computed) Pfaffian of the skew-symmetric tridiagonal matrix $T$ as\nwell as for solving systems of equations, across fields such as quantum\nelectronic structure and machine learning. Its application also often requires\npivoting in order to improve numerical stability. We compare and contrast\npreviously-published algorithms with those systematically derived using the\nFLAME methodology. Performant parallel CPU implementations are achieved by\nfusing operations at multiple levels in order to reduce memory traffic\noverhead. A key factor is the employment of new capabilities of the BLAS-like\nLibrary Instantiation Software (BLIS) framework, which now supports casting\nlevel-2 and level-3 BLAS-like operations by leveraging its gemm and other\nkernels, hierarchical parallelism, and cache blocking. A prototype, concise C++\nAPI facilitates the translation of correct-by-construction algorithms into\ncorrect code. Experiments verify that the resulting implementations greatly\nexceed the performance of previous work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The factorization of skew-symmetric matrices is a critically understudied\narea of dense linear algebra, particularly in comparison to that of general and\nsymmetric matrices. While some algorithms can be adapted from the symmetric\ncase, the cost of algorithms can be reduced by exploiting skew-symmetry. This\nwork examines the factorization of a skew-symmetric matrix $X$ into its\n$LTL^\\mathrm{T}$ decomposition, where $L$ is unit lower triangular and $T$ is\ntridiagonal. This is also known as a triangular tridiagonalization. This\noperation is a means for computing the determinant of $X$ as the square of the\n(cheaply-computed) Pfaffian of the skew-symmetric tridiagonal matrix $T$ as\nwell as for solving systems of equations, across fields such as quantum\nelectronic structure and machine learning. Its application also often requires\npivoting in order to improve numerical stability. We compare and contrast\npreviously-published algorithms with those systematically derived using the\nFLAME methodology. Performant parallel CPU implementations are achieved by\nfusing operations at multiple levels in order to reduce memory traffic\noverhead. A key factor is the employment of new capabilities of the BLAS-like\nLibrary Instantiation Software (BLIS) framework, which now supports casting\nlevel-2 and level-3 BLAS-like operations by leveraging its gemm and other\nkernels, hierarchical parallelism, and cache blocking. A prototype, concise C++\nAPI facilitates the translation of correct-by-construction algorithms into\ncorrect code. Experiments verify that the resulting implementations greatly\nexceed the performance of previous work."
                },
                "authors": [
                    {
                        "name": "Ishna Satyarth"
                    },
                    {
                        "name": "Chao Yin"
                    },
                    {
                        "name": "Devin A. Matthews"
                    },
                    {
                        "name": "Maggie Myers"
                    },
                    {
                        "name": "Robert van de Geijn"
                    },
                    {
                        "name": "RuQing G. Xu"
                    }
                ],
                "author_detail": {
                    "name": "RuQing G. Xu"
                },
                "author": "RuQing G. Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09859v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09859v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19145v1",
                "updated": "2025-03-24T21:00:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    21,
                    0,
                    37,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T21:00:37Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    21,
                    0,
                    37,
                    0,
                    83,
                    0
                ],
                "title": "Compositional Caching for Training-free Open-vocabulary Attribute\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compositional Caching for Training-free Open-vocabulary Attribute\n  Detection"
                },
                "summary": "Attribute detection is crucial for many computer vision tasks, as it enables\nsystems to describe properties such as color, texture, and material. Current\napproaches often rely on labor-intensive annotation processes which are\ninherently limited: objects can be described at an arbitrary level of detail\n(e.g., color vs. color shades), leading to ambiguities when the annotators are\nnot instructed carefully. Furthermore, they operate within a predefined set of\nattributes, reducing scalability and adaptability to unforeseen downstream\napplications. We present Compositional Caching (ComCa), a training-free method\nfor open-vocabulary attribute detection that overcomes these constraints. ComCa\nrequires only the list of target attributes and objects as input, using them to\npopulate an auxiliary cache of images by leveraging web-scale databases and\nLarge Language Models to determine attribute-object compatibility. To account\nfor the compositional nature of attributes, cache images receive soft attribute\nlabels. Those are aggregated at inference time based on the similarity between\nthe input and cache images, refining the predictions of underlying\nVision-Language Models (VLMs). Importantly, our approach is model-agnostic,\ncompatible with various VLMs. Experiments on public datasets demonstrate that\nComCa significantly outperforms zero-shot and cache-based baselines, competing\nwith recent training-based methods, proving that a carefully designed\ntraining-free approach can successfully address open-vocabulary attribute\ndetection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attribute detection is crucial for many computer vision tasks, as it enables\nsystems to describe properties such as color, texture, and material. Current\napproaches often rely on labor-intensive annotation processes which are\ninherently limited: objects can be described at an arbitrary level of detail\n(e.g., color vs. color shades), leading to ambiguities when the annotators are\nnot instructed carefully. Furthermore, they operate within a predefined set of\nattributes, reducing scalability and adaptability to unforeseen downstream\napplications. We present Compositional Caching (ComCa), a training-free method\nfor open-vocabulary attribute detection that overcomes these constraints. ComCa\nrequires only the list of target attributes and objects as input, using them to\npopulate an auxiliary cache of images by leveraging web-scale databases and\nLarge Language Models to determine attribute-object compatibility. To account\nfor the compositional nature of attributes, cache images receive soft attribute\nlabels. Those are aggregated at inference time based on the similarity between\nthe input and cache images, refining the predictions of underlying\nVision-Language Models (VLMs). Importantly, our approach is model-agnostic,\ncompatible with various VLMs. Experiments on public datasets demonstrate that\nComCa significantly outperforms zero-shot and cache-based baselines, competing\nwith recent training-based methods, proving that a carefully designed\ntraining-free approach can successfully address open-vocabulary attribute\ndetection."
                },
                "authors": [
                    {
                        "name": "Marco Garosi"
                    },
                    {
                        "name": "Alessandro Conti"
                    },
                    {
                        "name": "Gaowen Liu"
                    },
                    {
                        "name": "Elisa Ricci"
                    },
                    {
                        "name": "Massimiliano Mancini"
                    }
                ],
                "author_detail": {
                    "name": "Massimiliano Mancini"
                },
                "author": "Massimiliano Mancini",
                "arxiv_comment": "CVPR 2025. Project website at https://comca-attributes.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13773v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13773v2",
                "updated": "2025-03-24T18:50:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    18,
                    50,
                    9,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-17T23:38:29Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    23,
                    38,
                    29,
                    0,
                    76,
                    0
                ],
                "title": "Mitigating KV Cache Competition to Enhance User Experience in LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating KV Cache Competition to Enhance User Experience in LLM\n  Inference"
                },
                "summary": "In Large Language Model (LLM) serving, the KV-cache (KVC) bottleneck causes\nhigh tail Time-to-First-Token (TTFT) and Time-Between-Tokens (TBT), impairing\nuser experience, particularly in time-sensitive applications. However,\nsatisfying both TTFT and TBT service-level objectives (SLOs) is challenging. To\naddress this, we propose a system, named CacheOPT for mitigating KV Cache\ncompetition, based on key insights from our measurements, incorporating novel\ncomponents. First, it estimates a request's output length, bounding the\ndeviation with a high specified probability, adjusted based on the request\narrival rate. Second, it allocates the estimated KVC demand to a request, and\nreuses other requests' allocated KVC to avoid preemptions while reducing\nwaiting time. Third, it proactively allocates KVC before instead of at the time\na request exhausts its allocation and reserves KVC globally to prevent\npreemptions. Fourth, it chooses a request that has long TBT SLO, long job\nremaining time and short preemption time to preempt. Fifth, it selects the\nshortest-latency strategy between swapping and recomputation for preemptions.\nExperiments show that CacheOPT achieves up to 3.29$\\times$ and 2.83$\\times$\nlower tail TBT and tail TTFT, 47\\% and 53\\% higher TTFT and TBT SLO\nattainments, and supports up to 1.58$\\times$ higher request arrival rate than\nthe state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Large Language Model (LLM) serving, the KV-cache (KVC) bottleneck causes\nhigh tail Time-to-First-Token (TTFT) and Time-Between-Tokens (TBT), impairing\nuser experience, particularly in time-sensitive applications. However,\nsatisfying both TTFT and TBT service-level objectives (SLOs) is challenging. To\naddress this, we propose a system, named CacheOPT for mitigating KV Cache\ncompetition, based on key insights from our measurements, incorporating novel\ncomponents. First, it estimates a request's output length, bounding the\ndeviation with a high specified probability, adjusted based on the request\narrival rate. Second, it allocates the estimated KVC demand to a request, and\nreuses other requests' allocated KVC to avoid preemptions while reducing\nwaiting time. Third, it proactively allocates KVC before instead of at the time\na request exhausts its allocation and reserves KVC globally to prevent\npreemptions. Fourth, it chooses a request that has long TBT SLO, long job\nremaining time and short preemption time to preempt. Fifth, it selects the\nshortest-latency strategy between swapping and recomputation for preemptions.\nExperiments show that CacheOPT achieves up to 3.29$\\times$ and 2.83$\\times$\nlower tail TBT and tail TTFT, 47\\% and 53\\% higher TTFT and TBT SLO\nattainments, and supports up to 1.58$\\times$ higher request arrival rate than\nthe state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Tanmoy Sen"
                    },
                    {
                        "name": "Masahiro Tanaka"
                    }
                ],
                "author_detail": {
                    "name": "Masahiro Tanaka"
                },
                "author": "Masahiro Tanaka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13773v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13773v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06364v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06364v2",
                "updated": "2025-03-24T18:16:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    18,
                    16,
                    58,
                    0,
                    83,
                    0
                ],
                "published": "2024-11-10T05:12:51Z",
                "published_parsed": [
                    2024,
                    11,
                    10,
                    5,
                    12,
                    51,
                    6,
                    315,
                    0
                ],
                "title": "EconoServe: Maximizing Multi-Resource Utilization with SLO Guarantees in\n  LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EconoServe: Maximizing Multi-Resource Utilization with SLO Guarantees in\n  LLM Serving"
                },
                "summary": "As Large Language Models (LLMs) continue to grow, reducing costs and\nalleviating GPU demands has become increasingly critical. However, existing\nschedulers primarily target either GPU compute or Key-Value Cache (KVC)\nutilization, failing to fully optimize both GPU compute and KVC usage during\neach iteration or guarantee timely KVC allocations when needed. To address\nthese challenges, we conducted a trace-based experimental analysis and made\ninsightful observations, leading to the design of a system called EconoServe.\nEconoServe maximizes multi-resource utilization while ensuring service-level\nobjective (SLO) guarantees in LLM serving. To enable adding prompts to a batch\nto maximize GPU utilization in each iteration, EconoServe maintains separate\nwaiting queues for prompt processing tasks (PTs) and generation tasks (GTs). It\nbatches GTs with the same predicted response lengths (RL) to save scheduling\ntime and allocates KVC space for the predicted RL to avoid KVC allocation\nfailures. It further has a novel KVC pipelining method, allowing sharing\nallocated but unused KVC space to enhance KVC utilization. In addition, it\nprioritizes queued requests that occupy more KVC to release KVC earlier and\nsatisfy request service-level-objective (SLO). Experimental results demonstrate\nthat EconoServe increases throughput by up to 4$\\times$ with the same level of\nlatency, generates up to 91\\% lower job completion time and up to 91\\% higher\nSLO satisfaction ratio compared to vLLM. It also reduces the number of GPUs\nused in DistServe by up to 78\\% while maintaining the same level of goodput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) continue to grow, reducing costs and\nalleviating GPU demands has become increasingly critical. However, existing\nschedulers primarily target either GPU compute or Key-Value Cache (KVC)\nutilization, failing to fully optimize both GPU compute and KVC usage during\neach iteration or guarantee timely KVC allocations when needed. To address\nthese challenges, we conducted a trace-based experimental analysis and made\ninsightful observations, leading to the design of a system called EconoServe.\nEconoServe maximizes multi-resource utilization while ensuring service-level\nobjective (SLO) guarantees in LLM serving. To enable adding prompts to a batch\nto maximize GPU utilization in each iteration, EconoServe maintains separate\nwaiting queues for prompt processing tasks (PTs) and generation tasks (GTs). It\nbatches GTs with the same predicted response lengths (RL) to save scheduling\ntime and allocates KVC space for the predicted RL to avoid KVC allocation\nfailures. It further has a novel KVC pipelining method, allowing sharing\nallocated but unused KVC space to enhance KVC utilization. In addition, it\nprioritizes queued requests that occupy more KVC to release KVC earlier and\nsatisfy request service-level-objective (SLO). Experimental results demonstrate\nthat EconoServe increases throughput by up to 4$\\times$ with the same level of\nlatency, generates up to 91\\% lower job completion time and up to 91\\% higher\nSLO satisfaction ratio compared to vLLM. It also reduces the number of GPUs\nused in DistServe by up to 78\\% while maintaining the same level of goodput."
                },
                "authors": [
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Tanmoy Sen"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Sen"
                },
                "author": "Tanmoy Sen",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06364v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06364v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18893v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18893v1",
                "updated": "2025-03-24T17:06:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    6,
                    37,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T17:06:37Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    6,
                    37,
                    0,
                    83,
                    0
                ],
                "title": "xKV: Cross-Layer SVD for KV-Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xKV: Cross-Layer SVD for KV-Cache Compression"
                },
                "summary": "Large Language Models (LLMs) with long context windows enable powerful\napplications but come at the cost of high memory consumption to store the Key\nand Value states (KV-Cache). Recent studies attempted to merge KV-cache from\nmultiple layers into shared representations, yet these approaches either\nrequire expensive pretraining or rely on assumptions of high per-token cosine\nsimilarity across layers which generally does not hold in practice. We find\nthat the dominant singular vectors are remarkably well-aligned across multiple\nlayers of the KV-Cache. Exploiting this insight, we propose xKV, a simple\npost-training method that applies Singular Value Decomposition (SVD) on the\nKV-Cache of grouped layers. xKV consolidates the KV-Cache of multiple layers\ninto a shared low-rank subspace, significantly reducing KV-Cache sizes. Through\nextensive evaluations on the RULER long-context benchmark with widely-used LLMs\n(e.g., Llama-3.1 and Qwen2.5), xKV achieves up to 6.8x higher compression rates\nthan state-of-the-art inter-layer technique while improving accuracy by 2.7%.\nMoreover, xKV is compatible with the emerging Multi-Head Latent Attention (MLA)\n(e.g., DeepSeek-Coder-V2), yielding a notable 3x compression rates on coding\ntasks without performance degradation. These results highlight xKV's strong\ncapability and versatility in addressing memory bottlenecks for long-context\nLLM inference. Our code is publicly available at:\nhttps://github.com/abdelfattah-lab/xKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) with long context windows enable powerful\napplications but come at the cost of high memory consumption to store the Key\nand Value states (KV-Cache). Recent studies attempted to merge KV-cache from\nmultiple layers into shared representations, yet these approaches either\nrequire expensive pretraining or rely on assumptions of high per-token cosine\nsimilarity across layers which generally does not hold in practice. We find\nthat the dominant singular vectors are remarkably well-aligned across multiple\nlayers of the KV-Cache. Exploiting this insight, we propose xKV, a simple\npost-training method that applies Singular Value Decomposition (SVD) on the\nKV-Cache of grouped layers. xKV consolidates the KV-Cache of multiple layers\ninto a shared low-rank subspace, significantly reducing KV-Cache sizes. Through\nextensive evaluations on the RULER long-context benchmark with widely-used LLMs\n(e.g., Llama-3.1 and Qwen2.5), xKV achieves up to 6.8x higher compression rates\nthan state-of-the-art inter-layer technique while improving accuracy by 2.7%.\nMoreover, xKV is compatible with the emerging Multi-Head Latent Attention (MLA)\n(e.g., DeepSeek-Coder-V2), yielding a notable 3x compression rates on coding\ntasks without performance degradation. These results highlight xKV's strong\ncapability and versatility in addressing memory bottlenecks for long-context\nLLM inference. Our code is publicly available at:\nhttps://github.com/abdelfattah-lab/xKV."
                },
                "authors": [
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Wei-Cheng Lin"
                    },
                    {
                        "name": "Kai-Chiang Wu"
                    },
                    {
                        "name": "Luis Ceze"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed S. Abdelfattah"
                },
                "author": "Mohamed S. Abdelfattah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18893v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18893v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13064v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13064v2",
                "updated": "2025-03-24T16:47:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    47,
                    48,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-17T11:10:49Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    11,
                    10,
                    49,
                    0,
                    76,
                    0
                ],
                "title": "HERMES: High-Performance RISC-V Memory Hierarchy for ML Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HERMES: High-Performance RISC-V Memory Hierarchy for ML Workloads"
                },
                "summary": "The growth of machine learning (ML) workloads has underscored the importance\nof efficient memory hierarchies to address bandwidth, latency, and scalability\nchallenges. HERMES focuses on optimizing memory subsystems for RISC-V\narchitectures to meet the computational needs of ML models such as CNNs, RNNs,\nand Transformers. This project explores state-of-the-art techniques such as\nadvanced prefetching, tensor-aware caching, and hybrid memory models. The\ncornerstone of HERMES is the integration of shared L3 caches with fine-grained\ncoherence protocols equipped with specialized pathways to deep-learning\naccelerators such as Gemmini. Simulation tools like Gem5 and DRAMSim2 were used\nto evaluate baseline performance and scalability under representative ML\nworkloads. The findings of this study highlight the design choices, and the\nanticipated challenges, paving the way for low-latency scalable memory\noperations for ML applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growth of machine learning (ML) workloads has underscored the importance\nof efficient memory hierarchies to address bandwidth, latency, and scalability\nchallenges. HERMES focuses on optimizing memory subsystems for RISC-V\narchitectures to meet the computational needs of ML models such as CNNs, RNNs,\nand Transformers. This project explores state-of-the-art techniques such as\nadvanced prefetching, tensor-aware caching, and hybrid memory models. The\ncornerstone of HERMES is the integration of shared L3 caches with fine-grained\ncoherence protocols equipped with specialized pathways to deep-learning\naccelerators such as Gemmini. Simulation tools like Gem5 and DRAMSim2 were used\nto evaluate baseline performance and scalability under representative ML\nworkloads. The findings of this study highlight the design choices, and the\nanticipated challenges, paving the way for low-latency scalable memory\noperations for ML applications."
                },
                "authors": [
                    {
                        "name": "Pranav Suryadevara"
                    }
                ],
                "author_detail": {
                    "name": "Pranav Suryadevara"
                },
                "author": "Pranav Suryadevara",
                "arxiv_comment": "5 pages, 5 figures. Individual Project",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13064v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13064v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.3.2; C.1.3; C.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18862v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18862v1",
                "updated": "2025-03-24T16:38:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    38,
                    31,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T16:38:31Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    38,
                    31,
                    0,
                    83,
                    0
                ],
                "title": "Exploring the Integration of Key-Value Attention Into Pure and Hybrid\n  Transformers for Semantic Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Integration of Key-Value Attention Into Pure and Hybrid\n  Transformers for Semantic Segmentation"
                },
                "summary": "While CNNs were long considered state of the art for image processing, the\nintroduction of Transformer architectures has challenged this position. While\nachieving excellent results in image classification and segmentation,\nTransformers remain inherently reliant on large training datasets and remain\ncomputationally expensive. A newly introduced Transformer derivative named KV\nTransformer shows promising results in synthetic, NLP, and image classification\ntasks, while reducing complexity and memory usage. This is especially conducive\nto use cases where local inference is required, such as medical screening\napplications. We endeavoured to further evaluate the merit of KV Transformers\non semantic segmentation tasks, specifically in the domain of medical imaging.\nBy directly comparing traditional and KV variants of the same base\narchitectures, we provide further insight into the practical tradeoffs of\nreduced model complexity. We observe a notable reduction in parameter count and\nmultiply accumulate operations, while achieving similar performance from most\nof the KV variant models when directly compared to their QKV implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While CNNs were long considered state of the art for image processing, the\nintroduction of Transformer architectures has challenged this position. While\nachieving excellent results in image classification and segmentation,\nTransformers remain inherently reliant on large training datasets and remain\ncomputationally expensive. A newly introduced Transformer derivative named KV\nTransformer shows promising results in synthetic, NLP, and image classification\ntasks, while reducing complexity and memory usage. This is especially conducive\nto use cases where local inference is required, such as medical screening\napplications. We endeavoured to further evaluate the merit of KV Transformers\non semantic segmentation tasks, specifically in the domain of medical imaging.\nBy directly comparing traditional and KV variants of the same base\narchitectures, we provide further insight into the practical tradeoffs of\nreduced model complexity. We observe a notable reduction in parameter count and\nmultiply accumulate operations, while achieving similar performance from most\nof the KV variant models when directly compared to their QKV implementation."
                },
                "authors": [
                    {
                        "name": "DeShin Hwa"
                    },
                    {
                        "name": "Tobias Holmes"
                    },
                    {
                        "name": "Klaus Drechsler"
                    }
                ],
                "author_detail": {
                    "name": "Klaus Drechsler"
                },
                "author": "Klaus Drechsler",
                "arxiv_doi": "10.1007/978-3-658-47422-5_71",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-658-47422-5_71",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.18862v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18862v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "6 pages, 3 figures, Preprint. Final version published in:\n  Bildverarbeitung f\\\"ur die Medizin 2025, Springer. DOI:\n  https://doi.org/10.1007/978-3-658-47422-5_71",
                "arxiv_journal_ref": "Bildverarbeitung f\\\"ur die Medizin 2025. BVM 2025. Informatik\n  aktuell. Springer Vieweg, Wiesbaden, pp 305-310",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18773v1",
                "updated": "2025-03-24T15:22:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    22,
                    41,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T15:22:41Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    22,
                    41,
                    0,
                    83,
                    0
                ],
                "title": "BitDecoding: Unlocking Tensor Cores for Long-Context LLMs Decoding with\n  Low-Bit KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitDecoding: Unlocking Tensor Cores for Long-Context LLMs Decoding with\n  Low-Bit KV Cache"
                },
                "summary": "The growing adoption of long-context Large Language Models (LLMs) has\nintroduced significant memory and computational challenges in autoregressive\ndecoding due to the expanding Key-Value (KV) cache. KV cache quantization has\nemerged as a promising solution, with prior work showing that 4-bit or even\n2-bit quantization can maintain model accuracy while reducing memory costs.\nHowever, despite these benefits, preliminary implementations for the low-bit KV\ncache struggle to deliver the expected speedup due to quantization and\ndequantization overheads and the lack of Tensor Cores utilization. In this\nwork, we propose BitDecoding, a GPU-optimized framework that unlocks Tensor\nCores for efficient decoding with low-bit KV cache. Efficiently leveraging\nTensor Cores for low-bit KV cache is challenging due to the dynamic nature of\nKV cache generation at each decoding step. BitDecoding addresses these\nchallenges with a Tensor Cores-Centric BitFusion Scheme that ensures data\nlayout compatibility to enable high utilization of Tensor Cores. Additionally,\nBitDecoding incorporates a warp-efficient parallel decoding kernel and a\nfine-grained asynchronous pipeline, minimizing dequantization overhead and\nimproving computational efficiency. Experiments show that BitDecoding achieves\nup to 7.5x speedup on RTX 4090, 4.8x on A100, and 8.9x on H100, compared to\nFP16 FlashDecoding-v2. It also outperforms the state-of-the-art low-bit KV\ncache implementation (QServe) by up to 4.3x. On LLaMA-3.1-8B with a 128K\nsequence length, BitDecoding reduces single-batch decoding latency by 3x,\ndemonstrating its effectiveness in long-context generation scenarios. The code\nis available at https://github.com/DD-DuDa/BitDecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing adoption of long-context Large Language Models (LLMs) has\nintroduced significant memory and computational challenges in autoregressive\ndecoding due to the expanding Key-Value (KV) cache. KV cache quantization has\nemerged as a promising solution, with prior work showing that 4-bit or even\n2-bit quantization can maintain model accuracy while reducing memory costs.\nHowever, despite these benefits, preliminary implementations for the low-bit KV\ncache struggle to deliver the expected speedup due to quantization and\ndequantization overheads and the lack of Tensor Cores utilization. In this\nwork, we propose BitDecoding, a GPU-optimized framework that unlocks Tensor\nCores for efficient decoding with low-bit KV cache. Efficiently leveraging\nTensor Cores for low-bit KV cache is challenging due to the dynamic nature of\nKV cache generation at each decoding step. BitDecoding addresses these\nchallenges with a Tensor Cores-Centric BitFusion Scheme that ensures data\nlayout compatibility to enable high utilization of Tensor Cores. Additionally,\nBitDecoding incorporates a warp-efficient parallel decoding kernel and a\nfine-grained asynchronous pipeline, minimizing dequantization overhead and\nimproving computational efficiency. Experiments show that BitDecoding achieves\nup to 7.5x speedup on RTX 4090, 4.8x on A100, and 8.9x on H100, compared to\nFP16 FlashDecoding-v2. It also outperforms the state-of-the-art low-bit KV\ncache implementation (QServe) by up to 4.3x. On LLaMA-3.1-8B with a 128K\nsequence length, BitDecoding reduces single-batch decoding latency by 3x,\ndemonstrating its effectiveness in long-context generation scenarios. The code\nis available at https://github.com/DD-DuDa/BitDecoding."
                },
                "authors": [
                    {
                        "name": "Dayou Du"
                    },
                    {
                        "name": "Shijie Cao"
                    },
                    {
                        "name": "Jianyi Cheng"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05941v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05941v2",
                "updated": "2025-03-24T13:09:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    13,
                    9,
                    3,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-07T21:16:41Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    21,
                    16,
                    41,
                    4,
                    66,
                    0
                ],
                "title": "Choosing Augmentation Parameters in OSQP- A New Approach based on\n  Conjugate Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Choosing Augmentation Parameters in OSQP- A New Approach based on\n  Conjugate Directions"
                },
                "summary": "This work proposes a new method to select the augmentation parameters in the\noperator splitting quadratic program (OSQP) algorithm so as to reduce the\ncomputation time of overall algorithm. The selection is based upon the\ninformation of conjugate directions of the coefficient matrix of a linear\nsystem of equations present in the algorithm. This selection makes it possible\nto cache these conjugate directions, instead of computing them at each\niteration, resulting in faster computation of the solution of the linear system\nthus reducing the overall computation time. This reduction is demonstrated by a\nnumerical example.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work proposes a new method to select the augmentation parameters in the\noperator splitting quadratic program (OSQP) algorithm so as to reduce the\ncomputation time of overall algorithm. The selection is based upon the\ninformation of conjugate directions of the coefficient matrix of a linear\nsystem of equations present in the algorithm. This selection makes it possible\nto cache these conjugate directions, instead of computing them at each\niteration, resulting in faster computation of the solution of the linear system\nthus reducing the overall computation time. This reduction is demonstrated by a\nnumerical example."
                },
                "authors": [
                    {
                        "name": "Avinash Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Avinash Kumar"
                },
                "author": "Avinash Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05941v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05941v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18599v1",
                "updated": "2025-03-24T11:56:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    56,
                    50,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T11:56:50Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    56,
                    50,
                    0,
                    83,
                    0
                ],
                "title": "Oaken: Fast and Efficient LLM Serving with Online-Offline Hybrid KV\n  Cache Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oaken: Fast and Efficient LLM Serving with Online-Offline Hybrid KV\n  Cache Quantization"
                },
                "summary": "Modern Large Language Model serving system batches multiple requests to\nachieve high throughput, while batching attention operations is challenging,\nrendering memory bandwidth a critical bottleneck. The community relies on\nhigh-end GPUs with multiple high-bandwidth memory channels. Unfortunately,\nHBM's high bandwidth often comes at the expense of limited memory capacity,\nwhich reduces core utilization and increases costs. Recent advancements\nenabling longer contexts for LLMs have substantially increased the key-value\ncache size, further intensifying the pressures on memory capacity. The\nliterature has explored KV cache quantization techniques, which commonly use\nlow bitwidth for most values, selectively using higher bitwidth for outlier\nvalues. While this approach helps achieve high accuracy and low bitwidth\nsimultaneously, it comes with the limitation that cost for online outlier\ndetection is excessively high, negating the advantages. We propose Oaken, an\nacceleration solution that achieves high accuracy and high performance\nsimultaneously through co-designing algorithm and hardware. To effectively find\na sweet spot in the accuracy-performance trade-off space of KV cache\nquantization, Oaken employs an online-offline hybrid approach, setting outlier\nthresholds offline, which are then used to determine the quantization scale\nonline. To translate the proposed algorithmic technique into tangible\nperformance gains, Oaken also comes with custom quantization engines and memory\nmanagement units that can be integrated with any LLM accelerators. We built an\nOaken accelerator on top of an LLM accelerator, LPU, and conducted a\ncomprehensive evaluation. Our experiments show that for a batch size of 256,\nOaken achieves up to 1.58x throughput improvement over NVIDIA A100 GPU,\nincurring a minimal accuracy loss of only 0.54\\% on average, compared to\nstate-of-the-art KV cache quantization techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Large Language Model serving system batches multiple requests to\nachieve high throughput, while batching attention operations is challenging,\nrendering memory bandwidth a critical bottleneck. The community relies on\nhigh-end GPUs with multiple high-bandwidth memory channels. Unfortunately,\nHBM's high bandwidth often comes at the expense of limited memory capacity,\nwhich reduces core utilization and increases costs. Recent advancements\nenabling longer contexts for LLMs have substantially increased the key-value\ncache size, further intensifying the pressures on memory capacity. The\nliterature has explored KV cache quantization techniques, which commonly use\nlow bitwidth for most values, selectively using higher bitwidth for outlier\nvalues. While this approach helps achieve high accuracy and low bitwidth\nsimultaneously, it comes with the limitation that cost for online outlier\ndetection is excessively high, negating the advantages. We propose Oaken, an\nacceleration solution that achieves high accuracy and high performance\nsimultaneously through co-designing algorithm and hardware. To effectively find\na sweet spot in the accuracy-performance trade-off space of KV cache\nquantization, Oaken employs an online-offline hybrid approach, setting outlier\nthresholds offline, which are then used to determine the quantization scale\nonline. To translate the proposed algorithmic technique into tangible\nperformance gains, Oaken also comes with custom quantization engines and memory\nmanagement units that can be integrated with any LLM accelerators. We built an\nOaken accelerator on top of an LLM accelerator, LPU, and conducted a\ncomprehensive evaluation. Our experiments show that for a batch size of 256,\nOaken achieves up to 1.58x throughput improvement over NVIDIA A100 GPU,\nincurring a minimal accuracy loss of only 0.54\\% on average, compared to\nstate-of-the-art KV cache quantization techniques."
                },
                "authors": [
                    {
                        "name": "Minsu Kim"
                    },
                    {
                        "name": "Seongmin Hong"
                    },
                    {
                        "name": "RyeoWook Ko"
                    },
                    {
                        "name": "Soongyu Choi"
                    },
                    {
                        "name": "Hunjong Lee"
                    },
                    {
                        "name": "Junsoo Kim"
                    },
                    {
                        "name": "Joo-Young Kim"
                    },
                    {
                        "name": "Jongse Park"
                    }
                ],
                "author_detail": {
                    "name": "Jongse Park"
                },
                "author": "Jongse Park",
                "arxiv_comment": "15 pages, 14 figures, and 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17333v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17333v2",
                "updated": "2025-03-24T11:00:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    0,
                    35,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-21T17:33:03Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    17,
                    33,
                    3,
                    4,
                    80,
                    0
                ],
                "title": "Register Dispersion: Reducing the Footprint of the Vector Register File\n  in Vector Engines of Low-Cost RISC-V CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Register Dispersion: Reducing the Footprint of the Vector Register File\n  in Vector Engines of Low-Cost RISC-V CPUs"
                },
                "summary": "The deployment of Machine Learning (ML) applications at the edge on\nresource-constrained devices has accentuated the need for efficient ML\nprocessing on low-cost processors. While traditional CPUs provide programming\nflexibility, their general-purpose architecture often lacks the throughput\nrequired for complex ML models. The augmentation of a RISC-V processor with a\nvector unit can provide substantial data-level parallelism. However, increasing\nthe data-level parallelism supported by vector processing would make the Vector\nRegister File (VRF) a major area consumer in ultra low-cost processors, since\n32 vector registers are required for RISC-V Vector ISA compliance. This work\nleverages the insight that many ML vectorized kernels require a small number of\nactive vector registers, and proposes the use of a physically smaller VRF that\ndynamically caches only the vector registers currently accessed by the\napplication. This approach, called Register Dispersion, maps the architectural\nvector registers to a smaller set of physical registers. The proposed\nISA-compliant VRF is significantly smaller than a full-size VRF and operates\nlike a conventional cache, i.e., it only stores the most recently accessed\nvector registers. Essential registers remain readily accessible within the\ncompact VRF, while the others are offloaded to the cache/memory sub-system. The\ncompact VRF design is demonstrated to yield substantial area and power savings,\nas compared to using a full VRF, with no or minimal impact on performance. This\neffective trade-off renders the inclusion of vector units in low-cost\nprocessors feasible and practical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of Machine Learning (ML) applications at the edge on\nresource-constrained devices has accentuated the need for efficient ML\nprocessing on low-cost processors. While traditional CPUs provide programming\nflexibility, their general-purpose architecture often lacks the throughput\nrequired for complex ML models. The augmentation of a RISC-V processor with a\nvector unit can provide substantial data-level parallelism. However, increasing\nthe data-level parallelism supported by vector processing would make the Vector\nRegister File (VRF) a major area consumer in ultra low-cost processors, since\n32 vector registers are required for RISC-V Vector ISA compliance. This work\nleverages the insight that many ML vectorized kernels require a small number of\nactive vector registers, and proposes the use of a physically smaller VRF that\ndynamically caches only the vector registers currently accessed by the\napplication. This approach, called Register Dispersion, maps the architectural\nvector registers to a smaller set of physical registers. The proposed\nISA-compliant VRF is significantly smaller than a full-size VRF and operates\nlike a conventional cache, i.e., it only stores the most recently accessed\nvector registers. Essential registers remain readily accessible within the\ncompact VRF, while the others are offloaded to the cache/memory sub-system. The\ncompact VRF design is demonstrated to yield substantial area and power savings,\nas compared to using a full VRF, with no or minimal impact on performance. This\neffective trade-off renders the inclusion of vector units in low-cost\nprocessors feasible and practical."
                },
                "authors": [
                    {
                        "name": "Vasileios Titopoulos"
                    },
                    {
                        "name": "George Alexakis"
                    },
                    {
                        "name": "Kosmas Alexandridis"
                    },
                    {
                        "name": "Chrysostomos Nicopoulos"
                    },
                    {
                        "name": "Giorgos Dimitrakopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Giorgos Dimitrakopoulos"
                },
                "author": "Giorgos Dimitrakopoulos",
                "arxiv_comment": "22nd ACM International Conference on Computing Frontiers (CF' 25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17333v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17333v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16653v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16653v2",
                "updated": "2025-03-24T03:18:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    3,
                    18,
                    49,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-20T19:10:37Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    19,
                    10,
                    37,
                    3,
                    79,
                    0
                ],
                "title": "iFlame: Interleaving Full and Linear Attention for Efficient Mesh\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "iFlame: Interleaving Full and Linear Attention for Efficient Mesh\n  Generation"
                },
                "summary": "This paper propose iFlame, a novel transformer-based network architecture for\nmesh generation. While attention-based models have demonstrated remarkable\nperformance in mesh generation, their quadratic computational complexity limits\nscalability, particularly for high-resolution 3D data. Conversely, linear\nattention mechanisms offer lower computational costs but often struggle to\ncapture long-range dependencies, resulting in suboptimal outcomes. To address\nthis trade-off, we propose an interleaving autoregressive mesh generation\nframework that combines the efficiency of linear attention with the expressive\npower of full attention mechanisms. To further enhance efficiency and leverage\nthe inherent structure of mesh representations, we integrate this interleaving\napproach into an hourglass architecture, which significantly boosts efficiency.\nOur approach reduces training time while achieving performance comparable to\npure attention-based models. To improve inference efficiency, we implemented a\ncaching algorithm that almost doubles the speed and reduces the KV cache size\nby seven-eighths compared to the original Transformer. We evaluate our\nframework on ShapeNet and Objaverse, demonstrating its ability to generate\nhigh-quality 3D meshes efficiently. Our results indicate that the proposed\ninterleaving framework effectively balances computational efficiency and\ngenerative performance, making it a practical solution for mesh generation. The\ntraining takes only 2 days with 4 GPUs on 39k data with a maximum of 4k faces\non Objaverse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper propose iFlame, a novel transformer-based network architecture for\nmesh generation. While attention-based models have demonstrated remarkable\nperformance in mesh generation, their quadratic computational complexity limits\nscalability, particularly for high-resolution 3D data. Conversely, linear\nattention mechanisms offer lower computational costs but often struggle to\ncapture long-range dependencies, resulting in suboptimal outcomes. To address\nthis trade-off, we propose an interleaving autoregressive mesh generation\nframework that combines the efficiency of linear attention with the expressive\npower of full attention mechanisms. To further enhance efficiency and leverage\nthe inherent structure of mesh representations, we integrate this interleaving\napproach into an hourglass architecture, which significantly boosts efficiency.\nOur approach reduces training time while achieving performance comparable to\npure attention-based models. To improve inference efficiency, we implemented a\ncaching algorithm that almost doubles the speed and reduces the KV cache size\nby seven-eighths compared to the original Transformer. We evaluate our\nframework on ShapeNet and Objaverse, demonstrating its ability to generate\nhigh-quality 3D meshes efficiently. Our results indicate that the proposed\ninterleaving framework effectively balances computational efficiency and\ngenerative performance, making it a practical solution for mesh generation. The\ntraining takes only 2 days with 4 GPUs on 39k data with a maximum of 4k faces\non Objaverse."
                },
                "authors": [
                    {
                        "name": "Hanxiao Wang"
                    },
                    {
                        "name": "Biao Zhang"
                    },
                    {
                        "name": "Weize Quan"
                    },
                    {
                        "name": "Dong-Ming Yan"
                    },
                    {
                        "name": "Peter Wonka"
                    }
                ],
                "author_detail": {
                    "name": "Peter Wonka"
                },
                "author": "Peter Wonka",
                "arxiv_comment": "Project website: https://wanghanxiao123.github.io/iFa/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16653v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16653v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18292v1",
                "updated": "2025-03-24T02:28:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    2,
                    28,
                    4,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T02:28:04Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    2,
                    28,
                    4,
                    0,
                    83,
                    0
                ],
                "title": "Jenga: Effective Memory Management for Serving LLM with Heterogeneity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jenga: Effective Memory Management for Serving LLM with Heterogeneity"
                },
                "summary": "Large language models (LLMs) are widely used but expensive to run, especially\nas inference workloads grow. To lower costs, maximizing the request batch size\nby managing GPU memory efficiently is crucial. While PagedAttention has\nrecently been proposed to improve the efficiency of memory management, we find\nthat the growing heterogeneity in the embeddings dimensions, attention, and\naccess patterns of modern LLM architectures introduces new challenges for\nmemory allocation.\n  In this paper, we present Jenga, a novel memory allocation framework for\nheterogeneous embeddings in LLMs. Jenga tackles two key challenges: (1)\nminimizing memory fragmentation when managing embeddings of different sizes,\nand (2) enabling flexible caching and eviction policies tailored to the\nspecific token-dependency patterns of various layers. Jenga employs a two-level\nmemory allocator, leveraging the least common multiple (LCM) of embedding sizes\nto optimize memory usage and providing APIs to express layer-specific caching\nlogic to enhance memory reuse.\n  We implemente Jenga on vLLM, a state-of-the-art LLM inference engine, and\nevaluate it with diverse LLMs, datasets, and GPU configurations. Evaluations\nshow that Jenga improves GPU memory utilization by up to 79.6%, and increases\nserving throughput by up to 4.92x (1.80x on average).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are widely used but expensive to run, especially\nas inference workloads grow. To lower costs, maximizing the request batch size\nby managing GPU memory efficiently is crucial. While PagedAttention has\nrecently been proposed to improve the efficiency of memory management, we find\nthat the growing heterogeneity in the embeddings dimensions, attention, and\naccess patterns of modern LLM architectures introduces new challenges for\nmemory allocation.\n  In this paper, we present Jenga, a novel memory allocation framework for\nheterogeneous embeddings in LLMs. Jenga tackles two key challenges: (1)\nminimizing memory fragmentation when managing embeddings of different sizes,\nand (2) enabling flexible caching and eviction policies tailored to the\nspecific token-dependency patterns of various layers. Jenga employs a two-level\nmemory allocator, leveraging the least common multiple (LCM) of embedding sizes\nto optimize memory usage and providing APIs to express layer-specific caching\nlogic to enhance memory reuse.\n  We implemente Jenga on vLLM, a state-of-the-art LLM inference engine, and\nevaluate it with diverse LLMs, datasets, and GPU configurations. Evaluations\nshow that Jenga improves GPU memory utilization by up to 79.6%, and increases\nserving throughput by up to 4.92x (1.80x on average)."
                },
                "authors": [
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Woosuk Kwon"
                    },
                    {
                        "name": "Xiangxi Mo"
                    },
                    {
                        "name": "Yufeng Wang"
                    },
                    {
                        "name": "Xiaoxuan Liu"
                    },
                    {
                        "name": "Kaichao You"
                    },
                    {
                        "name": "Zhuohan Li"
                    },
                    {
                        "name": "Mingsheng Long"
                    },
                    {
                        "name": "Jidong Zhai"
                    },
                    {
                        "name": "Joseph Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "arxiv_comment": "16 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20504v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20504v5",
                "updated": "2025-03-24T02:17:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    2,
                    17,
                    34,
                    0,
                    83,
                    0
                ],
                "published": "2024-12-29T15:42:24Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    15,
                    42,
                    24,
                    6,
                    364,
                    0
                ],
                "title": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding"
                },
                "summary": "Video Large Language Models (VideoLLMs) have made significant strides in\nvideo understanding but struggle with long videos due to the limitations of\ntheir backbone LLMs. Existing solutions rely on length extrapolation, which is\nmemory-constrained, or visual token compression, which primarily leverages\nlow-level temporal redundancy while overlooking the more effective high-level\nknowledge redundancy. To address this, we propose $\\textbf{ReTaKe}$, a\ntraining-free method with two novel modules DPSelect and PivotKV, to jointly\nreduce both temporal visual redundancy and knowledge redundancy for video\ncompression. To align with the way of human temporal perception, DPSelect\nidentifies keyframes based on inter-frame distance peaks. To leverage LLMs'\nlearned prior knowledge, PivotKV marks the keyframes as pivots and compress\nnon-pivot frames by pruning low-attention tokens in their KV cache. ReTaKe\nenables VideoLLMs to process 8 times longer frames (up to 2048), outperforming\nsimilar-sized models by 3-5% and even rivaling much larger ones on VideoMME,\nMLVU, LongVideoBench, and LVBench. Moreover, by overlapping compression\noperations with prefilling, ReTaKe introduces only ~10% prefilling latency\noverhead while reducing decoding latency by ~20%. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Large Language Models (VideoLLMs) have made significant strides in\nvideo understanding but struggle with long videos due to the limitations of\ntheir backbone LLMs. Existing solutions rely on length extrapolation, which is\nmemory-constrained, or visual token compression, which primarily leverages\nlow-level temporal redundancy while overlooking the more effective high-level\nknowledge redundancy. To address this, we propose $\\textbf{ReTaKe}$, a\ntraining-free method with two novel modules DPSelect and PivotKV, to jointly\nreduce both temporal visual redundancy and knowledge redundancy for video\ncompression. To align with the way of human temporal perception, DPSelect\nidentifies keyframes based on inter-frame distance peaks. To leverage LLMs'\nlearned prior knowledge, PivotKV marks the keyframes as pivots and compress\nnon-pivot frames by pruning low-attention tokens in their KV cache. ReTaKe\nenables VideoLLMs to process 8 times longer frames (up to 2048), outperforming\nsimilar-sized models by 3-5% and even rivaling much larger ones on VideoMME,\nMLVU, LongVideoBench, and LVBench. Moreover, by overlapping compression\noperations with prefilling, ReTaKe introduces only ~10% prefilling latency\noverhead while reducing decoding latency by ~20%. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe."
                },
                "authors": [
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Qingyi Si"
                    },
                    {
                        "name": "Jianlong Wu"
                    },
                    {
                        "name": "Shiyu Zhu"
                    },
                    {
                        "name": "Li Cao"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "arxiv_comment": "Rewrite the methods section. Add more ablation studies and results in\n  LongVideoBench. Update metadata",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20504v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20504v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18265v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18265v1",
                "updated": "2025-03-24T01:15:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    1,
                    15,
                    43,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T01:15:43Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    1,
                    15,
                    43,
                    0,
                    83,
                    0
                ],
                "title": "Risk Management for Distributed Arbitrage Systems: Integrating\n  Artificial Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Risk Management for Distributed Arbitrage Systems: Integrating\n  Artificial Intelligence"
                },
                "summary": "Effective risk management solutions become absolutely crucial when financial\nmarkets embrace distributed technology and decentralized financing (DeFi). This\nstudy offers a thorough survey and comparative analysis of the integration of\nartificial intelligence (AI) in risk management for distributed arbitrage\nsystems. We examine several modern caching techniques namely in memory caching,\ndistributed caching, and proxy caching and their functions in enhancing\nperformance in decentralized settings. Through literature review we examine the\nutilization of AI techniques for alleviating risks related to market\nvolatility, liquidity challenges, operational failures, regulatory compliance,\nand security threats. This comparison research evaluates various case studies\nfrom prominent DeFi technologies, emphasizing critical performance metrics like\nlatency reduction, load balancing, and system resilience. Additionally, we\nexamine the problems and trade offs associated with these technologies,\nemphasizing their effects on consistency, scalability, and fault tolerance. By\nmeticulously analyzing real world applications, specifically centering on the\nAave platform as our principal case study, we illustrate how the purposeful\namalgamation of AI with contemporary caching methodologies has revolutionized\nrisk management in distributed arbitrage systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective risk management solutions become absolutely crucial when financial\nmarkets embrace distributed technology and decentralized financing (DeFi). This\nstudy offers a thorough survey and comparative analysis of the integration of\nartificial intelligence (AI) in risk management for distributed arbitrage\nsystems. We examine several modern caching techniques namely in memory caching,\ndistributed caching, and proxy caching and their functions in enhancing\nperformance in decentralized settings. Through literature review we examine the\nutilization of AI techniques for alleviating risks related to market\nvolatility, liquidity challenges, operational failures, regulatory compliance,\nand security threats. This comparison research evaluates various case studies\nfrom prominent DeFi technologies, emphasizing critical performance metrics like\nlatency reduction, load balancing, and system resilience. Additionally, we\nexamine the problems and trade offs associated with these technologies,\nemphasizing their effects on consistency, scalability, and fault tolerance. By\nmeticulously analyzing real world applications, specifically centering on the\nAave platform as our principal case study, we illustrate how the purposeful\namalgamation of AI with contemporary caching methodologies has revolutionized\nrisk management in distributed arbitrage systems."
                },
                "authors": [
                    {
                        "name": "Akaash Vishal Hazarika"
                    },
                    {
                        "name": "Mahak Shah"
                    },
                    {
                        "name": "Swapnil Patil"
                    },
                    {
                        "name": "Pradyumna Shukla"
                    }
                ],
                "author_detail": {
                    "name": "Pradyumna Shukla"
                },
                "author": "Pradyumna Shukla",
                "arxiv_comment": "International Conference on AI and Financial Innovation AIFI-2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18265v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18265v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11; G.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18191v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18191v1",
                "updated": "2025-03-23T20:18:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    23,
                    20,
                    18,
                    16,
                    6,
                    82,
                    0
                ],
                "published": "2025-03-23T20:18:16Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    20,
                    18,
                    16,
                    6,
                    82,
                    0
                ],
                "title": "Enabling the Write-Back Page Cache with Strong Consistency in\n  Distributed Userspace File Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling the Write-Back Page Cache with Strong Consistency in\n  Distributed Userspace File Systems"
                },
                "summary": "The large-scale, multi-tenant nature of cloud computing requires distributed\nfile systems that offer stability, adaptability, and compatibility. FUSE-based\ndistributed file systems have emerged as a popular solution for the cloud,\noffering fast deployment, fault isolation, and POSIX compliance. However,\nFUSE's performance limitations, particularly its inability to reconcile page\ncaching with strong consistency in distributed environments, remain a\npersistent problem. Existing approaches either sacrifice consistency for\nperformance or rely on inefficient caching, limiting their practicality.\n  To this end, we present DistFUSE, the first FUSE-based distributed file\nsystem that relies on a write-back kernel-based page cache for performance and\nprovides strong consistency. DistFUSE achieves this by offloading userspace\nlock management to the kernel driver, allowing coordinated access to the\nkernel's page cache across nodes. This design eliminates blind local cache\nupdates and ensures cluster-wide consistency without compromising performance.\nOur evaluation shows DistFUSE improves throughput by up to 75% compared to\nbaseline approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The large-scale, multi-tenant nature of cloud computing requires distributed\nfile systems that offer stability, adaptability, and compatibility. FUSE-based\ndistributed file systems have emerged as a popular solution for the cloud,\noffering fast deployment, fault isolation, and POSIX compliance. However,\nFUSE's performance limitations, particularly its inability to reconcile page\ncaching with strong consistency in distributed environments, remain a\npersistent problem. Existing approaches either sacrifice consistency for\nperformance or rely on inefficient caching, limiting their practicality.\n  To this end, we present DistFUSE, the first FUSE-based distributed file\nsystem that relies on a write-back kernel-based page cache for performance and\nprovides strong consistency. DistFUSE achieves this by offloading userspace\nlock management to the kernel driver, allowing coordinated access to the\nkernel's page cache across nodes. This design eliminates blind local cache\nupdates and ensures cluster-wide consistency without compromising performance.\nOur evaluation shows DistFUSE improves throughput by up to 75% compared to\nbaseline approaches."
                },
                "authors": [
                    {
                        "name": "Haoyu Li"
                    },
                    {
                        "name": "Jingkai Fu"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Windsor Hsu"
                    },
                    {
                        "name": "Asaf Cidon"
                    }
                ],
                "author_detail": {
                    "name": "Asaf Cidon"
                },
                "author": "Asaf Cidon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18191v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18191v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18030v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18030v1",
                "updated": "2025-03-23T11:07:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    23,
                    11,
                    7,
                    24,
                    6,
                    82,
                    0
                ],
                "published": "2025-03-23T11:07:24Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    11,
                    7,
                    24,
                    6,
                    82,
                    0
                ],
                "title": "Formal Verification of Parameterized Systems based on Induction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formal Verification of Parameterized Systems based on Induction"
                },
                "summary": "Parameterized systems play a crucial role in the computer field, and their\nsecurity is of great significance. Formal verification of parameterized\nprotocols is especially challenging due to its \"parameterized\" feature, which\nbrings complexity and undecidability. Existing automated parameterized\nverification methods have limitations, such as facing difficulties in\nautomatically deriving parameterized invariants constrained by mixed Forall and\nExists quantifiers, or having challenges in completing the parameterized\nverification of large and complex protocols. This paper proposes a formal\nverification framework for parameterized systems based on induction, named\nwiseParaverifier. It starts from small concretizations of protocols, analyzes\ninductive counterexamples, and constructs counterexample formulas to guide the\nentire process of parameterized verification. It also presents a heuristic\nGeneralize method to quickly find auxiliary invariants, a method for promoting\ncomplex mixed quantifiers and merging parameterized invariants, and uses\nsymmetric reduction ideas to accelerate the verification process. Experimental\nresults show that wiseParaverifier can successfully complete automatic\ninductive verification on 7 cache coherence protocols and 10 distributed\nprotocols. It has strong verification capabilities and migration capabilities,\nand can provide concise and readable verification results, which is helpful for\nlearners to understand protocol behaviors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameterized systems play a crucial role in the computer field, and their\nsecurity is of great significance. Formal verification of parameterized\nprotocols is especially challenging due to its \"parameterized\" feature, which\nbrings complexity and undecidability. Existing automated parameterized\nverification methods have limitations, such as facing difficulties in\nautomatically deriving parameterized invariants constrained by mixed Forall and\nExists quantifiers, or having challenges in completing the parameterized\nverification of large and complex protocols. This paper proposes a formal\nverification framework for parameterized systems based on induction, named\nwiseParaverifier. It starts from small concretizations of protocols, analyzes\ninductive counterexamples, and constructs counterexample formulas to guide the\nentire process of parameterized verification. It also presents a heuristic\nGeneralize method to quickly find auxiliary invariants, a method for promoting\ncomplex mixed quantifiers and merging parameterized invariants, and uses\nsymmetric reduction ideas to accelerate the verification process. Experimental\nresults show that wiseParaverifier can successfully complete automatic\ninductive verification on 7 cache coherence protocols and 10 distributed\nprotocols. It has strong verification capabilities and migration capabilities,\nand can provide concise and readable verification results, which is helpful for\nlearners to understand protocol behaviors."
                },
                "authors": [
                    {
                        "name": "Jiaqi Xiu"
                    },
                    {
                        "name": "Yongjian Li"
                    }
                ],
                "author_detail": {
                    "name": "Yongjian Li"
                },
                "author": "Yongjian Li",
                "arxiv_comment": "9 pages,2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18030v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18030v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.10425v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.10425v2",
                "updated": "2025-03-23T06:14:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    23,
                    6,
                    14,
                    35,
                    6,
                    82,
                    0
                ],
                "published": "2023-12-16T11:40:49Z",
                "published_parsed": [
                    2023,
                    12,
                    16,
                    11,
                    40,
                    49,
                    5,
                    350,
                    0
                ],
                "title": "Knowledge Rumination for Client Utility Evaluation in Heterogeneous\n  Federated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Rumination for Client Utility Evaluation in Heterogeneous\n  Federated Learning"
                },
                "summary": "Federated Learning (FL) allows several clients to cooperatively train machine\nlearning models without disclosing the raw data. In practical applications,\nasynchronous FL (AFL) can address the straggler effect compared to synchronous\nFL. However, Non-IID data and stale models pose significant challenges to AFL,\nas they can diminish the practicality of the global model and even lead to\ntraining failures. In this work, we propose a novel AFL framework called\nFederated Historical Learning (FedHist), which effectively addresses the\nchallenges posed by both Non-IID data and gradient staleness based on the\nconcept of knowledge rumination. FedHist enhances the stability of local\ngradients by performing weighted fusion with historical global gradients cached\non the server. Relying on hindsight, it assigns aggregation weights to each\nparticipant in a multi-dimensional manner during each communication round. To\nfurther enhance the efficiency and stability of the training process, we\nintroduce an intelligent $\\ell_2$-norm amplification scheme, which dynamically\nregulates the learning progress based on the $\\ell_2$-norms of the submitted\ngradients. Extensive experiments indicate FedHist outperforms state-of-the-art\nmethods in terms of convergence performance and test accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) allows several clients to cooperatively train machine\nlearning models without disclosing the raw data. In practical applications,\nasynchronous FL (AFL) can address the straggler effect compared to synchronous\nFL. However, Non-IID data and stale models pose significant challenges to AFL,\nas they can diminish the practicality of the global model and even lead to\ntraining failures. In this work, we propose a novel AFL framework called\nFederated Historical Learning (FedHist), which effectively addresses the\nchallenges posed by both Non-IID data and gradient staleness based on the\nconcept of knowledge rumination. FedHist enhances the stability of local\ngradients by performing weighted fusion with historical global gradients cached\non the server. Relying on hindsight, it assigns aggregation weights to each\nparticipant in a multi-dimensional manner during each communication round. To\nfurther enhance the efficiency and stability of the training process, we\nintroduce an intelligent $\\ell_2$-norm amplification scheme, which dynamically\nregulates the learning progress based on the $\\ell_2$-norms of the submitted\ngradients. Extensive experiments indicate FedHist outperforms state-of-the-art\nmethods in terms of convergence performance and test accuracy."
                },
                "authors": [
                    {
                        "name": "Xiaorui Jiang"
                    },
                    {
                        "name": "Yu Gao"
                    },
                    {
                        "name": "Hengwei Xu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Yong Liao"
                    },
                    {
                        "name": "Pengyuan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Pengyuan Zhou"
                },
                "author": "Pengyuan Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.10425v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.10425v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17913v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17913v1",
                "updated": "2025-03-23T03:20:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    20,
                    25,
                    6,
                    82,
                    0
                ],
                "published": "2025-03-23T03:20:25Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    20,
                    25,
                    6,
                    82,
                    0
                ],
                "title": "Cache-Aware Cooperative Multicast Beamforming in Dynamic\n  Satellite-Terrestrial Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Aware Cooperative Multicast Beamforming in Dynamic\n  Satellite-Terrestrial Networks"
                },
                "summary": "With the burgeoning demand for data-intensive services, satellite-terrestrial\nnetworks (STNs) face increasing backhaul link congestion, deteriorating user\nquality of service (QoS), and escalating power consumption. Cache-aided STNs\nare acknowledged as a promising paradigm for accelerating content delivery to\nusers and alleviating the load of backhaul links. However, the dynamic nature\nof low earth orbit (LEO) satellites and the complex interference among\nsatellite beams and terrestrial base stations pose challenges in effectively\nmanaging limited edge resources. To address these issues, this paper proposes a\nmethod for dynamically scheduling caching and communication resources, aiming\nto reduce network costs in terms of transmission power consumption and backhaul\ntraffic, while meeting user QoS demands and resource constraints. We formulate\na mixed timescale problem to jointly optimize cache placement, LEO satellite\nbeam direction, and cooperative multicast beamforming among satellite beams and\nbase stations. To tackle this intricate problem, we propose a two-stage\nsolution framework, where the primary problem is decoupled into a short-term\ncontent delivery subproblem and a long-term cache placement subproblem. The\nformer subproblem is solved by designing an alternating optimization approach\nwith whale optimization and successive convex approximation methods according\nto the cache placement state, while cache content in STNs is updated using an\niterative algorithm that utilizes historical information. Simulation results\ndemonstrate the effectiveness of our proposed algorithms, showcasing their\nconvergence and significantly reducing transmission power consumption and\nbackhaul traffic by up to 52%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the burgeoning demand for data-intensive services, satellite-terrestrial\nnetworks (STNs) face increasing backhaul link congestion, deteriorating user\nquality of service (QoS), and escalating power consumption. Cache-aided STNs\nare acknowledged as a promising paradigm for accelerating content delivery to\nusers and alleviating the load of backhaul links. However, the dynamic nature\nof low earth orbit (LEO) satellites and the complex interference among\nsatellite beams and terrestrial base stations pose challenges in effectively\nmanaging limited edge resources. To address these issues, this paper proposes a\nmethod for dynamically scheduling caching and communication resources, aiming\nto reduce network costs in terms of transmission power consumption and backhaul\ntraffic, while meeting user QoS demands and resource constraints. We formulate\na mixed timescale problem to jointly optimize cache placement, LEO satellite\nbeam direction, and cooperative multicast beamforming among satellite beams and\nbase stations. To tackle this intricate problem, we propose a two-stage\nsolution framework, where the primary problem is decoupled into a short-term\ncontent delivery subproblem and a long-term cache placement subproblem. The\nformer subproblem is solved by designing an alternating optimization approach\nwith whale optimization and successive convex approximation methods according\nto the cache placement state, while cache content in STNs is updated using an\niterative algorithm that utilizes historical information. Simulation results\ndemonstrate the effectiveness of our proposed algorithms, showcasing their\nconvergence and significantly reducing transmission power consumption and\nbackhaul traffic by up to 52%."
                },
                "authors": [
                    {
                        "name": "Shuo Yuan"
                    },
                    {
                        "name": "Yaohua Sun"
                    },
                    {
                        "name": "Mugen Peng"
                    }
                ],
                "author_detail": {
                    "name": "Mugen Peng"
                },
                "author": "Mugen Peng",
                "arxiv_doi": "10.1109/TVT.2024.3463548",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TVT.2024.3463548",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.17913v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17913v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by IEEE Transactions on Vehicular Technology",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17911v1",
                "updated": "2025-03-23T03:16:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    16,
                    50,
                    6,
                    82,
                    0
                ],
                "published": "2025-03-23T03:16:50Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    16,
                    50,
                    6,
                    82,
                    0
                ],
                "title": "VSAG: An Optimized Search Framework for Graph-based Approximate Nearest\n  Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VSAG: An Optimized Search Framework for Graph-based Approximate Nearest\n  Neighbor Search"
                },
                "summary": "Approximate nearest neighbor search (ANNS) is a fundamental problem in vector\ndatabases and AI infrastructures. Recent graph-based ANNS algorithms have\nachieved high search accuracy with practical efficiency. Despite the\nadvancements, these algorithms still face performance bottlenecks in\nproduction, due to the random memory access patterns of graph-based search and\nthe high computational overheads of vector distance. In addition, the\nperformance of a graph-based ANNS algorithm is highly sensitive to parameters,\nwhile selecting the optimal parameters is cost-prohibitive, e.g., manual tuning\nrequires repeatedly re-building the index.\n  This paper introduces VSAG, an open-source framework that aims to enhance the\nin production performance of graph-based ANNS algorithms. VSAG has been\ndeployed at scale in the services of Ant Group, and it incorporates three key\noptimizations: (i) efficient memory access: it reduces L3 cache misses with\npre-fetching and cache-friendly vector organization; (ii) automated parameter\ntuning: it automatically selects performance-optimal parameters without\nrequiring index rebuilding; (iii) efficient distance computation: it leverages\nmodern hardware, scalar quantization, and smartly switches to low-precision\nrepresentation to dramatically reduce the distance computation costs. We\nevaluate VSAG on real-world datasets. The experimental results show that VSAG\nachieves the state-of-the-art performance and provides up to 4x speedup over\nHNSWlib (an industry-standard library) while ensuring the same accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate nearest neighbor search (ANNS) is a fundamental problem in vector\ndatabases and AI infrastructures. Recent graph-based ANNS algorithms have\nachieved high search accuracy with practical efficiency. Despite the\nadvancements, these algorithms still face performance bottlenecks in\nproduction, due to the random memory access patterns of graph-based search and\nthe high computational overheads of vector distance. In addition, the\nperformance of a graph-based ANNS algorithm is highly sensitive to parameters,\nwhile selecting the optimal parameters is cost-prohibitive, e.g., manual tuning\nrequires repeatedly re-building the index.\n  This paper introduces VSAG, an open-source framework that aims to enhance the\nin production performance of graph-based ANNS algorithms. VSAG has been\ndeployed at scale in the services of Ant Group, and it incorporates three key\noptimizations: (i) efficient memory access: it reduces L3 cache misses with\npre-fetching and cache-friendly vector organization; (ii) automated parameter\ntuning: it automatically selects performance-optimal parameters without\nrequiring index rebuilding; (iii) efficient distance computation: it leverages\nmodern hardware, scalar quantization, and smartly switches to low-precision\nrepresentation to dramatically reduce the distance computation costs. We\nevaluate VSAG on real-world datasets. The experimental results show that VSAG\nachieves the state-of-the-art performance and provides up to 4x speedup over\nHNSWlib (an industry-standard library) while ensuring the same accuracy."
                },
                "authors": [
                    {
                        "name": "Xiaoyao Zhong"
                    },
                    {
                        "name": "Haotian Li"
                    },
                    {
                        "name": "Jiabao Jin"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Deming Chu"
                    },
                    {
                        "name": "Xiangyu Wang"
                    },
                    {
                        "name": "Zhitao Shen"
                    },
                    {
                        "name": "Wei Jia"
                    },
                    {
                        "name": "George Gu"
                    },
                    {
                        "name": "Yi Xie"
                    },
                    {
                        "name": "Xuemin Lin"
                    },
                    {
                        "name": "Heng Tao Shen"
                    },
                    {
                        "name": "Jingkuan Song"
                    },
                    {
                        "name": "Peng Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Peng Cheng"
                },
                "author": "Peng Cheng",
                "arxiv_comment": "16 pages, the report of open-source library VSAG\n  (https://github.com/antgroup/vsag)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17895v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17895v1",
                "updated": "2025-03-23T01:17:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    23,
                    1,
                    17,
                    8,
                    6,
                    82,
                    0
                ],
                "published": "2025-03-23T01:17:08Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    1,
                    17,
                    8,
                    6,
                    82,
                    0
                ],
                "title": "Orientation-Dependent \\b{eta}-Ga2O3 Heterojunction Diode with Atomic\n  Layer Deposition (ALD) Grown NiO",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Orientation-Dependent \\b{eta}-Ga2O3 Heterojunction Diode with Atomic\n  Layer Deposition (ALD) Grown NiO"
                },
                "summary": "This work reports the demonstration of ALD-deposited NiO/\\b{eta}-Ga2O3\nheterojunction diodes (HJDs) on low doped drift layer and highly doped (001) &\n(100) n+ substrates with experimental observation of a parallel-plane junction\nelectric field as high as 7.5 MV/cm, revealing a crystal orientation dependence\nin \\b{eta}-Ga2O3. We use a novel metalorganic precursor\nbis(1,4-di-tert-butyl-1,3-diazadienyl) (nickel Ni(tBu2DAD)2) with ozone (O3) to\ndeposit NiO. The NiO/\\b{eta}-Ga2O3 HJD on 7.7 {\\mu}m-thick HVPE-grown drift\nregion exhibited an on-state current density of ~20 A/cm2 at 5 V, ~10-8 A/cm2\nreverse leakage at low reverse bias(-5 V), and a rectifying ratio(Jon/Joff) of\n~109. The HJD broke down at ~2.2 kV reverse bias, corresponding to a ~3.4 MV/cm\nparallel-plane junction electric field, with a noise floor reverse leakage\n(10-8~10-6 A/cm2, nA) at 80% of the device catastrophic breakdown voltage. The\nNiO/\\b{eta}-Ga2O3 HJDs on n+ (001) & (100) highly-doped substrates exhibited\nbreakdown voltages at 12.5-16.0 V and 28.5-70.5 V, respectively, with extracted\ncritical electric field (EC) at 2.30-2.76 MV/cm, and 4.33-7.50 MV/cm, revealing\na substrate crystal orientation dependence on breakdown electric field for\n\\b{eta}-Ga2O3. The 7.5 MV/cm EC reported here is one of the highest\nparallel-plane junction electric fields reported in literature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work reports the demonstration of ALD-deposited NiO/\\b{eta}-Ga2O3\nheterojunction diodes (HJDs) on low doped drift layer and highly doped (001) &\n(100) n+ substrates with experimental observation of a parallel-plane junction\nelectric field as high as 7.5 MV/cm, revealing a crystal orientation dependence\nin \\b{eta}-Ga2O3. We use a novel metalorganic precursor\nbis(1,4-di-tert-butyl-1,3-diazadienyl) (nickel Ni(tBu2DAD)2) with ozone (O3) to\ndeposit NiO. The NiO/\\b{eta}-Ga2O3 HJD on 7.7 {\\mu}m-thick HVPE-grown drift\nregion exhibited an on-state current density of ~20 A/cm2 at 5 V, ~10-8 A/cm2\nreverse leakage at low reverse bias(-5 V), and a rectifying ratio(Jon/Joff) of\n~109. The HJD broke down at ~2.2 kV reverse bias, corresponding to a ~3.4 MV/cm\nparallel-plane junction electric field, with a noise floor reverse leakage\n(10-8~10-6 A/cm2, nA) at 80% of the device catastrophic breakdown voltage. The\nNiO/\\b{eta}-Ga2O3 HJDs on n+ (001) & (100) highly-doped substrates exhibited\nbreakdown voltages at 12.5-16.0 V and 28.5-70.5 V, respectively, with extracted\ncritical electric field (EC) at 2.30-2.76 MV/cm, and 4.33-7.50 MV/cm, revealing\na substrate crystal orientation dependence on breakdown electric field for\n\\b{eta}-Ga2O3. The 7.5 MV/cm EC reported here is one of the highest\nparallel-plane junction electric fields reported in literature."
                },
                "authors": [
                    {
                        "name": "Yizheng Liu"
                    },
                    {
                        "name": "Shane M. W. Witsell"
                    },
                    {
                        "name": "John F. Conley"
                    },
                    {
                        "name": "Sriram Krishnamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Sriram Krishnamoorthy"
                },
                "author": "Sriram Krishnamoorthy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17895v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17895v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17603v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17603v1",
                "updated": "2025-03-22T01:17:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    22,
                    1,
                    17,
                    56,
                    5,
                    81,
                    0
                ],
                "published": "2025-03-22T01:17:56Z",
                "published_parsed": [
                    2025,
                    3,
                    22,
                    1,
                    17,
                    56,
                    5,
                    81,
                    0
                ],
                "title": "A Generative Caching System for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Generative Caching System for Large Language Models"
                },
                "summary": "Caching has the potential to be of significant benefit for accessing large\nlanguage models (LLMs) due to their high latencies which typically range from a\nsmall number of seconds to well over a minute. Furthermore, many LLMs charge\nmoney for queries; caching thus has a clear monetary benefit. This paper\npresents a new caching system for improving user experiences with LLMs. In\naddition to reducing both latencies and monetary costs for accessing LLMs, our\nsystem also provides important features that go beyond the performance benefits\ntypically associated with caches. A key feature we provide is generative\ncaching, wherein multiple cached responses can be synthesized to provide\nanswers to queries which have never been seen before. Our generative caches\nfunction as repositories of valuable information which can be mined and\nanalyzed. We also improve upon past semantic caching techniques by tailoring\nthe caching algorithms to optimally balance cost and latency reduction with the\nquality of responses provided. Performance tests indicate that our caches are\nconsiderably faster than GPTcache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching has the potential to be of significant benefit for accessing large\nlanguage models (LLMs) due to their high latencies which typically range from a\nsmall number of seconds to well over a minute. Furthermore, many LLMs charge\nmoney for queries; caching thus has a clear monetary benefit. This paper\npresents a new caching system for improving user experiences with LLMs. In\naddition to reducing both latencies and monetary costs for accessing LLMs, our\nsystem also provides important features that go beyond the performance benefits\ntypically associated with caches. A key feature we provide is generative\ncaching, wherein multiple cached responses can be synthesized to provide\nanswers to queries which have never been seen before. Our generative caches\nfunction as repositories of valuable information which can be mined and\nanalyzed. We also improve upon past semantic caching techniques by tailoring\nthe caching algorithms to optimally balance cost and latency reduction with the\nquality of responses provided. Performance tests indicate that our caches are\nconsiderably faster than GPTcache."
                },
                "authors": [
                    {
                        "name": "Arun Iyengar"
                    },
                    {
                        "name": "Ashish Kundu"
                    },
                    {
                        "name": "Ramana Kompella"
                    },
                    {
                        "name": "Sai Nandan Mamidi"
                    }
                ],
                "author_detail": {
                    "name": "Sai Nandan Mamidi"
                },
                "author": "Sai Nandan Mamidi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17603v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17603v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17602v1",
                "updated": "2025-03-22T01:16:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    22,
                    1,
                    16,
                    24,
                    5,
                    81,
                    0
                ],
                "published": "2025-03-22T01:16:24Z",
                "published_parsed": [
                    2025,
                    3,
                    22,
                    1,
                    16,
                    24,
                    5,
                    81,
                    0
                ],
                "title": "Multiport Support for Vortex OpenGPU Memory Hierarchy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiport Support for Vortex OpenGPU Memory Hierarchy"
                },
                "summary": "Modern day applications have grown in size and require more computational\npower. The rise of machine learning and AI increased the need for parallel\ncomputation, which has increased the need for GPGPUs. With the increasing\ndemand for computational power, GPGPUs' SIMT architecture has solved this with\nan increase in the number of threads and the number of cores in a GPU,\nincreasing the throughput of these processors to match the demand of the\napplications. However, this created a larger demand for the memory, making the\nmemory bandwidth a bottleneck. The introduction of High-Bandwidth Memory (HBM)\nwith its increased number of memory ports offers a potential solution for the\nGPU to exploit its memory parallelism to increase the memory bandwidth.\nHowever, effectively leveraging HBM's memory parallelism to maximize bandwidth\npresents a unique and complex challenge for GPU architectures on how to\ndistribute those ports among the streaming multiprocessors in the GPGPU. In\nthis work, we extend the Vortex OpenGPU microarchitecture to incorporate a\nmultiport memory hierarchy, spanning from the L1 cache to the last-level cache\n(LLC). In addition, we propose various arbitration strategies to optimize\nmemory transfers across the cache hierarchy. The results have shown that an\nincrease in memory ports increases IPC, achieving an average speedup of 2.34x\nwith 8 memory ports in the tested configuration while showing relatively small\narea overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern day applications have grown in size and require more computational\npower. The rise of machine learning and AI increased the need for parallel\ncomputation, which has increased the need for GPGPUs. With the increasing\ndemand for computational power, GPGPUs' SIMT architecture has solved this with\nan increase in the number of threads and the number of cores in a GPU,\nincreasing the throughput of these processors to match the demand of the\napplications. However, this created a larger demand for the memory, making the\nmemory bandwidth a bottleneck. The introduction of High-Bandwidth Memory (HBM)\nwith its increased number of memory ports offers a potential solution for the\nGPU to exploit its memory parallelism to increase the memory bandwidth.\nHowever, effectively leveraging HBM's memory parallelism to maximize bandwidth\npresents a unique and complex challenge for GPU architectures on how to\ndistribute those ports among the streaming multiprocessors in the GPGPU. In\nthis work, we extend the Vortex OpenGPU microarchitecture to incorporate a\nmultiport memory hierarchy, spanning from the L1 cache to the last-level cache\n(LLC). In addition, we propose various arbitration strategies to optimize\nmemory transfers across the cache hierarchy. The results have shown that an\nincrease in memory ports increases IPC, achieving an average speedup of 2.34x\nwith 8 memory ports in the tested configuration while showing relatively small\narea overhead."
                },
                "authors": [
                    {
                        "name": "Injae Shin"
                    },
                    {
                        "name": "Blaise Tine"
                    }
                ],
                "author_detail": {
                    "name": "Blaise Tine"
                },
                "author": "Blaise Tine",
                "arxiv_comment": "OSSMPIC2025, 1st workshop on Open Source Solutions for Massively\n  Parallel Integrated Circuits",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07578v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07578v2",
                "updated": "2025-03-21T21:10:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    21,
                    10,
                    2,
                    4,
                    80,
                    0
                ],
                "published": "2025-02-11T14:25:20Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    25,
                    20,
                    1,
                    42,
                    0
                ],
                "title": "PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language\n  Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language\n  Model Inference"
                },
                "summary": "Large Language Model (LLM) inference uses an autoregressive manner to\ngenerate one token at a time, which exhibits notably lower operational\nintensity compared to earlier Machine Learning (ML) models such as encoder-only\ntransformers and Convolutional Neural Networks. At the same time, LLMs possess\nlarge parameter sizes and use key-value caches to store context information.\nModern LLMs support context windows with up to 1 million tokens to generate\nversatile text, audio, and video content. A large key-value cache unique to\neach prompt requires a large memory capacity, limiting the inference batch\nsize. Both low operational intensity and limited batch size necessitate a high\nmemory bandwidth. However, contemporary hardware systems for ML model\ndeployment, such as GPUs and TPUs, are primarily optimized for compute\nthroughput. This mismatch challenges the efficient deployment of advanced LLMs\nand makes users pay for expensive compute resources that are poorly utilized\nfor the memory-bound LLM inference tasks.\n  We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which\nharnesses CXL memory expansion capabilities to accommodate substantial LLM\nsizes, and utilizes near-bank processing units to deliver high memory\nbandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable\nCXL network to support peer-to-peer and collective communication primitives\nacross CXL devices. We implement various parallelism strategies to distribute\nLLMs across these devices. Compared to GPU baselines with maximum supported\nbatch sizes and similar average power, CENT achieves 2.3$\\times$ higher\nthroughput and consumes 2.3$\\times$ less energy. CENT enhances the Total Cost\nof Ownership (TCO), generating 5.2$\\times$ more tokens per dollar than GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference uses an autoregressive manner to\ngenerate one token at a time, which exhibits notably lower operational\nintensity compared to earlier Machine Learning (ML) models such as encoder-only\ntransformers and Convolutional Neural Networks. At the same time, LLMs possess\nlarge parameter sizes and use key-value caches to store context information.\nModern LLMs support context windows with up to 1 million tokens to generate\nversatile text, audio, and video content. A large key-value cache unique to\neach prompt requires a large memory capacity, limiting the inference batch\nsize. Both low operational intensity and limited batch size necessitate a high\nmemory bandwidth. However, contemporary hardware systems for ML model\ndeployment, such as GPUs and TPUs, are primarily optimized for compute\nthroughput. This mismatch challenges the efficient deployment of advanced LLMs\nand makes users pay for expensive compute resources that are poorly utilized\nfor the memory-bound LLM inference tasks.\n  We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which\nharnesses CXL memory expansion capabilities to accommodate substantial LLM\nsizes, and utilizes near-bank processing units to deliver high memory\nbandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable\nCXL network to support peer-to-peer and collective communication primitives\nacross CXL devices. We implement various parallelism strategies to distribute\nLLMs across these devices. Compared to GPU baselines with maximum supported\nbatch sizes and similar average power, CENT achieves 2.3$\\times$ higher\nthroughput and consumes 2.3$\\times$ less energy. CENT enhances the Total Cost\nof Ownership (TCO), generating 5.2$\\times$ more tokens per dollar than GPUs."
                },
                "authors": [
                    {
                        "name": "Yufeng Gu"
                    },
                    {
                        "name": "Alireza Khadem"
                    },
                    {
                        "name": "Sumanth Umesh"
                    },
                    {
                        "name": "Ning Liang"
                    },
                    {
                        "name": "Xavier Servot"
                    },
                    {
                        "name": "Onur Mutlu"
                    },
                    {
                        "name": "Ravi Iyer"
                    },
                    {
                        "name": "Reetuparna Das"
                    }
                ],
                "author_detail": {
                    "name": "Reetuparna Das"
                },
                "author": "Reetuparna Das",
                "arxiv_doi": "10.1145/3676641.3716267",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676641.3716267",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.07578v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07578v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings of the 30th ACM International Conference on\n  Architectural Support for Programming Languages and Operating Systems, Volume\n  2 (ASPLOS'25)",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09003v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09003v2",
                "updated": "2025-03-21T19:26:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    19,
                    26,
                    12,
                    4,
                    80,
                    0
                ],
                "published": "2025-02-13T06:44:33Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    6,
                    44,
                    33,
                    3,
                    44,
                    0
                ],
                "title": "RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach\n  for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach\n  for Large Language Models"
                },
                "summary": "Supervised fine-tuning is a standard method for adapting pre-trained large\nlanguage models (LLMs) to downstream tasks. Quantization has been recently\nstudied as a post-training technique for efficient LLM deployment. To obtain\nquantized fine-tuned LLMs, conventional pipelines would first fine-tune the\npre-trained models, followed by post-training quantization. This often yields\nsuboptimal performance as it fails to leverage the synergy between fine-tuning\nand quantization. To effectively realize low-bit quantization of weights,\nactivations, and KV caches in LLMs, we propose an algorithm named Rotated\nStraight-Through-Estimator (RoSTE), which combines quantization-aware\nsupervised fine-tuning (QA-SFT) with an adaptive rotation strategy that\nidentifies an effective rotation configuration to reduce activation outliers.\nWe provide theoretical insights on RoSTE by analyzing its prediction error when\napplied to an overparameterized least square quantized training problem. Our\nfindings reveal that the prediction error is directly proportional to the\nquantization error of the converged weights, which can be effectively managed\nthrough an optimized rotation configuration. Experiments on Pythia, Qwen and\nLlama models of different sizes demonstrate the effectiveness of RoSTE.\nCompared to existing post-SFT quantization baselines, our method consistently\nachieves superior performances across various tasks and different LLM\narchitectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised fine-tuning is a standard method for adapting pre-trained large\nlanguage models (LLMs) to downstream tasks. Quantization has been recently\nstudied as a post-training technique for efficient LLM deployment. To obtain\nquantized fine-tuned LLMs, conventional pipelines would first fine-tune the\npre-trained models, followed by post-training quantization. This often yields\nsuboptimal performance as it fails to leverage the synergy between fine-tuning\nand quantization. To effectively realize low-bit quantization of weights,\nactivations, and KV caches in LLMs, we propose an algorithm named Rotated\nStraight-Through-Estimator (RoSTE), which combines quantization-aware\nsupervised fine-tuning (QA-SFT) with an adaptive rotation strategy that\nidentifies an effective rotation configuration to reduce activation outliers.\nWe provide theoretical insights on RoSTE by analyzing its prediction error when\napplied to an overparameterized least square quantized training problem. Our\nfindings reveal that the prediction error is directly proportional to the\nquantization error of the converged weights, which can be effectively managed\nthrough an optimized rotation configuration. Experiments on Pythia, Qwen and\nLlama models of different sizes demonstrate the effectiveness of RoSTE.\nCompared to existing post-SFT quantization baselines, our method consistently\nachieves superior performances across various tasks and different LLM\narchitectures."
                },
                "authors": [
                    {
                        "name": "Quan Wei"
                    },
                    {
                        "name": "Chung-Yiu Yau"
                    },
                    {
                        "name": "Hoi-To Wai"
                    },
                    {
                        "name": "Yang Katie Zhao"
                    },
                    {
                        "name": "Dongyeop Kang"
                    },
                    {
                        "name": "Youngsuk Park"
                    },
                    {
                        "name": "Mingyi Hong"
                    }
                ],
                "author_detail": {
                    "name": "Mingyi Hong"
                },
                "author": "Mingyi Hong",
                "arxiv_comment": "20 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09003v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09003v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12444v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12444v3",
                "updated": "2025-03-21T15:52:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    15,
                    52,
                    39,
                    4,
                    80,
                    0
                ],
                "published": "2024-12-17T01:12:35Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    1,
                    12,
                    35,
                    1,
                    352,
                    0
                ],
                "title": "LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers"
                },
                "summary": "Diffusion Transformers have emerged as the preeminent models for a wide array\nof generative tasks, demonstrating superior performance and efficacy across\nvarious applications. The promising results come at the cost of slow inference,\nas each denoising step requires running the whole transformer model with a\nlarge amount of parameters. In this paper, we show that performing the full\ncomputation of the model at each diffusion step is unnecessary, as some\ncomputations can be skipped by lazily reusing the results of previous steps.\nFurthermore, we show that the lower bound of similarity between outputs at\nconsecutive steps is notably high, and this similarity can be linearly\napproximated using the inputs. To verify our demonstrations, we propose the\n\\textbf{LazyDiT}, a lazy learning framework that efficiently leverages cached\nresults from earlier steps to skip redundant computations. Specifically, we\nincorporate lazy learning layers into the model, effectively trained to\nmaximize laziness, enabling dynamic skipping of redundant computations.\nExperimental results show that LazyDiT outperforms the DDIM sampler across\nmultiple diffusion transformer models at various resolutions. Furthermore, we\nimplement our method on mobile devices, achieving better performance than DDIM\nwith similar latency. Code: https://github.com/shawnricecake/lazydit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers have emerged as the preeminent models for a wide array\nof generative tasks, demonstrating superior performance and efficacy across\nvarious applications. The promising results come at the cost of slow inference,\nas each denoising step requires running the whole transformer model with a\nlarge amount of parameters. In this paper, we show that performing the full\ncomputation of the model at each diffusion step is unnecessary, as some\ncomputations can be skipped by lazily reusing the results of previous steps.\nFurthermore, we show that the lower bound of similarity between outputs at\nconsecutive steps is notably high, and this similarity can be linearly\napproximated using the inputs. To verify our demonstrations, we propose the\n\\textbf{LazyDiT}, a lazy learning framework that efficiently leverages cached\nresults from earlier steps to skip redundant computations. Specifically, we\nincorporate lazy learning layers into the model, effectively trained to\nmaximize laziness, enabling dynamic skipping of redundant computations.\nExperimental results show that LazyDiT outperforms the DDIM sampler across\nmultiple diffusion transformer models at various resolutions. Furthermore, we\nimplement our method on mobile devices, achieving better performance than DDIM\nwith similar latency. Code: https://github.com/shawnricecake/lazydit"
                },
                "authors": [
                    {
                        "name": "Xuan Shen"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Yufa Zhou"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Yanyu Li"
                    },
                    {
                        "name": "Yifan Gong"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Hao Tan"
                    },
                    {
                        "name": "Jason Kuen"
                    },
                    {
                        "name": "Henghui Ding"
                    },
                    {
                        "name": "Zhihao Shu"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Pu Zhao"
                    },
                    {
                        "name": "Yanzhi Wang"
                    },
                    {
                        "name": "Jiuxiang Gu"
                    }
                ],
                "author_detail": {
                    "name": "Jiuxiang Gu"
                },
                "author": "Jiuxiang Gu",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12444v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12444v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15102v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15102v3",
                "updated": "2025-03-21T15:47:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    15,
                    47,
                    53,
                    4,
                    80,
                    0
                ],
                "published": "2024-11-22T18:06:14Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    6,
                    14,
                    4,
                    327,
                    0
                ],
                "title": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution"
                },
                "summary": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods."
                },
                "authors": [
                    {
                        "name": "Fengyuan Liu"
                    },
                    {
                        "name": "Nikhil Kandpal"
                    },
                    {
                        "name": "Colin Raffel"
                    }
                ],
                "author_detail": {
                    "name": "Colin Raffel"
                },
                "author": "Colin Raffel",
                "arxiv_comment": "24 pages, 11 figures, ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15102v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15102v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00876v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00876v4",
                "updated": "2025-03-21T13:30:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    13,
                    30,
                    33,
                    4,
                    80,
                    0
                ],
                "published": "2024-12-01T16:32:31Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    16,
                    32,
                    31,
                    6,
                    336,
                    0
                ],
                "title": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava ."
                },
                "authors": [
                    {
                        "name": "Wenxuan Huang"
                    },
                    {
                        "name": "Zijie Zhai"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Shaosheng Cao"
                    },
                    {
                        "name": "Fei Zhao"
                    },
                    {
                        "name": "Xiangfeng Xu"
                    },
                    {
                        "name": "Zheyu Ye"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Shaohui Lin"
                    }
                ],
                "author_detail": {
                    "name": "Shaohui Lin"
                },
                "author": "Shaohui Lin",
                "arxiv_comment": "Accepted to ICLR 2025. Code is available at\n  https://github.com/Osilly/dynamic_llava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00876v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00876v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09398v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09398v3",
                "updated": "2025-03-21T12:51:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    12,
                    51,
                    15,
                    4,
                    80,
                    0
                ],
                "published": "2024-09-14T10:15:37Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    10,
                    15,
                    37,
                    5,
                    258,
                    0
                ],
                "title": "Language-Queried Target Sound Extraction Without Parallel Training Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-Queried Target Sound Extraction Without Parallel Training Data"
                },
                "summary": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a parallel-data-free training scheme,\nrequiring only unlabelled audio clips for TSE model training by utilizing the\ncontrastive language-audio pre-trained model (CLAP). In a vanilla\nparallel-data-free training stage, target audio is encoded using the\npre-trained CLAP audio encoder to form a condition embedding, while during\ntesting, user language queries are encoded by CLAP text encoder as the\ncondition embedding. This vanilla approach assumes perfect alignment between\ntext and audio embeddings, which is unrealistic. Two major challenges arise\nfrom training-testing mismatch: the persistent modality gap between text and\naudio and the risk of overfitting due to the exposure of rich acoustic details\nin target audio embedding during training. To address this, we propose a\nretrieval-augmented strategy. Specifically, we create an embedding cache using\naudio captions generated by a large language model (LLM). During training,\ntarget audio embeddings retrieve text embeddings from this cache to use as\ncondition embeddings, ensuring consistent modalities between training and\ntesting and eliminating information leakage. Extensive experiment results show\nthat our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a parallel-data-free training scheme,\nrequiring only unlabelled audio clips for TSE model training by utilizing the\ncontrastive language-audio pre-trained model (CLAP). In a vanilla\nparallel-data-free training stage, target audio is encoded using the\npre-trained CLAP audio encoder to form a condition embedding, while during\ntesting, user language queries are encoded by CLAP text encoder as the\ncondition embedding. This vanilla approach assumes perfect alignment between\ntext and audio embeddings, which is unrealistic. Two major challenges arise\nfrom training-testing mismatch: the persistent modality gap between text and\naudio and the risk of overfitting due to the exposure of rich acoustic details\nin target audio embedding during training. To address this, we propose a\nretrieval-augmented strategy. Specifically, we create an embedding cache using\naudio captions generated by a large language model (LLM). During training,\ntarget audio embeddings retrieve text embeddings from this cache to use as\ncondition embeddings, ensuring consistent modalities between training and\ntesting and eliminating information leakage. Extensive experiment results show\nthat our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability."
                },
                "authors": [
                    {
                        "name": "Hao Ma"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Xu Li"
                    },
                    {
                        "name": "Yukai Li"
                    },
                    {
                        "name": "Mingjie Shao"
                    },
                    {
                        "name": "Qiuqiang Kong"
                    },
                    {
                        "name": "Ju Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ju Liu"
                },
                "author": "Ju Liu",
                "arxiv_comment": "Accepted by ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09398v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09398v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16870v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16870v1",
                "updated": "2025-03-21T05:58:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    5,
                    58,
                    18,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T05:58:18Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    5,
                    58,
                    18,
                    4,
                    80,
                    0
                ],
                "title": "Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs"
                },
                "summary": "Knowledge distillation can be a cost-effective technique to distill knowledge\nin Large Language Models, if the teacher output logits can be pre-computed and\ncached. However, successfully applying this to pre-training remains largely\nunexplored. In this work, we prove that naive approaches for sparse knowledge\ndistillation such as caching Top-K probabilities, while intuitive, provide\nbiased estimates of teacher probability distribution to the student, resulting\nin suboptimal performance and calibration. We propose an\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\nwhich provides unbiased estimates, preserves the gradient in expectation, and\nrequires storing significantly sparser logits. Our method enables faster\ntraining of student models with marginal overhead (<10%) compared to\ncross-entropy based training, while maintaining competitive performance\ncompared to full distillation, across a range of model sizes from 300M to 3B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge distillation can be a cost-effective technique to distill knowledge\nin Large Language Models, if the teacher output logits can be pre-computed and\ncached. However, successfully applying this to pre-training remains largely\nunexplored. In this work, we prove that naive approaches for sparse knowledge\ndistillation such as caching Top-K probabilities, while intuitive, provide\nbiased estimates of teacher probability distribution to the student, resulting\nin suboptimal performance and calibration. We propose an\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\nwhich provides unbiased estimates, preserves the gradient in expectation, and\nrequires storing significantly sparser logits. Our method enables faster\ntraining of student models with marginal overhead (<10%) compared to\ncross-entropy based training, while maintaining competitive performance\ncompared to full distillation, across a range of model sizes from 300M to 3B."
                },
                "authors": [
                    {
                        "name": "Anshumann"
                    },
                    {
                        "name": "Mohd Abbas Zaidi"
                    },
                    {
                        "name": "Akhil Kedia"
                    },
                    {
                        "name": "Jinwoo Ahn"
                    },
                    {
                        "name": "Taehwak Kwon"
                    },
                    {
                        "name": "Kangwook Lee"
                    },
                    {
                        "name": "Haejun Lee"
                    },
                    {
                        "name": "Joohyung Lee"
                    }
                ],
                "author_detail": {
                    "name": "Joohyung Lee"
                },
                "author": "Joohyung Lee",
                "arxiv_comment": "Anshumann, Mohd Abbas Zaidi and Akhil Kedia have Equal Contribution",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16870v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16870v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16131v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16131v2",
                "updated": "2025-03-21T01:59:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    1,
                    59,
                    12,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-20T13:25:03Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    25,
                    3,
                    3,
                    79,
                    0
                ],
                "title": "MKG-Rank: Enhancing Large Language Models with Knowledge Graph for\n  Multilingual Medical Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MKG-Rank: Enhancing Large Language Models with Knowledge Graph for\n  Multilingual Medical Question Answering"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable progress in medical\nquestion answering (QA), yet their effectiveness remains predominantly limited\nto English due to imbalanced multilingual training data and scarce medical\nresources for low-resource languages. To address this critical language gap in\nmedical QA, we propose Multilingual Knowledge Graph-based Retrieval Ranking\n(MKG-Rank), a knowledge graph-enhanced framework that enables English-centric\nLLMs to perform multilingual medical QA. Through a word-level translation\nmechanism, our framework efficiently integrates comprehensive English-centric\nmedical knowledge graphs into LLM reasoning at a low cost, mitigating\ncross-lingual semantic distortion and achieving precise medical QA across\nlanguage barriers. To enhance efficiency, we introduce caching and multi-angle\nranking strategies to optimize the retrieval process, significantly reducing\nresponse times and prioritizing relevant medical knowledge. Extensive\nevaluations on multilingual medical QA benchmarks across Chinese, Japanese,\nKorean, and Swahili demonstrate that MKG-Rank consistently outperforms\nzero-shot LLMs, achieving maximum 35.03% increase in accuracy, while\nmaintaining an average retrieval time of only 0.0009 seconds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable progress in medical\nquestion answering (QA), yet their effectiveness remains predominantly limited\nto English due to imbalanced multilingual training data and scarce medical\nresources for low-resource languages. To address this critical language gap in\nmedical QA, we propose Multilingual Knowledge Graph-based Retrieval Ranking\n(MKG-Rank), a knowledge graph-enhanced framework that enables English-centric\nLLMs to perform multilingual medical QA. Through a word-level translation\nmechanism, our framework efficiently integrates comprehensive English-centric\nmedical knowledge graphs into LLM reasoning at a low cost, mitigating\ncross-lingual semantic distortion and achieving precise medical QA across\nlanguage barriers. To enhance efficiency, we introduce caching and multi-angle\nranking strategies to optimize the retrieval process, significantly reducing\nresponse times and prioritizing relevant medical knowledge. Extensive\nevaluations on multilingual medical QA benchmarks across Chinese, Japanese,\nKorean, and Swahili demonstrate that MKG-Rank consistently outperforms\nzero-shot LLMs, achieving maximum 35.03% increase in accuracy, while\nmaintaining an average retrieval time of only 0.0009 seconds."
                },
                "authors": [
                    {
                        "name": "Feiyang Li"
                    },
                    {
                        "name": "Yingjian Chen"
                    },
                    {
                        "name": "Haoran Liu"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Han Yuan"
                    },
                    {
                        "name": "Yuang Jiang"
                    },
                    {
                        "name": "Tianxiao Li"
                    },
                    {
                        "name": "Edison Marrese Taylor"
                    },
                    {
                        "name": "Hossein Rouhizadeh"
                    },
                    {
                        "name": "Yusuke Iwasawa"
                    },
                    {
                        "name": "Douglas Teodoro"
                    },
                    {
                        "name": "Yutaka Matsuo"
                    },
                    {
                        "name": "Irene Li"
                    }
                ],
                "author_detail": {
                    "name": "Irene Li"
                },
                "author": "Irene Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16131v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16131v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02430v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02430v3",
                "updated": "2025-03-20T21:49:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    21,
                    49,
                    15,
                    3,
                    79,
                    0
                ],
                "published": "2025-02-04T15:55:10Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    55,
                    10,
                    1,
                    35,
                    0
                ],
                "title": "A Scalable Crawling Algorithm Utilizing Noisy Change-Indicating Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Scalable Crawling Algorithm Utilizing Noisy Change-Indicating Signals"
                },
                "summary": "Web refresh crawling is the problem of keeping a cache of web pages fresh,\nthat is, having the most recent copy available when a page is requested, given\na limited bandwidth available to the crawler. Under the assumption that the\nchange and request events, resp., to each web page follow independent Poisson\nprocesses, the optimal scheduling policy was derived by Azar et al. 2018. In\nthis paper, we study an extension of this problem where side information\nindicating content changes, such as various types of web pings, for example,\nsignals from sitemaps, content delivery networks, etc., is available.\nIncorporating such side information into the crawling policy is challenging,\nbecause (i) the signals can be noisy with false positive events and with\nmissing change events; and (ii) the crawler should achieve a fair performance\nover web pages regardless of the quality of the side information, which might\ndiffer from web page to web page. We propose a scalable crawling algorithm\nwhich (i) uses the noisy side information in an optimal way under mild\nassumptions; (ii) can be deployed without heavy centralized computation; (iii)\nis able to crawl web pages at a constant total rate without spikes in the total\nbandwidth usage over any time interval, and automatically adapt to the new\noptimal solution when the total bandwidth changes without centralized\ncomputation. Experiments clearly demonstrate the versatility of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web refresh crawling is the problem of keeping a cache of web pages fresh,\nthat is, having the most recent copy available when a page is requested, given\na limited bandwidth available to the crawler. Under the assumption that the\nchange and request events, resp., to each web page follow independent Poisson\nprocesses, the optimal scheduling policy was derived by Azar et al. 2018. In\nthis paper, we study an extension of this problem where side information\nindicating content changes, such as various types of web pings, for example,\nsignals from sitemaps, content delivery networks, etc., is available.\nIncorporating such side information into the crawling policy is challenging,\nbecause (i) the signals can be noisy with false positive events and with\nmissing change events; and (ii) the crawler should achieve a fair performance\nover web pages regardless of the quality of the side information, which might\ndiffer from web page to web page. We propose a scalable crawling algorithm\nwhich (i) uses the noisy side information in an optimal way under mild\nassumptions; (ii) can be deployed without heavy centralized computation; (iii)\nis able to crawl web pages at a constant total rate without spikes in the total\nbandwidth usage over any time interval, and automatically adapt to the new\noptimal solution when the total bandwidth changes without centralized\ncomputation. Experiments clearly demonstrate the versatility of our approach."
                },
                "authors": [
                    {
                        "name": "Róbert Busa-Fekete"
                    },
                    {
                        "name": "Julian Zimmert"
                    },
                    {
                        "name": "András György"
                    },
                    {
                        "name": "Linhai Qiu"
                    },
                    {
                        "name": "Tzu-Wei Sung"
                    },
                    {
                        "name": "Hao Shen"
                    },
                    {
                        "name": "Hyomin Choi"
                    },
                    {
                        "name": "Sharmila Subramaniam"
                    },
                    {
                        "name": "Li Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Li Xiao"
                },
                "author": "Li Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02430v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02430v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16257v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16257v1",
                "updated": "2025-03-20T15:52:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    52,
                    43,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T15:52:43Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    52,
                    43,
                    3,
                    79,
                    0
                ],
                "title": "Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language\n  Models"
                },
                "summary": "Video large language models (VideoLLMs) have demonstrated the capability to\nprocess longer video inputs and enable complex reasoning and analysis. However,\ndue to the thousands of visual tokens from the video frames, key-value (KV)\ncache can significantly increase memory requirements, becoming a bottleneck for\ninference speed and memory usage. KV cache quantization is a widely used\napproach to address this problem. In this paper, we find that 2-bit KV\nquantization of VideoLLMs can hardly hurt the model performance, while the\nlimit of KV cache quantization in even lower bits has not been investigated. To\nbridge this gap, we introduce VidKV, a plug-and-play KV cache quantization\nmethod to compress the KV cache to lower than 2 bits. Specifically, (1) for\nkey, we propose a mixed-precision quantization strategy in the channel\ndimension, where we perform 2-bit quantization for anomalous channels and 1-bit\nquantization combined with FFT for normal channels; (2) for value, we implement\n1.58-bit quantization while selectively filtering semantically salient visual\ntokens for targeted preservation, for a better trade-off between precision and\nmodel performance. Importantly, our findings suggest that the value cache of\nVideoLLMs should be quantized in a per-channel fashion instead of the per-token\nfashion proposed by prior KV cache quantization works for LLMs. Empirically,\nextensive results with LLaVA-OV-7B and Qwen2.5-VL-7B on six benchmarks show\nthat VidKV effectively compresses the KV cache to 1.5-bit and 1.58-bit\nprecision with almost no performance drop compared to the FP16 counterparts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VideoLLMs) have demonstrated the capability to\nprocess longer video inputs and enable complex reasoning and analysis. However,\ndue to the thousands of visual tokens from the video frames, key-value (KV)\ncache can significantly increase memory requirements, becoming a bottleneck for\ninference speed and memory usage. KV cache quantization is a widely used\napproach to address this problem. In this paper, we find that 2-bit KV\nquantization of VideoLLMs can hardly hurt the model performance, while the\nlimit of KV cache quantization in even lower bits has not been investigated. To\nbridge this gap, we introduce VidKV, a plug-and-play KV cache quantization\nmethod to compress the KV cache to lower than 2 bits. Specifically, (1) for\nkey, we propose a mixed-precision quantization strategy in the channel\ndimension, where we perform 2-bit quantization for anomalous channels and 1-bit\nquantization combined with FFT for normal channels; (2) for value, we implement\n1.58-bit quantization while selectively filtering semantically salient visual\ntokens for targeted preservation, for a better trade-off between precision and\nmodel performance. Importantly, our findings suggest that the value cache of\nVideoLLMs should be quantized in a per-channel fashion instead of the per-token\nfashion proposed by prior KV cache quantization works for LLMs. Empirically,\nextensive results with LLaVA-OV-7B and Qwen2.5-VL-7B on six benchmarks show\nthat VidKV effectively compresses the KV cache to 1.5-bit and 1.58-bit\nprecision with almost no performance drop compared to the FP16 counterparts."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16257v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16257v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16163v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16163v1",
                "updated": "2025-03-20T14:01:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    14,
                    1,
                    56,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T14:01:56Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    14,
                    1,
                    56,
                    3,
                    79,
                    0
                ],
                "title": "SpeCache: Speculative Key-Value Caching for Efficient Generation of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpeCache: Speculative Key-Value Caching for Efficient Generation of LLMs"
                },
                "summary": "Transformer-based large language models (LLMs) have already achieved\nremarkable results on long-text tasks, but the limited GPU memory (VRAM)\nresources struggle to accommodate the linearly growing demand for key-value\n(KV) cache as the sequence length increases, which has become a bottleneck for\nthe application of LLMs on long sequences. Existing KV cache compression\nmethods include eviction, merging, or quantization of the KV cache to reduce\nits size. However, compression results in irreversible information forgetting,\npotentially affecting the accuracy of subsequent decoding. In this paper, we\npropose SpeCache, which takes full advantage of the large and easily expandable\nCPU memory to offload the complete KV cache, and dynamically fetches KV pairs\nback in each decoding step based on their importance measured by low-bit KV\ncache copy in VRAM. To avoid inference latency caused by CPU-GPU communication,\nSpeCache speculatively predicts the KV pairs that the next token might attend\nto, allowing us to prefetch them before the next decoding step which enables\nparallelization of prefetching and computation. Experiments on LongBench and\nNeedle-in-a-Haystack benchmarks verify that SpeCache effectively reduces VRAM\nusage while avoiding information forgetting for long sequences without\nre-training, even with a 10x high KV cache compression ratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) have already achieved\nremarkable results on long-text tasks, but the limited GPU memory (VRAM)\nresources struggle to accommodate the linearly growing demand for key-value\n(KV) cache as the sequence length increases, which has become a bottleneck for\nthe application of LLMs on long sequences. Existing KV cache compression\nmethods include eviction, merging, or quantization of the KV cache to reduce\nits size. However, compression results in irreversible information forgetting,\npotentially affecting the accuracy of subsequent decoding. In this paper, we\npropose SpeCache, which takes full advantage of the large and easily expandable\nCPU memory to offload the complete KV cache, and dynamically fetches KV pairs\nback in each decoding step based on their importance measured by low-bit KV\ncache copy in VRAM. To avoid inference latency caused by CPU-GPU communication,\nSpeCache speculatively predicts the KV pairs that the next token might attend\nto, allowing us to prefetch them before the next decoding step which enables\nparallelization of prefetching and computation. Experiments on LongBench and\nNeedle-in-a-Haystack benchmarks verify that SpeCache effectively reduces VRAM\nusage while avoiding information forgetting for long sequences without\nre-training, even with a 10x high KV cache compression ratio."
                },
                "authors": [
                    {
                        "name": "Shibo Jie"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Kai Han"
                    },
                    {
                        "name": "Zhi-Hong Deng"
                    },
                    {
                        "name": "Jing Han"
                    }
                ],
                "author_detail": {
                    "name": "Jing Han"
                },
                "author": "Jing Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16163v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16163v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16112v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16112v1",
                "updated": "2025-03-20T13:00:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    0,
                    36,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T13:00:36Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    0,
                    36,
                    3,
                    79,
                    0
                ],
                "title": "PromptMobile: Efficient Promptus for Low Bandwidth Mobile Video\n  Streaming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromptMobile: Efficient Promptus for Low Bandwidth Mobile Video\n  Streaming"
                },
                "summary": "Traditional video compression algorithms exhibit significant quality\ndegradation at extremely low bitrates. Promptus emerges as a new paradigm for\nvideo streaming, substantially cutting down the bandwidth essential for video\nstreaming. However, Promptus is computationally intensive and can not run in\nreal-time on mobile devices. This paper presents PromptMobile, an efficient\nacceleration framework tailored for on-device Promptus. Specifically, we\npropose (1) a two-stage efficient generation framework to reduce computational\ncost by 8.1x, (2) a fine-grained inter-frame caching to reduce redundant\ncomputations by 16.6\\%, (3) system-level optimizations to further enhance\nefficiency. The evaluations demonstrate that compared with the original\nPromptus, PromptMobile achieves a 13.6x increase in image generation speed.\nCompared with other streaming methods, PromptMobile achives an average LPIPS\nimprovement of 0.016 (compared with H.265), reducing 60\\% of severely distorted\nframes (compared to VQGAN).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional video compression algorithms exhibit significant quality\ndegradation at extremely low bitrates. Promptus emerges as a new paradigm for\nvideo streaming, substantially cutting down the bandwidth essential for video\nstreaming. However, Promptus is computationally intensive and can not run in\nreal-time on mobile devices. This paper presents PromptMobile, an efficient\nacceleration framework tailored for on-device Promptus. Specifically, we\npropose (1) a two-stage efficient generation framework to reduce computational\ncost by 8.1x, (2) a fine-grained inter-frame caching to reduce redundant\ncomputations by 16.6\\%, (3) system-level optimizations to further enhance\nefficiency. The evaluations demonstrate that compared with the original\nPromptus, PromptMobile achieves a 13.6x increase in image generation speed.\nCompared with other streaming methods, PromptMobile achives an average LPIPS\nimprovement of 0.016 (compared with H.265), reducing 60\\% of severely distorted\nframes (compared to VQGAN)."
                },
                "authors": [
                    {
                        "name": "Liming Liu"
                    },
                    {
                        "name": "Jiangkai Wu"
                    },
                    {
                        "name": "Haoyang Wang"
                    },
                    {
                        "name": "Peiheng Wang"
                    },
                    {
                        "name": "Xinggong Zhang"
                    },
                    {
                        "name": "Zongming Guo"
                    }
                ],
                "author_detail": {
                    "name": "Zongming Guo"
                },
                "author": "Zongming Guo",
                "arxiv_comment": "7 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16112v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16112v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15927v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15927v1",
                "updated": "2025-03-20T08:07:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    8,
                    7,
                    31,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T08:07:31Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    8,
                    7,
                    31,
                    3,
                    79,
                    0
                ],
                "title": "BlockDance: Reuse Structurally Similar Spatio-Temporal Features to\n  Accelerate Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BlockDance: Reuse Structurally Similar Spatio-Temporal Features to\n  Accelerate Diffusion Transformers"
                },
                "summary": "Diffusion models have demonstrated impressive generation capabilities,\nparticularly with recent advancements leveraging transformer architectures to\nimprove both visual and artistic quality. However, Diffusion Transformers\n(DiTs) continue to encounter challenges related to low inference speed,\nprimarily due to the iterative denoising process. To address this issue, we\npropose BlockDance, a training-free approach that explores feature similarities\nat adjacent time steps to accelerate DiTs. Unlike previous feature-reuse\nmethods that lack tailored reuse strategies for features at different scales,\nBlockDance prioritizes the identification of the most structurally similar\nfeatures, referred to as Structurally Similar Spatio-Temporal (STSS) features.\nThese features are primarily located within the structure-focused blocks of the\ntransformer during the later stages of denoising. BlockDance caches and reuses\nthese highly similar features to mitigate redundant computation, thereby\naccelerating DiTs while maximizing consistency with the generated results of\nthe original model. Furthermore, considering the diversity of generated content\nand the varying distributions of redundant features, we introduce\nBlockDance-Ada, a lightweight decision-making network tailored for\ninstance-specific acceleration. BlockDance-Ada dynamically allocates resources\nand provides superior content quality. Both BlockDance and BlockDance-Ada have\nproven effective across various generation tasks and models, achieving\naccelerations between 25% and 50% while maintaining generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have demonstrated impressive generation capabilities,\nparticularly with recent advancements leveraging transformer architectures to\nimprove both visual and artistic quality. However, Diffusion Transformers\n(DiTs) continue to encounter challenges related to low inference speed,\nprimarily due to the iterative denoising process. To address this issue, we\npropose BlockDance, a training-free approach that explores feature similarities\nat adjacent time steps to accelerate DiTs. Unlike previous feature-reuse\nmethods that lack tailored reuse strategies for features at different scales,\nBlockDance prioritizes the identification of the most structurally similar\nfeatures, referred to as Structurally Similar Spatio-Temporal (STSS) features.\nThese features are primarily located within the structure-focused blocks of the\ntransformer during the later stages of denoising. BlockDance caches and reuses\nthese highly similar features to mitigate redundant computation, thereby\naccelerating DiTs while maximizing consistency with the generated results of\nthe original model. Furthermore, considering the diversity of generated content\nand the varying distributions of redundant features, we introduce\nBlockDance-Ada, a lightweight decision-making network tailored for\ninstance-specific acceleration. BlockDance-Ada dynamically allocates resources\nand provides superior content quality. Both BlockDance and BlockDance-Ada have\nproven effective across various generation tasks and models, achieving\naccelerations between 25% and 50% while maintaining generation quality."
                },
                "authors": [
                    {
                        "name": "Hui Zhang"
                    },
                    {
                        "name": "Tingwei Gao"
                    },
                    {
                        "name": "Jie Shao"
                    },
                    {
                        "name": "Zuxuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zuxuan Wu"
                },
                "author": "Zuxuan Wu",
                "arxiv_comment": "Accepted by CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15927v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18921v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18921v2",
                "updated": "2025-03-20T05:23:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    5,
                    23,
                    42,
                    3,
                    79,
                    0
                ],
                "published": "2024-07-09T13:47:05Z",
                "published_parsed": [
                    2024,
                    7,
                    9,
                    13,
                    47,
                    5,
                    1,
                    191,
                    0
                ],
                "title": "Mobile Edge Intelligence for Large Language Models: A Contemporary\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile Edge Intelligence for Large Language Models: A Contemporary\n  Survey"
                },
                "summary": "On-device large language models (LLMs), referring to running LLMs on edge\ndevices, have raised considerable interest since they are more cost-effective,\nlatency-efficient, and privacy-preserving compared with the cloud paradigm.\nNonetheless, the performance of on-device LLMs is intrinsically constrained by\nresource limitations on edge devices. Sitting between cloud and on-device AI,\nmobile edge intelligence (MEI) presents a viable solution by provisioning AI\ncapabilities at the edge of mobile networks, enabling end users to offload\nheavy AI computation to capable edge servers nearby. This article provides a\ncontemporary survey on harnessing MEI for LLMs. We begin by illustrating\nseveral killer applications to demonstrate the urgent need for deploying LLMs\nat the network edge. Next, we present the preliminaries of LLMs and MEI,\nfollowed by resource-efficient LLM techniques. We then present an architectural\noverview of MEI for LLMs (MEI4LLM), outlining its core components and how it\nsupports the deployment of LLMs. Subsequently, we delve into various aspects of\nMEI4LLM, extensively covering edge LLM caching and delivery, edge LLM training,\nand edge LLM inference. Finally, we identify future research opportunities. We\nhope this article inspires researchers in the field to leverage mobile edge\ncomputing to facilitate LLM deployment, thereby unleashing the potential of\nLLMs across various privacy- and delay-sensitive applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On-device large language models (LLMs), referring to running LLMs on edge\ndevices, have raised considerable interest since they are more cost-effective,\nlatency-efficient, and privacy-preserving compared with the cloud paradigm.\nNonetheless, the performance of on-device LLMs is intrinsically constrained by\nresource limitations on edge devices. Sitting between cloud and on-device AI,\nmobile edge intelligence (MEI) presents a viable solution by provisioning AI\ncapabilities at the edge of mobile networks, enabling end users to offload\nheavy AI computation to capable edge servers nearby. This article provides a\ncontemporary survey on harnessing MEI for LLMs. We begin by illustrating\nseveral killer applications to demonstrate the urgent need for deploying LLMs\nat the network edge. Next, we present the preliminaries of LLMs and MEI,\nfollowed by resource-efficient LLM techniques. We then present an architectural\noverview of MEI for LLMs (MEI4LLM), outlining its core components and how it\nsupports the deployment of LLMs. Subsequently, we delve into various aspects of\nMEI4LLM, extensively covering edge LLM caching and delivery, edge LLM training,\nand edge LLM inference. Finally, we identify future research opportunities. We\nhope this article inspires researchers in the field to leverage mobile edge\ncomputing to facilitate LLM deployment, thereby unleashing the potential of\nLLMs across various privacy- and delay-sensitive applications."
                },
                "authors": [
                    {
                        "name": "Guanqiao Qu"
                    },
                    {
                        "name": "Qiyuan Chen"
                    },
                    {
                        "name": "Wei Wei"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Xianhao Chen"
                    },
                    {
                        "name": "Kaibin Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kaibin Huang"
                },
                "author": "Kaibin Huang",
                "arxiv_comment": "42 pages, 17 figures. This paper has been accepted by IEEE\n  Communications Surveys & Tutorials",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18921v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18921v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15908v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15908v2",
                "updated": "2025-03-19T10:19:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    10,
                    19,
                    30,
                    2,
                    78,
                    0
                ],
                "published": "2024-10-21T11:29:49Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    11,
                    29,
                    49,
                    0,
                    295,
                    0
                ],
                "title": "Formalising CXL Cache Coherence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formalising CXL Cache Coherence"
                },
                "summary": "We report our experience formally modelling and verifying CXL.cache, the\ninter-device cache coherence protocol of the Compute Express Link standard. We\nhave used the Isabelle proof assistant to create a formal model for CXL.cache\nbased on the prose English specification. This led to us identifying and\nproposing fixes to several problems we identified as unclear, ambiguous or\ninaccurate, some of which could lead to incoherence if left unfixed. Nearly all\nour issues and proposed fixes have been confirmed and tentatively accepted by\nthe CXL consortium for adoption, save for one which is still under discussion.\nTo validate the faithfulness of our model we performed scenario verification of\nessential restrictions such as \"Snoop-pushes-GO\", and produced a fully\nmechanised proof of a coherence property of the model. The considerable size of\nthis proof, comprising tens of thousands of lemmas, prompted us to develop new\nproof automation tools, which we have made available for other Isabelle users\nworking with similarly cumbersome proofs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report our experience formally modelling and verifying CXL.cache, the\ninter-device cache coherence protocol of the Compute Express Link standard. We\nhave used the Isabelle proof assistant to create a formal model for CXL.cache\nbased on the prose English specification. This led to us identifying and\nproposing fixes to several problems we identified as unclear, ambiguous or\ninaccurate, some of which could lead to incoherence if left unfixed. Nearly all\nour issues and proposed fixes have been confirmed and tentatively accepted by\nthe CXL consortium for adoption, save for one which is still under discussion.\nTo validate the faithfulness of our model we performed scenario verification of\nessential restrictions such as \"Snoop-pushes-GO\", and produced a fully\nmechanised proof of a coherence property of the model. The considerable size of\nthis proof, comprising tens of thousands of lemmas, prompted us to develop new\nproof automation tools, which we have made available for other Isabelle users\nworking with similarly cumbersome proofs."
                },
                "authors": [
                    {
                        "name": "Chengsong Tan"
                    },
                    {
                        "name": "Alastair F. Donaldson"
                    },
                    {
                        "name": "John Wickerson"
                    }
                ],
                "author_detail": {
                    "name": "John Wickerson"
                },
                "author": "John Wickerson",
                "arxiv_doi": "10.1145/3676641.3715999",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676641.3715999",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.15908v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15908v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages",
                "arxiv_journal_ref": "Proceedings of the 30th ACM International Conference on\n  Architectural Support for Programming Languages and Operating Systems, ASPLOS\n  2025, Rotterdam, Netherlands",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68 (Primary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1; F.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14881v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14881v1",
                "updated": "2025-03-19T04:18:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    4,
                    18,
                    57,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T04:18:57Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    4,
                    18,
                    57,
                    2,
                    78,
                    0
                ],
                "title": "Exploring the Limits of KV Cache Compression in Visual Autoregressive\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Limits of KV Cache Compression in Visual Autoregressive\n  Transformers"
                },
                "summary": "A fundamental challenge in Visual Autoregressive models is the substantial\nmemory overhead required during inference to store previously generated\nrepresentations. Despite various attempts to mitigate this issue through\ncompression techniques, prior works have not explicitly formalized the problem\nof KV-cache compression in this context. In this work, we take the first step\nin formally defining the KV-cache compression problem for Visual Autoregressive\ntransformers. We then establish a fundamental negative result, proving that any\nmechanism for sequential visual token generation under attention-based\narchitectures must use at least $\\Omega(n^2 d)$ memory, when $d = \\Omega(\\log\nn)$, where $n$ is the number of tokens generated and $d$ is the embedding\ndimensionality. This result demonstrates that achieving truly sub-quadratic\nmemory usage is impossible without additional structural constraints. Our proof\nis constructed via a reduction from a computational lower bound problem,\nleveraging randomized embedding techniques inspired by dimensionality reduction\nprinciples. Finally, we discuss how sparsity priors on visual representations\ncan influence memory efficiency, presenting both impossibility results and\npotential directions for mitigating memory overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A fundamental challenge in Visual Autoregressive models is the substantial\nmemory overhead required during inference to store previously generated\nrepresentations. Despite various attempts to mitigate this issue through\ncompression techniques, prior works have not explicitly formalized the problem\nof KV-cache compression in this context. In this work, we take the first step\nin formally defining the KV-cache compression problem for Visual Autoregressive\ntransformers. We then establish a fundamental negative result, proving that any\nmechanism for sequential visual token generation under attention-based\narchitectures must use at least $\\Omega(n^2 d)$ memory, when $d = \\Omega(\\log\nn)$, where $n$ is the number of tokens generated and $d$ is the embedding\ndimensionality. This result demonstrates that achieving truly sub-quadratic\nmemory usage is impossible without additional structural constraints. Our proof\nis constructed via a reduction from a computational lower bound problem,\nleveraging randomized embedding techniques inspired by dimensionality reduction\nprinciples. Finally, we discuss how sparsity priors on visual representations\ncan influence memory efficiency, presenting both impossibility results and\npotential directions for mitigating memory overhead."
                },
                "authors": [
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Xiaoyu Li"
                    },
                    {
                        "name": "Yekun Ke"
                    },
                    {
                        "name": "Yingyu Liang"
                    },
                    {
                        "name": "Zhenmei Shi"
                    },
                    {
                        "name": "Zhao Song"
                    }
                ],
                "author_detail": {
                    "name": "Zhao Song"
                },
                "author": "Zhao Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14881v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14881v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14805v1",
                "updated": "2025-03-19T00:30:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    0,
                    30,
                    43,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T00:30:43Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    0,
                    30,
                    43,
                    2,
                    78,
                    0
                ],
                "title": "Degradation of 2.4-kV $Ga_{2}O_{3}$ Schottky Barrier Diode at High\n  Temperatures up to 500 °C",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Degradation of 2.4-kV $Ga_{2}O_{3}$ Schottky Barrier Diode at High\n  Temperatures up to 500 °C"
                },
                "summary": "Ga2O3 Schottky barrier diodes featuring a field plate and a composite\nSiO2/SiNx dielectric layer beneath the field plate were fabricated, achieving a\nbreakdown voltage of 2.4 kV at room temperature. Electrical performance and\ndegradation were analyzed via I-V and C-V measurements from 25 {\\deg}C to 500\n{\\deg}C, revealing temperature-dependent transport, interface stability, and\ndevice stability. Upon returning to room temperature, the diodes exhibited\nnearly unchanged forward characteristics, while the breakdown voltage declined\nsignificantly from 2.4 kV to 700 V. This behavior indicates a\ntemperature-induced reduction in the barrier height. Detailed analysis revealed\nthat variable range hopping (VRH) dominated the leakage mechanism at moderate\ntemperatures, while thermal emission (TE) became increasingly significant at\ntemperatures exceeding 400 {\\deg}C.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ga2O3 Schottky barrier diodes featuring a field plate and a composite\nSiO2/SiNx dielectric layer beneath the field plate were fabricated, achieving a\nbreakdown voltage of 2.4 kV at room temperature. Electrical performance and\ndegradation were analyzed via I-V and C-V measurements from 25 {\\deg}C to 500\n{\\deg}C, revealing temperature-dependent transport, interface stability, and\ndevice stability. Upon returning to room temperature, the diodes exhibited\nnearly unchanged forward characteristics, while the breakdown voltage declined\nsignificantly from 2.4 kV to 700 V. This behavior indicates a\ntemperature-induced reduction in the barrier height. Detailed analysis revealed\nthat variable range hopping (VRH) dominated the leakage mechanism at moderate\ntemperatures, while thermal emission (TE) became increasingly significant at\ntemperatures exceeding 400 {\\deg}C."
                },
                "authors": [
                    {
                        "name": "Hunter Ellis"
                    },
                    {
                        "name": "Wei Jia"
                    },
                    {
                        "name": "Imteaz Rahaman"
                    },
                    {
                        "name": "Apostoli Hillas"
                    },
                    {
                        "name": "Botong Li"
                    },
                    {
                        "name": "Michael A. Scarpulla"
                    },
                    {
                        "name": "Berardi Sensale Rodriguez"
                    },
                    {
                        "name": "Kai Fu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Fu"
                },
                "author": "Kai Fu",
                "arxiv_comment": "7 Pages, 9 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14708v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14708v1",
                "updated": "2025-03-18T20:16:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    20,
                    16,
                    50,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T20:16:50Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    20,
                    16,
                    50,
                    1,
                    77,
                    0
                ],
                "title": "NeCTAr: A Heterogeneous RISC-V SoC for Language Model Inference in Intel\n  16",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeCTAr: A Heterogeneous RISC-V SoC for Language Model Inference in Intel\n  16"
                },
                "summary": "This paper introduces NeCTAr (Near-Cache Transformer Accelerator), a 16nm\nheterogeneous multicore RISC-V SoC for sparse and dense machine learning\nkernels with both near-core and near-memory accelerators. A prototype chip runs\nat 400MHz at 0.85V and performs matrix-vector multiplications with 109 GOPs/W.\nThe effectiveness of the design is demonstrated by running inference on a\nsparse language model, ReLU-Llama.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces NeCTAr (Near-Cache Transformer Accelerator), a 16nm\nheterogeneous multicore RISC-V SoC for sparse and dense machine learning\nkernels with both near-core and near-memory accelerators. A prototype chip runs\nat 400MHz at 0.85V and performs matrix-vector multiplications with 109 GOPs/W.\nThe effectiveness of the design is demonstrated by running inference on a\nsparse language model, ReLU-Llama."
                },
                "authors": [
                    {
                        "name": "Viansa Schmulbach"
                    },
                    {
                        "name": "Jason Kim"
                    },
                    {
                        "name": "Ethan Gao"
                    },
                    {
                        "name": "Lucy Revina"
                    },
                    {
                        "name": "Nikhil Jha"
                    },
                    {
                        "name": "Ethan Wu"
                    },
                    {
                        "name": "Borivoje Nikolic"
                    }
                ],
                "author_detail": {
                    "name": "Borivoje Nikolic"
                },
                "author": "Borivoje Nikolic",
                "arxiv_doi": "10.1109/HCS61935.2024.10665203",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/HCS61935.2024.10665203",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.14708v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14708v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14647v1",
                "updated": "2025-03-18T18:52:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    18,
                    52,
                    3,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T18:52:03Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    18,
                    52,
                    3,
                    1,
                    77,
                    0
                ],
                "title": "Towards More Economical Context-Augmented LLM Generation by Reusing\n  Stored KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards More Economical Context-Augmented LLM Generation by Reusing\n  Stored KV Cache"
                },
                "summary": "Across large language model (LLM) applications, we observe an emerging trend\nfor reusing KV caches to save the prefill delays of processing repeated input\ntexts in different LLM inputs. This has led to a broad design space, including\ncolocating stored KV caches with (or close to) GPUs to various KV cache\ncompression. However, a key question remains unanswered: can these delay\nreductions also be economically favorable? Specifically, we ask whether a\ndeveloper can use public cloud services to store precomputed KV caches and\nreuse them to save delay without incurring more costs in terms of compute,\nstorage, and network. To answer this question, we propose an validated\nanalytical model for the cloud cost (in compute, storage, and network) of\nstoring and reusing KV caches based on various workload parameters, such as\nreuse frequency, generated text lengths, model sizes, etc. Preliminary results\nshow that KV cache reusing is able to save both delay and cloud cost across a\nrange of workloads with long context. And we call more efforts on building more\neconomical context augmented LLM by KV cache reusing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Across large language model (LLM) applications, we observe an emerging trend\nfor reusing KV caches to save the prefill delays of processing repeated input\ntexts in different LLM inputs. This has led to a broad design space, including\ncolocating stored KV caches with (or close to) GPUs to various KV cache\ncompression. However, a key question remains unanswered: can these delay\nreductions also be economically favorable? Specifically, we ask whether a\ndeveloper can use public cloud services to store precomputed KV caches and\nreuse them to save delay without incurring more costs in terms of compute,\nstorage, and network. To answer this question, we propose an validated\nanalytical model for the cloud cost (in compute, storage, and network) of\nstoring and reusing KV caches based on various workload parameters, such as\nreuse frequency, generated text lengths, model sizes, etc. Preliminary results\nshow that KV cache reusing is able to save both delay and cloud cost across a\nrange of workloads with long context. And we call more efforts on building more\neconomical context augmented LLM by KV cache reusing."
                },
                "authors": [
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08640v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08640v2",
                "updated": "2025-03-18T17:13:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    13,
                    42,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-11T17:30:58Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    30,
                    58,
                    1,
                    70,
                    0
                ],
                "title": "Efficient Many-Shot In-Context Learning with Dynamic Block-Sparse\n  Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Many-Shot In-Context Learning with Dynamic Block-Sparse\n  Attention"
                },
                "summary": "Many-shot in-context learning has recently shown promise as an alternative to\nfinetuning, with the major advantage that the same model can be served for\nmultiple tasks. However, this shifts the computational burden from\ntraining-time to inference-time, making deployment of many-shot ICL challenging\nto justify in-practice. This cost is further increased if a custom\ndemonstration set is retrieved for each inference example. We present Dynamic\nBlock-Sparse Attention, a training-free framework for retrieval-based many-shot\nin-context learning. By combining carefully designed block-sparse attention and\nretrieval of cached groups of demonstrations, we achieve comparable per-example\nlatency to finetuning while maintaining on average >95% of the best method's\naccuracy across strong ICL and finetuning baselines. We hope that this will\nfurther enable the deployment of many-shot ICL at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many-shot in-context learning has recently shown promise as an alternative to\nfinetuning, with the major advantage that the same model can be served for\nmultiple tasks. However, this shifts the computational burden from\ntraining-time to inference-time, making deployment of many-shot ICL challenging\nto justify in-practice. This cost is further increased if a custom\ndemonstration set is retrieved for each inference example. We present Dynamic\nBlock-Sparse Attention, a training-free framework for retrieval-based many-shot\nin-context learning. By combining carefully designed block-sparse attention and\nretrieval of cached groups of demonstrations, we achieve comparable per-example\nlatency to finetuning while maintaining on average >95% of the best method's\naccuracy across strong ICL and finetuning baselines. We hope that this will\nfurther enable the deployment of many-shot ICL at scale."
                },
                "authors": [
                    {
                        "name": "Emily Xiao"
                    },
                    {
                        "name": "Chin-Jou Li"
                    },
                    {
                        "name": "Yilin Zhang"
                    },
                    {
                        "name": "Graham Neubig"
                    },
                    {
                        "name": "Amanda Bertsch"
                    }
                ],
                "author_detail": {
                    "name": "Amanda Bertsch"
                },
                "author": "Amanda Bertsch",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08640v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08640v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09573v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09573v2",
                "updated": "2025-03-18T15:58:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    15,
                    58,
                    18,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-12T17:43:40Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    43,
                    40,
                    2,
                    71,
                    0
                ],
                "title": "Block Diffusion: Interpolating Between Autoregressive and Diffusion\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block Diffusion: Interpolating Between Autoregressive and Diffusion\n  Language Models"
                },
                "summary": "Diffusion language models offer unique benefits over autoregressive models\ndue to their potential for parallelized generation and controllability, yet\nthey lag in likelihood modeling and are limited to fixed-length generation. In\nthis work, we introduce a class of block diffusion language models that\ninterpolate between discrete denoising diffusion and autoregressive models.\nBlock diffusion overcomes key limitations of both approaches by supporting\nflexible-length generation and improving inference efficiency with KV caching\nand parallel token sampling. We propose a recipe for building effective block\ndiffusion models that includes an efficient training algorithm, estimators of\ngradient variance, and data-driven noise schedules to minimize the variance.\nBlock diffusion sets a new state-of-the-art performance among diffusion models\non language modeling benchmarks and enables generation of arbitrary-length\nsequences. We provide the code, along with the model weights and blog post on\nthe project page: https://m-arriola.com/bd3lms/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models offer unique benefits over autoregressive models\ndue to their potential for parallelized generation and controllability, yet\nthey lag in likelihood modeling and are limited to fixed-length generation. In\nthis work, we introduce a class of block diffusion language models that\ninterpolate between discrete denoising diffusion and autoregressive models.\nBlock diffusion overcomes key limitations of both approaches by supporting\nflexible-length generation and improving inference efficiency with KV caching\nand parallel token sampling. We propose a recipe for building effective block\ndiffusion models that includes an efficient training algorithm, estimators of\ngradient variance, and data-driven noise schedules to minimize the variance.\nBlock diffusion sets a new state-of-the-art performance among diffusion models\non language modeling benchmarks and enables generation of arbitrary-length\nsequences. We provide the code, along with the model weights and blog post on\nthe project page: https://m-arriola.com/bd3lms/"
                },
                "authors": [
                    {
                        "name": "Marianne Arriola"
                    },
                    {
                        "name": "Aaron Gokaslan"
                    },
                    {
                        "name": "Justin T Chiu"
                    },
                    {
                        "name": "Zhihan Yang"
                    },
                    {
                        "name": "Zhixuan Qi"
                    },
                    {
                        "name": "Jiaqi Han"
                    },
                    {
                        "name": "Subham Sekhar Sahoo"
                    },
                    {
                        "name": "Volodymyr Kuleshov"
                    }
                ],
                "author_detail": {
                    "name": "Volodymyr Kuleshov"
                },
                "author": "Volodymyr Kuleshov",
                "arxiv_comment": "ICLR 2025 Oral. We provide the code at\n  https://github.com/kuleshov-group/bd3lms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09573v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09573v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18753v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18753v2",
                "updated": "2025-03-18T09:43:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    9,
                    43,
                    33,
                    1,
                    77,
                    0
                ],
                "published": "2024-07-26T14:08:53Z",
                "published_parsed": [
                    2024,
                    7,
                    26,
                    14,
                    8,
                    53,
                    4,
                    208,
                    0
                ],
                "title": "Suffixient Arrays: a New Efficient Suffix Array Compression Technique",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Suffixient Arrays: a New Efficient Suffix Array Compression Technique"
                },
                "summary": "The Suffix Array is a classic text index enabling on-line pattern matching\nqueries via simple binary search. The main drawback of the Suffix Array is that\nit takes linear space in the text's length, even if the text itself is\nextremely compressible. Several works in the literature showed that the Suffix\nArray can be compressed, but they all rely on complex succinct data structures\nwhich in practice tend to exhibit poor cache locality and thus significantly\nslow down queries. In this paper, we propose a new simple and very efficient\nsolution to this problem by presenting the \\emph{Suffixient Array}: a tiny\nsubset of the Suffix Array \\emph{sufficient} to locate on-line one pattern\noccurrence (in general, all its Maximal Exact Matches) via binary search,\nprovided that random access to the text is available. We prove that: (i) the\nSuffixient Array length $\\chi$ is a strong repetitiveness measure, (ii) unlike\nmost existing repetition-aware indexes such as the $r$-index, our new index is\nefficient in the I/O model, and (iii) Suffixient Arrays can be computed in\nlinear time and compressed working space. We show experimentally that, when\nusing well-established compressed random access data structures on repetitive\ncollections, the Suffixient Array $\\SuA$ is \\emph{simultaneously} (i) faster\nand orders of magnitude smaller than the Suffix Array $\\SA$ and (ii) smaller\nand \\emph{one to two orders of magnitude faster} than the $r$-index. With an\naverage pattern matching query time as low as 3.5 ns per character, our new\nindex gets very close to the ultimate lower bound: the RAM throughput of our\nworkstation (1.18 ns per character).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Suffix Array is a classic text index enabling on-line pattern matching\nqueries via simple binary search. The main drawback of the Suffix Array is that\nit takes linear space in the text's length, even if the text itself is\nextremely compressible. Several works in the literature showed that the Suffix\nArray can be compressed, but they all rely on complex succinct data structures\nwhich in practice tend to exhibit poor cache locality and thus significantly\nslow down queries. In this paper, we propose a new simple and very efficient\nsolution to this problem by presenting the \\emph{Suffixient Array}: a tiny\nsubset of the Suffix Array \\emph{sufficient} to locate on-line one pattern\noccurrence (in general, all its Maximal Exact Matches) via binary search,\nprovided that random access to the text is available. We prove that: (i) the\nSuffixient Array length $\\chi$ is a strong repetitiveness measure, (ii) unlike\nmost existing repetition-aware indexes such as the $r$-index, our new index is\nefficient in the I/O model, and (iii) Suffixient Arrays can be computed in\nlinear time and compressed working space. We show experimentally that, when\nusing well-established compressed random access data structures on repetitive\ncollections, the Suffixient Array $\\SuA$ is \\emph{simultaneously} (i) faster\nand orders of magnitude smaller than the Suffix Array $\\SA$ and (ii) smaller\nand \\emph{one to two orders of magnitude faster} than the $r$-index. With an\naverage pattern matching query time as low as 3.5 ns per character, our new\nindex gets very close to the ultimate lower bound: the RAM throughput of our\nworkstation (1.18 ns per character)."
                },
                "authors": [
                    {
                        "name": "Davide Cenzato"
                    },
                    {
                        "name": "Lore Depuydt"
                    },
                    {
                        "name": "Travis Gagie"
                    },
                    {
                        "name": "Sung-Hwan Kim"
                    },
                    {
                        "name": "Giovanni Manzini"
                    },
                    {
                        "name": "Francisco Olivares"
                    },
                    {
                        "name": "Nicola Prezza"
                    }
                ],
                "author_detail": {
                    "name": "Nicola Prezza"
                },
                "author": "Nicola Prezza",
                "arxiv_comment": "40 pages, 7 figure, 1 table and 7 pseudocodes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18753v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18753v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13145v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13145v2",
                "updated": "2025-03-18T07:02:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    7,
                    2,
                    33,
                    1,
                    77,
                    0
                ],
                "published": "2025-02-18T18:59:57Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    59,
                    57,
                    1,
                    49,
                    0
                ],
                "title": "Multimodal Mamba: Decoder-only Multimodal State Space Model via\n  Quadratic to Linear Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Mamba: Decoder-only Multimodal State Space Model via\n  Quadratic to Linear Distillation"
                },
                "summary": "Recent Multimodal Large Language Models (MLLMs) have achieved remarkable\nperformance but face deployment challenges due to their quadratic computational\ncomplexity, growing Key-Value cache requirements, and reliance on separate\nvision encoders. We propose mmMamba, a framework for developing\nlinear-complexity native multimodal state space models through progressive\ndistillation from existing MLLMs using moderate academic computational\nresources. Our approach enables the direct conversion of trained decoder-only\nMLLMs to linear-complexity architectures without requiring pre-trained\nRNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba\nfrom trained Transformer and a three-stage distillation recipe, which can\neffectively transfer the knowledge from Transformer to Mamba while preserving\nmultimodal capabilities. Our method also supports flexible hybrid architectures\nthat combine Transformer and Mamba layers for customizable\nefficiency-performance trade-offs. Distilled from the Transformer-based\ndecoder-only HoVLE, mmMamba-linear achieves competitive performance against\nexisting linear and quadratic-complexity VLMs, while mmMamba-hybrid further\nimproves performance significantly, approaching HoVLE's capabilities. At 103K\ntokens, mmMamba-linear demonstrates 20.6$\\times$ speedup and 75.8% GPU memory\nreduction compared to HoVLE, while mmMamba-hybrid achieves 13.5$\\times$ speedup\nand 60.2% memory savings. Code and models are released at\nhttps://github.com/hustvl/mmMamba",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Multimodal Large Language Models (MLLMs) have achieved remarkable\nperformance but face deployment challenges due to their quadratic computational\ncomplexity, growing Key-Value cache requirements, and reliance on separate\nvision encoders. We propose mmMamba, a framework for developing\nlinear-complexity native multimodal state space models through progressive\ndistillation from existing MLLMs using moderate academic computational\nresources. Our approach enables the direct conversion of trained decoder-only\nMLLMs to linear-complexity architectures without requiring pre-trained\nRNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba\nfrom trained Transformer and a three-stage distillation recipe, which can\neffectively transfer the knowledge from Transformer to Mamba while preserving\nmultimodal capabilities. Our method also supports flexible hybrid architectures\nthat combine Transformer and Mamba layers for customizable\nefficiency-performance trade-offs. Distilled from the Transformer-based\ndecoder-only HoVLE, mmMamba-linear achieves competitive performance against\nexisting linear and quadratic-complexity VLMs, while mmMamba-hybrid further\nimproves performance significantly, approaching HoVLE's capabilities. At 103K\ntokens, mmMamba-linear demonstrates 20.6$\\times$ speedup and 75.8% GPU memory\nreduction compared to HoVLE, while mmMamba-hybrid achieves 13.5$\\times$ speedup\nand 60.2% memory savings. Code and models are released at\nhttps://github.com/hustvl/mmMamba"
                },
                "authors": [
                    {
                        "name": "Bencheng Liao"
                    },
                    {
                        "name": "Hongyuan Tao"
                    },
                    {
                        "name": "Qian Zhang"
                    },
                    {
                        "name": "Tianheng Cheng"
                    },
                    {
                        "name": "Yingyue Li"
                    },
                    {
                        "name": "Haoran Yin"
                    },
                    {
                        "name": "Wenyu Liu"
                    },
                    {
                        "name": "Xinggang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinggang Wang"
                },
                "author": "Xinggang Wang",
                "arxiv_comment": "Code and model are available at https://github.com/hustvl/mmMamba",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13145v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13145v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19108v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19108v2",
                "updated": "2025-03-18T04:49:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    4,
                    49,
                    23,
                    1,
                    77,
                    0
                ],
                "published": "2024-11-28T12:50:05Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    12,
                    50,
                    5,
                    3,
                    333,
                    0
                ],
                "title": "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model"
                },
                "summary": "As a fundamental backbone for video generation, diffusion models are\nchallenged by low inference speed due to the sequential nature of denoising.\nPrevious methods speed up the models by caching and reusing model outputs at\nuniformly selected timesteps. However, such a strategy neglects the fact that\ndifferences among model outputs are not uniform across timesteps, which hinders\nselecting the appropriate model outputs to cache, leading to a poor balance\nbetween inference efficiency and visual quality. In this study, we introduce\nTimestep Embedding Aware Cache (TeaCache), a training-free caching approach\nthat estimates and leverages the fluctuating differences among model outputs\nacross timesteps. Rather than directly using the time-consuming model outputs,\nTeaCache focuses on model inputs, which have a strong correlation with the\nmodeloutputs while incurring negligible computational cost. TeaCache first\nmodulates the noisy inputs using the timestep embeddings to ensure their\ndifferences better approximating those of model outputs. TeaCache then\nintroduces a rescaling strategy to refine the estimated differences and\nutilizes them to indicate output caching. Experiments show that TeaCache\nachieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07%\nVbench score) degradation of visual quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a fundamental backbone for video generation, diffusion models are\nchallenged by low inference speed due to the sequential nature of denoising.\nPrevious methods speed up the models by caching and reusing model outputs at\nuniformly selected timesteps. However, such a strategy neglects the fact that\ndifferences among model outputs are not uniform across timesteps, which hinders\nselecting the appropriate model outputs to cache, leading to a poor balance\nbetween inference efficiency and visual quality. In this study, we introduce\nTimestep Embedding Aware Cache (TeaCache), a training-free caching approach\nthat estimates and leverages the fluctuating differences among model outputs\nacross timesteps. Rather than directly using the time-consuming model outputs,\nTeaCache focuses on model inputs, which have a strong correlation with the\nmodeloutputs while incurring negligible computational cost. TeaCache first\nmodulates the noisy inputs using the timestep embeddings to ensure their\ndifferences better approximating those of model outputs. TeaCache then\nintroduces a rescaling strategy to refine the estimated differences and\nutilizes them to indicate output caching. Experiments show that TeaCache\nachieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07%\nVbench score) degradation of visual quality."
                },
                "authors": [
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Shiwei Zhang"
                    },
                    {
                        "name": "Xiaofeng Wang"
                    },
                    {
                        "name": "Yujie Wei"
                    },
                    {
                        "name": "Haonan Qiu"
                    },
                    {
                        "name": "Yuzhong Zhao"
                    },
                    {
                        "name": "Yingya Zhang"
                    },
                    {
                        "name": "Qixiang Ye"
                    },
                    {
                        "name": "Fang Wan"
                    }
                ],
                "author_detail": {
                    "name": "Fang Wan"
                },
                "author": "Fang Wan",
                "arxiv_comment": "Accepted in CVPR 2025. Project: https://liewfeng.github.io/TeaCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19108v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19108v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10511v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10511v3",
                "updated": "2025-03-18T01:58:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    1,
                    58,
                    36,
                    1,
                    77,
                    0
                ],
                "published": "2024-06-15T05:28:55Z",
                "published_parsed": [
                    2024,
                    6,
                    15,
                    5,
                    28,
                    55,
                    5,
                    167,
                    0
                ],
                "title": "Efficient Hardware Accelerator Based on Medium Granularity Dataflow for\n  SpTRSV",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Hardware Accelerator Based on Medium Granularity Dataflow for\n  SpTRSV"
                },
                "summary": "Sparse triangular solve (SpTRSV) is widely used in various domains. Numerous\nstudies have been conducted using CPUs, GPUs, and specific hardware\naccelerators, where dataflows can be categorized into coarse and fine\ngranularity. Coarse dataflows offer good spatial locality but suffer from low\nparallelism, while fine dataflows provide high parallelism but disrupt the\nspatial structure, leading to increased nodes and poor data reuse. This paper\nproposes a novel hardware accelerator for SpTRSV or SpTRSV-like DAGs. The\naccelerator implements a medium granularity dataflow through hardware-software\ncodesign and achieves both excellent spatial locality and high parallelism.\nAdditionally, a partial sum caching mechanism is introduced to reduce the\nblocking frequency of processing elements (PEs), and a reordering algorithm of\nintra-node edges computation is developed to enhance data reuse. Experimental\nresults on 245 benchmarks with node counts reaching up to 85,392 demonstrate\nthat this work achieves average performance improvements of 7.0$\\times$ (up to\n27.8$\\times$) over CPUs and 5.8$\\times$ (up to 98.8$\\times$) over GPUs.\nCompared to the state-of-the-art technique (DPU-v2), this work shows a\n2.5$\\times$ (up to 5.9$\\times$) average performance improvement and 1.7$\\times$\n(up to 4.1$\\times$) average energy efficiency enhancement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse triangular solve (SpTRSV) is widely used in various domains. Numerous\nstudies have been conducted using CPUs, GPUs, and specific hardware\naccelerators, where dataflows can be categorized into coarse and fine\ngranularity. Coarse dataflows offer good spatial locality but suffer from low\nparallelism, while fine dataflows provide high parallelism but disrupt the\nspatial structure, leading to increased nodes and poor data reuse. This paper\nproposes a novel hardware accelerator for SpTRSV or SpTRSV-like DAGs. The\naccelerator implements a medium granularity dataflow through hardware-software\ncodesign and achieves both excellent spatial locality and high parallelism.\nAdditionally, a partial sum caching mechanism is introduced to reduce the\nblocking frequency of processing elements (PEs), and a reordering algorithm of\nintra-node edges computation is developed to enhance data reuse. Experimental\nresults on 245 benchmarks with node counts reaching up to 85,392 demonstrate\nthat this work achieves average performance improvements of 7.0$\\times$ (up to\n27.8$\\times$) over CPUs and 5.8$\\times$ (up to 98.8$\\times$) over GPUs.\nCompared to the state-of-the-art technique (DPU-v2), this work shows a\n2.5$\\times$ (up to 5.9$\\times$) average performance improvement and 1.7$\\times$\n(up to 4.1$\\times$) average energy efficiency enhancement."
                },
                "authors": [
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Xiaofeng Yang"
                    },
                    {
                        "name": "Shengli Lu"
                    }
                ],
                "author_detail": {
                    "name": "Shengli Lu"
                },
                "author": "Shengli Lu",
                "arxiv_doi": "10.1109/TVLSI.2024.3497166",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TVLSI.2024.3497166",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.10511v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10511v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Trans. Very Large Scale Integr. (VLSI) Syst. 33 (2025)\n  807-820",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13737v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13737v1",
                "updated": "2025-03-17T21:47:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    21,
                    47,
                    43,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T21:47:43Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    21,
                    47,
                    43,
                    0,
                    76,
                    0
                ],
                "title": "AccelGen: Heterogeneous SLO-Guaranteed High-Throughput LLM Inference\n  Serving for Diverse Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AccelGen: Heterogeneous SLO-Guaranteed High-Throughput LLM Inference\n  Serving for Diverse Applications"
                },
                "summary": "In this paper, we consider a mixed-prompt scenario for a large language model\n(LLM) inference serving system that supports diverse applications with both\nshort prompts and long prompts and heterogeneous SLOs for iteration time. To\nimprove throughput when handling long prompts, previous research introduces a\nchunking method, but has not addressed heterogeneous SLOs. To address the\nlimitation, we propose AccelGen, a high-throughput LLM inference serving system\nwith heterogeneous SLO guarantees for diverse applications. AccelGen introduces\nfour core components: (1) SLO-guaranteed dynamic chunking, which dynamically\nadjusts chunk sizes to maximize GPU compute utilization while meeting\niteration-level SLOs; (2) Iteration-level SLO-based task prioritization, which\nprioritizes tight-SLO requests and batches requests with similar SLOs; (3)\nMulti-resource-aware batching, which selects queued requests to maximize the\nutilizations of both GPU compute resource and key-value cache (KVC).\nTrace-driven real experiments demonstrate that AccelGen achieves 1.42-11.21X\nhigher throughput, 1.43-13.71X higher goodput, 37-90% higher SLO attainment,\nand 1.61-12.22X lower response latency compared to the state-of-the-art\napproaches. It achieves performance near the Oracle, which optimally maximizes\ngoodput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we consider a mixed-prompt scenario for a large language model\n(LLM) inference serving system that supports diverse applications with both\nshort prompts and long prompts and heterogeneous SLOs for iteration time. To\nimprove throughput when handling long prompts, previous research introduces a\nchunking method, but has not addressed heterogeneous SLOs. To address the\nlimitation, we propose AccelGen, a high-throughput LLM inference serving system\nwith heterogeneous SLO guarantees for diverse applications. AccelGen introduces\nfour core components: (1) SLO-guaranteed dynamic chunking, which dynamically\nadjusts chunk sizes to maximize GPU compute utilization while meeting\niteration-level SLOs; (2) Iteration-level SLO-based task prioritization, which\nprioritizes tight-SLO requests and batches requests with similar SLOs; (3)\nMulti-resource-aware batching, which selects queued requests to maximize the\nutilizations of both GPU compute resource and key-value cache (KVC).\nTrace-driven real experiments demonstrate that AccelGen achieves 1.42-11.21X\nhigher throughput, 1.43-13.71X higher goodput, 37-90% higher SLO attainment,\nand 1.61-12.22X lower response latency compared to the state-of-the-art\napproaches. It achieves performance near the Oracle, which optimally maximizes\ngoodput."
                },
                "authors": [
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Tanmoy Sen"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Sen"
                },
                "author": "Tanmoy Sen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13737v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13737v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13723v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13723v1",
                "updated": "2025-03-17T21:11:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    21,
                    11,
                    30,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T21:11:30Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    21,
                    11,
                    30,
                    0,
                    76,
                    0
                ],
                "title": "Fast Maximum Likelihood Positioning for a Staggered Layer Scintillation\n  PET Detector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Maximum Likelihood Positioning for a Staggered Layer Scintillation\n  PET Detector"
                },
                "summary": "In this study, we propose a fast implementation of a Maximum Likelihood\nPositioning (MLP) algorithm to estimate the energy and identify the active\nscintillator pixel in staggered layer scintillation detectors for PET. The\nstaggered layer design with pixelated scintillators enables the determination\nof the gamma's depth of interaction and facilitates an iteration-free\nformulation of the MLP algorithm. The efficacy of the algorithm optimization\nwas tested on a scintillation detector block designed for an ultra-high field\nBrainPET 7T, comprising three scintillator pixel layers. The three layers\ncontain 24 x 24, 24 x 23 and 23 x 22 scintillator pixels, respectively, with a\npixel pitch of 2 mm in both directions and layer thicknesses of 9, 8 and 7 mm.\nCalibration measurements, in combination with an automated calibration script,\nwere used to obtain the expected counts of scintillation photons required in\nthe MLP algorithm. Using Single-Instruction-Multiple-Data parallelization,\nmulti-threading and optimized cache lines, a maximum processing speed of\napproximately 22.5 million singles per second was achieved on a platform with\nfour Intel Xeon Platinum 8168 CPUs and 60 threads, encompassing all required\nprocessing steps. The automatic calibration failed for 1 to 15 individual\nscintillator pixels in approximately 10 per cent of the 120 scintillation\ndetector blocks, necessitating manual correction. After applying the energy\ncorrection to the positioned single events, an energy resolution of of 12 +/- 2\nper cent FWHM was obtained for the entire scintillation block. This value is\nvery close to the energy resolutions measured for the individual scintillator\npixels, proving that the MLP accurately identifies the scintillating pixel and\nthat the energy correction method effectively compensates for the light\ncollection variations of the SiPM array.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we propose a fast implementation of a Maximum Likelihood\nPositioning (MLP) algorithm to estimate the energy and identify the active\nscintillator pixel in staggered layer scintillation detectors for PET. The\nstaggered layer design with pixelated scintillators enables the determination\nof the gamma's depth of interaction and facilitates an iteration-free\nformulation of the MLP algorithm. The efficacy of the algorithm optimization\nwas tested on a scintillation detector block designed for an ultra-high field\nBrainPET 7T, comprising three scintillator pixel layers. The three layers\ncontain 24 x 24, 24 x 23 and 23 x 22 scintillator pixels, respectively, with a\npixel pitch of 2 mm in both directions and layer thicknesses of 9, 8 and 7 mm.\nCalibration measurements, in combination with an automated calibration script,\nwere used to obtain the expected counts of scintillation photons required in\nthe MLP algorithm. Using Single-Instruction-Multiple-Data parallelization,\nmulti-threading and optimized cache lines, a maximum processing speed of\napproximately 22.5 million singles per second was achieved on a platform with\nfour Intel Xeon Platinum 8168 CPUs and 60 threads, encompassing all required\nprocessing steps. The automatic calibration failed for 1 to 15 individual\nscintillator pixels in approximately 10 per cent of the 120 scintillation\ndetector blocks, necessitating manual correction. After applying the energy\ncorrection to the positioned single events, an energy resolution of of 12 +/- 2\nper cent FWHM was obtained for the entire scintillation block. This value is\nvery close to the energy resolutions measured for the individual scintillator\npixels, proving that the MLP accurately identifies the scintillating pixel and\nthat the energy correction method effectively compensates for the light\ncollection variations of the SiPM array."
                },
                "authors": [
                    {
                        "name": "Christoph W. Lerche"
                    },
                    {
                        "name": "Wenwei Bi"
                    },
                    {
                        "name": "Mirjam Schoeneck"
                    },
                    {
                        "name": "Debora Niekaemper"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Elisabeth Pfaehler"
                    },
                    {
                        "name": "Lutz Tellmann"
                    },
                    {
                        "name": "Juergen J. Scheins"
                    },
                    {
                        "name": "N. Jon Shah"
                    }
                ],
                "author_detail": {
                    "name": "N. Jon Shah"
                },
                "arxiv_affiliation": "Department of Neurology RWTH Aachen University Aachen Germany",
                "author": "N. Jon Shah",
                "arxiv_comment": "20 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13723v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13723v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "92C55 (Primary) 94A08 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13873v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13873v2",
                "updated": "2025-03-17T20:31:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    20,
                    31,
                    46,
                    0,
                    76,
                    0
                ],
                "published": "2025-02-19T16:54:58Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    16,
                    54,
                    58,
                    2,
                    50,
                    0
                ],
                "title": "NVR: Vector Runahead on NPUs for Sparse Memory Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NVR: Vector Runahead on NPUs for Sparse Memory Access"
                },
                "summary": "Deep Neural Networks are increasingly leveraging sparsity to reduce the\nscaling up of model parameter size. However, reducing wall-clock time through\nsparsity and pruning remains challenging due to irregular memory access\npatterns, leading to frequent cache misses. In this paper, we present NPU\nVector Runahead (NVR), a prefetching mechanism tailored for NPUs to address\ncache miss problems in sparse DNN workloads. Rather than optimising memory\npatterns with high overhead and poor portability, NVR adapts runahead execution\nto the unique architecture of NPUs. NVR provides a general micro-architectural\nsolution for sparse DNN workloads without requiring compiler or algorithmic\nsupport, operating as a decoupled, speculative, lightweight hardware sub-thread\nalongside the NPU, with minimal hardware overhead (under 5%). NVR achieves an\naverage 90% reduction in cache misses compared to SOTA prefetching in\ngeneral-purpose processors, delivering 4x average speedup on sparse workloads\nversus NPUs without prefetching. Moreover, we investigate the advantages of\nincorporating a small cache (16KB) into the NPU combined with NVR. Our\nevaluation shows that expanding this modest cache delivers 5x higher\nperformance benefits than increasing the L2 cache size by the same amount.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Neural Networks are increasingly leveraging sparsity to reduce the\nscaling up of model parameter size. However, reducing wall-clock time through\nsparsity and pruning remains challenging due to irregular memory access\npatterns, leading to frequent cache misses. In this paper, we present NPU\nVector Runahead (NVR), a prefetching mechanism tailored for NPUs to address\ncache miss problems in sparse DNN workloads. Rather than optimising memory\npatterns with high overhead and poor portability, NVR adapts runahead execution\nto the unique architecture of NPUs. NVR provides a general micro-architectural\nsolution for sparse DNN workloads without requiring compiler or algorithmic\nsupport, operating as a decoupled, speculative, lightweight hardware sub-thread\nalongside the NPU, with minimal hardware overhead (under 5%). NVR achieves an\naverage 90% reduction in cache misses compared to SOTA prefetching in\ngeneral-purpose processors, delivering 4x average speedup on sparse workloads\nversus NPUs without prefetching. Moreover, we investigate the advantages of\nincorporating a small cache (16KB) into the NPU combined with NVR. Our\nevaluation shows that expanding this modest cache delivers 5x higher\nperformance benefits than increasing the L2 cache size by the same amount."
                },
                "authors": [
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Zhengpeng Zhao"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Yushu Du"
                    },
                    {
                        "name": "Yuan Cheng"
                    },
                    {
                        "name": "Bing Guo"
                    },
                    {
                        "name": "He Xiao"
                    },
                    {
                        "name": "Chenhao Ma"
                    },
                    {
                        "name": "Xiaomeng Han"
                    },
                    {
                        "name": "Dean You"
                    },
                    {
                        "name": "Jiapeng Guan"
                    },
                    {
                        "name": "Ran Wei"
                    },
                    {
                        "name": "Dawei Yang"
                    },
                    {
                        "name": "Zhe Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Jiang"
                },
                "author": "Zhe Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13873v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13873v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2503.09647v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09647v3",
                "updated": "2025-04-01T17:54:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    54,
                    27,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-12T08:41:36Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    8,
                    41,
                    36,
                    2,
                    71,
                    0
                ],
                "title": "Leveraging LLMS for Top-Down Sector Allocation In Automated Trading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging LLMS for Top-Down Sector Allocation In Automated Trading"
                },
                "summary": "This paper introduces a methodology leveraging Large Language Models (LLMs)\nfor sector-level portfolio allocation through systematic analysis of\nmacroeconomic conditions and market sentiment. Our framework emphasizes\ntop-down sector allocation by processing multiple data streams simultaneously,\nincluding policy documents, economic indicators, and sentiment patterns.\nEmpirical results demonstrate superior risk-adjusted returns compared to\ntraditional cross momentum strategies, achieving a Sharpe ratio of 2.51 and\nportfolio return of 8.79% versus -0.61 and -1.39% respectively. These results\nsuggest that LLM-based systematic macro analysis presents a viable approach for\nenhancing automated portfolio allocation decisions at the sector level.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a methodology leveraging Large Language Models (LLMs)\nfor sector-level portfolio allocation through systematic analysis of\nmacroeconomic conditions and market sentiment. Our framework emphasizes\ntop-down sector allocation by processing multiple data streams simultaneously,\nincluding policy documents, economic indicators, and sentiment patterns.\nEmpirical results demonstrate superior risk-adjusted returns compared to\ntraditional cross momentum strategies, achieving a Sharpe ratio of 2.51 and\nportfolio return of 8.79% versus -0.61 and -1.39% respectively. These results\nsuggest that LLM-based systematic macro analysis presents a viable approach for\nenhancing automated portfolio allocation decisions at the sector level."
                },
                "authors": [
                    {
                        "name": "Ryan Quek Wei Heng"
                    },
                    {
                        "name": "Edoardo Vittori"
                    },
                    {
                        "name": "Keane Ong"
                    },
                    {
                        "name": "Rui Mao"
                    },
                    {
                        "name": "Erik Cambria"
                    },
                    {
                        "name": "Gianmarco Mengaldo"
                    }
                ],
                "author_detail": {
                    "name": "Gianmarco Mengaldo"
                },
                "author": "Gianmarco Mengaldo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09647v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09647v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.PM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10380v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10380v3",
                "updated": "2025-04-01T17:25:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    25,
                    53,
                    1,
                    91,
                    0
                ],
                "published": "2024-07-15T01:21:56Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    1,
                    21,
                    56,
                    0,
                    197,
                    0
                ],
                "title": "NTSEBENCH: Cognitive Reasoning Benchmark for Vision Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NTSEBENCH: Cognitive Reasoning Benchmark for Vision Language Models"
                },
                "summary": "Cognitive textual and visual reasoning tasks, including puzzles, series, and\nanalogies, demand the ability to quickly reason, decipher, and evaluate\npatterns both textually and spatially. Due to extensive training on vast\namounts of human-curated data, LLMs and VLMs excel in common-sense reasoning\ntasks, however still struggle with more complex reasoning that demands deeper\ncognitive understanding. We introduce NTSEBench, a new dataset designed to\nevaluate cognitive multi-modal reasoning and problem-solving skills of large\nmodels. The dataset contains 2728 multiple-choice questions, accompanied by a\ntotal of 4,642 images, categorized into 26 different types. These questions are\ndrawn from the nationwide NTSE examination in India and feature a mix of visual\nand textual general aptitude challenges, designed to assess intelligence and\ncritical thinking skills beyond mere rote learning. We establish baselines on\nthe dataset using state-of-the-art LLMs and VLMs. To facilitate a comparison\nbetween open source and propriety models, we propose four distinct modeling\nstrategies to handle different modalities -- text and images -- in the dataset\ninstances.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive textual and visual reasoning tasks, including puzzles, series, and\nanalogies, demand the ability to quickly reason, decipher, and evaluate\npatterns both textually and spatially. Due to extensive training on vast\namounts of human-curated data, LLMs and VLMs excel in common-sense reasoning\ntasks, however still struggle with more complex reasoning that demands deeper\ncognitive understanding. We introduce NTSEBench, a new dataset designed to\nevaluate cognitive multi-modal reasoning and problem-solving skills of large\nmodels. The dataset contains 2728 multiple-choice questions, accompanied by a\ntotal of 4,642 images, categorized into 26 different types. These questions are\ndrawn from the nationwide NTSE examination in India and feature a mix of visual\nand textual general aptitude challenges, designed to assess intelligence and\ncritical thinking skills beyond mere rote learning. We establish baselines on\nthe dataset using state-of-the-art LLMs and VLMs. To facilitate a comparison\nbetween open source and propriety models, we propose four distinct modeling\nstrategies to handle different modalities -- text and images -- in the dataset\ninstances."
                },
                "authors": [
                    {
                        "name": "Pranshu Pandya"
                    },
                    {
                        "name": "Vatsal Gupta"
                    },
                    {
                        "name": "Agney S Talwarr"
                    },
                    {
                        "name": "Tushar Kataria"
                    },
                    {
                        "name": "Dan Roth"
                    },
                    {
                        "name": "Vivek Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Vivek Gupta"
                },
                "author": "Vivek Gupta",
                "arxiv_comment": "28 pages, 3 figures, 12 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10380v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10380v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.06501v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.06501v3",
                "updated": "2025-04-01T16:54:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    16,
                    54,
                    54,
                    1,
                    91,
                    0
                ],
                "published": "2024-07-09T02:06:30Z",
                "published_parsed": [
                    2024,
                    7,
                    9,
                    2,
                    6,
                    30,
                    1,
                    191,
                    0
                ],
                "title": "STORYSUMM: Evaluating Faithfulness in Story Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STORYSUMM: Evaluating Faithfulness in Story Summarization"
                },
                "summary": "Human evaluation has been the gold standard for checking faithfulness in\nabstractive summarization. However, with a challenging source domain like\nnarrative, multiple annotators can agree a summary is faithful, while missing\ndetails that are obvious errors only once pointed out. We therefore introduce a\nnew dataset, STORYSUMM, comprising LLM summaries of short stories with\nlocalized faithfulness labels and error explanations. This benchmark is for\nevaluation methods, testing whether a given method can detect challenging\ninconsistencies. Using this dataset, we first show that any one human\nannotation protocol is likely to miss inconsistencies, and we advocate for\npursuing a range of methods when establishing ground truth for a summarization\ndataset. We finally test recent automatic metrics and find that none of them\nachieve more than 70% balanced accuracy on this task, demonstrating that it is\na challenging benchmark for future work in faithfulness evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human evaluation has been the gold standard for checking faithfulness in\nabstractive summarization. However, with a challenging source domain like\nnarrative, multiple annotators can agree a summary is faithful, while missing\ndetails that are obvious errors only once pointed out. We therefore introduce a\nnew dataset, STORYSUMM, comprising LLM summaries of short stories with\nlocalized faithfulness labels and error explanations. This benchmark is for\nevaluation methods, testing whether a given method can detect challenging\ninconsistencies. Using this dataset, we first show that any one human\nannotation protocol is likely to miss inconsistencies, and we advocate for\npursuing a range of methods when establishing ground truth for a summarization\ndataset. We finally test recent automatic metrics and find that none of them\nachieve more than 70% balanced accuracy on this task, demonstrating that it is\na challenging benchmark for future work in faithfulness evaluation."
                },
                "authors": [
                    {
                        "name": "Melanie Subbiah"
                    },
                    {
                        "name": "Faisal Ladhak"
                    },
                    {
                        "name": "Akankshya Mishra"
                    },
                    {
                        "name": "Griffin Adams"
                    },
                    {
                        "name": "Lydia B. Chilton"
                    },
                    {
                        "name": "Kathleen McKeown"
                    }
                ],
                "author_detail": {
                    "name": "Kathleen McKeown"
                },
                "author": "Kathleen McKeown",
                "arxiv_comment": "EMNLP Main 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.06501v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.06501v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13727v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13727v2",
                "updated": "2025-04-01T16:24:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    16,
                    24,
                    24,
                    1,
                    91,
                    0
                ],
                "published": "2024-10-17T16:33:01Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    16,
                    33,
                    1,
                    3,
                    291,
                    0
                ],
                "title": "LLM-Human Pipeline for Cultural Context Grounding of Conversations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Human Pipeline for Cultural Context Grounding of Conversations"
                },
                "summary": "Conversations often adhere to well-understood social norms that vary across\ncultures. For example, while \"addressing parents by name\" is commonplace in the\nWest, it is rare in most Asian cultures. Adherence or violation of such norms\noften dictates the tenor of conversations. Humans are able to navigate social\nsituations requiring cultural awareness quite adeptly. However, it is a hard\ntask for NLP models.\n  In this paper, we tackle this problem by introducing a \"Cultural Context\nSchema\" for conversations. It comprises (1) conversational information such as\nemotions, dialogue acts, etc., and (2) cultural information such as social\nnorms, violations, etc. We generate ~110k social norm and violation\ndescriptions for ~23k conversations from Chinese culture using LLMs. We refine\nthem using automated verification strategies which are evaluated against\nculturally aware human judgements. We organize these descriptions into\nmeaningful structures we call \"Norm Concepts\", using an interactive\nhuman-in-loop framework. We ground the norm concepts and the descriptions in\nconversations using symbolic annotation. Finally, we use the obtained dataset\nfor downstream tasks such as emotion, sentiment, and dialogue act detection. We\nshow that it significantly improves the empirical performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversations often adhere to well-understood social norms that vary across\ncultures. For example, while \"addressing parents by name\" is commonplace in the\nWest, it is rare in most Asian cultures. Adherence or violation of such norms\noften dictates the tenor of conversations. Humans are able to navigate social\nsituations requiring cultural awareness quite adeptly. However, it is a hard\ntask for NLP models.\n  In this paper, we tackle this problem by introducing a \"Cultural Context\nSchema\" for conversations. It comprises (1) conversational information such as\nemotions, dialogue acts, etc., and (2) cultural information such as social\nnorms, violations, etc. We generate ~110k social norm and violation\ndescriptions for ~23k conversations from Chinese culture using LLMs. We refine\nthem using automated verification strategies which are evaluated against\nculturally aware human judgements. We organize these descriptions into\nmeaningful structures we call \"Norm Concepts\", using an interactive\nhuman-in-loop framework. We ground the norm concepts and the descriptions in\nconversations using symbolic annotation. Finally, we use the obtained dataset\nfor downstream tasks such as emotion, sentiment, and dialogue act detection. We\nshow that it significantly improves the empirical performance."
                },
                "authors": [
                    {
                        "name": "Rajkumar Pujari"
                    },
                    {
                        "name": "Dan Goldwasser"
                    }
                ],
                "author_detail": {
                    "name": "Dan Goldwasser"
                },
                "author": "Dan Goldwasser",
                "arxiv_comment": "Oral at NAACL 2025 Main conference. Albuquerque, USA. Apr 29 - May 4,\n  2025. 19 pages, 9 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13727v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13727v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14642v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14642v2",
                "updated": "2025-04-01T16:18:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    16,
                    18,
                    55,
                    1,
                    91,
                    0
                ],
                "published": "2024-12-19T08:51:16Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    8,
                    51,
                    16,
                    3,
                    354,
                    0
                ],
                "title": "TOMG-Bench: Evaluating LLMs on Text-based Open Molecule Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TOMG-Bench: Evaluating LLMs on Text-based Open Molecule Generation"
                },
                "summary": "In this paper, we propose Text-based Open Molecule Generation Benchmark\n(TOMG-Bench), the first benchmark to evaluate the open-domain molecule\ngeneration capability of LLMs. TOMG-Bench encompasses a dataset of three major\ntasks: molecule editing (MolEdit), molecule optimization (MolOpt), and\ncustomized molecule generation (MolCustom). Each major task further contains\nthree subtasks, while each subtask comprises 5,000 test samples. Given the\ninherent complexity of open molecule generation evaluation, we also developed\nan automated evaluation system that helps measure both the quality and the\naccuracy of the generated molecules. Our comprehensive benchmarking of 25 LLMs\nreveals the current limitations as well as potential areas for improvement in\ntext-guided molecule discovery. Furthermore, we propose OpenMolIns, a\nspecialized instruction tuning dataset established for solving challenges\nraised by TOMG-Bench. Fine-tuned on OpenMolIns, Llama3.1-8B could outperform\nall the open-source general LLMs, even surpassing GPT-3.5-turbo by 46.5\\% on\nTOMG-Bench. Our codes and datasets are available through\nhttps://github.com/phenixace/TOMG-Bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose Text-based Open Molecule Generation Benchmark\n(TOMG-Bench), the first benchmark to evaluate the open-domain molecule\ngeneration capability of LLMs. TOMG-Bench encompasses a dataset of three major\ntasks: molecule editing (MolEdit), molecule optimization (MolOpt), and\ncustomized molecule generation (MolCustom). Each major task further contains\nthree subtasks, while each subtask comprises 5,000 test samples. Given the\ninherent complexity of open molecule generation evaluation, we also developed\nan automated evaluation system that helps measure both the quality and the\naccuracy of the generated molecules. Our comprehensive benchmarking of 25 LLMs\nreveals the current limitations as well as potential areas for improvement in\ntext-guided molecule discovery. Furthermore, we propose OpenMolIns, a\nspecialized instruction tuning dataset established for solving challenges\nraised by TOMG-Bench. Fine-tuned on OpenMolIns, Llama3.1-8B could outperform\nall the open-source general LLMs, even surpassing GPT-3.5-turbo by 46.5\\% on\nTOMG-Bench. Our codes and datasets are available through\nhttps://github.com/phenixace/TOMG-Bench."
                },
                "authors": [
                    {
                        "name": "Jiatong Li"
                    },
                    {
                        "name": "Junxian Li"
                    },
                    {
                        "name": "Yunqing Liu"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "arxiv_comment": "The first benchmark for text-based open molecule generation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14642v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14642v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14561v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14561v4",
                "updated": "2025-04-01T16:04:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    16,
                    4,
                    53,
                    1,
                    91,
                    0
                ],
                "published": "2024-07-18T17:59:01Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    17,
                    59,
                    1,
                    3,
                    200,
                    0
                ],
                "title": "NNsight and NDIF: Democratizing Access to Open-Weight Foundation Model\n  Internals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NNsight and NDIF: Democratizing Access to Open-Weight Foundation Model\n  Internals"
                },
                "summary": "We introduce NNsight and NDIF, technologies that work in tandem to enable\nscientific study of the representations and computations learned by very large\nneural networks. NNsight is an open-source system that extends PyTorch to\nintroduce deferred remote execution. The National Deep Inference Fabric (NDIF)\nis a scalable inference service that executes NNsight requests, allowing users\nto share GPU resources and pretrained models. These technologies are enabled by\nthe Intervention Graph, an architecture developed to decouple experimental\ndesign from model runtime. Together, this framework provides transparent and\nefficient access to the internals of deep neural networks such as very large\nlanguage models (LLMs) without imposing the cost or complexity of hosting\ncustomized models individually. We conduct a quantitative survey of the machine\nlearning literature that reveals a growing gap in the study of the internals of\nlarge-scale AI. We demonstrate the design and use of our framework to address\nthis gap by enabling a range of research methods on huge models. Finally, we\nconduct benchmarks to compare performance with previous approaches.\n  Code, documentation, and tutorials are available at https://nnsight.net/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce NNsight and NDIF, technologies that work in tandem to enable\nscientific study of the representations and computations learned by very large\nneural networks. NNsight is an open-source system that extends PyTorch to\nintroduce deferred remote execution. The National Deep Inference Fabric (NDIF)\nis a scalable inference service that executes NNsight requests, allowing users\nto share GPU resources and pretrained models. These technologies are enabled by\nthe Intervention Graph, an architecture developed to decouple experimental\ndesign from model runtime. Together, this framework provides transparent and\nefficient access to the internals of deep neural networks such as very large\nlanguage models (LLMs) without imposing the cost or complexity of hosting\ncustomized models individually. We conduct a quantitative survey of the machine\nlearning literature that reveals a growing gap in the study of the internals of\nlarge-scale AI. We demonstrate the design and use of our framework to address\nthis gap by enabling a range of research methods on huge models. Finally, we\nconduct benchmarks to compare performance with previous approaches.\n  Code, documentation, and tutorials are available at https://nnsight.net/."
                },
                "authors": [
                    {
                        "name": "Jaden Fiotto-Kaufman"
                    },
                    {
                        "name": "Alexander R. Loftus"
                    },
                    {
                        "name": "Eric Todd"
                    },
                    {
                        "name": "Jannik Brinkmann"
                    },
                    {
                        "name": "Koyena Pal"
                    },
                    {
                        "name": "Dmitrii Troitskii"
                    },
                    {
                        "name": "Michael Ripa"
                    },
                    {
                        "name": "Adam Belfki"
                    },
                    {
                        "name": "Can Rager"
                    },
                    {
                        "name": "Caden Juang"
                    },
                    {
                        "name": "Aaron Mueller"
                    },
                    {
                        "name": "Samuel Marks"
                    },
                    {
                        "name": "Arnab Sen Sharma"
                    },
                    {
                        "name": "Francesca Lucchetti"
                    },
                    {
                        "name": "Nikhil Prakash"
                    },
                    {
                        "name": "Carla Brodley"
                    },
                    {
                        "name": "Arjun Guha"
                    },
                    {
                        "name": "Jonathan Bell"
                    },
                    {
                        "name": "Byron C. Wallace"
                    },
                    {
                        "name": "David Bau"
                    }
                ],
                "author_detail": {
                    "name": "David Bau"
                },
                "author": "David Bau",
                "arxiv_comment": "Code at https://nnsight.net",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14561v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14561v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.04197v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.04197v3",
                "updated": "2025-04-01T16:02:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    16,
                    2,
                    3,
                    1,
                    91,
                    0
                ],
                "published": "2024-03-07T03:58:28Z",
                "published_parsed": [
                    2024,
                    3,
                    7,
                    3,
                    58,
                    28,
                    3,
                    67,
                    0
                ],
                "title": "Large Language Models are In-Context Molecule Learners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are In-Context Molecule Learners"
                },
                "summary": "Large Language Models (LLMs) have demonstrated exceptional performance in\nbiochemical tasks, especially the molecule caption translation task, which aims\nto bridge the gap between molecules and natural language texts. However,\nprevious methods in adapting LLMs to the molecule-caption translation task\nrequired extra domain-specific pre-training stages, suffered weak alignment\nbetween molecular and textual spaces, or imposed stringent demands on the scale\nof LLMs. To resolve the challenges, we propose In-Context Molecule Adaptation\n(ICMA), as a new paradigm allowing LLMs to learn the molecule-text alignment\nfrom context examples via In-Context Molecule Tuning. Specifically, ICMA\nincorporates the following three stages: Hybrid Context Retrieval,\nPost-retrieval Re-ranking, and In-context Molecule Tuning. Initially, Hybrid\nContext Retrieval utilizes BM25 Caption Retrieval and Molecule Graph Retrieval\nto retrieve similar informative context examples. Additionally, Post-retrieval\nRe-ranking is composed of Sequence Reversal and Random Walk selection to\nfurther improve the quality of retrieval results. Finally, In-Context Molecule\nTuning unlocks the in-context learning and reasoning capability of LLMs with\nthe retrieved examples and adapts the parameters of LLMs for better alignment\nbetween molecules and texts. Experimental results demonstrate that ICMA can\nempower LLMs to achieve state-of-the-art or comparable performance without\nextra training corpora and intricate structures, showing that LLMs are\ninherently in-context molecule learners.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated exceptional performance in\nbiochemical tasks, especially the molecule caption translation task, which aims\nto bridge the gap between molecules and natural language texts. However,\nprevious methods in adapting LLMs to the molecule-caption translation task\nrequired extra domain-specific pre-training stages, suffered weak alignment\nbetween molecular and textual spaces, or imposed stringent demands on the scale\nof LLMs. To resolve the challenges, we propose In-Context Molecule Adaptation\n(ICMA), as a new paradigm allowing LLMs to learn the molecule-text alignment\nfrom context examples via In-Context Molecule Tuning. Specifically, ICMA\nincorporates the following three stages: Hybrid Context Retrieval,\nPost-retrieval Re-ranking, and In-context Molecule Tuning. Initially, Hybrid\nContext Retrieval utilizes BM25 Caption Retrieval and Molecule Graph Retrieval\nto retrieve similar informative context examples. Additionally, Post-retrieval\nRe-ranking is composed of Sequence Reversal and Random Walk selection to\nfurther improve the quality of retrieval results. Finally, In-Context Molecule\nTuning unlocks the in-context learning and reasoning capability of LLMs with\nthe retrieved examples and adapts the parameters of LLMs for better alignment\nbetween molecules and texts. Experimental results demonstrate that ICMA can\nempower LLMs to achieve state-of-the-art or comparable performance without\nextra training corpora and intricate structures, showing that LLMs are\ninherently in-context molecule learners."
                },
                "authors": [
                    {
                        "name": "Jiatong Li"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Zhihao Ding"
                    },
                    {
                        "name": "Wenqi Fan"
                    },
                    {
                        "name": "Yuqiang Li"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "arxiv_comment": "Accepted by IEEE TKDE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.04197v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.04197v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20019v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20019v2",
                "updated": "2025-04-01T15:33:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    15,
                    33,
                    23,
                    1,
                    91,
                    0
                ],
                "published": "2024-12-28T04:42:54Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    4,
                    42,
                    54,
                    5,
                    363,
                    0
                ],
                "title": "Universal Bootstrap for Spectral Statistics: Beyond Gaussian\n  Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Universal Bootstrap for Spectral Statistics: Beyond Gaussian\n  Approximation"
                },
                "summary": "Spectral analysis plays a crucial role in high-dimensional statistics, where\ndetermining the asymptotic distribution of various spectral statistics remains\na challenging task. Due to the difficulties of deriving the analytic form,\nrecent advances have explored data-driven bootstrap methods for this purpose.\nHowever, widely used Gaussian approximation-based bootstrap methods, such as\nthe empirical bootstrap and multiplier bootstrap, have been shown to be\ninconsistent in approximating the distributions of spectral statistics in\nhigh-dimensional settings. To address this issue, we propose a universal\nbootstrap procedure based on the concept of universality from random matrix\ntheory. Our method consistently approximates a broad class of spectral\nstatistics across both high- and ultra-high-dimensional regimes, accommodating\nscenarios where the dimension-to-sample-size ratio $p/n$ converges to a nonzero\nconstant or diverges to infinity without requiring structural assumptions on\nthe population covariance matrix, such as eigenvalue decay or low effective\nrank. We showcase this universal bootstrap method for high-dimensional\ncovariance inference. Extensive simulations and a real-world data study support\nour findings, highlighting the favorable finite sample performance of the\nproposed universal bootstrap procedure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spectral analysis plays a crucial role in high-dimensional statistics, where\ndetermining the asymptotic distribution of various spectral statistics remains\na challenging task. Due to the difficulties of deriving the analytic form,\nrecent advances have explored data-driven bootstrap methods for this purpose.\nHowever, widely used Gaussian approximation-based bootstrap methods, such as\nthe empirical bootstrap and multiplier bootstrap, have been shown to be\ninconsistent in approximating the distributions of spectral statistics in\nhigh-dimensional settings. To address this issue, we propose a universal\nbootstrap procedure based on the concept of universality from random matrix\ntheory. Our method consistently approximates a broad class of spectral\nstatistics across both high- and ultra-high-dimensional regimes, accommodating\nscenarios where the dimension-to-sample-size ratio $p/n$ converges to a nonzero\nconstant or diverges to infinity without requiring structural assumptions on\nthe population covariance matrix, such as eigenvalue decay or low effective\nrank. We showcase this universal bootstrap method for high-dimensional\ncovariance inference. Extensive simulations and a real-world data study support\nour findings, highlighting the favorable finite sample performance of the\nproposed universal bootstrap procedure."
                },
                "authors": [
                    {
                        "name": "Guoyu Zhang"
                    },
                    {
                        "name": "Dandan Jiang"
                    },
                    {
                        "name": "Fang Yao"
                    }
                ],
                "author_detail": {
                    "name": "Fang Yao"
                },
                "author": "Fang Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20019v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20019v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19231v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19231v2",
                "updated": "2025-04-01T15:27:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    15,
                    27,
                    51,
                    1,
                    91,
                    0
                ],
                "published": "2025-02-26T15:42:06Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    15,
                    42,
                    6,
                    2,
                    57,
                    0
                ],
                "title": "AI-Powered Bayesian Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-Powered Bayesian Inference"
                },
                "summary": "The advent of Generative Artificial Intelligence (GAI) has heralded an\ninflection point that changed how society thinks about knowledge acquisition.\nWhile GAI cannot be fully trusted for decision-making, it may still provide\nvaluable information that can be integrated into a decision pipeline. Rather\nthan seeing the lack of certitude and inherent randomness of GAI as a problem,\nwe view it as an opportunity. Indeed, variable answers to given prompts can be\nleveraged to construct a prior distribution which reflects assuredness of AI\npredictions. This prior distribution may be combined with tailored datasets for\na fully Bayesian analysis with an AI-driven prior. In this paper, we explore\nsuch a possibility within a non-parametric Bayesian framework. The basic idea\nconsists of assigning a Dirichlet process prior distribution on the\ndata-generating distribution with AI generative model as its baseline.\nHyper-parameters of the prior can be tuned out-of-sample to assess the\ninformativeness of the AI prior. Posterior simulation is achieved by computing\na suitably randomized functional on an augmented data that consists of observed\n(labeled) data as well as fake data whose labels have been imputed using AI.\nThis strategy can be parallelized and rapidly produces iid samples from the\nposterior by optimization as opposed to sampling from conditionals. Our method\nenables (predictive) inference and uncertainty quantification leveraging AI\npredictions in a coherent probabilistic manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of Generative Artificial Intelligence (GAI) has heralded an\ninflection point that changed how society thinks about knowledge acquisition.\nWhile GAI cannot be fully trusted for decision-making, it may still provide\nvaluable information that can be integrated into a decision pipeline. Rather\nthan seeing the lack of certitude and inherent randomness of GAI as a problem,\nwe view it as an opportunity. Indeed, variable answers to given prompts can be\nleveraged to construct a prior distribution which reflects assuredness of AI\npredictions. This prior distribution may be combined with tailored datasets for\na fully Bayesian analysis with an AI-driven prior. In this paper, we explore\nsuch a possibility within a non-parametric Bayesian framework. The basic idea\nconsists of assigning a Dirichlet process prior distribution on the\ndata-generating distribution with AI generative model as its baseline.\nHyper-parameters of the prior can be tuned out-of-sample to assess the\ninformativeness of the AI prior. Posterior simulation is achieved by computing\na suitably randomized functional on an augmented data that consists of observed\n(labeled) data as well as fake data whose labels have been imputed using AI.\nThis strategy can be parallelized and rapidly produces iid samples from the\nposterior by optimization as opposed to sampling from conditionals. Our method\nenables (predictive) inference and uncertainty quantification leveraging AI\npredictions in a coherent probabilistic manner."
                },
                "authors": [
                    {
                        "name": "Veronika Ročková"
                    },
                    {
                        "name": "Sean O'Hagan"
                    }
                ],
                "author_detail": {
                    "name": "Sean O'Hagan"
                },
                "author": "Sean O'Hagan",
                "arxiv_comment": "37 pages, 4 figures; added additional experiments, asymptotic theory\n  and exposition, corrected typos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19231v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19231v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12481v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12481v2",
                "updated": "2025-04-01T15:16:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    15,
                    16,
                    34,
                    1,
                    91,
                    0
                ],
                "published": "2024-07-17T11:06:27Z",
                "published_parsed": [
                    2024,
                    7,
                    17,
                    11,
                    6,
                    27,
                    2,
                    199,
                    0
                ],
                "title": "Krutrim LLM: A Novel Tokenization Strategy for Multilingual Indic\n  Languages with Petabyte-Scale Data Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Krutrim LLM: A Novel Tokenization Strategy for Multilingual Indic\n  Languages with Petabyte-Scale Data Processing"
                },
                "summary": "We present a novel approach to data preparation for developing multilingual\nIndic large language model. Our meticulous data acquisition spans open-source\nand proprietary sources, including Common Crawl, Indic books, news articles,\nand Wikipedia, ensuring a diverse and rich linguistic representation. For each\nIndic language, we design a custom preprocessing pipeline to effectively\neliminate redundant and low-quality text content. Additionally, we perform\ndeduplication on Common Crawl data to address the redundancy present in 70% of\nthe crawled web pages. This study focuses on developing high-quality data,\noptimizing tokenization for our multilingual dataset for Indic large language\nmodels with 3B and 7B parameters, engineered for superior performance in Indic\nlanguages. We introduce a novel multilingual tokenizer training strategy,\ndemonstrating our custom-trained Indic tokenizer outperforms the\nstate-of-the-art OpenAI Tiktoken tokenizer, achieving a superior token-to-word\nratio for Indic languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel approach to data preparation for developing multilingual\nIndic large language model. Our meticulous data acquisition spans open-source\nand proprietary sources, including Common Crawl, Indic books, news articles,\nand Wikipedia, ensuring a diverse and rich linguistic representation. For each\nIndic language, we design a custom preprocessing pipeline to effectively\neliminate redundant and low-quality text content. Additionally, we perform\ndeduplication on Common Crawl data to address the redundancy present in 70% of\nthe crawled web pages. This study focuses on developing high-quality data,\noptimizing tokenization for our multilingual dataset for Indic large language\nmodels with 3B and 7B parameters, engineered for superior performance in Indic\nlanguages. We introduce a novel multilingual tokenizer training strategy,\ndemonstrating our custom-trained Indic tokenizer outperforms the\nstate-of-the-art OpenAI Tiktoken tokenizer, achieving a superior token-to-word\nratio for Indic languages."
                },
                "authors": [
                    {
                        "name": "Rahul Kumar"
                    },
                    {
                        "name": "Shubham Kakde"
                    },
                    {
                        "name": "Divyansh Rajput"
                    },
                    {
                        "name": "Daud Ibrahim"
                    },
                    {
                        "name": "Rishabh Nahata"
                    },
                    {
                        "name": "Pidathala Sowjanya"
                    },
                    {
                        "name": "Deepak Kumarr"
                    },
                    {
                        "name": "Gautam Bhargava"
                    },
                    {
                        "name": "Chandra Khatri"
                    }
                ],
                "author_detail": {
                    "name": "Chandra Khatri"
                },
                "author": "Chandra Khatri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12481v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12481v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03480v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03480v2",
                "updated": "2025-04-01T15:10:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    15,
                    10,
                    31,
                    1,
                    91,
                    0
                ],
                "published": "2024-04-04T14:33:37Z",
                "published_parsed": [
                    2024,
                    4,
                    4,
                    14,
                    33,
                    37,
                    3,
                    95,
                    0
                ],
                "title": "Evolutionary theory of convective organization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolutionary theory of convective organization"
                },
                "summary": "The conceptual landscape of convection has two simple gateways: optimal\nfunction and random form. Optimal convection adjusts toward a univariate ideal\ncalled neutrality. Convection form involves elements (parcels, bubbles, drafts)\nwhose most parsimonious assumption is random. Between these gates lies a\nwilderness of realizable flow configurations. The only simple principle is\nnatural selection by fitness, a scalar whose gradient is a local direction in\nan abstract configuration space. Random or high-entropy patterns occupy most of\nconfiguration space and occur spontaneously. With time, convection can discover\nless facile but more efficient (organized) configurations, by sequential\nselection. Here two data exercises explore that self-organization process, in\nshallow and deep moist convection. For shallow convection, causal network\npostulates are explored in a large set of cyclic-domain large-eddy simulations\n(LES; the Cloud Botany set). When an evolutionary pathway (mainly layer\ndeepening in these simulations) leads to precipitation, mesoscale patterns\nblossom rapidly. For deep convection, expanding rings of conditional cell\nprobability around prior cells are estimated from satellite imagery over South\nAmerica and the South Pacific. In a Monte Carlo model iterating such a\nconditional probability kernel, hundreds of hourly cells take days to discover\na self-sustaining squall configuration the kernel affords. Larger-scale\nimplications include overshoots (redefinition of neutrality) and tens-of-hours\ntimescales to both adjustment and noise (indeterminacy). If functional\norganization can be inferred from horizontal patterns, the abundance of\nhorizontal texture information in satellite cloud imagery could find\nquantitative value.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The conceptual landscape of convection has two simple gateways: optimal\nfunction and random form. Optimal convection adjusts toward a univariate ideal\ncalled neutrality. Convection form involves elements (parcels, bubbles, drafts)\nwhose most parsimonious assumption is random. Between these gates lies a\nwilderness of realizable flow configurations. The only simple principle is\nnatural selection by fitness, a scalar whose gradient is a local direction in\nan abstract configuration space. Random or high-entropy patterns occupy most of\nconfiguration space and occur spontaneously. With time, convection can discover\nless facile but more efficient (organized) configurations, by sequential\nselection. Here two data exercises explore that self-organization process, in\nshallow and deep moist convection. For shallow convection, causal network\npostulates are explored in a large set of cyclic-domain large-eddy simulations\n(LES; the Cloud Botany set). When an evolutionary pathway (mainly layer\ndeepening in these simulations) leads to precipitation, mesoscale patterns\nblossom rapidly. For deep convection, expanding rings of conditional cell\nprobability around prior cells are estimated from satellite imagery over South\nAmerica and the South Pacific. In a Monte Carlo model iterating such a\nconditional probability kernel, hundreds of hourly cells take days to discover\na self-sustaining squall configuration the kernel affords. Larger-scale\nimplications include overshoots (redefinition of neutrality) and tens-of-hours\ntimescales to both adjustment and noise (indeterminacy). If functional\norganization can be inferred from horizontal patterns, the abundance of\nhorizontal texture information in satellite cloud imagery could find\nquantitative value."
                },
                "authors": [
                    {
                        "name": "Brian E. Mapes"
                    }
                ],
                "author_detail": {
                    "name": "Brian E. Mapes"
                },
                "author": "Brian E. Mapes",
                "arxiv_comment": "31 pages with 10 figures, revised 3/31/2025 for publication in J.\n  Atmos. Sci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03480v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03480v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "nlin.AO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "nlin.AO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nlin.PS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15035v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15035v2",
                "updated": "2025-04-01T15:02:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    15,
                    2,
                    40,
                    1,
                    91,
                    0
                ],
                "published": "2024-12-19T16:46:54Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    16,
                    46,
                    54,
                    3,
                    354,
                    0
                ],
                "title": "LLMs Lost in Translation: M-ALERT uncovers Cross-Linguistic Safety Gaps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Lost in Translation: M-ALERT uncovers Cross-Linguistic Safety Gaps"
                },
                "summary": "Building safe Large Language Models (LLMs) across multiple languages is\nessential in ensuring both safe access and linguistic diversity. To this end,\nwe introduce M-ALERT, a multilingual benchmark that evaluates the safety of\nLLMs in five languages: English, French, German, Italian, and Spanish. M-ALERT\nincludes 15k high-quality prompts per language, totaling 75k, following the\ndetailed ALERT taxonomy. Our extensive experiments on 10 state-of-the-art LLMs\nhighlight the importance of language-specific safety analysis, revealing that\nmodels often exhibit significant inconsistencies in safety across languages and\ncategories. For instance, Llama3.2 shows high unsafety in the category\ncrime_tax for Italian but remains safe in other languages. Similar differences\ncan be observed across all models. In contrast, certain categories, such as\nsubstance_cannabis and crime_propaganda, consistently trigger unsafe responses\nacross models and languages. These findings underscore the need for robust\nmultilingual safety practices in LLMs to ensure safe and responsible usage\nacross diverse user communities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building safe Large Language Models (LLMs) across multiple languages is\nessential in ensuring both safe access and linguistic diversity. To this end,\nwe introduce M-ALERT, a multilingual benchmark that evaluates the safety of\nLLMs in five languages: English, French, German, Italian, and Spanish. M-ALERT\nincludes 15k high-quality prompts per language, totaling 75k, following the\ndetailed ALERT taxonomy. Our extensive experiments on 10 state-of-the-art LLMs\nhighlight the importance of language-specific safety analysis, revealing that\nmodels often exhibit significant inconsistencies in safety across languages and\ncategories. For instance, Llama3.2 shows high unsafety in the category\ncrime_tax for Italian but remains safe in other languages. Similar differences\ncan be observed across all models. In contrast, certain categories, such as\nsubstance_cannabis and crime_propaganda, consistently trigger unsafe responses\nacross models and languages. These findings underscore the need for robust\nmultilingual safety practices in LLMs to ensure safe and responsible usage\nacross diverse user communities."
                },
                "authors": [
                    {
                        "name": "Felix Friedrich"
                    },
                    {
                        "name": "Simone Tedeschi"
                    },
                    {
                        "name": "Patrick Schramowski"
                    },
                    {
                        "name": "Manuel Brack"
                    },
                    {
                        "name": "Roberto Navigli"
                    },
                    {
                        "name": "Huu Nguyen"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Kristian Kersting"
                    }
                ],
                "author_detail": {
                    "name": "Kristian Kersting"
                },
                "author": "Kristian Kersting",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15035v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15035v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23829v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23829v2",
                "updated": "2025-04-01T14:48:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    14,
                    48,
                    2,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-31T08:22:49Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    8,
                    22,
                    49,
                    0,
                    90,
                    0
                ],
                "title": "Crossing the Reward Bridge: Expanding RL with Verifiable Rewards Across\n  Diverse Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Crossing the Reward Bridge: Expanding RL with Verifiable Rewards Across\n  Diverse Domains"
                },
                "summary": "Reinforcement learning with verifiable rewards (RLVR) has demonstrated\nsignificant success in enhancing mathematical reasoning and coding performance\nof large language models (LLMs), especially when structured reference answers\nare accessible for verification. However, its extension to broader, less\nstructured domains remains unexplored. In this work, we investigate the\neffectiveness and scalability of RLVR across diverse real-world domains\nincluding medicine, chemistry, psychology, economics, and education, where\nstructured reference answers are typically unavailable. We reveal that binary\nverification judgments on broad-domain tasks exhibit high consistency across\nvarious LLMs provided expert-written reference answers exist. Motivated by this\nfinding, we utilize a generative scoring technique that yields soft,\nmodel-based reward signals to overcome limitations posed by binary\nverifications, especially in free-form, unstructured answer scenarios. We\nfurther demonstrate the feasibility of training cross-domain generative reward\nmodels using relatively small (7B) LLMs without the need for extensive\ndomain-specific annotation. Through comprehensive experiments, our RLVR\nframework establishes clear performance gains, significantly outperforming\nstate-of-the-art open-source aligned models such as Qwen2.5-72B and\nDeepSeek-R1-Distill-Qwen-32B across domains in free-form settings. Our approach\nnotably enhances the robustness, flexibility, and scalability of RLVR,\nrepresenting a substantial step towards practical reinforcement learning\napplications in complex, noisy-label scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning with verifiable rewards (RLVR) has demonstrated\nsignificant success in enhancing mathematical reasoning and coding performance\nof large language models (LLMs), especially when structured reference answers\nare accessible for verification. However, its extension to broader, less\nstructured domains remains unexplored. In this work, we investigate the\neffectiveness and scalability of RLVR across diverse real-world domains\nincluding medicine, chemistry, psychology, economics, and education, where\nstructured reference answers are typically unavailable. We reveal that binary\nverification judgments on broad-domain tasks exhibit high consistency across\nvarious LLMs provided expert-written reference answers exist. Motivated by this\nfinding, we utilize a generative scoring technique that yields soft,\nmodel-based reward signals to overcome limitations posed by binary\nverifications, especially in free-form, unstructured answer scenarios. We\nfurther demonstrate the feasibility of training cross-domain generative reward\nmodels using relatively small (7B) LLMs without the need for extensive\ndomain-specific annotation. Through comprehensive experiments, our RLVR\nframework establishes clear performance gains, significantly outperforming\nstate-of-the-art open-source aligned models such as Qwen2.5-72B and\nDeepSeek-R1-Distill-Qwen-32B across domains in free-form settings. Our approach\nnotably enhances the robustness, flexibility, and scalability of RLVR,\nrepresenting a substantial step towards practical reinforcement learning\napplications in complex, noisy-label scenarios."
                },
                "authors": [
                    {
                        "name": "Yi Su"
                    },
                    {
                        "name": "Dian Yu"
                    },
                    {
                        "name": "Linfeng Song"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Zhaopeng Tu"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Dong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yu"
                },
                "author": "Dong Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23829v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23829v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13543v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13543v2",
                "updated": "2025-04-01T14:45:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    14,
                    45,
                    22,
                    1,
                    91,
                    0
                ],
                "published": "2024-11-20T18:54:32Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    18,
                    54,
                    32,
                    2,
                    325,
                    0
                ],
                "title": "BALROG: Benchmarking Agentic LLM and VLM Reasoning On Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BALROG: Benchmarking Agentic LLM and VLM Reasoning On Games"
                },
                "summary": "Large Language Models (LLMs) and Vision Language Models (VLMs) possess\nextensive knowledge and exhibit promising reasoning abilities, however, they\nstill struggle to perform well in complex, dynamic environments. Real-world\ntasks require handling intricate interactions, advanced spatial reasoning,\nlong-term planning, and continuous exploration of new strategies-areas in which\nwe lack effective methodologies for comprehensively evaluating these\ncapabilities. To address this gap, we introduce BALROG, a novel benchmark\ndesigned to assess the agentic capabilities of LLMs and VLMs through a diverse\nset of challenging games. Our benchmark incorporates a range of existing\nreinforcement learning environments with varying levels of difficulty,\nincluding tasks that are solvable by non-expert humans in seconds to extremely\nchallenging ones that may take years to master (e.g., the NetHack Learning\nEnvironment). We devise fine-grained metrics to measure performance and conduct\nan extensive evaluation of several popular open-source and closed-source LLMs\nand VLMs. Our findings indicate that while current models achieve partial\nsuccess in the easier games, they struggle significantly with more challenging\ntasks. Notably, we observe severe deficiencies in vision-based decision-making,\nas several models perform worse when visual representations of the environments\nare provided. We release BALROG as an open and user-friendly benchmark to\nfacilitate future research and development in the agentic community. Code and\nLeaderboard at balrogai.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) and Vision Language Models (VLMs) possess\nextensive knowledge and exhibit promising reasoning abilities, however, they\nstill struggle to perform well in complex, dynamic environments. Real-world\ntasks require handling intricate interactions, advanced spatial reasoning,\nlong-term planning, and continuous exploration of new strategies-areas in which\nwe lack effective methodologies for comprehensively evaluating these\ncapabilities. To address this gap, we introduce BALROG, a novel benchmark\ndesigned to assess the agentic capabilities of LLMs and VLMs through a diverse\nset of challenging games. Our benchmark incorporates a range of existing\nreinforcement learning environments with varying levels of difficulty,\nincluding tasks that are solvable by non-expert humans in seconds to extremely\nchallenging ones that may take years to master (e.g., the NetHack Learning\nEnvironment). We devise fine-grained metrics to measure performance and conduct\nan extensive evaluation of several popular open-source and closed-source LLMs\nand VLMs. Our findings indicate that while current models achieve partial\nsuccess in the easier games, they struggle significantly with more challenging\ntasks. Notably, we observe severe deficiencies in vision-based decision-making,\nas several models perform worse when visual representations of the environments\nare provided. We release BALROG as an open and user-friendly benchmark to\nfacilitate future research and development in the agentic community. Code and\nLeaderboard at balrogai.com."
                },
                "authors": [
                    {
                        "name": "Davide Paglieri"
                    },
                    {
                        "name": "Bartłomiej Cupiał"
                    },
                    {
                        "name": "Samuel Coward"
                    },
                    {
                        "name": "Ulyana Piterbarg"
                    },
                    {
                        "name": "Maciej Wolczyk"
                    },
                    {
                        "name": "Akbir Khan"
                    },
                    {
                        "name": "Eduardo Pignatelli"
                    },
                    {
                        "name": "Łukasz Kuciński"
                    },
                    {
                        "name": "Lerrel Pinto"
                    },
                    {
                        "name": "Rob Fergus"
                    },
                    {
                        "name": "Jakob Nicolaus Foerster"
                    },
                    {
                        "name": "Jack Parker-Holder"
                    },
                    {
                        "name": "Tim Rocktäschel"
                    }
                ],
                "author_detail": {
                    "name": "Tim Rocktäschel"
                },
                "author": "Tim Rocktäschel",
                "arxiv_comment": "Published as a conference paper at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13543v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13543v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03275v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03275v3",
                "updated": "2025-04-01T14:33:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    14,
                    33,
                    53,
                    1,
                    91,
                    0
                ],
                "published": "2024-04-04T07:59:24Z",
                "published_parsed": [
                    2024,
                    4,
                    4,
                    7,
                    59,
                    24,
                    3,
                    95,
                    0
                ],
                "title": "DELTA: Decomposed Efficient Long-Term Robot Task Planning using Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DELTA: Decomposed Efficient Long-Term Robot Task Planning using Large\n  Language Models"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have sparked a revolution\nacross many research fields. In robotics, the integration of common-sense\nknowledge from LLMs into task and motion planning has drastically advanced the\nfield by unlocking unprecedented levels of context awareness. Despite their\nvast collection of knowledge, large language models may generate infeasible\nplans due to hallucinations or missing domain information. To address these\nchallenges and improve plan feasibility and computational efficiency, we\nintroduce DELTA, a novel LLM-informed task planning approach. By using scene\ngraphs as environment representations within LLMs, DELTA achieves rapid\ngeneration of precise planning problem descriptions. To enhance planning\nperformance, DELTA decomposes long-term task goals with LLMs into an\nautoregressive sequence of sub-goals, enabling automated task planners to\nefficiently solve complex problems. In our extensive evaluation, we show that\nDELTA enables an efficient and fully automatic task planning pipeline,\nachieving higher planning success rates and significantly shorter planning\ntimes compared to the state of the art. Project webpage:\nhttps://delta-llm.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have sparked a revolution\nacross many research fields. In robotics, the integration of common-sense\nknowledge from LLMs into task and motion planning has drastically advanced the\nfield by unlocking unprecedented levels of context awareness. Despite their\nvast collection of knowledge, large language models may generate infeasible\nplans due to hallucinations or missing domain information. To address these\nchallenges and improve plan feasibility and computational efficiency, we\nintroduce DELTA, a novel LLM-informed task planning approach. By using scene\ngraphs as environment representations within LLMs, DELTA achieves rapid\ngeneration of precise planning problem descriptions. To enhance planning\nperformance, DELTA decomposes long-term task goals with LLMs into an\nautoregressive sequence of sub-goals, enabling automated task planners to\nefficiently solve complex problems. In our extensive evaluation, we show that\nDELTA enables an efficient and fully automatic task planning pipeline,\nachieving higher planning success rates and significantly shorter planning\ntimes compared to the state of the art. Project webpage:\nhttps://delta-llm.github.io/"
                },
                "authors": [
                    {
                        "name": "Yuchen Liu"
                    },
                    {
                        "name": "Luigi Palmieri"
                    },
                    {
                        "name": "Sebastian Koch"
                    },
                    {
                        "name": "Ilche Georgievski"
                    },
                    {
                        "name": "Marco Aiello"
                    }
                ],
                "author_detail": {
                    "name": "Marco Aiello"
                },
                "author": "Marco Aiello",
                "arxiv_comment": "Accepted at ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03275v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03275v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13275v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13275v2",
                "updated": "2025-04-01T14:21:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    14,
                    21,
                    15,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-17T15:27:02Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    27,
                    2,
                    0,
                    76,
                    0
                ],
                "title": "Knowledge-Aware Iterative Retrieval for Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-Aware Iterative Retrieval for Multi-Agent Systems"
                },
                "summary": "We introduce a novel large language model (LLM)-driven agent framework, which\niteratively refines queries and filters contextual evidence by leveraging\ndynamically evolving knowledge. A defining feature of the system is its\ndecoupling of external sources from an internal knowledge cache that is\nprogressively updated to guide both query generation and evidence selection.\nThis design mitigates bias-reinforcement loops and enables dynamic, trackable\nsearch exploration paths, thereby optimizing the trade-off between exploring\ndiverse information and maintaining accuracy through autonomous agent\ndecision-making. Our approach is evaluated on a broad range of open-domain\nquestion answering benchmarks, including multi-step tasks that mirror\nreal-world scenarios where integrating information from multiple sources is\ncritical, especially given the vulnerabilities of LLMs that lack explicit\nreasoning or planning capabilities. The results show that the proposed system\nnot only outperforms single-step baselines regardless of task difficulty but\nalso, compared to conventional iterative retrieval methods, demonstrates\npronounced advantages in complex tasks through precise evidence-based reasoning\nand enhanced efficiency. The proposed system supports both competitive and\ncollaborative sharing of updated context, enabling multi-agent extension. The\nbenefits of multi-agent configurations become especially prominent as task\ndifficulty increases. The number of convergence steps scales with task\ndifficulty, suggesting cost-effective scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel large language model (LLM)-driven agent framework, which\niteratively refines queries and filters contextual evidence by leveraging\ndynamically evolving knowledge. A defining feature of the system is its\ndecoupling of external sources from an internal knowledge cache that is\nprogressively updated to guide both query generation and evidence selection.\nThis design mitigates bias-reinforcement loops and enables dynamic, trackable\nsearch exploration paths, thereby optimizing the trade-off between exploring\ndiverse information and maintaining accuracy through autonomous agent\ndecision-making. Our approach is evaluated on a broad range of open-domain\nquestion answering benchmarks, including multi-step tasks that mirror\nreal-world scenarios where integrating information from multiple sources is\ncritical, especially given the vulnerabilities of LLMs that lack explicit\nreasoning or planning capabilities. The results show that the proposed system\nnot only outperforms single-step baselines regardless of task difficulty but\nalso, compared to conventional iterative retrieval methods, demonstrates\npronounced advantages in complex tasks through precise evidence-based reasoning\nand enhanced efficiency. The proposed system supports both competitive and\ncollaborative sharing of updated context, enabling multi-agent extension. The\nbenefits of multi-agent configurations become especially prominent as task\ndifficulty increases. The number of convergence steps scales with task\ndifficulty, suggesting cost-effective scalability."
                },
                "authors": [
                    {
                        "name": "Seyoung Song"
                    }
                ],
                "author_detail": {
                    "name": "Seyoung Song"
                },
                "author": "Seyoung Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13275v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13275v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.7; I.2.11; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12190v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12190v2",
                "updated": "2025-04-01T14:18:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    14,
                    18,
                    14,
                    1,
                    91,
                    0
                ],
                "published": "2025-01-21T14:54:28Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    14,
                    54,
                    28,
                    1,
                    21,
                    0
                ],
                "title": "Modelling polarized X-ray pulses from accreting millisecond pulsars with\n  X-PSI, using different hot spot locations and shapes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modelling polarized X-ray pulses from accreting millisecond pulsars with\n  X-PSI, using different hot spot locations and shapes"
                },
                "summary": "We present an analysis of polarized X-ray pulses based on simulated data for\naccreting millisecond pulsars (AMPs). We used the open-source X-ray Pulse\nSimulation and Inference code (previously applied to NICER observations), which\nwe upgraded to allow polarization analysis. We provide estimates of how well\nneutron star (NS) parameters can be constrained for the Imaging X-ray\nPolarimetry Explorer (IXPE) and find that strong limits on the hot region\ngeometries can be hard to obtain if the emitting hot region is large and the\nnumber of polarized photons relatively small. However, if the star is bright\nenough and the hot regions are small and located so that polarization degree is\nhigher, the observer inclination and hot spot colatitude can be constrained to\na precision of within a few degrees. We also found that the shape of the hot\nregion, whether a circle or a ring, cannot be distinguished in our most\noptimistic scenario. Nevertheless, future X-ray polarization missions are\nexpected to improve the constraints, and already the recent AMP polarization\ndetections by IXPE should help to infer the NS mass and radius when combined\nwith modelling of X-ray pulse data sets that do not contain polarization\ninformation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an analysis of polarized X-ray pulses based on simulated data for\naccreting millisecond pulsars (AMPs). We used the open-source X-ray Pulse\nSimulation and Inference code (previously applied to NICER observations), which\nwe upgraded to allow polarization analysis. We provide estimates of how well\nneutron star (NS) parameters can be constrained for the Imaging X-ray\nPolarimetry Explorer (IXPE) and find that strong limits on the hot region\ngeometries can be hard to obtain if the emitting hot region is large and the\nnumber of polarized photons relatively small. However, if the star is bright\nenough and the hot regions are small and located so that polarization degree is\nhigher, the observer inclination and hot spot colatitude can be constrained to\na precision of within a few degrees. We also found that the shape of the hot\nregion, whether a circle or a ring, cannot be distinguished in our most\noptimistic scenario. Nevertheless, future X-ray polarization missions are\nexpected to improve the constraints, and already the recent AMP polarization\ndetections by IXPE should help to infer the NS mass and radius when combined\nwith modelling of X-ray pulse data sets that do not contain polarization\ninformation."
                },
                "authors": [
                    {
                        "name": "Tuomo Salmi"
                    },
                    {
                        "name": "Bas Dorsman"
                    },
                    {
                        "name": "Anna L. Watts"
                    },
                    {
                        "name": "Anna Bobrikova"
                    },
                    {
                        "name": "Alessandro Di Marco"
                    },
                    {
                        "name": "Vladislav Loktev"
                    },
                    {
                        "name": "Alessandro Papitto"
                    },
                    {
                        "name": "Maura Pilia"
                    },
                    {
                        "name": "Juri Poutanen"
                    },
                    {
                        "name": "John Rankin"
                    }
                ],
                "author_detail": {
                    "name": "John Rankin"
                },
                "author": "John Rankin",
                "arxiv_doi": "10.1093/mnras/staf441",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/mnras/staf441",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.12190v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12190v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages, 3 figures, 1 table, published in MNRAS",
                "arxiv_journal_ref": "MNRAS, 538, 2562 (2025)",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18104v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18104v2",
                "updated": "2025-04-01T14:15:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    14,
                    15,
                    3,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-23T15:22:37Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    15,
                    22,
                    37,
                    6,
                    82,
                    0
                ],
                "title": "Challenging Dataset and Multi-modal Gated Mixture of Experts Model for\n  Remote Sensing Copy-Move Forgery Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Challenging Dataset and Multi-modal Gated Mixture of Experts Model for\n  Remote Sensing Copy-Move Forgery Understanding"
                },
                "summary": "The Remote Sensing Copy-Move Question Answering (RSCMQA) task focuses on\ninterpreting complex tampering scenarios and inferring the relationships\nbetween objects. Currently, publicly available datasets often use randomly\ngenerated tampered images, which lack spatial logic and do not meet the\npractical needs of defense security and land resource monitoring. To address\nthis, we propose a high-quality manually annotated RSCMQA dataset, Real-RSCM,\nwhich provides more realistic evaluation metrics for the identification and\nunderstanding of remote sensing image tampering. The tampered images in the\nReal-RSCM dataset are subtle, authentic, and challenging, posing significant\ndifficulties for model discrimination capabilities. To overcome these\nchallenges, we introduce a multimodal gated mixture of experts model (CM-MMoE),\nwhich guides multi-expert models to discern tampered information in images\nthrough multi-level visual semantics and textual joint modeling. Extensive\nexperiments demonstrate that CM-MMoE provides a stronger benchmark for the\nRSCMQA task compared to general VQA and CMQA models. Our dataset and code are\navailable at https://github.com/shenyedepisa/CM-MMoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Remote Sensing Copy-Move Question Answering (RSCMQA) task focuses on\ninterpreting complex tampering scenarios and inferring the relationships\nbetween objects. Currently, publicly available datasets often use randomly\ngenerated tampered images, which lack spatial logic and do not meet the\npractical needs of defense security and land resource monitoring. To address\nthis, we propose a high-quality manually annotated RSCMQA dataset, Real-RSCM,\nwhich provides more realistic evaluation metrics for the identification and\nunderstanding of remote sensing image tampering. The tampered images in the\nReal-RSCM dataset are subtle, authentic, and challenging, posing significant\ndifficulties for model discrimination capabilities. To overcome these\nchallenges, we introduce a multimodal gated mixture of experts model (CM-MMoE),\nwhich guides multi-expert models to discern tampered information in images\nthrough multi-level visual semantics and textual joint modeling. Extensive\nexperiments demonstrate that CM-MMoE provides a stronger benchmark for the\nRSCMQA task compared to general VQA and CMQA models. Our dataset and code are\navailable at https://github.com/shenyedepisa/CM-MMoE."
                },
                "authors": [
                    {
                        "name": "Ze Zhang"
                    },
                    {
                        "name": "Enyuan Zhao"
                    },
                    {
                        "name": "Yi Jiang"
                    },
                    {
                        "name": "Jie Nie"
                    },
                    {
                        "name": "Xinyue Liang"
                    }
                ],
                "author_detail": {
                    "name": "Xinyue Liang"
                },
                "author": "Xinyue Liang",
                "arxiv_comment": "6 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18104v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18104v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.16260v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.16260v4",
                "updated": "2025-04-01T14:06:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    14,
                    6,
                    0,
                    1,
                    91,
                    0
                ],
                "published": "2023-12-26T05:56:13Z",
                "published_parsed": [
                    2023,
                    12,
                    26,
                    5,
                    56,
                    13,
                    1,
                    360,
                    0
                ],
                "title": "Multinomial Link Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multinomial Link Models"
                },
                "summary": "We propose a new family of regression models for analyzing categorical\nresponses, called multinomial link models. It consists of four classes, namely,\nmixed-link models that generalize existing multinomial logistic models and\ntheir extensions, two-group models that can incorporate the observations with\nNA or unknown responses, multinomial conditional link models that handle\nlongitudinal categorical responses, and po-npo mixture models that are more\nflexible than partial proportional odds models. By characterizing the feasible\nparameter space, deriving necessary and sufficient conditions, and developing\nvalidated algorithms to guarantee the finding of feasible maximum likelihood\nestimates, we solve the infeasibility issue of existing statistical software\nwhen estimating parameters for cumulative link models. We also provide explicit\nformulae and detailed algorithms for computing the Fisher information matrix\nand selecting the best models among the new family. The applications to real\ndatasets show that the new models can fit the data significantly better,\ncorrect misleading conclusions due to missing responses, and make more\ninformative statistical inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a new family of regression models for analyzing categorical\nresponses, called multinomial link models. It consists of four classes, namely,\nmixed-link models that generalize existing multinomial logistic models and\ntheir extensions, two-group models that can incorporate the observations with\nNA or unknown responses, multinomial conditional link models that handle\nlongitudinal categorical responses, and po-npo mixture models that are more\nflexible than partial proportional odds models. By characterizing the feasible\nparameter space, deriving necessary and sufficient conditions, and developing\nvalidated algorithms to guarantee the finding of feasible maximum likelihood\nestimates, we solve the infeasibility issue of existing statistical software\nwhen estimating parameters for cumulative link models. We also provide explicit\nformulae and detailed algorithms for computing the Fisher information matrix\nand selecting the best models among the new family. The applications to real\ndatasets show that the new models can fit the data significantly better,\ncorrect misleading conclusions due to missing responses, and make more\ninformative statistical inference."
                },
                "authors": [
                    {
                        "name": "Tianmeng Wang"
                    },
                    {
                        "name": "Liping Tong"
                    },
                    {
                        "name": "Jie Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Yang"
                },
                "author": "Jie Yang",
                "arxiv_comment": "65 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.16260v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.16260v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24115v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24115v2",
                "updated": "2025-04-01T14:04:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    14,
                    4,
                    47,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-31T14:06:17Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    14,
                    6,
                    17,
                    0,
                    90,
                    0
                ],
                "title": "TeleAntiFraud-28k: An Audio-Text Slow-Thinking Dataset for Telecom Fraud\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TeleAntiFraud-28k: An Audio-Text Slow-Thinking Dataset for Telecom Fraud\n  Detection"
                },
                "summary": "The detection of telecom fraud faces significant challenges due to the lack\nof high-quality multimodal training data that integrates audio signals with\nreasoning-oriented textual analysis. To address this gap, we present\nTeleAntiFraud-28k, the first open-source audio-text slow-thinking dataset\nspecifically designed for automated telecom fraud analysis. Our dataset is\nconstructed through three strategies: (1) Privacy-preserved text-truth sample\ngeneration using automatically speech recognition (ASR)-transcribed call\nrecordings (with anonymized original audio), ensuring real-world consistency\nthrough text-to-speech (TTS) model regeneration; (2) Semantic enhancement via\nlarge language model (LLM)-based self-instruction sampling on authentic ASR\noutputs to expand scenario coverage; (3) Multi-agent adversarial synthesis that\nsimulates emerging fraud tactics through predefined communication scenarios and\nfraud typologies. The generated dataset contains 28,511 rigorously processed\nspeech-text pairs, complete with detailed annotations for fraud reasoning. The\ndataset is divided into three tasks: scenario classification, fraud detection,\nfraud type classification. Furthermore, we construct TeleAntiFraud-Bench, a\nstandardized evaluation benchmark comprising proportionally sampled instances\nfrom the dataset, to facilitate systematic testing of model performance on\ntelecom fraud detection tasks. We also contribute a production-optimized\nsupervised fine-tuning (SFT) model trained on hybrid real/synthetic data, while\nopen-sourcing the data processing framework to enable community-driven dataset\nexpansion. This work establishes a foundational framework for multimodal\nanti-fraud research while addressing critical challenges in data privacy and\nscenario diversity. The project will be released at\nhttps://github.com/JimmyMa99/TeleAntiFraud.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The detection of telecom fraud faces significant challenges due to the lack\nof high-quality multimodal training data that integrates audio signals with\nreasoning-oriented textual analysis. To address this gap, we present\nTeleAntiFraud-28k, the first open-source audio-text slow-thinking dataset\nspecifically designed for automated telecom fraud analysis. Our dataset is\nconstructed through three strategies: (1) Privacy-preserved text-truth sample\ngeneration using automatically speech recognition (ASR)-transcribed call\nrecordings (with anonymized original audio), ensuring real-world consistency\nthrough text-to-speech (TTS) model regeneration; (2) Semantic enhancement via\nlarge language model (LLM)-based self-instruction sampling on authentic ASR\noutputs to expand scenario coverage; (3) Multi-agent adversarial synthesis that\nsimulates emerging fraud tactics through predefined communication scenarios and\nfraud typologies. The generated dataset contains 28,511 rigorously processed\nspeech-text pairs, complete with detailed annotations for fraud reasoning. The\ndataset is divided into three tasks: scenario classification, fraud detection,\nfraud type classification. Furthermore, we construct TeleAntiFraud-Bench, a\nstandardized evaluation benchmark comprising proportionally sampled instances\nfrom the dataset, to facilitate systematic testing of model performance on\ntelecom fraud detection tasks. We also contribute a production-optimized\nsupervised fine-tuning (SFT) model trained on hybrid real/synthetic data, while\nopen-sourcing the data processing framework to enable community-driven dataset\nexpansion. This work establishes a foundational framework for multimodal\nanti-fraud research while addressing critical challenges in data privacy and\nscenario diversity. The project will be released at\nhttps://github.com/JimmyMa99/TeleAntiFraud."
                },
                "authors": [
                    {
                        "name": "Zhiming Ma"
                    },
                    {
                        "name": "Peidong Wang"
                    },
                    {
                        "name": "Minhua Huang"
                    },
                    {
                        "name": "Jingpeng Wang"
                    },
                    {
                        "name": "Kai Wu"
                    },
                    {
                        "name": "Xiangzhao Lv"
                    },
                    {
                        "name": "Yachun Pang"
                    },
                    {
                        "name": "Yin Yang"
                    },
                    {
                        "name": "Wenjie Tang"
                    },
                    {
                        "name": "Yuchen Kang"
                    }
                ],
                "author_detail": {
                    "name": "Yuchen Kang"
                },
                "author": "Yuchen Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24115v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24115v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05447v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05447v2",
                "updated": "2025-04-01T14:03:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    14,
                    3,
                    15,
                    1,
                    91,
                    0
                ],
                "published": "2024-12-06T22:05:39Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    22,
                    5,
                    39,
                    4,
                    341,
                    0
                ],
                "title": "TOBUGraph: Knowledge Graph-Based Retrieval for Enhanced LLM Performance\n  Beyond RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TOBUGraph: Knowledge Graph-Based Retrieval for Enhanced LLM Performance\n  Beyond RAG"
                },
                "summary": "Retrieval-Augmented Generation (RAG) is one of the leading and most widely\nused techniques for enhancing LLM retrieval capabilities, but it still faces\nsignificant limitations in commercial use cases. RAG primarily relies on the\nquery-chunk text-to-text similarity in the embedding space for retrieval and\ncan fail to capture deeper semantic relationships across chunks, is highly\nsensitive to chunking strategies, and is prone to hallucinations. To address\nthese challenges, we propose TOBUGraph, a graph-based retrieval framework that\nfirst constructs the knowledge graph from unstructured data dynamically and\nautomatically. Using LLMs, TOBUGraph extracts structured knowledge and diverse\nrelationships among data, going beyond RAG's text-to-text similarity. Retrieval\nis achieved through graph traversal, leveraging the extracted relationships and\nstructures to enhance retrieval accuracy, eliminating the need for chunking\nconfigurations while reducing hallucination. We demonstrate TOBUGraph's\neffectiveness in TOBU, a real-world application in production for personal\nmemory organization and retrieval. Our evaluation using real user data\ndemonstrates that TOBUGraph outperforms multiple RAG implementations in both\nprecision and recall, significantly improving user experience through improved\nretrieval accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) is one of the leading and most widely\nused techniques for enhancing LLM retrieval capabilities, but it still faces\nsignificant limitations in commercial use cases. RAG primarily relies on the\nquery-chunk text-to-text similarity in the embedding space for retrieval and\ncan fail to capture deeper semantic relationships across chunks, is highly\nsensitive to chunking strategies, and is prone to hallucinations. To address\nthese challenges, we propose TOBUGraph, a graph-based retrieval framework that\nfirst constructs the knowledge graph from unstructured data dynamically and\nautomatically. Using LLMs, TOBUGraph extracts structured knowledge and diverse\nrelationships among data, going beyond RAG's text-to-text similarity. Retrieval\nis achieved through graph traversal, leveraging the extracted relationships and\nstructures to enhance retrieval accuracy, eliminating the need for chunking\nconfigurations while reducing hallucination. We demonstrate TOBUGraph's\neffectiveness in TOBU, a real-world application in production for personal\nmemory organization and retrieval. Our evaluation using real user data\ndemonstrates that TOBUGraph outperforms multiple RAG implementations in both\nprecision and recall, significantly improving user experience through improved\nretrieval accuracy."
                },
                "authors": [
                    {
                        "name": "Savini Kashmira"
                    },
                    {
                        "name": "Jayanaka L. Dantanarayana"
                    },
                    {
                        "name": "Joshua Brodsky"
                    },
                    {
                        "name": "Ashish Mahendra"
                    },
                    {
                        "name": "Yiping Kang"
                    },
                    {
                        "name": "Krisztian Flautner"
                    },
                    {
                        "name": "Lingjia Tang"
                    },
                    {
                        "name": "Jason Mars"
                    }
                ],
                "author_detail": {
                    "name": "Jason Mars"
                },
                "author": "Jason Mars",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05447v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05447v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18497v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18497v2",
                "updated": "2025-04-01T13:34:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    13,
                    34,
                    59,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-24T09:52:36Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    9,
                    52,
                    36,
                    0,
                    83,
                    0
                ],
                "title": "Statistically Testing Training Data for Unwanted Error Patterns using\n  Rule-Oriented Regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistically Testing Training Data for Unwanted Error Patterns using\n  Rule-Oriented Regression"
                },
                "summary": "Artificial intelligence models trained from data can only be as good as the\nunderlying data is. Biases in training data propagating through to the output\nof a machine learning model are a well-documented and well-understood\nphenomenon, but the machinery to prevent these undesired effects is much less\ndeveloped. Efforts to ensure data is clean during collection, such as using\nbias-aware sampling, are most effective when the entity controlling data\ncollection also trains the AI. In cases where the data is already available,\nhow do we find out if the data was already manipulated, i.e., ``poisoned'', so\nthat an undesired behavior would be trained into a machine learning model? This\nis a challenge fundamentally different to (just) improving approximation\naccuracy or efficiency, and we provide a method to test training data for\nflaws, to establish a trustworthy ground-truth for a subsequent training of\nmachine learning models (of any kind). Unlike the well-studied problem of\napproximating data using fuzzy rules that are generated from the data, our\nmethod hinges on a prior definition of rules to happen before seeing the data\nto be tested. Therefore, the proposed method can also discover hidden error\npatterns, which may also have substantial influence. Our approach extends the\nabilities of conventional statistical testing by letting the ``test-condition''\nbe any Boolean condition to describe a pattern in the data, whose presence we\nwish to determine. The method puts fuzzy inference into a regression model, to\nget the best of the two: explainability from fuzzy logic with statistical\nproperties and diagnostics from the regression, and finally also being\napplicable to ``small data'', hence not requiring large datasets as deep\nlearning methods do. We provide an open source implementation for demonstration\nand experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence models trained from data can only be as good as the\nunderlying data is. Biases in training data propagating through to the output\nof a machine learning model are a well-documented and well-understood\nphenomenon, but the machinery to prevent these undesired effects is much less\ndeveloped. Efforts to ensure data is clean during collection, such as using\nbias-aware sampling, are most effective when the entity controlling data\ncollection also trains the AI. In cases where the data is already available,\nhow do we find out if the data was already manipulated, i.e., ``poisoned'', so\nthat an undesired behavior would be trained into a machine learning model? This\nis a challenge fundamentally different to (just) improving approximation\naccuracy or efficiency, and we provide a method to test training data for\nflaws, to establish a trustworthy ground-truth for a subsequent training of\nmachine learning models (of any kind). Unlike the well-studied problem of\napproximating data using fuzzy rules that are generated from the data, our\nmethod hinges on a prior definition of rules to happen before seeing the data\nto be tested. Therefore, the proposed method can also discover hidden error\npatterns, which may also have substantial influence. Our approach extends the\nabilities of conventional statistical testing by letting the ``test-condition''\nbe any Boolean condition to describe a pattern in the data, whose presence we\nwish to determine. The method puts fuzzy inference into a regression model, to\nget the best of the two: explainability from fuzzy logic with statistical\nproperties and diagnostics from the regression, and finally also being\napplicable to ``small data'', hence not requiring large datasets as deep\nlearning methods do. We provide an open source implementation for demonstration\nand experiments."
                },
                "authors": [
                    {
                        "name": "Stefan Rass"
                    },
                    {
                        "name": "Martin Dallinger"
                    }
                ],
                "author_detail": {
                    "name": "Martin Dallinger"
                },
                "author": "Martin Dallinger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18497v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18497v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T10 (Primary), 68M25, 62J86 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17290v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17290v2",
                "updated": "2025-04-01T12:59:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    12,
                    59,
                    50,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-21T16:41:10Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    16,
                    41,
                    10,
                    4,
                    80,
                    0
                ],
                "title": "Calibration Strategies for Robust Causal Estimation: Theoretical and\n  Empirical Insights on Propensity Score Based Estimators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Calibration Strategies for Robust Causal Estimation: Theoretical and\n  Empirical Insights on Propensity Score Based Estimators"
                },
                "summary": "The partitioning of data for estimation and calibration critically impacts\nthe performance of propensity score based estimators like inverse probability\nweighting (IPW) and double/debiased machine learning (DML) frameworks. We\nextend recent advances in calibration techniques for propensity score\nestimation, improving the robustness of propensity scores in challenging\nsettings such as limited overlap, small sample sizes, or unbalanced data. Our\ncontributions are twofold: First, we provide a theoretical analysis of the\nproperties of calibrated estimators in the context of DML. To this end, we\nrefine existing calibration frameworks for propensity score models, with a\nparticular emphasis on the role of sample-splitting schemes in ensuring valid\ncausal inference. Second, through extensive simulations, we show that\ncalibration reduces variance of inverse-based propensity score estimators while\nalso mitigating bias in IPW, even in small-sample regimes. Notably, calibration\nimproves stability for flexible learners (e.g., gradient boosting) while\npreserving the doubly robust properties of DML. A key insight is that, even\nwhen methods perform well without calibration, incorporating a calibration step\ndoes not degrade performance, provided that an appropriate sample-splitting\napproach is chosen.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The partitioning of data for estimation and calibration critically impacts\nthe performance of propensity score based estimators like inverse probability\nweighting (IPW) and double/debiased machine learning (DML) frameworks. We\nextend recent advances in calibration techniques for propensity score\nestimation, improving the robustness of propensity scores in challenging\nsettings such as limited overlap, small sample sizes, or unbalanced data. Our\ncontributions are twofold: First, we provide a theoretical analysis of the\nproperties of calibrated estimators in the context of DML. To this end, we\nrefine existing calibration frameworks for propensity score models, with a\nparticular emphasis on the role of sample-splitting schemes in ensuring valid\ncausal inference. Second, through extensive simulations, we show that\ncalibration reduces variance of inverse-based propensity score estimators while\nalso mitigating bias in IPW, even in small-sample regimes. Notably, calibration\nimproves stability for flexible learners (e.g., gradient boosting) while\npreserving the doubly robust properties of DML. A key insight is that, even\nwhen methods perform well without calibration, incorporating a calibration step\ndoes not degrade performance, provided that an appropriate sample-splitting\napproach is chosen."
                },
                "authors": [
                    {
                        "name": "Jan Rabenseifner"
                    },
                    {
                        "name": "Sven Klaassen"
                    },
                    {
                        "name": "Jannis Kueck"
                    },
                    {
                        "name": "Philipp Bach"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Bach"
                },
                "author": "Philipp Bach",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17290v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17290v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06621v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06621v4",
                "updated": "2025-04-01T12:53:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    12,
                    53,
                    30,
                    1,
                    91,
                    0
                ],
                "published": "2024-08-13T04:18:32Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    4,
                    18,
                    32,
                    1,
                    226,
                    0
                ],
                "title": "Towards Robust and Parameter-Efficient Knowledge Unlearning for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Robust and Parameter-Efficient Knowledge Unlearning for LLMs"
                },
                "summary": "Large Language Models (LLMs) have demonstrated strong reasoning and\nmemorization capabilities via pretraining on massive textual corpora. However,\nthis poses risk of privacy and copyright violations, highlighting the need for\nefficient machine unlearning methods that remove sensitive data without\nretraining from scratch. While Gradient Ascent (GA) is commonly used to unlearn\nby reducing the likelihood of generating unwanted content, it leads to unstable\noptimization and catastrophic forgetting of retrained knowledge. We find that\ncombining GA with low-rank adaptation results in poor trade-offs between\ncomputational cost and generative performance. To address these challenges, we\npropose Low-rank Knowledge Unlearning (LoKU), a novel framework that enables\nrobust and efficient unlearning for LLMs. First, we introduce Inverted Hinge\nLoss, which suppresses unwanted tokens while maintaining fluency by boosting\nthe probability of the next most likely token. Second, we develop a\ndata-adaptive initialization for LoRA adapters via low-rank approximation\nweighted with relative Fisher information, thereby focusing updates on\nparameters critical for removing targeted knowledge. Experiments on the\nTraining Data Extraction Challenge dataset using GPT-Neo models as well as on\nthe TOFU benchmark with Phi-1.5B and Llama2-7B models demonstrate that our\napproach effectively removes sensitive information while maintaining reasoning\nand generative capabilities with minimal impact. Our implementation can be\nfound in https://github.com/csm9493/efficient-llm-unlearning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated strong reasoning and\nmemorization capabilities via pretraining on massive textual corpora. However,\nthis poses risk of privacy and copyright violations, highlighting the need for\nefficient machine unlearning methods that remove sensitive data without\nretraining from scratch. While Gradient Ascent (GA) is commonly used to unlearn\nby reducing the likelihood of generating unwanted content, it leads to unstable\noptimization and catastrophic forgetting of retrained knowledge. We find that\ncombining GA with low-rank adaptation results in poor trade-offs between\ncomputational cost and generative performance. To address these challenges, we\npropose Low-rank Knowledge Unlearning (LoKU), a novel framework that enables\nrobust and efficient unlearning for LLMs. First, we introduce Inverted Hinge\nLoss, which suppresses unwanted tokens while maintaining fluency by boosting\nthe probability of the next most likely token. Second, we develop a\ndata-adaptive initialization for LoRA adapters via low-rank approximation\nweighted with relative Fisher information, thereby focusing updates on\nparameters critical for removing targeted knowledge. Experiments on the\nTraining Data Extraction Challenge dataset using GPT-Neo models as well as on\nthe TOFU benchmark with Phi-1.5B and Llama2-7B models demonstrate that our\napproach effectively removes sensitive information while maintaining reasoning\nand generative capabilities with minimal impact. Our implementation can be\nfound in https://github.com/csm9493/efficient-llm-unlearning."
                },
                "authors": [
                    {
                        "name": "Sungmin Cha"
                    },
                    {
                        "name": "Sungjun Cho"
                    },
                    {
                        "name": "Dasol Hwang"
                    },
                    {
                        "name": "Moontae Lee"
                    }
                ],
                "author_detail": {
                    "name": "Moontae Lee"
                },
                "author": "Moontae Lee",
                "arxiv_comment": "ICLR 2025 camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06621v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06621v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23157v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23157v2",
                "updated": "2025-04-01T12:53:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    12,
                    53,
                    16,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-29T17:29:30Z",
                "published_parsed": [
                    2025,
                    3,
                    29,
                    17,
                    29,
                    30,
                    5,
                    88,
                    0
                ],
                "title": "Reasoning-SQL: Reinforcement Learning with SQL Tailored Partial Rewards\n  for Reasoning-Enhanced Text-to-SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning-SQL: Reinforcement Learning with SQL Tailored Partial Rewards\n  for Reasoning-Enhanced Text-to-SQL"
                },
                "summary": "Text-to-SQL is a challenging task involving multiple reasoning-intensive\nsubtasks, including natural language understanding, database schema\ncomprehension, and precise SQL query formulation. Existing approaches often\nrely on handcrafted reasoning paths with inductive biases that can limit their\noverall effectiveness. Motivated by the recent success of reasoning-enhanced\nmodels such as DeepSeek R1 and OpenAI o1, which effectively leverage\nreward-driven self-exploration to enhance reasoning capabilities and\ngeneralization, we propose a novel set of partial rewards tailored specifically\nfor the Text-to-SQL task. Our reward set includes schema-linking, AI feedback,\nn-gram similarity, and syntax check, explicitly designed to address the reward\nsparsity issue prevalent in reinforcement learning (RL). Leveraging group\nrelative policy optimization (GRPO), our approach explicitly encourages large\nlanguage models (LLMs) to develop intrinsic reasoning skills necessary for\naccurate SQL query generation. With models of different sizes, we demonstrate\nthat RL-only training with our proposed rewards consistently achieves higher\naccuracy and superior generalization compared to supervised fine-tuning (SFT).\nRemarkably, our RL-trained 14B-parameter model significantly outperforms larger\nproprietary models, e.g. o3-mini by 4% and Gemini-1.5-Pro-002 by 3% on the BIRD\nbenchmark. These highlight the efficacy of our proposed RL-training framework\nwith partial rewards for enhancing both accuracy and reasoning capabilities in\nText-to-SQL tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-SQL is a challenging task involving multiple reasoning-intensive\nsubtasks, including natural language understanding, database schema\ncomprehension, and precise SQL query formulation. Existing approaches often\nrely on handcrafted reasoning paths with inductive biases that can limit their\noverall effectiveness. Motivated by the recent success of reasoning-enhanced\nmodels such as DeepSeek R1 and OpenAI o1, which effectively leverage\nreward-driven self-exploration to enhance reasoning capabilities and\ngeneralization, we propose a novel set of partial rewards tailored specifically\nfor the Text-to-SQL task. Our reward set includes schema-linking, AI feedback,\nn-gram similarity, and syntax check, explicitly designed to address the reward\nsparsity issue prevalent in reinforcement learning (RL). Leveraging group\nrelative policy optimization (GRPO), our approach explicitly encourages large\nlanguage models (LLMs) to develop intrinsic reasoning skills necessary for\naccurate SQL query generation. With models of different sizes, we demonstrate\nthat RL-only training with our proposed rewards consistently achieves higher\naccuracy and superior generalization compared to supervised fine-tuning (SFT).\nRemarkably, our RL-trained 14B-parameter model significantly outperforms larger\nproprietary models, e.g. o3-mini by 4% and Gemini-1.5-Pro-002 by 3% on the BIRD\nbenchmark. These highlight the efficacy of our proposed RL-training framework\nwith partial rewards for enhancing both accuracy and reasoning capabilities in\nText-to-SQL tasks."
                },
                "authors": [
                    {
                        "name": "Mohammadreza Pourreza"
                    },
                    {
                        "name": "Shayan Talaei"
                    },
                    {
                        "name": "Ruoxi Sun"
                    },
                    {
                        "name": "Xingchen Wan"
                    },
                    {
                        "name": "Hailong Li"
                    },
                    {
                        "name": "Azalia Mirhoseini"
                    },
                    {
                        "name": "Amin Saberi"
                    },
                    {
                        "name": "Sercan \"O. Arik"
                    }
                ],
                "author_detail": {
                    "name": "Sercan \"O. Arik"
                },
                "author": "Sercan \"O. Arik",
                "arxiv_comment": "Mohammadreza Pourreza and Shayan Talaei contributed equally to this\n  work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23157v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23157v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09078v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09078v5",
                "updated": "2025-04-01T12:48:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    12,
                    48,
                    43,
                    1,
                    91,
                    0
                ],
                "published": "2024-12-12T09:01:18Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    9,
                    1,
                    18,
                    3,
                    347,
                    0
                ],
                "title": "Forest-of-Thought: Scaling Test-Time Compute for Enhancing LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forest-of-Thought: Scaling Test-Time Compute for Enhancing LLM Reasoning"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable abilities across\nvarious language tasks, but solving complex reasoning problems remains a\nsignificant challenge. While existing methods, such as Chain-of-Thought (CoT)\nand Tree-of-Thought (ToT), enhance reasoning by decomposing problems or\nstructuring prompts, they typically perform a single pass of reasoning and may\nfail to revisit flawed paths, compromising accuracy. To address this\nlimitation, we propose a novel reasoning framework called Forest-of-Thought\n(FoT), which integrates multiple reasoning trees to leverage collective\ndecision-making for solving complex logical problems. FoT employs sparse\nactivation strategies to select the most relevant reasoning paths, improving\nboth efficiency and accuracy. Additionally, we introduce a dynamic\nself-correction strategy that enables real-time error correction, along with\nconsensus-guided decision-making strategies to optimize both correctness and\ncomputational resources. Experimental results demonstrate that the FoT\nframework, combined with these strategies, significantly enhances the reasoning\ncapabilities of LLMs, enabling them to solve complex tasks with greater\nprecision and efficiency. Code will be available at\nhttps://github.com/iamhankai/Forest-of-Thought.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable abilities across\nvarious language tasks, but solving complex reasoning problems remains a\nsignificant challenge. While existing methods, such as Chain-of-Thought (CoT)\nand Tree-of-Thought (ToT), enhance reasoning by decomposing problems or\nstructuring prompts, they typically perform a single pass of reasoning and may\nfail to revisit flawed paths, compromising accuracy. To address this\nlimitation, we propose a novel reasoning framework called Forest-of-Thought\n(FoT), which integrates multiple reasoning trees to leverage collective\ndecision-making for solving complex logical problems. FoT employs sparse\nactivation strategies to select the most relevant reasoning paths, improving\nboth efficiency and accuracy. Additionally, we introduce a dynamic\nself-correction strategy that enables real-time error correction, along with\nconsensus-guided decision-making strategies to optimize both correctness and\ncomputational resources. Experimental results demonstrate that the FoT\nframework, combined with these strategies, significantly enhances the reasoning\ncapabilities of LLMs, enabling them to solve complex tasks with greater\nprecision and efficiency. Code will be available at\nhttps://github.com/iamhankai/Forest-of-Thought."
                },
                "authors": [
                    {
                        "name": "Zhenni Bi"
                    },
                    {
                        "name": "Kai Han"
                    },
                    {
                        "name": "Chuanjian Liu"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Yunhe Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yunhe Wang"
                },
                "author": "Yunhe Wang",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09078v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09078v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.07773v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.07773v4",
                "updated": "2025-04-01T12:46:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    12,
                    46,
                    3,
                    1,
                    91,
                    0
                ],
                "published": "2024-04-11T14:08:45Z",
                "published_parsed": [
                    2024,
                    4,
                    11,
                    14,
                    8,
                    45,
                    3,
                    102,
                    0
                ],
                "title": "ConsistencyDet: A Few-step Denoising Framework for Object Detection\n  Using the Consistency Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConsistencyDet: A Few-step Denoising Framework for Object Detection\n  Using the Consistency Model"
                },
                "summary": "Object detection, a quintessential task in the realm of perceptual computing,\ncan be tackled using a generative methodology. In the present study, we\nintroduce a novel framework designed to articulate object detection as a\ndenoising diffusion process, which operates on the perturbed bounding boxes of\nannotated entities. This framework, termed \\textbf{ConsistencyDet}, leverages\nan innovative denoising concept known as the Consistency Model. The hallmark of\nthis model is its self-consistency feature, which empowers the model to map\ndistorted information from any time step back to its pristine state, thereby\nrealizing a \\textbf{``few-step denoising''} mechanism. Such an attribute\nmarkedly elevates the operational efficiency of the model, setting it apart\nfrom the conventional Diffusion Model. Throughout the training phase,\nConsistencyDet initiates the diffusion sequence with noise-infused boxes\nderived from the ground-truth annotations and conditions the model to perform\nthe denoising task. Subsequently, in the inference stage, the model employs a\ndenoising sampling strategy that commences with bounding boxes randomly sampled\nfrom a normal distribution. Through iterative refinement, the model transforms\nan assortment of arbitrarily generated boxes into definitive detections.\nComprehensive evaluations employing standard benchmarks, such as MS-COCO and\nLVIS, corroborate that ConsistencyDet surpasses other leading-edge detectors in\nperformance metrics. Our code is available at\nhttps://anonymous.4open.science/r/ConsistencyDet-37D5.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object detection, a quintessential task in the realm of perceptual computing,\ncan be tackled using a generative methodology. In the present study, we\nintroduce a novel framework designed to articulate object detection as a\ndenoising diffusion process, which operates on the perturbed bounding boxes of\nannotated entities. This framework, termed \\textbf{ConsistencyDet}, leverages\nan innovative denoising concept known as the Consistency Model. The hallmark of\nthis model is its self-consistency feature, which empowers the model to map\ndistorted information from any time step back to its pristine state, thereby\nrealizing a \\textbf{``few-step denoising''} mechanism. Such an attribute\nmarkedly elevates the operational efficiency of the model, setting it apart\nfrom the conventional Diffusion Model. Throughout the training phase,\nConsistencyDet initiates the diffusion sequence with noise-infused boxes\nderived from the ground-truth annotations and conditions the model to perform\nthe denoising task. Subsequently, in the inference stage, the model employs a\ndenoising sampling strategy that commences with bounding boxes randomly sampled\nfrom a normal distribution. Through iterative refinement, the model transforms\nan assortment of arbitrarily generated boxes into definitive detections.\nComprehensive evaluations employing standard benchmarks, such as MS-COCO and\nLVIS, corroborate that ConsistencyDet surpasses other leading-edge detectors in\nperformance metrics. Our code is available at\nhttps://anonymous.4open.science/r/ConsistencyDet-37D5."
                },
                "authors": [
                    {
                        "name": "Lifan Jiang"
                    },
                    {
                        "name": "Zhihui Wang"
                    },
                    {
                        "name": "Changmiao Wang"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Jiaxu Leng"
                    },
                    {
                        "name": "Xindong Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xindong Wu"
                },
                "author": "Xindong Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.07773v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.07773v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11923v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11923v2",
                "updated": "2025-04-01T12:45:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    12,
                    45,
                    58,
                    1,
                    91,
                    0
                ],
                "published": "2024-12-16T16:09:35Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    9,
                    35,
                    0,
                    351,
                    0
                ],
                "title": "PICLe: Pseudo-Annotations for In-Context Learning in Low-Resource Named\n  Entity Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PICLe: Pseudo-Annotations for In-Context Learning in Low-Resource Named\n  Entity Detection"
                },
                "summary": "In-context learning (ICL) enables Large Language Models (LLMs) to perform\ntasks using few demonstrations, facilitating task adaptation when labeled\nexamples are hard to obtain. However, ICL is sensitive to the choice of\ndemonstrations, and it remains unclear which demonstration attributes enable\nin-context generalization. In this work, we conduct a perturbation study of\nin-context demonstrations for low-resource Named Entity Detection (NED). Our\nsurprising finding is that in-context demonstrations with partially correct\nannotated entity mentions can be as effective for task transfer as fully\ncorrect demonstrations. Based off our findings, we propose Pseudo-annotated\nIn-Context Learning (PICLe), a framework for in-context learning with noisy,\npseudo-annotated demonstrations. PICLe leverages LLMs to annotate many\ndemonstrations in a zero-shot first pass. We then cluster these synthetic\ndemonstrations, sample specific sets of in-context demonstrations from each\ncluster, and predict entity mentions using each set independently. Finally, we\nuse self-verification to select the final set of entity mentions. We evaluate\nPICLe on five biomedical NED datasets and show that, with zero human\nannotation, PICLe outperforms ICL in low-resource settings where limited gold\nexamples can be used as in-context demonstrations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) enables Large Language Models (LLMs) to perform\ntasks using few demonstrations, facilitating task adaptation when labeled\nexamples are hard to obtain. However, ICL is sensitive to the choice of\ndemonstrations, and it remains unclear which demonstration attributes enable\nin-context generalization. In this work, we conduct a perturbation study of\nin-context demonstrations for low-resource Named Entity Detection (NED). Our\nsurprising finding is that in-context demonstrations with partially correct\nannotated entity mentions can be as effective for task transfer as fully\ncorrect demonstrations. Based off our findings, we propose Pseudo-annotated\nIn-Context Learning (PICLe), a framework for in-context learning with noisy,\npseudo-annotated demonstrations. PICLe leverages LLMs to annotate many\ndemonstrations in a zero-shot first pass. We then cluster these synthetic\ndemonstrations, sample specific sets of in-context demonstrations from each\ncluster, and predict entity mentions using each set independently. Finally, we\nuse self-verification to select the final set of entity mentions. We evaluate\nPICLe on five biomedical NED datasets and show that, with zero human\nannotation, PICLe outperforms ICL in low-resource settings where limited gold\nexamples can be used as in-context demonstrations."
                },
                "authors": [
                    {
                        "name": "Sepideh Mamooler"
                    },
                    {
                        "name": "Syrielle Montariol"
                    },
                    {
                        "name": "Alexander Mathis"
                    },
                    {
                        "name": "Antoine Bosselut"
                    }
                ],
                "author_detail": {
                    "name": "Antoine Bosselut"
                },
                "author": "Antoine Bosselut",
                "arxiv_comment": "In Proceedings of NAACL2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11923v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11923v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22968v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22968v2",
                "updated": "2025-04-01T12:37:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    12,
                    37,
                    16,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-29T04:17:58Z",
                "published_parsed": [
                    2025,
                    3,
                    29,
                    4,
                    17,
                    58,
                    5,
                    88,
                    0
                ],
                "title": "HRET: A Self-Evolving LLM Evaluation Toolkit for Korean",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HRET: A Self-Evolving LLM Evaluation Toolkit for Korean"
                },
                "summary": "Recent advancements in Korean large language models (LLMs) have spurred\nnumerous benchmarks and evaluation methodologies, yet the lack of a\nstandardized evaluation framework has led to inconsistent results and limited\ncomparability. To address this, we introduce HRET Haerae Evaluation Toolkit, an\nopen-source, self-evolving evaluation framework tailored specifically for\nKorean LLMs. HRET unifies diverse evaluation methods, including logit-based\nscoring, exact-match, language-inconsistency penalization, and LLM-as-a-Judge\nassessments. Its modular, registry-based architecture integrates major\nbenchmarks (HAE-RAE Bench, KMMLU, KUDGE, HRM8K) and multiple inference backends\n(vLLM, HuggingFace, OpenAI-compatible endpoints). With automated pipelines for\ncontinuous evolution, HRET provides a robust foundation for reproducible, fair,\nand transparent Korean NLP research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Korean large language models (LLMs) have spurred\nnumerous benchmarks and evaluation methodologies, yet the lack of a\nstandardized evaluation framework has led to inconsistent results and limited\ncomparability. To address this, we introduce HRET Haerae Evaluation Toolkit, an\nopen-source, self-evolving evaluation framework tailored specifically for\nKorean LLMs. HRET unifies diverse evaluation methods, including logit-based\nscoring, exact-match, language-inconsistency penalization, and LLM-as-a-Judge\nassessments. Its modular, registry-based architecture integrates major\nbenchmarks (HAE-RAE Bench, KMMLU, KUDGE, HRM8K) and multiple inference backends\n(vLLM, HuggingFace, OpenAI-compatible endpoints). With automated pipelines for\ncontinuous evolution, HRET provides a robust foundation for reproducible, fair,\nand transparent Korean NLP research."
                },
                "authors": [
                    {
                        "name": "Hanwool Lee"
                    },
                    {
                        "name": "Soo Yong Kim"
                    },
                    {
                        "name": "Dasol Choi"
                    },
                    {
                        "name": "SangWon Baek"
                    },
                    {
                        "name": "Seunghyeok Hong"
                    },
                    {
                        "name": "Ilgyun Jeong"
                    },
                    {
                        "name": "Inseon Hwang"
                    },
                    {
                        "name": "Naeun Lee"
                    },
                    {
                        "name": "Guijin Son"
                    }
                ],
                "author_detail": {
                    "name": "Guijin Son"
                },
                "author": "Guijin Son",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22968v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22968v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16644v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16644v3",
                "updated": "2025-04-01T12:35:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    12,
                    35,
                    25,
                    1,
                    91,
                    0
                ],
                "published": "2024-09-25T05:44:44Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    5,
                    44,
                    44,
                    2,
                    269,
                    0
                ],
                "title": "Enabling Auditory Large Language Models for Automatic Speech Quality\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Auditory Large Language Models for Automatic Speech Quality\n  Evaluation"
                },
                "summary": "Speech quality assessment typically requires evaluating audio from multiple\naspects, such as mean opinion score (MOS) and speaker similarity (SIM) \\etc.,\nwhich can be challenging to cover using one small model designed for a single\ntask. In this paper, we propose leveraging recently introduced auditory large\nlanguage models (LLMs) for automatic speech quality assessment. By employing\ntask-specific prompts, auditory LLMs are finetuned to predict MOS, SIM and A/B\ntesting results, which are commonly used for evaluating text-to-speech systems.\nAdditionally, the finetuned auditory LLM is able to generate natural language\ndescriptions assessing aspects like noisiness, distortion, discontinuity, and\noverall quality, providing more interpretable outputs. Extensive experiments\nhave been performed on the NISQA, BVCC, SOMOS and VoxSim speech quality\ndatasets, using open-source auditory LLMs such as SALMONN, Qwen-Audio, and\nQwen2-Audio. For the natural language descriptions task, a commercial model\nGoogle Gemini 1.5 Pro is also evaluated. The results demonstrate that auditory\nLLMs achieve competitive performance compared to state-of-the-art task-specific\nsmall models in predicting MOS and SIM, while also delivering promising results\nin A/B testing and natural language descriptions. Our data processing scripts\nand finetuned model checkpoints can be found at\nhttps://github.com/bytedance/SALMONN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speech quality assessment typically requires evaluating audio from multiple\naspects, such as mean opinion score (MOS) and speaker similarity (SIM) \\etc.,\nwhich can be challenging to cover using one small model designed for a single\ntask. In this paper, we propose leveraging recently introduced auditory large\nlanguage models (LLMs) for automatic speech quality assessment. By employing\ntask-specific prompts, auditory LLMs are finetuned to predict MOS, SIM and A/B\ntesting results, which are commonly used for evaluating text-to-speech systems.\nAdditionally, the finetuned auditory LLM is able to generate natural language\ndescriptions assessing aspects like noisiness, distortion, discontinuity, and\noverall quality, providing more interpretable outputs. Extensive experiments\nhave been performed on the NISQA, BVCC, SOMOS and VoxSim speech quality\ndatasets, using open-source auditory LLMs such as SALMONN, Qwen-Audio, and\nQwen2-Audio. For the natural language descriptions task, a commercial model\nGoogle Gemini 1.5 Pro is also evaluated. The results demonstrate that auditory\nLLMs achieve competitive performance compared to state-of-the-art task-specific\nsmall models in predicting MOS and SIM, while also delivering promising results\nin A/B testing and natural language descriptions. Our data processing scripts\nand finetuned model checkpoints can be found at\nhttps://github.com/bytedance/SALMONN."
                },
                "authors": [
                    {
                        "name": "Siyin Wang"
                    },
                    {
                        "name": "Wenyi Yu"
                    },
                    {
                        "name": "Yudong Yang"
                    },
                    {
                        "name": "Changli Tang"
                    },
                    {
                        "name": "Yixuan Li"
                    },
                    {
                        "name": "Jimin Zhuang"
                    },
                    {
                        "name": "Xianzhao Chen"
                    },
                    {
                        "name": "Xiaohai Tian"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Guangzhi Sun"
                    },
                    {
                        "name": "Lu Lu"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Chao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Zhang"
                },
                "author": "Chao Zhang",
                "arxiv_comment": "Accepted by ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16644v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16644v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20290v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20290v2",
                "updated": "2025-04-01T12:33:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    12,
                    33,
                    53,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-26T07:32:20Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    7,
                    32,
                    20,
                    2,
                    85,
                    0
                ],
                "title": "QualiSpeech: A Speech Quality Assessment Dataset with Natural Language\n  Reasoning and Descriptions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QualiSpeech: A Speech Quality Assessment Dataset with Natural Language\n  Reasoning and Descriptions"
                },
                "summary": "This paper explores a novel perspective to speech quality assessment by\nleveraging natural language descriptions, offering richer, more nuanced\ninsights than traditional numerical scoring methods. Natural language feedback\nprovides instructive recommendations and detailed evaluations, yet existing\ndatasets lack the comprehensive annotations needed for this approach. To bridge\nthis gap, we introduce QualiSpeech, a comprehensive low-level speech quality\nassessment dataset encompassing 11 key aspects and detailed natural language\ncomments that include reasoning and contextual insights. Additionally, we\npropose the QualiSpeech Benchmark to evaluate the low-level speech\nunderstanding capabilities of auditory large language models (LLMs).\nExperimental results demonstrate that finetuned auditory LLMs can reliably\ngenerate detailed descriptions of noise and distortion, effectively identifying\ntheir types and temporal characteristics. The results further highlight the\npotential for incorporating reasoning to enhance the accuracy and reliability\nof quality assessments. The dataset will be released at\nhttps://huggingface.co/datasets/tsinghua-ee/QualiSpeech.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores a novel perspective to speech quality assessment by\nleveraging natural language descriptions, offering richer, more nuanced\ninsights than traditional numerical scoring methods. Natural language feedback\nprovides instructive recommendations and detailed evaluations, yet existing\ndatasets lack the comprehensive annotations needed for this approach. To bridge\nthis gap, we introduce QualiSpeech, a comprehensive low-level speech quality\nassessment dataset encompassing 11 key aspects and detailed natural language\ncomments that include reasoning and contextual insights. Additionally, we\npropose the QualiSpeech Benchmark to evaluate the low-level speech\nunderstanding capabilities of auditory large language models (LLMs).\nExperimental results demonstrate that finetuned auditory LLMs can reliably\ngenerate detailed descriptions of noise and distortion, effectively identifying\ntheir types and temporal characteristics. The results further highlight the\npotential for incorporating reasoning to enhance the accuracy and reliability\nof quality assessments. The dataset will be released at\nhttps://huggingface.co/datasets/tsinghua-ee/QualiSpeech."
                },
                "authors": [
                    {
                        "name": "Siyin Wang"
                    },
                    {
                        "name": "Wenyi Yu"
                    },
                    {
                        "name": "Xianzhao Chen"
                    },
                    {
                        "name": "Xiaohai Tian"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Lu Lu"
                    },
                    {
                        "name": "Yu Tsao"
                    },
                    {
                        "name": "Junichi Yamagishi"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Chao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Zhang"
                },
                "author": "Chao Zhang",
                "arxiv_comment": "23 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20290v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20290v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12049v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12049v4",
                "updated": "2025-04-01T12:19:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    12,
                    19,
                    49,
                    1,
                    91,
                    0
                ],
                "published": "2024-10-15T20:37:34Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    20,
                    37,
                    34,
                    1,
                    289,
                    0
                ],
                "title": "Sabiá-3 Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sabiá-3 Technical Report"
                },
                "summary": "This report presents Sabi\\'a-3, our new flagship language model, and\nSabiazinho-3, a more cost-effective sibling. The models were trained on a large\nbrazilian-centric corpus. Evaluations across diverse professional and academic\nbenchmarks show a strong performance on Portuguese and Brazil-related tasks.\nSabi\\'a-3 shows large improvements in comparison to our previous best of model,\nSabia-2 Medium, especially in reasoning-intensive tasks. Notably, Sabi\\'a-3's\naverage performance matches frontier LLMs, while it is offered at a three to\nfour times lower cost per token, reinforcing the benefits of domain\nspecialization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This report presents Sabi\\'a-3, our new flagship language model, and\nSabiazinho-3, a more cost-effective sibling. The models were trained on a large\nbrazilian-centric corpus. Evaluations across diverse professional and academic\nbenchmarks show a strong performance on Portuguese and Brazil-related tasks.\nSabi\\'a-3 shows large improvements in comparison to our previous best of model,\nSabia-2 Medium, especially in reasoning-intensive tasks. Notably, Sabi\\'a-3's\naverage performance matches frontier LLMs, while it is offered at a three to\nfour times lower cost per token, reinforcing the benefits of domain\nspecialization."
                },
                "authors": [
                    {
                        "name": "Hugo Abonizio"
                    },
                    {
                        "name": "Thales Sales Almeida"
                    },
                    {
                        "name": "Thiago Laitz"
                    },
                    {
                        "name": "Roseval Malaquias Junior"
                    },
                    {
                        "name": "Giovana Kerche Bonás"
                    },
                    {
                        "name": "Rodrigo Nogueira"
                    },
                    {
                        "name": "Ramon Pires"
                    }
                ],
                "author_detail": {
                    "name": "Ramon Pires"
                },
                "author": "Ramon Pires",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12049v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12049v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13269v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13269v2",
                "updated": "2025-04-01T12:13:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    12,
                    13,
                    46,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-17T15:22:19Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    22,
                    19,
                    0,
                    76,
                    0
                ],
                "title": "DAgent: A Relational Database-Driven Data Analysis Report Generation\n  Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DAgent: A Relational Database-Driven Data Analysis Report Generation\n  Agent"
                },
                "summary": "Relational database-driven data analysis (RDB-DA) report generation, which\naims to generate data analysis reports after querying relational databases, has\nbeen widely applied in fields such as finance and healthcare. Typically, these\ntasks are manually completed by data scientists, making the process very\nlabor-intensive and showing a clear need for automation. Although existing\nmethods (e.g., Table QA or Text-to-SQL) have been proposed to reduce human\ndependency, they cannot handle complex analytical tasks that require multi-step\nreasoning, cross-table associations, and synthesizing insights into reports.\nMoreover, there is no dataset available for developing automatic RDB-DA report\ngeneration. To fill this gap, this paper proposes an LLM agent system for\nRDB-DA report generation tasks, dubbed DAgent; moreover, we construct a\nbenchmark for automatic data analysis report generation, which includes a new\ndataset DA-Dataset and evaluation metrics. DAgent integrates planning, tools,\nand memory modules to decompose natural language questions into logically\nindependent sub-queries, accurately retrieve key information from relational\ndatabases, and generate analytical reports that meet the requirements of\ncompleteness, correctness, and conciseness through multi-step reasoning and\neffective data integration. Experimental analysis on the DA-Dataset\ndemonstrates that DAgent's superiority in retrieval performance and analysis\nreport generation quality, showcasing its strong potential for tackling complex\ndatabase analysis report generation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relational database-driven data analysis (RDB-DA) report generation, which\naims to generate data analysis reports after querying relational databases, has\nbeen widely applied in fields such as finance and healthcare. Typically, these\ntasks are manually completed by data scientists, making the process very\nlabor-intensive and showing a clear need for automation. Although existing\nmethods (e.g., Table QA or Text-to-SQL) have been proposed to reduce human\ndependency, they cannot handle complex analytical tasks that require multi-step\nreasoning, cross-table associations, and synthesizing insights into reports.\nMoreover, there is no dataset available for developing automatic RDB-DA report\ngeneration. To fill this gap, this paper proposes an LLM agent system for\nRDB-DA report generation tasks, dubbed DAgent; moreover, we construct a\nbenchmark for automatic data analysis report generation, which includes a new\ndataset DA-Dataset and evaluation metrics. DAgent integrates planning, tools,\nand memory modules to decompose natural language questions into logically\nindependent sub-queries, accurately retrieve key information from relational\ndatabases, and generate analytical reports that meet the requirements of\ncompleteness, correctness, and conciseness through multi-step reasoning and\neffective data integration. Experimental analysis on the DA-Dataset\ndemonstrates that DAgent's superiority in retrieval performance and analysis\nreport generation quality, showcasing its strong potential for tackling complex\ndatabase analysis report generation tasks."
                },
                "authors": [
                    {
                        "name": "Wenyi Xu"
                    },
                    {
                        "name": "Yuren Mao"
                    },
                    {
                        "name": "Xiaolu Zhang"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Xuemei Dong"
                    },
                    {
                        "name": "Mengfei Zhang"
                    },
                    {
                        "name": "Yunjun Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yunjun Gao"
                },
                "author": "Yunjun Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13269v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13269v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17216v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17216v2",
                "updated": "2025-04-01T10:49:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    10,
                    49,
                    56,
                    1,
                    91,
                    0
                ],
                "published": "2024-06-25T02:05:29Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    2,
                    5,
                    29,
                    1,
                    177,
                    0
                ],
                "title": "Machine Unlearning Fails to Remove Data Poisoning Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine Unlearning Fails to Remove Data Poisoning Attacks"
                },
                "summary": "We revisit the efficacy of several practical methods for approximate machine\nunlearning developed for large-scale deep learning. In addition to complying\nwith data deletion requests, one often-cited potential application for\nunlearning methods is to remove the effects of poisoned data. We experimentally\ndemonstrate that, while existing unlearning methods have been demonstrated to\nbe effective in a number of settings, they fail to remove the effects of data\npoisoning across a variety of types of poisoning attacks (indiscriminate,\ntargeted, and a newly-introduced Gaussian poisoning attack) and models (image\nclassifiers and LLMs); even when granted a relatively large compute budget. In\norder to precisely characterize unlearning efficacy, we introduce new\nevaluation metrics for unlearning based on data poisoning. Our results suggest\nthat a broader perspective, including a wider variety of evaluations, are\nrequired to avoid a false sense of confidence in machine unlearning procedures\nfor deep learning without provable guarantees. Moreover, while unlearning\nmethods show some signs of being useful to efficiently remove poisoned data\nwithout having to retrain, our work suggests that these methods are not yet\n``ready for prime time,'' and currently provide limited benefit over\nretraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We revisit the efficacy of several practical methods for approximate machine\nunlearning developed for large-scale deep learning. In addition to complying\nwith data deletion requests, one often-cited potential application for\nunlearning methods is to remove the effects of poisoned data. We experimentally\ndemonstrate that, while existing unlearning methods have been demonstrated to\nbe effective in a number of settings, they fail to remove the effects of data\npoisoning across a variety of types of poisoning attacks (indiscriminate,\ntargeted, and a newly-introduced Gaussian poisoning attack) and models (image\nclassifiers and LLMs); even when granted a relatively large compute budget. In\norder to precisely characterize unlearning efficacy, we introduce new\nevaluation metrics for unlearning based on data poisoning. Our results suggest\nthat a broader perspective, including a wider variety of evaluations, are\nrequired to avoid a false sense of confidence in machine unlearning procedures\nfor deep learning without provable guarantees. Moreover, while unlearning\nmethods show some signs of being useful to efficiently remove poisoned data\nwithout having to retrain, our work suggests that these methods are not yet\n``ready for prime time,'' and currently provide limited benefit over\nretraining."
                },
                "authors": [
                    {
                        "name": "Martin Pawelczyk"
                    },
                    {
                        "name": "Jimmy Z. Di"
                    },
                    {
                        "name": "Yiwei Lu"
                    },
                    {
                        "name": "Ayush Sekhari"
                    },
                    {
                        "name": "Gautam Kamath"
                    },
                    {
                        "name": "Seth Neel"
                    }
                ],
                "author_detail": {
                    "name": "Seth Neel"
                },
                "author": "Seth Neel",
                "arxiv_comment": "Published at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17216v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17216v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22517v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22517v2",
                "updated": "2025-04-01T10:42:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    10,
                    42,
                    11,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-28T15:21:24Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    15,
                    21,
                    24,
                    4,
                    87,
                    0
                ],
                "title": "Exploiting Mixture-of-Experts Redundancy Unlocks Multimodal Generative\n  Abilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploiting Mixture-of-Experts Redundancy Unlocks Multimodal Generative\n  Abilities"
                },
                "summary": "In this work, we undertake the challenge of augmenting the existing\ngenerative capabilities of pre-trained text-only large language models (LLMs)\nwith multi-modal generation capability while satisfying two core constraints:\nC1 preserving the preservation of original language generative capabilities\nwith negligible performance degradation, and C2 adhering to a small parameter\nbudget to learn the new modality, ensuring scalability and efficiency. In\ncontrast to current approaches that add dedicated modules, thereby\nsignificantly increasing the parameter count, we propose a method that\nleverages the underutilized capacity inherent in deep models. Specifically, we\nexploit the parameter redundancy within Mixture-of-Experts (MoEs) as a source\nof additional capacity for learning a new modality, enabling better parameter\nefficiency (C1). Moreover, we preserve the original language generation\ncapabilities by applying low-rank adaptation exclusively to the tokens of the\nnew modality (C2). Furthermore, we introduce a novel parameter initialization\nscheme based on the Gromov-Wasserstein distance to improve convergence and\ntraining stability. Through an extensive analysis of the routing mechanism, we\nuncover the emergence of modality-specific pathways and decreased redundancy\nwithin the experts that can efficiently unlock multi-modal generative\ncapabilities. Overall, our method can be seamlessly applied to a wide range of\ncontemporary LLMs, providing a new pathway for transitioning from uni-modal to\nmulti-modal architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we undertake the challenge of augmenting the existing\ngenerative capabilities of pre-trained text-only large language models (LLMs)\nwith multi-modal generation capability while satisfying two core constraints:\nC1 preserving the preservation of original language generative capabilities\nwith negligible performance degradation, and C2 adhering to a small parameter\nbudget to learn the new modality, ensuring scalability and efficiency. In\ncontrast to current approaches that add dedicated modules, thereby\nsignificantly increasing the parameter count, we propose a method that\nleverages the underutilized capacity inherent in deep models. Specifically, we\nexploit the parameter redundancy within Mixture-of-Experts (MoEs) as a source\nof additional capacity for learning a new modality, enabling better parameter\nefficiency (C1). Moreover, we preserve the original language generation\ncapabilities by applying low-rank adaptation exclusively to the tokens of the\nnew modality (C2). Furthermore, we introduce a novel parameter initialization\nscheme based on the Gromov-Wasserstein distance to improve convergence and\ntraining stability. Through an extensive analysis of the routing mechanism, we\nuncover the emergence of modality-specific pathways and decreased redundancy\nwithin the experts that can efficiently unlock multi-modal generative\ncapabilities. Overall, our method can be seamlessly applied to a wide range of\ncontemporary LLMs, providing a new pathway for transitioning from uni-modal to\nmulti-modal architectures."
                },
                "authors": [
                    {
                        "name": "Raman Dutt"
                    },
                    {
                        "name": "Harleen Hanspal"
                    },
                    {
                        "name": "Guoxuan Xia"
                    },
                    {
                        "name": "Petru-Daniel Tudosiu"
                    },
                    {
                        "name": "Alexander Black"
                    },
                    {
                        "name": "Yongxin Yang"
                    },
                    {
                        "name": "Steven McDonagh"
                    },
                    {
                        "name": "Sarah Parisot"
                    }
                ],
                "author_detail": {
                    "name": "Sarah Parisot"
                },
                "author": "Sarah Parisot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22517v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22517v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01684v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01684v3",
                "updated": "2025-04-01T10:40:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    10,
                    40,
                    1,
                    1,
                    91,
                    0
                ],
                "published": "2025-02-02T07:42:45Z",
                "published_parsed": [
                    2025,
                    2,
                    2,
                    7,
                    42,
                    45,
                    6,
                    33,
                    0
                ],
                "title": "Leveraging Joint Predictive Embedding and Bayesian Inference in Graph\n  Self Supervised Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Joint Predictive Embedding and Bayesian Inference in Graph\n  Self Supervised Learning"
                },
                "summary": "Graph representation learning has emerged as a cornerstone for tasks like\nnode classification and link prediction, yet prevailing self-supervised\nlearning (SSL) methods face challenges such as computational inefficiency,\nreliance on contrastive objectives, and representation collapse. Existing\napproaches often depend on feature reconstruction, negative sampling, or\ncomplex decoders, which introduce training overhead and hinder generalization.\nFurther, current techniques which address such limitations fail to account for\nthe contribution of node embeddings to a certain prediction in the absence of\nlabeled nodes. To address these limitations, we propose a novel joint embedding\npredictive framework for graph SSL that eliminates contrastive objectives and\nnegative sampling while preserving semantic and structural information.\nAdditionally, we introduce a semantic-aware objective term that incorporates\npseudo-labels derived from Gaussian Mixture Models (GMMs), enhancing node\ndiscriminability by evaluating latent feature contributions. Extensive\nexperiments demonstrate that our framework outperforms state-of-the-art graph\nSSL methods across benchmarks, achieving superior performance without\ncontrastive loss or complex decoders. Key innovations include (1) a\nnon-contrastive, view-invariant joint embedding predictive architecture, (2)\nLeveraging single context and multiple targets relationship between subgraphs,\nand (3) GMM-based pseudo-label scoring to capture semantic contributions. This\nwork advances graph SSL by offering a computationally efficient,\ncollapse-resistant paradigm that bridges spatial and semantic graph features\nfor downstream tasks. The code for our paper can be found at\nhttps://github.com/Deceptrax123/JPEB-GSSL",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph representation learning has emerged as a cornerstone for tasks like\nnode classification and link prediction, yet prevailing self-supervised\nlearning (SSL) methods face challenges such as computational inefficiency,\nreliance on contrastive objectives, and representation collapse. Existing\napproaches often depend on feature reconstruction, negative sampling, or\ncomplex decoders, which introduce training overhead and hinder generalization.\nFurther, current techniques which address such limitations fail to account for\nthe contribution of node embeddings to a certain prediction in the absence of\nlabeled nodes. To address these limitations, we propose a novel joint embedding\npredictive framework for graph SSL that eliminates contrastive objectives and\nnegative sampling while preserving semantic and structural information.\nAdditionally, we introduce a semantic-aware objective term that incorporates\npseudo-labels derived from Gaussian Mixture Models (GMMs), enhancing node\ndiscriminability by evaluating latent feature contributions. Extensive\nexperiments demonstrate that our framework outperforms state-of-the-art graph\nSSL methods across benchmarks, achieving superior performance without\ncontrastive loss or complex decoders. Key innovations include (1) a\nnon-contrastive, view-invariant joint embedding predictive architecture, (2)\nLeveraging single context and multiple targets relationship between subgraphs,\nand (3) GMM-based pseudo-label scoring to capture semantic contributions. This\nwork advances graph SSL by offering a computationally efficient,\ncollapse-resistant paradigm that bridges spatial and semantic graph features\nfor downstream tasks. The code for our paper can be found at\nhttps://github.com/Deceptrax123/JPEB-GSSL"
                },
                "authors": [
                    {
                        "name": "Srinitish Srinivasan"
                    },
                    {
                        "name": "Omkumar CU"
                    }
                ],
                "author_detail": {
                    "name": "Omkumar CU"
                },
                "author": "Omkumar CU",
                "arxiv_comment": "Preprint. Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01684v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01684v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19129v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19129v2",
                "updated": "2025-04-01T10:34:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    10,
                    34,
                    1,
                    1,
                    91,
                    0
                ],
                "published": "2024-11-28T13:23:02Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    13,
                    23,
                    2,
                    3,
                    333,
                    0
                ],
                "title": "Can Neutron Star Tidal Effects Obscure Deviations from General\n  Relativity?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Neutron Star Tidal Effects Obscure Deviations from General\n  Relativity?"
                },
                "summary": "One of the main goals of gravitational-wave astrophysics is to study gravity\nin the strong-field regime and constrain deviations from general relativity\n(GR). Any such deviation affects not only binary dynamics and\ngravitational-wave emission but also the structure and tidal properties of\ncompact objects. In the case of neutron stars, masses, radii, and tidal\ndeformabilities can all differ significantly between different theories of\ngravity. Currently, the measurement uncertainties in neutron star radii and\ntidal deformabilities are quite large. However, much less is known about how\nthe large uncertainty in the nuclear equation of state (EOS) might affect tests\nof GR using binary neutron star mergers. Conversely, using the wrong theory of\ngravity might lead to incorrect constraints on the nuclear EOS. Here, we study\nthis problem within scalar-tensor (ST) theory. We apply the recently derived\n$\\ell = 2$ tidal Love numbers in this theory to parameter estimation of\nGW170817. Correspondingly, we test if physics beyond GR could bias measurements\nof the nuclear EOS and neutron star radii. We find that parameter inference for\nboth the GR and ST cases return consistent component masses and tidal\ndeformabilities. The radius and the EOS posteriors, however, differ between the\ntwo theories, but neither is excluded by current observational limits. This\nindicates that measurements of the nuclear EOS may be biased and that\ndeviations from GR could go undetected when analyzing current binary neutron\nstar mergers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One of the main goals of gravitational-wave astrophysics is to study gravity\nin the strong-field regime and constrain deviations from general relativity\n(GR). Any such deviation affects not only binary dynamics and\ngravitational-wave emission but also the structure and tidal properties of\ncompact objects. In the case of neutron stars, masses, radii, and tidal\ndeformabilities can all differ significantly between different theories of\ngravity. Currently, the measurement uncertainties in neutron star radii and\ntidal deformabilities are quite large. However, much less is known about how\nthe large uncertainty in the nuclear equation of state (EOS) might affect tests\nof GR using binary neutron star mergers. Conversely, using the wrong theory of\ngravity might lead to incorrect constraints on the nuclear EOS. Here, we study\nthis problem within scalar-tensor (ST) theory. We apply the recently derived\n$\\ell = 2$ tidal Love numbers in this theory to parameter estimation of\nGW170817. Correspondingly, we test if physics beyond GR could bias measurements\nof the nuclear EOS and neutron star radii. We find that parameter inference for\nboth the GR and ST cases return consistent component masses and tidal\ndeformabilities. The radius and the EOS posteriors, however, differ between the\ntwo theories, but neither is excluded by current observational limits. This\nindicates that measurements of the nuclear EOS may be biased and that\ndeviations from GR could go undetected when analyzing current binary neutron\nstar mergers."
                },
                "authors": [
                    {
                        "name": "Stephanie M. Brown"
                    },
                    {
                        "name": "Badri Krishnan"
                    },
                    {
                        "name": "Rahul Somasundaram"
                    },
                    {
                        "name": "Ingo Tews"
                    }
                ],
                "author_detail": {
                    "name": "Ingo Tews"
                },
                "author": "Ingo Tews",
                "arxiv_doi": "10.3847/1538-4357/adb966",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4357/adb966",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.19129v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19129v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "6 pages. 2 figures",
                "arxiv_journal_ref": "ApJ 982 133 (2025)",
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23859v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23859v2",
                "updated": "2025-04-01T10:23:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    10,
                    23,
                    7,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-31T09:06:23Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    9,
                    6,
                    23,
                    0,
                    90,
                    0
                ],
                "title": "Evaluating small vision-language models as AI assistants for radio\n  astronomical source analysis tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating small vision-language models as AI assistants for radio\n  astronomical source analysis tasks"
                },
                "summary": "The advent of next-generation radio telescopes is set to transform radio\nastronomy by producing massive data volumes that challenge traditional\nprocessing methods. Deep learning techniques have shown strong potential in\nautomating radio analysis tasks, yet are often constrained by the limited\navailability of large annotated datasets. Recent progress in self-supervised\nlearning has led to foundational radio vision models, but adapting them for new\ntasks typically requires coding expertise, limiting their accessibility to a\nbroader astronomical community. Text-based AI interfaces offer a promising\nalternative by enabling task-specific queries and example-driven learning. In\nthis context, Large Language Models (LLMs), with their remarkable zero-shot\ncapabilities, are increasingly used in scientific domains. However, deploying\nlarge-scale models remains resource-intensive, and there is a growing demand\nfor AI systems that can reason over both visual and textual data in\nastronomical analysis. This study explores small-scale Vision-Language Models\n(VLMs) as AI assistants for radio astronomy, combining LLM capabilities with\nvision transformers. We fine-tuned the LLaVA VLM on a dataset of 59k radio\nimages from multiple surveys, enriched with 38k image-caption pairs from the\nliterature. The fine-tuned models show clear improvements over base models in\nradio-specific tasks, achieving ~30% F1-score gains in extended source\ndetection, but they underperform pure vision models and exhibit ~20% drop on\ngeneral multimodal tasks. Inclusion of caption data and LoRA fine-tuning\nenhances instruction-following and helps recover ~10% accuracy on standard\nbenchmarks. This work lays the foundation for future advancements in radio\nVLMs, highlighting their potential and limitations, such as the need for better\nmultimodal alignment, higher-quality datasets, and mitigation of catastrophic\nforgetting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of next-generation radio telescopes is set to transform radio\nastronomy by producing massive data volumes that challenge traditional\nprocessing methods. Deep learning techniques have shown strong potential in\nautomating radio analysis tasks, yet are often constrained by the limited\navailability of large annotated datasets. Recent progress in self-supervised\nlearning has led to foundational radio vision models, but adapting them for new\ntasks typically requires coding expertise, limiting their accessibility to a\nbroader astronomical community. Text-based AI interfaces offer a promising\nalternative by enabling task-specific queries and example-driven learning. In\nthis context, Large Language Models (LLMs), with their remarkable zero-shot\ncapabilities, are increasingly used in scientific domains. However, deploying\nlarge-scale models remains resource-intensive, and there is a growing demand\nfor AI systems that can reason over both visual and textual data in\nastronomical analysis. This study explores small-scale Vision-Language Models\n(VLMs) as AI assistants for radio astronomy, combining LLM capabilities with\nvision transformers. We fine-tuned the LLaVA VLM on a dataset of 59k radio\nimages from multiple surveys, enriched with 38k image-caption pairs from the\nliterature. The fine-tuned models show clear improvements over base models in\nradio-specific tasks, achieving ~30% F1-score gains in extended source\ndetection, but they underperform pure vision models and exhibit ~20% drop on\ngeneral multimodal tasks. Inclusion of caption data and LoRA fine-tuning\nenhances instruction-following and helps recover ~10% accuracy on standard\nbenchmarks. This work lays the foundation for future advancements in radio\nVLMs, highlighting their potential and limitations, such as the need for better\nmultimodal alignment, higher-quality datasets, and mitigation of catastrophic\nforgetting."
                },
                "authors": [
                    {
                        "name": "S. Riggi"
                    },
                    {
                        "name": "T. Cecconello"
                    },
                    {
                        "name": "A. Pilzer"
                    },
                    {
                        "name": "S. Palazzo"
                    },
                    {
                        "name": "N. Gupta"
                    },
                    {
                        "name": "A. M. Hopkins"
                    },
                    {
                        "name": "C. Trigilio"
                    },
                    {
                        "name": "G. Umana"
                    }
                ],
                "author_detail": {
                    "name": "G. Umana"
                },
                "author": "G. Umana",
                "arxiv_comment": "17 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23859v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23859v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01503v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01503v3",
                "updated": "2025-04-01T10:22:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    10,
                    22,
                    26,
                    1,
                    91,
                    0
                ],
                "published": "2024-11-03T09:49:12Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    9,
                    49,
                    12,
                    6,
                    308,
                    0
                ],
                "title": "A Highly Scalable LLM Clusters with Optical Interconnect",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Highly Scalable LLM Clusters with Optical Interconnect"
                },
                "summary": "We propose \\emph{LumosCore} to build high-bandwidth and large-scale data\ncenter networks for LLM jobs. By replacing the core-layer electrical packet\nswitches by optical circuit switches, \\emph{LumosCore} could achieves $2\\times$\nincrease in bandwidth or $8\\times$ increase in network size. We offer the\ndetailed design of \\emph{LumosCore} at both deployment stage and running stage.\nAt deployment stage, we propose Interleaved Wiring, which is compatible with\nall possible logical topologies. At running stage, we design polynomial-time\nalgorithms for GPU placement, logical topology generating and OCS\nreconfiguration to minimize network contention and reduce impact to scheduled\njobs. We evaluate \\emph{LumosCore} using both testbed experiments and\nlarge-scale simulation. Compared to traditional hybrid optical/electrical\narchitectures, \\emph{LumosCore} increases the end-to-end training throughput by\nup to 39.5\\% on a 128-node testbed. Compared to the state-of-art Clos\narchitectures, \\emph{LumosCore} reduces the average job completion time by up\nto 34.1\\% in a 16k simulation platform.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose \\emph{LumosCore} to build high-bandwidth and large-scale data\ncenter networks for LLM jobs. By replacing the core-layer electrical packet\nswitches by optical circuit switches, \\emph{LumosCore} could achieves $2\\times$\nincrease in bandwidth or $8\\times$ increase in network size. We offer the\ndetailed design of \\emph{LumosCore} at both deployment stage and running stage.\nAt deployment stage, we propose Interleaved Wiring, which is compatible with\nall possible logical topologies. At running stage, we design polynomial-time\nalgorithms for GPU placement, logical topology generating and OCS\nreconfiguration to minimize network contention and reduce impact to scheduled\njobs. We evaluate \\emph{LumosCore} using both testbed experiments and\nlarge-scale simulation. Compared to traditional hybrid optical/electrical\narchitectures, \\emph{LumosCore} increases the end-to-end training throughput by\nup to 39.5\\% on a 128-node testbed. Compared to the state-of-art Clos\narchitectures, \\emph{LumosCore} reduces the average job completion time by up\nto 34.1\\% in a 16k simulation platform."
                },
                "authors": [
                    {
                        "name": "Xinchi Han"
                    },
                    {
                        "name": "Yongxi Lv"
                    },
                    {
                        "name": "Shizhen Zhao"
                    },
                    {
                        "name": "Zhuotao Liu"
                    },
                    {
                        "name": "Ximeng Liu"
                    },
                    {
                        "name": "Xinbing Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinbing Wang"
                },
                "author": "Xinbing Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01503v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01503v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00591v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00591v2",
                "updated": "2025-04-01T10:21:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    10,
                    21,
                    48,
                    1,
                    91,
                    0
                ],
                "published": "2024-09-01T02:53:24Z",
                "published_parsed": [
                    2024,
                    9,
                    1,
                    2,
                    53,
                    24,
                    6,
                    245,
                    0
                ],
                "title": "Attention-Guided Multi-scale Interaction Network for Face\n  Super-Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention-Guided Multi-scale Interaction Network for Face\n  Super-Resolution"
                },
                "summary": "Recently, CNN and Transformer hybrid networks demonstrated excellent\nperformance in face super-resolution (FSR) tasks. Since numerous features at\ndifferent scales in hybrid networks, how to fuse these multi-scale features and\npromote their complementarity is crucial for enhancing FSR. However, existing\nhybrid network-based FSR methods ignore this, only simply combining the\nTransformer and CNN. To address this issue, we propose an attention-guided\nMulti-scale interaction network (AMINet), which contains local and global\nfeature interactions and encoder-decoder phase feature interactions.\nSpecifically, we propose a Local and Global Feature Interaction Module (LGFI)\nto promote fusions of global features and different receptive fields' local\nfeatures extracted by our Residual Depth Feature Extraction Module (RDFE).\nAdditionally, we propose a Selective Kernel Attention Fusion Module (SKAF) to\nadaptively select fusions of different features within LGFI and encoder-decoder\nphases. Our above design allows the free flow of multi-scale features from\nwithin modules and between encoder and decoder, which can promote the\ncomplementarity of different scale features to enhance FSR. Comprehensive\nexperiments confirm that our method consistently performs well with less\ncomputational consumption and faster inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, CNN and Transformer hybrid networks demonstrated excellent\nperformance in face super-resolution (FSR) tasks. Since numerous features at\ndifferent scales in hybrid networks, how to fuse these multi-scale features and\npromote their complementarity is crucial for enhancing FSR. However, existing\nhybrid network-based FSR methods ignore this, only simply combining the\nTransformer and CNN. To address this issue, we propose an attention-guided\nMulti-scale interaction network (AMINet), which contains local and global\nfeature interactions and encoder-decoder phase feature interactions.\nSpecifically, we propose a Local and Global Feature Interaction Module (LGFI)\nto promote fusions of global features and different receptive fields' local\nfeatures extracted by our Residual Depth Feature Extraction Module (RDFE).\nAdditionally, we propose a Selective Kernel Attention Fusion Module (SKAF) to\nadaptively select fusions of different features within LGFI and encoder-decoder\nphases. Our above design allows the free flow of multi-scale features from\nwithin modules and between encoder and decoder, which can promote the\ncomplementarity of different scale features to enhance FSR. Comprehensive\nexperiments confirm that our method consistently performs well with less\ncomputational consumption and faster inference."
                },
                "authors": [
                    {
                        "name": "Xujie Wan"
                    },
                    {
                        "name": "Wenjie Li"
                    },
                    {
                        "name": "Guangwei Gao"
                    },
                    {
                        "name": "Huimin Lu"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Chia-Wen Lin"
                    }
                ],
                "author_detail": {
                    "name": "Chia-Wen Lin"
                },
                "author": "Chia-Wen Lin",
                "arxiv_comment": "13 pages, 11 figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00591v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00591v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.08035v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.08035v2",
                "updated": "2025-04-01T10:19:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    10,
                    19,
                    16,
                    1,
                    91,
                    0
                ],
                "published": "2024-07-10T20:32:50Z",
                "published_parsed": [
                    2024,
                    7,
                    10,
                    20,
                    32,
                    50,
                    2,
                    192,
                    0
                ],
                "title": "FsPONER: Few-shot Prompt Optimization for Named Entity Recognition in\n  Domain-specific Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FsPONER: Few-shot Prompt Optimization for Named Entity Recognition in\n  Domain-specific Scenarios"
                },
                "summary": "Large Language Models (LLMs) have provided a new pathway for Named Entity\nRecognition (NER) tasks. Compared with fine-tuning, LLM-powered prompting\nmethods avoid the need for training, conserve substantial computational\nresources, and rely on minimal annotated data. Previous studies have achieved\ncomparable performance to fully supervised BERT-based fine-tuning approaches on\ngeneral NER benchmarks. However, none of the previous approaches has\ninvestigated the efficiency of LLM-based few-shot learning in domain-specific\nscenarios. To address this gap, we introduce FsPONER, a novel approach for\noptimizing few-shot prompts, and evaluate its performance on domain-specific\nNER datasets, with a focus on industrial manufacturing and maintenance, while\nusing multiple LLMs -- GPT-4-32K, GPT-3.5-Turbo, LLaMA 2-chat, and Vicuna.\nFsPONER consists of three few-shot selection methods based on random sampling,\nTF-IDF vectors, and a combination of both. We compare these methods with a\ngeneral-purpose GPT-NER method as the number of few-shot examples increases and\nevaluate their optimal NER performance against fine-tuned BERT and LLaMA\n2-chat. In the considered real-world scenarios with data scarcity, FsPONER with\nTF-IDF surpasses fine-tuned models by approximately 10% in F1 score.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have provided a new pathway for Named Entity\nRecognition (NER) tasks. Compared with fine-tuning, LLM-powered prompting\nmethods avoid the need for training, conserve substantial computational\nresources, and rely on minimal annotated data. Previous studies have achieved\ncomparable performance to fully supervised BERT-based fine-tuning approaches on\ngeneral NER benchmarks. However, none of the previous approaches has\ninvestigated the efficiency of LLM-based few-shot learning in domain-specific\nscenarios. To address this gap, we introduce FsPONER, a novel approach for\noptimizing few-shot prompts, and evaluate its performance on domain-specific\nNER datasets, with a focus on industrial manufacturing and maintenance, while\nusing multiple LLMs -- GPT-4-32K, GPT-3.5-Turbo, LLaMA 2-chat, and Vicuna.\nFsPONER consists of three few-shot selection methods based on random sampling,\nTF-IDF vectors, and a combination of both. We compare these methods with a\ngeneral-purpose GPT-NER method as the number of few-shot examples increases and\nevaluate their optimal NER performance against fine-tuned BERT and LLaMA\n2-chat. In the considered real-world scenarios with data scarcity, FsPONER with\nTF-IDF surpasses fine-tuned models by approximately 10% in F1 score."
                },
                "authors": [
                    {
                        "name": "Yongjian Tang"
                    },
                    {
                        "name": "Rakebul Hasan"
                    },
                    {
                        "name": "Thomas Runkler"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Runkler"
                },
                "author": "Thomas Runkler",
                "arxiv_comment": "accepted in the main track at the 27th European Conference on\n  Artificial Intelligence (ECAI-2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.08035v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.08035v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.16984v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.16984v7",
                "updated": "2025-04-01T09:43:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    9,
                    43,
                    41,
                    1,
                    91,
                    0
                ],
                "published": "2023-11-28T17:35:38Z",
                "published_parsed": [
                    2023,
                    11,
                    28,
                    17,
                    35,
                    38,
                    1,
                    332,
                    0
                ],
                "title": "FedECA: A Federated External Control Arm Method for Causal Inference\n  with Time-To-Event Data in Distributed Settings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedECA: A Federated External Control Arm Method for Causal Inference\n  with Time-To-Event Data in Distributed Settings"
                },
                "summary": "External control arms (ECA) can inform the early clinical development of\nexperimental drugs and provide efficacy evidence for regulatory approval.\nHowever, the main challenge in implementing ECA lies in accessing real-world or\nhistorical clinical trials data. Indeed, regulations protecting patients'\nrights by strictly controlling data processing make pooling data from multiple\nsources in a central server often difficult. To address these limitations, we\ndevelop a new method, 'FedECA' that leverages federated learning (FL) to enable\ninverse probability of treatment weighting (IPTW) for time-to-event outcomes on\nseparate cohorts without needing to pool data. To showcase the potential of\nFedECA, we apply it in different settings of increasing complexity culminating\nwith a real-world use-case in which FedECA is used to compare the treatment\neffect of two approved chemotherapy regimens using data from three separate\ncohorts of patients with metastatic pancreatic cancer. By sharing our code, we\nhope FedECA will foster the creation of federated research networks and thus\naccelerate drug development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "External control arms (ECA) can inform the early clinical development of\nexperimental drugs and provide efficacy evidence for regulatory approval.\nHowever, the main challenge in implementing ECA lies in accessing real-world or\nhistorical clinical trials data. Indeed, regulations protecting patients'\nrights by strictly controlling data processing make pooling data from multiple\nsources in a central server often difficult. To address these limitations, we\ndevelop a new method, 'FedECA' that leverages federated learning (FL) to enable\ninverse probability of treatment weighting (IPTW) for time-to-event outcomes on\nseparate cohorts without needing to pool data. To showcase the potential of\nFedECA, we apply it in different settings of increasing complexity culminating\nwith a real-world use-case in which FedECA is used to compare the treatment\neffect of two approved chemotherapy regimens using data from three separate\ncohorts of patients with metastatic pancreatic cancer. By sharing our code, we\nhope FedECA will foster the creation of federated research networks and thus\naccelerate drug development."
                },
                "authors": [
                    {
                        "name": "Jean Ogier du Terrail"
                    },
                    {
                        "name": "Quentin Klopfenstein"
                    },
                    {
                        "name": "Honghao Li"
                    },
                    {
                        "name": "Imke Mayer"
                    },
                    {
                        "name": "Nicolas Loiseau"
                    },
                    {
                        "name": "Mohammad Hallal"
                    },
                    {
                        "name": "Michael Debouver"
                    },
                    {
                        "name": "Thibault Camalon"
                    },
                    {
                        "name": "Thibault Fouqueray"
                    },
                    {
                        "name": "Jorge Arellano Castro"
                    },
                    {
                        "name": "Zahia Yanes"
                    },
                    {
                        "name": "Laetitia Dahan"
                    },
                    {
                        "name": "Julien Taïeb"
                    },
                    {
                        "name": "Pierre Laurent-Puig"
                    },
                    {
                        "name": "Jean-Baptiste Bachet"
                    },
                    {
                        "name": "Shulin Zhao"
                    },
                    {
                        "name": "Remy Nicolle"
                    },
                    {
                        "name": "Jérome Cros"
                    },
                    {
                        "name": "Daniel Gonzalez"
                    },
                    {
                        "name": "Robert Carreras-Torres"
                    },
                    {
                        "name": "Adelaida Garcia Velasco"
                    },
                    {
                        "name": "Kawther Abdilleh"
                    },
                    {
                        "name": "Sudheer Doss"
                    },
                    {
                        "name": "Félix Balazard"
                    },
                    {
                        "name": "Mathieu Andreux"
                    }
                ],
                "author_detail": {
                    "name": "Mathieu Andreux"
                },
                "author": "Mathieu Andreux",
                "arxiv_comment": "code available at: https://github.com/owkin/fedeca, bug in SMD\n  computation present in v1 and v2 fixed, many experiments on real data added +\n  fix in YODA experiments using imputed data instead of raw data (v3->v4) +\n  affiliations fix + more precise wording for acknowledgments, real-world\n  experiment results fixed by excluding data with bias + text polished (v5->v6)\n  + updating abstract(v6->v7)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.16984v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.16984v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00596v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00596v2",
                "updated": "2025-04-01T09:33:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    9,
                    33,
                    55,
                    1,
                    91,
                    0
                ],
                "published": "2024-11-30T22:02:12Z",
                "published_parsed": [
                    2024,
                    11,
                    30,
                    22,
                    2,
                    12,
                    5,
                    335,
                    0
                ],
                "title": "PhyT2V: LLM-Guided Iterative Self-Refinement for Physics-Grounded\n  Text-to-Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhyT2V: LLM-Guided Iterative Self-Refinement for Physics-Grounded\n  Text-to-Video Generation"
                },
                "summary": "Text-to-video (T2V) generation has been recently enabled by transformer-based\ndiffusion models, but current T2V models lack capabilities in adhering to the\nreal-world common knowledge and physical rules, due to their limited\nunderstanding of physical realism and deficiency in temporal modeling. Existing\nsolutions are either data-driven or require extra model inputs, but cannot be\ngeneralizable to out-of-distribution domains. In this paper, we present PhyT2V,\na new data-independent T2V technique that expands the current T2V model's\ncapability of video generation to out-of-distribution domains, by enabling\nchain-of-thought and step-back reasoning in T2V prompting. Our experiments show\nthat PhyT2V improves existing T2V models' adherence to real-world physical\nrules by 2.3x, and achieves 35% improvement compared to T2V prompt enhancers.\nThe source codes are available at: https://github.com/pittisl/PhyT2V.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-video (T2V) generation has been recently enabled by transformer-based\ndiffusion models, but current T2V models lack capabilities in adhering to the\nreal-world common knowledge and physical rules, due to their limited\nunderstanding of physical realism and deficiency in temporal modeling. Existing\nsolutions are either data-driven or require extra model inputs, but cannot be\ngeneralizable to out-of-distribution domains. In this paper, we present PhyT2V,\na new data-independent T2V technique that expands the current T2V model's\ncapability of video generation to out-of-distribution domains, by enabling\nchain-of-thought and step-back reasoning in T2V prompting. Our experiments show\nthat PhyT2V improves existing T2V models' adherence to real-world physical\nrules by 2.3x, and achieves 35% improvement compared to T2V prompt enhancers.\nThe source codes are available at: https://github.com/pittisl/PhyT2V."
                },
                "authors": [
                    {
                        "name": "Qiyao Xue"
                    },
                    {
                        "name": "Xiangyu Yin"
                    },
                    {
                        "name": "Boyuan Yang"
                    },
                    {
                        "name": "Wei Gao"
                    }
                ],
                "author_detail": {
                    "name": "Wei Gao"
                },
                "author": "Wei Gao",
                "arxiv_comment": "28 pages",
                "arxiv_journal_ref": "in Proceedings of the IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition (CVPR), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00596v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00596v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17003v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17003v3",
                "updated": "2025-04-01T09:33:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    9,
                    33,
                    19,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-21T10:09:16Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    10,
                    9,
                    16,
                    4,
                    80,
                    0
                ],
                "title": "A Survey on Personalized Alignment -- The Missing Piece for Large\n  Language Models in Real-World Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Personalized Alignment -- The Missing Piece for Large\n  Language Models in Real-World Applications"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities, yet\ntheir transition to real-world applications reveals a critical limitation: the\ninability to adapt to individual preferences while maintaining alignment with\nuniversal human values. Current alignment techniques adopt a one-size-fits-all\napproach that fails to accommodate users' diverse backgrounds and needs. This\npaper presents the first comprehensive survey of personalized alignment-a\nparadigm that enables LLMs to adapt their behavior within ethical boundaries\nbased on individual preferences. We propose a unified framework comprising\npreference memory management, personalized generation, and feedback-based\nalignment, systematically analyzing implementation approaches and evaluating\ntheir effectiveness across various scenarios. By examining current techniques,\npotential risks, and future challenges, this survey provides a structured\nfoundation for developing more adaptable and ethically-aligned LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities, yet\ntheir transition to real-world applications reveals a critical limitation: the\ninability to adapt to individual preferences while maintaining alignment with\nuniversal human values. Current alignment techniques adopt a one-size-fits-all\napproach that fails to accommodate users' diverse backgrounds and needs. This\npaper presents the first comprehensive survey of personalized alignment-a\nparadigm that enables LLMs to adapt their behavior within ethical boundaries\nbased on individual preferences. We propose a unified framework comprising\npreference memory management, personalized generation, and feedback-based\nalignment, systematically analyzing implementation approaches and evaluating\ntheir effectiveness across various scenarios. By examining current techniques,\npotential risks, and future challenges, this survey provides a structured\nfoundation for developing more adaptable and ethically-aligned LLMs."
                },
                "authors": [
                    {
                        "name": "Jian Guan"
                    },
                    {
                        "name": "Junfei Wu"
                    },
                    {
                        "name": "Jia-Nan Li"
                    },
                    {
                        "name": "Chuanqi Cheng"
                    },
                    {
                        "name": "Wei Wu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wu"
                },
                "author": "Wei Wu",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17003v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17003v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17940v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17940v2",
                "updated": "2025-04-01T09:23:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    9,
                    23,
                    0,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-23T04:47:15Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    4,
                    47,
                    15,
                    6,
                    82,
                    0
                ],
                "title": "FisherTune: Fisher-Guided Robust Tuning of Vision Foundation Models for\n  Domain Generalized Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FisherTune: Fisher-Guided Robust Tuning of Vision Foundation Models for\n  Domain Generalized Segmentation"
                },
                "summary": "Vision Foundation Models (VFMs) excel in generalization due to large-scale\npretraining, but fine-tuning them for Domain Generalized Semantic Segmentation\n(DGSS) while maintaining this ability remains challenging. Existing approaches\neither selectively fine-tune parameters or freeze the VFMs and update only the\nadapters, both of which may underutilize the VFMs' full potential in DGSS\ntasks. We observe that domain-sensitive parameters in VFMs, arising from task\nand distribution differences, can hinder generalization. To address this, we\npropose \\textbf{FisherTune}, a robust fine-tuning method guided by the\nDomain-Related Fisher Information Matrix (DR-FIM). DR-FIM measures parameter\nsensitivity across tasks and domains, enabling selective updates that preserve\ngeneralization and enhance DGSS adaptability. FisherTune incorporates\nvariational inference to stabilize DR-FIM estimation, treating parameters as\nGaussian-distributed variables and leveraging pre-trained priors. Extensive\nexperiments show that FisherTune achieves superior cross-domain segmentation\nwhile maintaining generalization, outperforming selective-parameter and\nadapter-based methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Foundation Models (VFMs) excel in generalization due to large-scale\npretraining, but fine-tuning them for Domain Generalized Semantic Segmentation\n(DGSS) while maintaining this ability remains challenging. Existing approaches\neither selectively fine-tune parameters or freeze the VFMs and update only the\nadapters, both of which may underutilize the VFMs' full potential in DGSS\ntasks. We observe that domain-sensitive parameters in VFMs, arising from task\nand distribution differences, can hinder generalization. To address this, we\npropose \\textbf{FisherTune}, a robust fine-tuning method guided by the\nDomain-Related Fisher Information Matrix (DR-FIM). DR-FIM measures parameter\nsensitivity across tasks and domains, enabling selective updates that preserve\ngeneralization and enhance DGSS adaptability. FisherTune incorporates\nvariational inference to stabilize DR-FIM estimation, treating parameters as\nGaussian-distributed variables and leveraging pre-trained priors. Extensive\nexperiments show that FisherTune achieves superior cross-domain segmentation\nwhile maintaining generalization, outperforming selective-parameter and\nadapter-based methods."
                },
                "authors": [
                    {
                        "name": "Dong Zhao"
                    },
                    {
                        "name": "Jinlong Li"
                    },
                    {
                        "name": "Shuang Wang"
                    },
                    {
                        "name": "Mengyao Wu"
                    },
                    {
                        "name": "Qi Zang"
                    },
                    {
                        "name": "Nicu Sebe"
                    },
                    {
                        "name": "Zhun Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Zhun Zhong"
                },
                "author": "Zhun Zhong",
                "arxiv_journal_ref": "Conference on Computer Vision and Pattern Recognition 2025\n  Conference on Computer Vision and Pattern Recognition 2025 Conference on\n  Computer Vision and Pattern Recognition 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17940v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17940v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16308v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16308v4",
                "updated": "2025-04-01T08:48:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    8,
                    48,
                    53,
                    1,
                    91,
                    0
                ],
                "published": "2024-11-25T11:53:55Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    11,
                    53,
                    55,
                    0,
                    330,
                    0
                ],
                "title": "An End-to-End Robust Point Cloud Semantic Segmentation Network with\n  Single-Step Conditional Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An End-to-End Robust Point Cloud Semantic Segmentation Network with\n  Single-Step Conditional Diffusion Models"
                },
                "summary": "Existing conditional Denoising Diffusion Probabilistic Models (DDPMs) with a\nNoise-Conditional Framework (NCF) remain challenging for 3D scene understanding\ntasks, as the complex geometric details in scenes increase the difficulty of\nfitting the gradients of the data distribution (the scores) from semantic\nlabels. This also results in longer training and inference time for DDPMs\ncompared to non-DDPMs. From a different perspective, we delve deeply into the\nmodel paradigm dominated by the Conditional Network. In this paper, we propose\nan end-to-end robust semantic Segmentation Network based on a Conditional-Noise\nFramework (CNF) of DDPMs, named CDSegNet. Specifically, CDSegNet models the\nNoise Network (NN) as a learnable noise-feature generator. This enables the\nConditional Network (CN) to understand 3D scene semantics under multi-level\nfeature perturbations, enhancing the generalization in unseen scenes.\nMeanwhile, benefiting from the noise system of DDPMs, CDSegNet exhibits strong\nnoise and sparsity robustness in experiments. Moreover, thanks to CNF, CDSegNet\ncan generate the semantic labels in a single-step inference like non-DDPMs, due\nto avoiding directly fitting the scores from semantic labels in the dominant\nnetwork of CDSegNet. On public indoor and outdoor benchmarks, CDSegNet\nsignificantly outperforms existing methods, achieving state-of-the-art\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing conditional Denoising Diffusion Probabilistic Models (DDPMs) with a\nNoise-Conditional Framework (NCF) remain challenging for 3D scene understanding\ntasks, as the complex geometric details in scenes increase the difficulty of\nfitting the gradients of the data distribution (the scores) from semantic\nlabels. This also results in longer training and inference time for DDPMs\ncompared to non-DDPMs. From a different perspective, we delve deeply into the\nmodel paradigm dominated by the Conditional Network. In this paper, we propose\nan end-to-end robust semantic Segmentation Network based on a Conditional-Noise\nFramework (CNF) of DDPMs, named CDSegNet. Specifically, CDSegNet models the\nNoise Network (NN) as a learnable noise-feature generator. This enables the\nConditional Network (CN) to understand 3D scene semantics under multi-level\nfeature perturbations, enhancing the generalization in unseen scenes.\nMeanwhile, benefiting from the noise system of DDPMs, CDSegNet exhibits strong\nnoise and sparsity robustness in experiments. Moreover, thanks to CNF, CDSegNet\ncan generate the semantic labels in a single-step inference like non-DDPMs, due\nto avoiding directly fitting the scores from semantic labels in the dominant\nnetwork of CDSegNet. On public indoor and outdoor benchmarks, CDSegNet\nsignificantly outperforms existing methods, achieving state-of-the-art\nperformance."
                },
                "authors": [
                    {
                        "name": "Wentao Qu"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "YongShun Gong"
                    },
                    {
                        "name": "Xiaoshui Huang"
                    },
                    {
                        "name": "Liang Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Liang Xiao"
                },
                "author": "Liang Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16308v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16308v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16855v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16855v2",
                "updated": "2025-04-01T08:48:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    8,
                    48,
                    4,
                    1,
                    91,
                    0
                ],
                "published": "2024-12-22T04:40:24Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    4,
                    40,
                    24,
                    6,
                    357,
                    0
                ],
                "title": "GME: Improving Universal Multimodal Retrieval by Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GME: Improving Universal Multimodal Retrieval by Multimodal LLMs"
                },
                "summary": "Universal Multimodal Retrieval (UMR) aims to enable search across various\nmodalities using a unified model, where queries and candidates can consist of\npure text, images, or a combination of both. Previous work has attempted to\nadopt multimodal large language models (MLLMs) to realize UMR using only text\ndata. However, our preliminary experiments demonstrate that more diverse\nmultimodal training data can further unlock the potential of MLLMs. Despite its\neffectiveness, the existing multimodal training data is highly imbalanced in\nterms of modality, which motivates us to develop a training data synthesis\npipeline and construct a large-scale, high-quality fused-modal training\ndataset. Based on the synthetic training data, we develop the General\nMultimodal Embedder (GME), an MLLM-based dense retriever designed for UMR.\nFurthermore, we construct a comprehensive UMR Benchmark (UMRB) to evaluate the\neffectiveness of our approach. Experimental results show that our method\nachieves state-of-the-art performance among existing UMR methods. Last, we\nprovide in-depth analyses of model scaling and training strategies, and perform\nablation studies on both the model and synthetic data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Universal Multimodal Retrieval (UMR) aims to enable search across various\nmodalities using a unified model, where queries and candidates can consist of\npure text, images, or a combination of both. Previous work has attempted to\nadopt multimodal large language models (MLLMs) to realize UMR using only text\ndata. However, our preliminary experiments demonstrate that more diverse\nmultimodal training data can further unlock the potential of MLLMs. Despite its\neffectiveness, the existing multimodal training data is highly imbalanced in\nterms of modality, which motivates us to develop a training data synthesis\npipeline and construct a large-scale, high-quality fused-modal training\ndataset. Based on the synthetic training data, we develop the General\nMultimodal Embedder (GME), an MLLM-based dense retriever designed for UMR.\nFurthermore, we construct a comprehensive UMR Benchmark (UMRB) to evaluate the\neffectiveness of our approach. Experimental results show that our method\nachieves state-of-the-art performance among existing UMR methods. Last, we\nprovide in-depth analyses of model scaling and training strategies, and perform\nablation studies on both the model and synthetic data."
                },
                "authors": [
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Yanzhao Zhang"
                    },
                    {
                        "name": "Wen Xie"
                    },
                    {
                        "name": "Mingxin Li"
                    },
                    {
                        "name": "Ziqi Dai"
                    },
                    {
                        "name": "Dingkun Long"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Meishan Zhang"
                    },
                    {
                        "name": "Wenjie Li"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "Accepted to CVPR 2025, models at\n  https://huggingface.co/Alibaba-NLP/gme-Qwen2-VL-2B-Instruct",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16855v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16855v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15877v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15877v4",
                "updated": "2025-04-01T08:36:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    8,
                    36,
                    44,
                    1,
                    91,
                    0
                ],
                "published": "2024-06-22T15:52:04Z",
                "published_parsed": [
                    2024,
                    6,
                    22,
                    15,
                    52,
                    4,
                    5,
                    174,
                    0
                ],
                "title": "BigCodeBench: Benchmarking Code Generation with Diverse Function Calls\n  and Complex Instructions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BigCodeBench: Benchmarking Code Generation with Diverse Function Calls\n  and Complex Instructions"
                },
                "summary": "Task automation has been greatly empowered by the recent advances in Large\nLanguage Models (LLMs) via Python code, where the tasks ranging from software\nengineering development to general-purpose reasoning. While current benchmarks\nhave shown that LLMs can solve tasks using programs like human developers, the\nmajority of their evaluations are limited to short and self-contained\nalgorithmic tasks or standalone function calls. Solving challenging and\npractical tasks requires the capability of utilizing diverse function calls as\ntools to efficiently implement functionalities like data analysis and web\ndevelopment. In addition, using multiple tools to solve a task needs\ncompositional reasoning by accurately understanding complex instructions.\nFulfilling both of these characteristics can pose a great challenge for LLMs.To\nassess how well LLMs can solve challenging and practical tasks via programs, we\nintroduce BigCodeBench, a benchmark that challenges LLMs to invoke multiple\nfunction calls as tools from 139 libraries and 7 domains for 1,140 fine-grained\ntasks. To evaluate LLMs rigorously, each task encompasses 5.6 test cases with\nan average branch coverage of 99%. In addition, we propose a\nnatural-language-oriented variant of BigCodeBench, BigCodeBench-Instruct, that\nautomatically transforms the original docstrings into short instructions only\nwith essential information. Our extensive evaluation of 60 LLMs shows that LLMs\nare not yet capable of following complex instructions to use function calls\nprecisely, with scores up to 60%, significantly lower than the human\nperformance of 97%. The results underscore the need for further advancements in\nthis area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task automation has been greatly empowered by the recent advances in Large\nLanguage Models (LLMs) via Python code, where the tasks ranging from software\nengineering development to general-purpose reasoning. While current benchmarks\nhave shown that LLMs can solve tasks using programs like human developers, the\nmajority of their evaluations are limited to short and self-contained\nalgorithmic tasks or standalone function calls. Solving challenging and\npractical tasks requires the capability of utilizing diverse function calls as\ntools to efficiently implement functionalities like data analysis and web\ndevelopment. In addition, using multiple tools to solve a task needs\ncompositional reasoning by accurately understanding complex instructions.\nFulfilling both of these characteristics can pose a great challenge for LLMs.To\nassess how well LLMs can solve challenging and practical tasks via programs, we\nintroduce BigCodeBench, a benchmark that challenges LLMs to invoke multiple\nfunction calls as tools from 139 libraries and 7 domains for 1,140 fine-grained\ntasks. To evaluate LLMs rigorously, each task encompasses 5.6 test cases with\nan average branch coverage of 99%. In addition, we propose a\nnatural-language-oriented variant of BigCodeBench, BigCodeBench-Instruct, that\nautomatically transforms the original docstrings into short instructions only\nwith essential information. Our extensive evaluation of 60 LLMs shows that LLMs\nare not yet capable of following complex instructions to use function calls\nprecisely, with scores up to 60%, significantly lower than the human\nperformance of 97%. The results underscore the need for further advancements in\nthis area."
                },
                "authors": [
                    {
                        "name": "Terry Yue Zhuo"
                    },
                    {
                        "name": "Minh Chien Vu"
                    },
                    {
                        "name": "Jenny Chim"
                    },
                    {
                        "name": "Han Hu"
                    },
                    {
                        "name": "Wenhao Yu"
                    },
                    {
                        "name": "Ratnadira Widyasari"
                    },
                    {
                        "name": "Imam Nur Bani Yusuf"
                    },
                    {
                        "name": "Haolan Zhan"
                    },
                    {
                        "name": "Junda He"
                    },
                    {
                        "name": "Indraneil Paul"
                    },
                    {
                        "name": "Simon Brunner"
                    },
                    {
                        "name": "Chen Gong"
                    },
                    {
                        "name": "Thong Hoang"
                    },
                    {
                        "name": "Armel Randy Zebaze"
                    },
                    {
                        "name": "Xiaoheng Hong"
                    },
                    {
                        "name": "Wen-Ding Li"
                    },
                    {
                        "name": "Jean Kaddour"
                    },
                    {
                        "name": "Ming Xu"
                    },
                    {
                        "name": "Zhihan Zhang"
                    },
                    {
                        "name": "Prateek Yadav"
                    },
                    {
                        "name": "Naman Jain"
                    },
                    {
                        "name": "Alex Gu"
                    },
                    {
                        "name": "Zhoujun Cheng"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Zijian Wang"
                    },
                    {
                        "name": "Binyuan Hui"
                    },
                    {
                        "name": "Niklas Muennighoff"
                    },
                    {
                        "name": "David Lo"
                    },
                    {
                        "name": "Daniel Fried"
                    },
                    {
                        "name": "Xiaoning Du"
                    },
                    {
                        "name": "Harm de Vries"
                    },
                    {
                        "name": "Leandro Von Werra"
                    }
                ],
                "author_detail": {
                    "name": "Leandro Von Werra"
                },
                "author": "Leandro Von Werra",
                "arxiv_comment": "Accpeted at ICLR 2025 (Oral), built with love by the BigCode\n  community :)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15877v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15877v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23959v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23959v2",
                "updated": "2025-04-01T08:34:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    8,
                    34,
                    57,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-31T11:18:27Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    18,
                    27,
                    0,
                    90,
                    0
                ],
                "title": "Local Information Matters: Inference Acceleration For Grounded\n  Conversation Generation Models Through Adaptive Local-Aware Token Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Local Information Matters: Inference Acceleration For Grounded\n  Conversation Generation Models Through Adaptive Local-Aware Token Pruning"
                },
                "summary": "Grounded Conversation Generation (GCG) is an emerging vision-language task\nthat requires models to generate natural language responses seamlessly\nintertwined with corresponding object segmentation masks. Recent models, such\nas GLaMM and OMG-LLaVA, achieve pixel-level grounding but incur significant\ncomputational costs due to processing a large number of visual tokens. Existing\ntoken pruning methods, like FastV and PyramidDrop, fail to preserve the local\nvisual features critical for accurate grounding, leading to substantial\nperformance drops in GCG tasks. To address this, we propose Adaptive\nLocal-Aware Token Pruning (ALTP), a simple yet effective framework that\naccelerates GCG models by prioritizing local object information. ALTP\nintroduces two key components: (1) Detail Density Capture (DDC), which uses\nsuperpixel segmentation to retain tokens in object-centric regions, preserving\nfine-grained details, and (2) Dynamic Density Formation (DDF), which\ndynamically allocates tokens based on information density, ensuring higher\nretention in semantically rich areas. Extensive experiments on the GranDf\ndataset demonstrate that ALTP significantly outperforms existing token pruning\nmethods, such as FastV and PyramidDrop, on both GLaMM and OMG-LLaVA models.\nNotably, when applied to GLaMM, ALTP achieves a 90% reduction in visual tokens\nwith a 4.9% improvement in AP50 and a 5.0% improvement in Recall compared to\nPyramidDrop. Similarly, on OMG-LLaVA, ALTP improves AP by 2.1% and mIOU by 3.0%\nat a 90% token reduction compared with PDrop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grounded Conversation Generation (GCG) is an emerging vision-language task\nthat requires models to generate natural language responses seamlessly\nintertwined with corresponding object segmentation masks. Recent models, such\nas GLaMM and OMG-LLaVA, achieve pixel-level grounding but incur significant\ncomputational costs due to processing a large number of visual tokens. Existing\ntoken pruning methods, like FastV and PyramidDrop, fail to preserve the local\nvisual features critical for accurate grounding, leading to substantial\nperformance drops in GCG tasks. To address this, we propose Adaptive\nLocal-Aware Token Pruning (ALTP), a simple yet effective framework that\naccelerates GCG models by prioritizing local object information. ALTP\nintroduces two key components: (1) Detail Density Capture (DDC), which uses\nsuperpixel segmentation to retain tokens in object-centric regions, preserving\nfine-grained details, and (2) Dynamic Density Formation (DDF), which\ndynamically allocates tokens based on information density, ensuring higher\nretention in semantically rich areas. Extensive experiments on the GranDf\ndataset demonstrate that ALTP significantly outperforms existing token pruning\nmethods, such as FastV and PyramidDrop, on both GLaMM and OMG-LLaVA models.\nNotably, when applied to GLaMM, ALTP achieves a 90% reduction in visual tokens\nwith a 4.9% improvement in AP50 and a 5.0% improvement in Recall compared to\nPyramidDrop. Similarly, on OMG-LLaVA, ALTP improves AP by 2.1% and mIOU by 3.0%\nat a 90% token reduction compared with PDrop."
                },
                "authors": [
                    {
                        "name": "Bizhe Bai"
                    },
                    {
                        "name": "Jianjian Cao"
                    },
                    {
                        "name": "Yadan Luo"
                    },
                    {
                        "name": "Tao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tao Chen"
                },
                "author": "Tao Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23959v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23959v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10028v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10028v3",
                "updated": "2025-04-02T05:38:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    5,
                    38,
                    32,
                    2,
                    92,
                    0
                ],
                "published": "2024-12-13T10:39:27Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    10,
                    39,
                    27,
                    4,
                    348,
                    0
                ],
                "title": "Mr. DETR: Instructive Multi-Route Training for Detection Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mr. DETR: Instructive Multi-Route Training for Detection Transformers"
                },
                "summary": "Existing methods enhance the training of detection transformers by\nincorporating an auxiliary one-to-many assignment. In this work, we treat the\nmodel as a multi-task framework, simultaneously performing one-to-one and\none-to-many predictions. We investigate the roles of each component in the\ntransformer decoder across these two training targets, including\nself-attention, cross-attention, and feed-forward network. Our empirical\nresults demonstrate that any independent component in the decoder can\neffectively learn both targets simultaneously, even when other components are\nshared. This finding leads us to propose a multi-route training mechanism,\nfeaturing a primary route for one-to-one prediction and two auxiliary training\nroutes for one-to-many prediction. We enhance the training mechanism with a\nnovel instructive self-attention that dynamically and flexibly guides object\nqueries for one-to-many prediction. The auxiliary routes are removed during\ninference, ensuring no impact on model architecture or inference cost. We\nconduct extensive experiments on various baselines, achieving consistent\nimprovements as shown in Figure 1. Project page:\nhttps://visual-ai.github.io/mrdetr",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing methods enhance the training of detection transformers by\nincorporating an auxiliary one-to-many assignment. In this work, we treat the\nmodel as a multi-task framework, simultaneously performing one-to-one and\none-to-many predictions. We investigate the roles of each component in the\ntransformer decoder across these two training targets, including\nself-attention, cross-attention, and feed-forward network. Our empirical\nresults demonstrate that any independent component in the decoder can\neffectively learn both targets simultaneously, even when other components are\nshared. This finding leads us to propose a multi-route training mechanism,\nfeaturing a primary route for one-to-one prediction and two auxiliary training\nroutes for one-to-many prediction. We enhance the training mechanism with a\nnovel instructive self-attention that dynamically and flexibly guides object\nqueries for one-to-many prediction. The auxiliary routes are removed during\ninference, ensuring no impact on model architecture or inference cost. We\nconduct extensive experiments on various baselines, achieving consistent\nimprovements as shown in Figure 1. Project page:\nhttps://visual-ai.github.io/mrdetr"
                },
                "authors": [
                    {
                        "name": "Chang-Bin Zhang"
                    },
                    {
                        "name": "Yujie Zhong"
                    },
                    {
                        "name": "Kai Han"
                    }
                ],
                "author_detail": {
                    "name": "Kai Han"
                },
                "author": "Kai Han",
                "arxiv_comment": "Accepted by CVPR 2025, Project page:\n  https://visual-ai.github.io/mrdetr, Code:\n  https://github.com/Visual-AI/Mr.DETR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10028v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10028v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14279v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14279v2",
                "updated": "2025-04-01T08:31:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    8,
                    31,
                    22,
                    1,
                    91,
                    0
                ],
                "published": "2024-10-18T08:35:57Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    8,
                    35,
                    57,
                    4,
                    292,
                    0
                ],
                "title": "ControlSR: Taming Diffusion Models for Consistent Real-World Image Super\n  Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ControlSR: Taming Diffusion Models for Consistent Real-World Image Super\n  Resolution"
                },
                "summary": "We present ControlSR, a new method that can tame Diffusion Models for\nconsistent real-world image super-resolution (Real-ISR). Previous Real-ISR\nmodels mostly focus on how to activate more generative priors of text-to-image\ndiffusion models to make the output high-resolution (HR) images look better.\nHowever, since these methods rely too much on the generative priors, the\ncontent of the output images is often inconsistent with the input LR ones. To\nmitigate the above issue, in this work, we tame Diffusion Models by effectively\nutilizing LR information to impose stronger constraints on the control signals\nfrom ControlNet in the latent space. We show that our method can produce\nhigher-quality control signals, which enables the super-resolution results to\nbe more consistent with the LR image and leads to clearer visual results. In\naddition, we also propose an inference strategy that imposes constraints in the\nlatent space using LR information, allowing for the simultaneous improvement of\nfidelity and generative ability. Experiments demonstrate that our model can\nachieve better performance across multiple metrics on several test sets and\ngenerate more consistent SR results with LR images than existing methods. Our\ncode is available at https://github.com/HVision-NKU/ControlSR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present ControlSR, a new method that can tame Diffusion Models for\nconsistent real-world image super-resolution (Real-ISR). Previous Real-ISR\nmodels mostly focus on how to activate more generative priors of text-to-image\ndiffusion models to make the output high-resolution (HR) images look better.\nHowever, since these methods rely too much on the generative priors, the\ncontent of the output images is often inconsistent with the input LR ones. To\nmitigate the above issue, in this work, we tame Diffusion Models by effectively\nutilizing LR information to impose stronger constraints on the control signals\nfrom ControlNet in the latent space. We show that our method can produce\nhigher-quality control signals, which enables the super-resolution results to\nbe more consistent with the LR image and leads to clearer visual results. In\naddition, we also propose an inference strategy that imposes constraints in the\nlatent space using LR information, allowing for the simultaneous improvement of\nfidelity and generative ability. Experiments demonstrate that our model can\nachieve better performance across multiple metrics on several test sets and\ngenerate more consistent SR results with LR images than existing methods. Our\ncode is available at https://github.com/HVision-NKU/ControlSR."
                },
                "authors": [
                    {
                        "name": "Yuhao Wan"
                    },
                    {
                        "name": "Peng-Tao Jiang"
                    },
                    {
                        "name": "Qibin Hou"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Jinwei Chen"
                    },
                    {
                        "name": "Ming-Ming Cheng"
                    },
                    {
                        "name": "Bo Li"
                    }
                ],
                "author_detail": {
                    "name": "Bo Li"
                },
                "author": "Bo Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14279v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14279v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19178v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19178v2",
                "updated": "2025-04-01T08:18:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    8,
                    18,
                    34,
                    1,
                    91,
                    0
                ],
                "published": "2025-02-26T14:34:00Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    34,
                    0,
                    2,
                    57,
                    0
                ],
                "title": "UQABench: Evaluating User Embedding for Prompting LLMs in Personalized\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UQABench: Evaluating User Embedding for Prompting LLMs in Personalized\n  Question Answering"
                },
                "summary": "Large language models (LLMs) achieve remarkable success in natural language\nprocessing (NLP). In practical scenarios like recommendations, as users\nincreasingly seek personalized experiences, it becomes crucial to incorporate\nuser interaction history into the context of LLMs to enhance personalization.\nHowever, from a practical utility perspective, user interactions' extensive\nlength and noise present challenges when used directly as text prompts. A\npromising solution is to compress and distill interactions into compact\nembeddings, serving as soft prompts to assist LLMs in generating personalized\nresponses. Although this approach brings efficiency, a critical concern\nemerges: Can user embeddings adequately capture valuable information and prompt\nLLMs? To address this concern, we propose \\name, a benchmark designed to\nevaluate the effectiveness of user embeddings in prompting LLMs for\npersonalization. We establish a fair and standardized evaluation process,\nencompassing pre-training, fine-tuning, and evaluation stages. To thoroughly\nevaluate user embeddings, we design three dimensions of tasks: sequence\nunderstanding, action prediction, and interest perception. These evaluation\ntasks cover the industry's demands in traditional recommendation tasks, such as\nimproving prediction accuracy, and its aspirations for LLM-based methods, such\nas accurately understanding user interests and enhancing the user experience.\nWe conduct extensive experiments on various state-of-the-art methods for\nmodeling user embeddings. Additionally, we reveal the scaling laws of\nleveraging user embeddings to prompt LLMs. The benchmark is available online.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) achieve remarkable success in natural language\nprocessing (NLP). In practical scenarios like recommendations, as users\nincreasingly seek personalized experiences, it becomes crucial to incorporate\nuser interaction history into the context of LLMs to enhance personalization.\nHowever, from a practical utility perspective, user interactions' extensive\nlength and noise present challenges when used directly as text prompts. A\npromising solution is to compress and distill interactions into compact\nembeddings, serving as soft prompts to assist LLMs in generating personalized\nresponses. Although this approach brings efficiency, a critical concern\nemerges: Can user embeddings adequately capture valuable information and prompt\nLLMs? To address this concern, we propose \\name, a benchmark designed to\nevaluate the effectiveness of user embeddings in prompting LLMs for\npersonalization. We establish a fair and standardized evaluation process,\nencompassing pre-training, fine-tuning, and evaluation stages. To thoroughly\nevaluate user embeddings, we design three dimensions of tasks: sequence\nunderstanding, action prediction, and interest perception. These evaluation\ntasks cover the industry's demands in traditional recommendation tasks, such as\nimproving prediction accuracy, and its aspirations for LLM-based methods, such\nas accurately understanding user interests and enhancing the user experience.\nWe conduct extensive experiments on various state-of-the-art methods for\nmodeling user embeddings. Additionally, we reveal the scaling laws of\nleveraging user embeddings to prompt LLMs. The benchmark is available online."
                },
                "authors": [
                    {
                        "name": "Langming Liu"
                    },
                    {
                        "name": "Shilei Liu"
                    },
                    {
                        "name": "Yujin Yuan"
                    },
                    {
                        "name": "Yizhen Zhang"
                    },
                    {
                        "name": "Bencheng Yan"
                    },
                    {
                        "name": "Zhiyuan Zeng"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Jiaqi Liu"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Pengjie Wang"
                    },
                    {
                        "name": "Jian Xu"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "arxiv_comment": "10 pages, 3 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19178v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19178v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05763v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05763v3",
                "updated": "2025-04-01T08:18:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    8,
                    18,
                    1,
                    1,
                    91,
                    0
                ],
                "published": "2025-01-10T07:41:47Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    7,
                    41,
                    47,
                    4,
                    10,
                    0
                ],
                "title": "StarGen: A Spatiotemporal Autoregression Framework with Video Diffusion\n  Model for Scalable and Controllable Scene Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StarGen: A Spatiotemporal Autoregression Framework with Video Diffusion\n  Model for Scalable and Controllable Scene Generation"
                },
                "summary": "Recent advances in large reconstruction and generative models have\nsignificantly improved scene reconstruction and novel view generation. However,\ndue to compute limitations, each inference with these large models is confined\nto a small area, making long-range consistent scene generation challenging. To\naddress this, we propose StarGen, a novel framework that employs a pre-trained\nvideo diffusion model in an autoregressive manner for long-range scene\ngeneration. The generation of each video clip is conditioned on the 3D warping\nof spatially adjacent images and the temporally overlapping image from\npreviously generated clips, improving spatiotemporal consistency in long-range\nscene generation with precise pose control. The spatiotemporal condition is\ncompatible with various input conditions, facilitating diverse tasks, including\nsparse view interpolation, perpetual view generation, and layout-conditioned\ncity generation. Quantitative and qualitative evaluations demonstrate StarGen's\nsuperior scalability, fidelity, and pose accuracy compared to state-of-the-art\nmethods. Project page: https://zju3dv.github.io/StarGen.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large reconstruction and generative models have\nsignificantly improved scene reconstruction and novel view generation. However,\ndue to compute limitations, each inference with these large models is confined\nto a small area, making long-range consistent scene generation challenging. To\naddress this, we propose StarGen, a novel framework that employs a pre-trained\nvideo diffusion model in an autoregressive manner for long-range scene\ngeneration. The generation of each video clip is conditioned on the 3D warping\nof spatially adjacent images and the temporally overlapping image from\npreviously generated clips, improving spatiotemporal consistency in long-range\nscene generation with precise pose control. The spatiotemporal condition is\ncompatible with various input conditions, facilitating diverse tasks, including\nsparse view interpolation, perpetual view generation, and layout-conditioned\ncity generation. Quantitative and qualitative evaluations demonstrate StarGen's\nsuperior scalability, fidelity, and pose accuracy compared to state-of-the-art\nmethods. Project page: https://zju3dv.github.io/StarGen."
                },
                "authors": [
                    {
                        "name": "Shangjin Zhai"
                    },
                    {
                        "name": "Zhichao Ye"
                    },
                    {
                        "name": "Jialin Liu"
                    },
                    {
                        "name": "Weijian Xie"
                    },
                    {
                        "name": "Jiaqi Hu"
                    },
                    {
                        "name": "Zhen Peng"
                    },
                    {
                        "name": "Hua Xue"
                    },
                    {
                        "name": "Danpeng Chen"
                    },
                    {
                        "name": "Xiaomeng Wang"
                    },
                    {
                        "name": "Lei Yang"
                    },
                    {
                        "name": "Nan Wang"
                    },
                    {
                        "name": "Haomin Liu"
                    },
                    {
                        "name": "Guofeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Guofeng Zhang"
                },
                "author": "Guofeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05763v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05763v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.11910v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.11910v2",
                "updated": "2025-04-01T07:37:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    7,
                    37,
                    55,
                    1,
                    91,
                    0
                ],
                "published": "2024-02-19T07:50:54Z",
                "published_parsed": [
                    2024,
                    2,
                    19,
                    7,
                    50,
                    54,
                    0,
                    50,
                    0
                ],
                "title": "Enhancing Large Language Models for Text-to-Testcase Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Large Language Models for Text-to-Testcase Generation"
                },
                "summary": "Context: Test-driven development (TDD) is a widely employed software\ndevelopment practice that involves developing test cases based on requirements\nprior to writing the code. Although various methods for automated test case\ngeneration have been proposed, they are not specifically tailored for TDD,\nwhere requirements instead of code serve as input. Objective: In this paper, we\nintroduce a text-to-testcase generation approach based on a large language\nmodel (GPT-3.5) that is fine-tuned on our curated dataset with an effective\nprompt design. Method: Our approach involves enhancing the capabilities of\nbasic GPT-3.5 for text-to-testcase generation task that is fine-tuned on our\ncurated dataset with an effective prompting design. We evaluated the\neffectiveness of our approach using a span of five large-scale open-source\nsoftware projects. Results: Our approach generated 7k test cases for open\nsource projects, achieving 78.5% syntactic correctness, 67.09% requirement\nalignment, and 61.7% code coverage, which substantially outperforms all other\nLLMs (basic GPT-3.5, Bloom, and CodeT5). In addition, our ablation study\ndemonstrates the substantial performance improvement of the fine-tuning and\nprompting components of the GPT-3.5 model. Conclusions: These findings lead us\nto conclude that fine-tuning and prompting should be considered in the future\nwhen building a language model for the text-to-testcase generation task",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context: Test-driven development (TDD) is a widely employed software\ndevelopment practice that involves developing test cases based on requirements\nprior to writing the code. Although various methods for automated test case\ngeneration have been proposed, they are not specifically tailored for TDD,\nwhere requirements instead of code serve as input. Objective: In this paper, we\nintroduce a text-to-testcase generation approach based on a large language\nmodel (GPT-3.5) that is fine-tuned on our curated dataset with an effective\nprompt design. Method: Our approach involves enhancing the capabilities of\nbasic GPT-3.5 for text-to-testcase generation task that is fine-tuned on our\ncurated dataset with an effective prompting design. We evaluated the\neffectiveness of our approach using a span of five large-scale open-source\nsoftware projects. Results: Our approach generated 7k test cases for open\nsource projects, achieving 78.5% syntactic correctness, 67.09% requirement\nalignment, and 61.7% code coverage, which substantially outperforms all other\nLLMs (basic GPT-3.5, Bloom, and CodeT5). In addition, our ablation study\ndemonstrates the substantial performance improvement of the fine-tuning and\nprompting components of the GPT-3.5 model. Conclusions: These findings lead us\nto conclude that fine-tuning and prompting should be considered in the future\nwhen building a language model for the text-to-testcase generation task"
                },
                "authors": [
                    {
                        "name": "Saranya Alagarsamy"
                    },
                    {
                        "name": "Chakkrit Tantithamthavorn"
                    },
                    {
                        "name": "Wannita Takerngsaksiri"
                    },
                    {
                        "name": "Chetan Arora"
                    },
                    {
                        "name": "Aldeida Aleti"
                    }
                ],
                "author_detail": {
                    "name": "Aldeida Aleti"
                },
                "author": "Aldeida Aleti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.11910v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.11910v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13208v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13208v2",
                "updated": "2025-04-01T07:04:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    7,
                    4,
                    25,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-17T14:20:48Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    14,
                    20,
                    48,
                    0,
                    76,
                    0
                ],
                "title": "Improving Complex Reasoning with Dynamic Prompt Corruption: A soft\n  prompt Optimization Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Complex Reasoning with Dynamic Prompt Corruption: A soft\n  prompt Optimization Approach"
                },
                "summary": "Prompt-tuning (PT) for large language models (LLMs) can facilitate the\nperformance on various conventional NLP tasks with significantly fewer\ntrainable parameters. However, our investigation reveals that PT provides\nlimited improvement and may even degrade the primitive performance of LLMs on\ncomplex reasoning tasks. Such a phenomenon suggests that soft prompts can\npositively impact certain instances while negatively affecting others,\nparticularly during the later phases of reasoning. To address these challenges,\nWe first identify an information accumulation within the soft prompts. Through\ndetailed analysis, we demonstrate that this phenomenon is often accompanied by\nerroneous information flow patterns in the deeper layers of the model, which\nultimately lead to incorrect reasoning outcomes. we propose a novel method\ncalled Dynamic Prompt Corruption (DPC) to take better advantage of soft prompts\nin complex reasoning tasks, which dynamically adjusts the influence of soft\nprompts based on their impact on the reasoning process. Specifically, DPC\nconsists of two stages: Dynamic Trigger and Dynamic Corruption. First, Dynamic\nTrigger measures the impact of soft prompts, identifying whether beneficial or\ndetrimental. Then, Dynamic Corruption mitigates the negative effects of soft\nprompts by selectively masking key tokens that interfere with the reasoning\nprocess. We validate the proposed approach through extensive experiments on\nvarious LLMs and reasoning tasks, including GSM8K, MATH, and AQuA. Experimental\nresults demonstrate that DPC can consistently enhance the performance of PT,\nachieving 4%-8% accuracy gains compared to vanilla prompt tuning, highlighting\nthe effectiveness of our approach and its potential to enhance complex\nreasoning in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt-tuning (PT) for large language models (LLMs) can facilitate the\nperformance on various conventional NLP tasks with significantly fewer\ntrainable parameters. However, our investigation reveals that PT provides\nlimited improvement and may even degrade the primitive performance of LLMs on\ncomplex reasoning tasks. Such a phenomenon suggests that soft prompts can\npositively impact certain instances while negatively affecting others,\nparticularly during the later phases of reasoning. To address these challenges,\nWe first identify an information accumulation within the soft prompts. Through\ndetailed analysis, we demonstrate that this phenomenon is often accompanied by\nerroneous information flow patterns in the deeper layers of the model, which\nultimately lead to incorrect reasoning outcomes. we propose a novel method\ncalled Dynamic Prompt Corruption (DPC) to take better advantage of soft prompts\nin complex reasoning tasks, which dynamically adjusts the influence of soft\nprompts based on their impact on the reasoning process. Specifically, DPC\nconsists of two stages: Dynamic Trigger and Dynamic Corruption. First, Dynamic\nTrigger measures the impact of soft prompts, identifying whether beneficial or\ndetrimental. Then, Dynamic Corruption mitigates the negative effects of soft\nprompts by selectively masking key tokens that interfere with the reasoning\nprocess. We validate the proposed approach through extensive experiments on\nvarious LLMs and reasoning tasks, including GSM8K, MATH, and AQuA. Experimental\nresults demonstrate that DPC can consistently enhance the performance of PT,\nachieving 4%-8% accuracy gains compared to vanilla prompt tuning, highlighting\nthe effectiveness of our approach and its potential to enhance complex\nreasoning in LLMs."
                },
                "authors": [
                    {
                        "name": "Sinan Fan"
                    },
                    {
                        "name": "Liang Xie"
                    },
                    {
                        "name": "Chen Shen"
                    },
                    {
                        "name": "Ge Teng"
                    },
                    {
                        "name": "Xiaosong Yuan"
                    },
                    {
                        "name": "Xiaofeng Zhang"
                    },
                    {
                        "name": "Chenxi Huang"
                    },
                    {
                        "name": "Wenxiao Wang"
                    },
                    {
                        "name": "Xiaofei He"
                    },
                    {
                        "name": "Jieping Ye"
                    }
                ],
                "author_detail": {
                    "name": "Jieping Ye"
                },
                "author": "Jieping Ye",
                "arxiv_comment": "Accepted by ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13208v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13208v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23811v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23811v2",
                "updated": "2025-04-01T06:56:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    6,
                    56,
                    57,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-31T07:44:26Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    7,
                    44,
                    26,
                    0,
                    90,
                    0
                ],
                "title": "Did ChatGPT or Copilot use alter the style of internet news headlines? A\n  time series regression analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Did ChatGPT or Copilot use alter the style of internet news headlines? A\n  time series regression analysis"
                },
                "summary": "The release of advanced Large Language Models (LLMs) such as ChatGPT and\nCopilot is changing the way text is created and may influence the content that\nwe find on the web. This study investigated whether the release of these two\npopular LLMs coincided with a change in writing style in headlines and links on\nworldwide news websites. 175 NLP features were obtained for each text in a\ndataset of 451 million headlines/links. An interrupted time series analysis was\napplied for each of the 175 NLP features to evaluate whether there were any\nstatistically significant sustained changes after the release dates of ChatGPT\nand/or Copilot. There were a total of 44 features that did not appear to have\nany significant sustained change after the release of ChatGPT/Copilot. A total\nof 91 other features did show significant change with ChatGPT and/or Copilot\nalthough significance with earlier control LLM release dates (GPT-1/2/3,\nGopher) removed them from consideration. This initial analysis suggests these\nlanguage models may have had a limited impact on the style of individual news\nheadlines/links, with respect to only some NLP measures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The release of advanced Large Language Models (LLMs) such as ChatGPT and\nCopilot is changing the way text is created and may influence the content that\nwe find on the web. This study investigated whether the release of these two\npopular LLMs coincided with a change in writing style in headlines and links on\nworldwide news websites. 175 NLP features were obtained for each text in a\ndataset of 451 million headlines/links. An interrupted time series analysis was\napplied for each of the 175 NLP features to evaluate whether there were any\nstatistically significant sustained changes after the release dates of ChatGPT\nand/or Copilot. There were a total of 44 features that did not appear to have\nany significant sustained change after the release of ChatGPT/Copilot. A total\nof 91 other features did show significant change with ChatGPT and/or Copilot\nalthough significance with earlier control LLM release dates (GPT-1/2/3,\nGopher) removed them from consideration. This initial analysis suggests these\nlanguage models may have had a limited impact on the style of individual news\nheadlines/links, with respect to only some NLP measures."
                },
                "authors": [
                    {
                        "name": "Chris Brogly"
                    },
                    {
                        "name": "Connor McElroy"
                    }
                ],
                "author_detail": {
                    "name": "Connor McElroy"
                },
                "author": "Connor McElroy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23811v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23811v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18942v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18942v2",
                "updated": "2025-04-01T06:52:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    6,
                    52,
                    58,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-24T17:59:04Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    59,
                    4,
                    0,
                    83,
                    0
                ],
                "title": "Video-T1: Test-Time Scaling for Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-T1: Test-Time Scaling for Video Generation"
                },
                "summary": "With the scale capability of increasing training data, model size, and\ncomputational cost, video generation has achieved impressive results in digital\ncreation, enabling users to express creativity across various domains.\nRecently, researchers in Large Language Models (LLMs) have expanded the scaling\nto test-time, which can significantly improve LLM performance by using more\ninference-time computation. Instead of scaling up video foundation models\nthrough expensive training costs, we explore the power of Test-Time Scaling\n(TTS) in video generation, aiming to answer the question: if a video generation\nmodel is allowed to use non-trivial amount of inference-time compute, how much\ncan it improve generation quality given a challenging text prompt. In this\nwork, we reinterpret the test-time scaling of video generation as a searching\nproblem to sample better trajectories from Gaussian noise space to the target\nvideo distribution. Specifically, we build the search space with test-time\nverifiers to provide feedback and heuristic algorithms to guide searching\nprocess. Given a text prompt, we first explore an intuitive linear search\nstrategy by increasing noise candidates at inference time. As full-step\ndenoising all frames simultaneously requires heavy test-time computation costs,\nwe further design a more efficient TTS method for video generation called\nTree-of-Frames (ToF) that adaptively expands and prunes video branches in an\nautoregressive manner. Extensive experiments on text-conditioned video\ngeneration benchmarks demonstrate that increasing test-time compute\nconsistently leads to significant improvements in the quality of videos.\nProject page: https://liuff19.github.io/Video-T1",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the scale capability of increasing training data, model size, and\ncomputational cost, video generation has achieved impressive results in digital\ncreation, enabling users to express creativity across various domains.\nRecently, researchers in Large Language Models (LLMs) have expanded the scaling\nto test-time, which can significantly improve LLM performance by using more\ninference-time computation. Instead of scaling up video foundation models\nthrough expensive training costs, we explore the power of Test-Time Scaling\n(TTS) in video generation, aiming to answer the question: if a video generation\nmodel is allowed to use non-trivial amount of inference-time compute, how much\ncan it improve generation quality given a challenging text prompt. In this\nwork, we reinterpret the test-time scaling of video generation as a searching\nproblem to sample better trajectories from Gaussian noise space to the target\nvideo distribution. Specifically, we build the search space with test-time\nverifiers to provide feedback and heuristic algorithms to guide searching\nprocess. Given a text prompt, we first explore an intuitive linear search\nstrategy by increasing noise candidates at inference time. As full-step\ndenoising all frames simultaneously requires heavy test-time computation costs,\nwe further design a more efficient TTS method for video generation called\nTree-of-Frames (ToF) that adaptively expands and prunes video branches in an\nautoregressive manner. Extensive experiments on text-conditioned video\ngeneration benchmarks demonstrate that increasing test-time compute\nconsistently leads to significant improvements in the quality of videos.\nProject page: https://liuff19.github.io/Video-T1"
                },
                "authors": [
                    {
                        "name": "Fangfu Liu"
                    },
                    {
                        "name": "Hanyang Wang"
                    },
                    {
                        "name": "Yimo Cai"
                    },
                    {
                        "name": "Kaiyan Zhang"
                    },
                    {
                        "name": "Xiaohang Zhan"
                    },
                    {
                        "name": "Yueqi Duan"
                    }
                ],
                "author_detail": {
                    "name": "Yueqi Duan"
                },
                "author": "Yueqi Duan",
                "arxiv_comment": "Project page: https://liuff19.github.io/Video-T1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18942v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18942v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07298v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07298v2",
                "updated": "2025-04-01T06:30:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    6,
                    30,
                    16,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-10T13:18:05Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    13,
                    18,
                    5,
                    0,
                    69,
                    0
                ],
                "title": "ALLVB: All-in-One Long Video Understanding Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ALLVB: All-in-One Long Video Understanding Benchmark"
                },
                "summary": "From image to video understanding, the capabilities of Multi-modal LLMs\n(MLLMs) are increasingly powerful. However, most existing video understanding\nbenchmarks are relatively short, which makes them inadequate for effectively\nevaluating the long-sequence modeling capabilities of MLLMs. This highlights\nthe urgent need for a comprehensive and integrated long video understanding\nbenchmark to assess the ability of MLLMs thoroughly. To this end, we propose\nALLVB (ALL-in-One Long Video Understanding Benchmark). ALLVB's main\ncontributions include: 1) It integrates 9 major video understanding tasks.\nThese tasks are converted into video QA formats, allowing a single benchmark to\nevaluate 9 different video understanding capabilities of MLLMs, highlighting\nthe versatility, comprehensiveness, and challenging nature of ALLVB. 2) A fully\nautomated annotation pipeline using GPT-4o is designed, requiring only human\nquality control, which facilitates the maintenance and expansion of the\nbenchmark. 3) It contains 1,376 videos across 16 categories, averaging nearly 2\nhours each, with a total of 252k QAs. To the best of our knowledge, it is the\nlargest long video understanding benchmark in terms of the number of videos,\naverage duration, and number of QAs. We have tested various mainstream MLLMs on\nALLVB, and the results indicate that even the most advanced commercial models\nhave significant room for improvement. This reflects the benchmark's\nchallenging nature and demonstrates the substantial potential for development\nin long video understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From image to video understanding, the capabilities of Multi-modal LLMs\n(MLLMs) are increasingly powerful. However, most existing video understanding\nbenchmarks are relatively short, which makes them inadequate for effectively\nevaluating the long-sequence modeling capabilities of MLLMs. This highlights\nthe urgent need for a comprehensive and integrated long video understanding\nbenchmark to assess the ability of MLLMs thoroughly. To this end, we propose\nALLVB (ALL-in-One Long Video Understanding Benchmark). ALLVB's main\ncontributions include: 1) It integrates 9 major video understanding tasks.\nThese tasks are converted into video QA formats, allowing a single benchmark to\nevaluate 9 different video understanding capabilities of MLLMs, highlighting\nthe versatility, comprehensiveness, and challenging nature of ALLVB. 2) A fully\nautomated annotation pipeline using GPT-4o is designed, requiring only human\nquality control, which facilitates the maintenance and expansion of the\nbenchmark. 3) It contains 1,376 videos across 16 categories, averaging nearly 2\nhours each, with a total of 252k QAs. To the best of our knowledge, it is the\nlargest long video understanding benchmark in terms of the number of videos,\naverage duration, and number of QAs. We have tested various mainstream MLLMs on\nALLVB, and the results indicate that even the most advanced commercial models\nhave significant room for improvement. This reflects the benchmark's\nchallenging nature and demonstrates the substantial potential for development\nin long video understanding."
                },
                "authors": [
                    {
                        "name": "Xichen Tan"
                    },
                    {
                        "name": "Yuanjing Luo"
                    },
                    {
                        "name": "Yunfan Ye"
                    },
                    {
                        "name": "Fang Liu"
                    },
                    {
                        "name": "Zhiping Cai"
                    }
                ],
                "author_detail": {
                    "name": "Zhiping Cai"
                },
                "author": "Zhiping Cai",
                "arxiv_comment": "AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07298v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07298v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18045v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18045v2",
                "updated": "2025-04-01T06:29:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    6,
                    29,
                    56,
                    1,
                    91,
                    0
                ],
                "published": "2025-02-25T10:10:01Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    10,
                    10,
                    1,
                    1,
                    56,
                    0
                ],
                "title": "Near-instantaneous Atmospheric Retrievals and Model Comparison with\n  FASTER",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Near-instantaneous Atmospheric Retrievals and Model Comparison with\n  FASTER"
                },
                "summary": "In the era of the James Webb Space Telescope (JWST), the dramatic improvement\nin the spectra of exoplanetary atmospheres demands a corresponding leap forward\nin our ability to analyze them: atmospheric retrievals need to be performed on\nthousands of spectra, applying to each large ensembles of models (that explore\natmospheric chemistry, thermal profiles and cloud models) to identify the best\none(s). In this limit, traditional Bayesian inference methods such as nested\nsampling become prohibitively expensive. We introduce FASTER (Fast Amortized\nSimulation-based Transiting Exoplanet Retrieval), a neural-network based method\nfor performing atmospheric retrieval and Bayesian model comparison at a\nfraction of the computational cost of classical techniques. We demonstrate that\nthe marginal posterior distributions of all parameters within a model as well\nas the posterior probabilities of the models we consider match those computed\nusing nested sampling both on mock spectra, and for the real NIRSpec PRISM\nspectrum of WASP-39b. The true power of the FASTER framework comes from its\namortized nature, which allows the trained networks to perform practically\ninstantaneous Bayesian inference and model comparison over ensembles of spectra\n-- real or simulated -- at minimal additional computational cost. This offers\nvaluable insight into the expected results of model comparison (e.g.,\ndistinguishing cloudy from cloud-free and isothermal from non-isothermal\nmodels), as well as their dependence on the underlying parameters, which is\ncomputationally unfeasible with nested sampling. This approach will constitute\nas large a leap in spectral analysis as the original retrieval methods based on\nMarkov Chain Monte Carlo have proven to be.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of the James Webb Space Telescope (JWST), the dramatic improvement\nin the spectra of exoplanetary atmospheres demands a corresponding leap forward\nin our ability to analyze them: atmospheric retrievals need to be performed on\nthousands of spectra, applying to each large ensembles of models (that explore\natmospheric chemistry, thermal profiles and cloud models) to identify the best\none(s). In this limit, traditional Bayesian inference methods such as nested\nsampling become prohibitively expensive. We introduce FASTER (Fast Amortized\nSimulation-based Transiting Exoplanet Retrieval), a neural-network based method\nfor performing atmospheric retrieval and Bayesian model comparison at a\nfraction of the computational cost of classical techniques. We demonstrate that\nthe marginal posterior distributions of all parameters within a model as well\nas the posterior probabilities of the models we consider match those computed\nusing nested sampling both on mock spectra, and for the real NIRSpec PRISM\nspectrum of WASP-39b. The true power of the FASTER framework comes from its\namortized nature, which allows the trained networks to perform practically\ninstantaneous Bayesian inference and model comparison over ensembles of spectra\n-- real or simulated -- at minimal additional computational cost. This offers\nvaluable insight into the expected results of model comparison (e.g.,\ndistinguishing cloudy from cloud-free and isothermal from non-isothermal\nmodels), as well as their dependence on the underlying parameters, which is\ncomputationally unfeasible with nested sampling. This approach will constitute\nas large a leap in spectral analysis as the original retrieval methods based on\nMarkov Chain Monte Carlo have proven to be."
                },
                "authors": [
                    {
                        "name": "Anna Lueber"
                    },
                    {
                        "name": "Konstantin Karchev"
                    },
                    {
                        "name": "Chloe Fisher"
                    },
                    {
                        "name": "Matthias Heim"
                    },
                    {
                        "name": "Roberto Trotta"
                    },
                    {
                        "name": "Kevin Heng"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Heng"
                },
                "author": "Kevin Heng",
                "arxiv_comment": "15 pages, 7 figures, 1 table. Accepted by Astrophysical Journal\n  Letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18045v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18045v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20197v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20197v2",
                "updated": "2025-04-01T06:06:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    6,
                    6,
                    38,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-26T03:44:03Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    3,
                    44,
                    3,
                    2,
                    85,
                    0
                ],
                "title": "Enhancing the Robustness of LLM-Generated Code: Empirical Study and\n  Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing the Robustness of LLM-Generated Code: Empirical Study and\n  Framework"
                },
                "summary": "Ensuring the robustness of code generated by large language models (LLMs) is\ncrucial for real-world reliability. However, existing evaluations predominantly\nfocus on correctness, often neglecting key robustness concerns such as missing\ninput validation and insufficient error handling. In this paper, we present the\nfirst empirical study on the robustness of LLM-generated code. We introduce\nnovel robustness metrics and analyze four state-of-the-art code LLMs, revealing\nthat, on average, 43.1% of their generated code is less robust than\nhuman-written counterparts. Notably, over 90% of robustness deficiencies stem\nfrom missing conditional checks, with 70% of these omissions occurring in the\nfirst line of code. Additionally, in 69% of cases where a conditional statement\nis necessary but absent, the \"if\" token still ranks third or higher in the\nmodel's predicted token probabilities, indicating an implicit recognition of\ncontrol structures. Building on these findings, we propose RobGen, a framework\ndesigned to enhance code robustness without requiring model retraining. RobGen\nleverages two model-agnostic techniques: RobGen-Adj, which dynamically adjusts\ntoken probabilities during decoding to encourage the inclusion of control\nstructures, and RobGen-Ins, which improves generated code by inserting missing\nconditionals after generation. Experimental results demonstrate that RobGen\nreduces the proportion of less robust model-generated code by 20.0%,\nsignificantly enhancing code reliability across diverse tasks. As a lightweight\nand adaptable solution, RobGen effectively mitigates robustness challenges in\nLLM-generated code. All code and data are available at\nhttps://github.com/SYSUSELab/RobGen.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring the robustness of code generated by large language models (LLMs) is\ncrucial for real-world reliability. However, existing evaluations predominantly\nfocus on correctness, often neglecting key robustness concerns such as missing\ninput validation and insufficient error handling. In this paper, we present the\nfirst empirical study on the robustness of LLM-generated code. We introduce\nnovel robustness metrics and analyze four state-of-the-art code LLMs, revealing\nthat, on average, 43.1% of their generated code is less robust than\nhuman-written counterparts. Notably, over 90% of robustness deficiencies stem\nfrom missing conditional checks, with 70% of these omissions occurring in the\nfirst line of code. Additionally, in 69% of cases where a conditional statement\nis necessary but absent, the \"if\" token still ranks third or higher in the\nmodel's predicted token probabilities, indicating an implicit recognition of\ncontrol structures. Building on these findings, we propose RobGen, a framework\ndesigned to enhance code robustness without requiring model retraining. RobGen\nleverages two model-agnostic techniques: RobGen-Adj, which dynamically adjusts\ntoken probabilities during decoding to encourage the inclusion of control\nstructures, and RobGen-Ins, which improves generated code by inserting missing\nconditionals after generation. Experimental results demonstrate that RobGen\nreduces the proportion of less robust model-generated code by 20.0%,\nsignificantly enhancing code reliability across diverse tasks. As a lightweight\nand adaptable solution, RobGen effectively mitigates robustness challenges in\nLLM-generated code. All code and data are available at\nhttps://github.com/SYSUSELab/RobGen."
                },
                "authors": [
                    {
                        "name": "Zike Li"
                    },
                    {
                        "name": "Mingwei Liu"
                    },
                    {
                        "name": "Anji Li"
                    },
                    {
                        "name": "Kaifeng He"
                    },
                    {
                        "name": "Yanlin Wang"
                    },
                    {
                        "name": "Xin Peng"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20197v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20197v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2209.06327v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2209.06327v7",
                "updated": "2025-04-01T05:51:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    5,
                    51,
                    30,
                    1,
                    91,
                    0
                ],
                "published": "2022-09-13T22:20:41Z",
                "published_parsed": [
                    2022,
                    9,
                    13,
                    22,
                    20,
                    41,
                    1,
                    256,
                    0
                ],
                "title": "PROVGEN: A Privacy-Preserving Approach for Outcome Validation in Genomic\n  Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PROVGEN: A Privacy-Preserving Approach for Outcome Validation in Genomic\n  Research"
                },
                "summary": "As genomic research has grown increasingly popular in recent years, dataset\nsharing has remained limited due to privacy concerns. This limitation hinders\nthe reproducibility and validation of research outcomes, both of which are\nessential for identifying computational errors during the research process. In\nthis paper, we introduce PROVGEN, a privacy-preserving method for sharing\ngenomic datasets that facilitates reproducibility and outcome validation in\ngenome-wide association studies (GWAS). Our approach encodes genomic data into\nbinary space and applies a two-stage process. First, we generate a\ndifferentially private version of the dataset using an XOR-based mechanism that\nincorporates biological characteristics. Second, we restore data utility by\nadjusting the Minor Allele Frequency (MAF) values in the noisy dataset to align\nwith published MAFs using optimal transport. Finally, we convert the processed\nbinary data back into its genomic representation and publish the resulting\ndataset. We evaluate PROVGEN on three real-world genomic datasets and compare\nit with local differential privacy and three synthesis-based methods. We show\nthat our proposed scheme outperforms all existing methods in detecting GWAS\noutcome errors, achieves better data utility, and provides higher privacy\nprotection against membership inference attacks (MIAs). By adopting our method,\ngenomic researchers will be inclined to share differentially private datasets\nwhile maintaining high data quality for reproducibility of their findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As genomic research has grown increasingly popular in recent years, dataset\nsharing has remained limited due to privacy concerns. This limitation hinders\nthe reproducibility and validation of research outcomes, both of which are\nessential for identifying computational errors during the research process. In\nthis paper, we introduce PROVGEN, a privacy-preserving method for sharing\ngenomic datasets that facilitates reproducibility and outcome validation in\ngenome-wide association studies (GWAS). Our approach encodes genomic data into\nbinary space and applies a two-stage process. First, we generate a\ndifferentially private version of the dataset using an XOR-based mechanism that\nincorporates biological characteristics. Second, we restore data utility by\nadjusting the Minor Allele Frequency (MAF) values in the noisy dataset to align\nwith published MAFs using optimal transport. Finally, we convert the processed\nbinary data back into its genomic representation and publish the resulting\ndataset. We evaluate PROVGEN on three real-world genomic datasets and compare\nit with local differential privacy and three synthesis-based methods. We show\nthat our proposed scheme outperforms all existing methods in detecting GWAS\noutcome errors, achieves better data utility, and provides higher privacy\nprotection against membership inference attacks (MIAs). By adopting our method,\ngenomic researchers will be inclined to share differentially private datasets\nwhile maintaining high data quality for reproducibility of their findings."
                },
                "authors": [
                    {
                        "name": "Yuzhou Jiang"
                    },
                    {
                        "name": "Tianxi Ji"
                    },
                    {
                        "name": "Erman Ayday"
                    }
                ],
                "author_detail": {
                    "name": "Erman Ayday"
                },
                "author": "Erman Ayday",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2209.06327v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2209.06327v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19803v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19803v2",
                "updated": "2025-04-01T05:51:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    5,
                    51,
                    20,
                    1,
                    91,
                    0
                ],
                "published": "2024-05-30T08:12:21Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    8,
                    12,
                    21,
                    3,
                    151,
                    0
                ],
                "title": "Dynamic Factor Analysis of High-dimensional Recurrent Events",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Factor Analysis of High-dimensional Recurrent Events"
                },
                "summary": "Recurrent event time data arise in many studies, including biomedicine,\npublic health, marketing, and social media analysis. High-dimensional recurrent\nevent data involving many event types and observations have become prevalent\nwith advances in information technology. This paper proposes a semiparametric\ndynamic factor model for the dimension reduction of high-dimensional recurrent\nevent data. The proposed model imposes a low-dimensional structure on the mean\nintensity functions of the event types while allowing for dependencies. A\nnearly rate-optimal smoothing-based estimator is proposed. An information\ncriterion that consistently selects the number of factors is also developed.\nSimulation studies demonstrate the effectiveness of these inference tools. The\nproposed method is applied to grocery shopping data, for which an interpretable\nfactor structure is obtained.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recurrent event time data arise in many studies, including biomedicine,\npublic health, marketing, and social media analysis. High-dimensional recurrent\nevent data involving many event types and observations have become prevalent\nwith advances in information technology. This paper proposes a semiparametric\ndynamic factor model for the dimension reduction of high-dimensional recurrent\nevent data. The proposed model imposes a low-dimensional structure on the mean\nintensity functions of the event types while allowing for dependencies. A\nnearly rate-optimal smoothing-based estimator is proposed. An information\ncriterion that consistently selects the number of factors is also developed.\nSimulation studies demonstrate the effectiveness of these inference tools. The\nproposed method is applied to grocery shopping data, for which an interpretable\nfactor structure is obtained."
                },
                "authors": [
                    {
                        "name": "Fangyi Chen"
                    },
                    {
                        "name": "Yunxiao Chen"
                    },
                    {
                        "name": "Zhiliang Ying"
                    },
                    {
                        "name": "Kangjie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Kangjie Zhou"
                },
                "author": "Kangjie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.19803v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19803v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15752v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15752v2",
                "updated": "2025-04-01T05:37:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    5,
                    37,
                    24,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-20T00:07:06Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    0,
                    7,
                    6,
                    3,
                    79,
                    0
                ],
                "title": "Using Language Models to Decipher the Motivation Behind Human Behaviors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Language Models to Decipher the Motivation Behind Human Behaviors"
                },
                "summary": "AI presents a novel tool for deciphering the motivations behind human\nbehaviors. We show that by varying prompts to a large language model, we can\nelicit a full range of human behaviors in a variety of different scenarios in\nterms of classic economic games. Then by analyzing which prompts are needed to\nelicit which behaviors, we can infer (decipher) the motivations behind the\nhuman behaviors. We also show how one can analyze the prompts to reveal\nrelationships between the classic economic games, providing new insight into\nwhat different economic scenarios induce people to think about. We also show\nhow this deciphering process can be used to understand differences in the\nbehavioral tendencies of different populations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI presents a novel tool for deciphering the motivations behind human\nbehaviors. We show that by varying prompts to a large language model, we can\nelicit a full range of human behaviors in a variety of different scenarios in\nterms of classic economic games. Then by analyzing which prompts are needed to\nelicit which behaviors, we can infer (decipher) the motivations behind the\nhuman behaviors. We also show how one can analyze the prompts to reveal\nrelationships between the classic economic games, providing new insight into\nwhat different economic scenarios induce people to think about. We also show\nhow this deciphering process can be used to understand differences in the\nbehavioral tendencies of different populations."
                },
                "authors": [
                    {
                        "name": "Yutong Xie"
                    },
                    {
                        "name": "Qiaozhu Mei"
                    },
                    {
                        "name": "Walter Yuan"
                    },
                    {
                        "name": "Matthew O. Jackson"
                    }
                ],
                "author_detail": {
                    "name": "Matthew O. Jackson"
                },
                "author": "Matthew O. Jackson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15752v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15752v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06994v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06994v2",
                "updated": "2025-04-01T05:33:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    5,
                    33,
                    5,
                    1,
                    91,
                    0
                ],
                "published": "2024-12-09T21:01:45Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    21,
                    1,
                    45,
                    0,
                    344,
                    0
                ],
                "title": "Phaedrus: Predicting Dynamic Application Behavior with Lightweight\n  Generative Models and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phaedrus: Predicting Dynamic Application Behavior with Lightweight\n  Generative Models and LLMs"
                },
                "summary": "Application profiling is an indispensable technique for many software\ndevelopment tasks, such as code and memory layout optimizations, where\noptimization decisions are tailored to specific program profiles.\nUnfortunately, modern applications codebases exhibit highly variant behavior\nacross different inputs, creating challenges for conventional profiling\napproaches that rely on a single representative execution instance. In this\npaper, we propose \\textbf{Phaedrus}, a new \\textit{compiler-assisted deep\nlearning framework} designed to predict dynamic program behaviors across varied\nexecution instances, specifically focusing on dynamic function call\nprediction.Such predicted call sequences are then used for producing optimized\ncode pertinent to a given input.\n  Traditional profile-guided optimization methods struggle with the\ninput-dependent variability of modern applications, where profiling on\ndifferent inputs yields divergent application behaviors. To address this,\nPhaedrus proposes two new approaches: \\textit{Application Behavior Synthesis},\na profile-less approach where Large Language Models (LLMs) directly infer\ndynamic functions based on source code \\& static compiler analysis, bypassing\nthe need for traditional profiling, and \\textit{Application Profile\nGeneralization}, which uses generative models trained on compressed and\naugmented \\textit{Whole Program Path} (WPP) based function profiles to predict\napplication behavior under unseen inputs. Our experiments show that\n\\textit{Phaedrus} can achieve upto $10^7X$ reduction in WPP function profile\nsizes, can predict most frequently executed functions that cover upto 85-99\\%\nof the execution time, along with an average of 13.68\\% (upto 65\\%) reduction\nin application binary size, and an average of 2.8\\% performance improvement\nover the traditional profile-guided optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application profiling is an indispensable technique for many software\ndevelopment tasks, such as code and memory layout optimizations, where\noptimization decisions are tailored to specific program profiles.\nUnfortunately, modern applications codebases exhibit highly variant behavior\nacross different inputs, creating challenges for conventional profiling\napproaches that rely on a single representative execution instance. In this\npaper, we propose \\textbf{Phaedrus}, a new \\textit{compiler-assisted deep\nlearning framework} designed to predict dynamic program behaviors across varied\nexecution instances, specifically focusing on dynamic function call\nprediction.Such predicted call sequences are then used for producing optimized\ncode pertinent to a given input.\n  Traditional profile-guided optimization methods struggle with the\ninput-dependent variability of modern applications, where profiling on\ndifferent inputs yields divergent application behaviors. To address this,\nPhaedrus proposes two new approaches: \\textit{Application Behavior Synthesis},\na profile-less approach where Large Language Models (LLMs) directly infer\ndynamic functions based on source code \\& static compiler analysis, bypassing\nthe need for traditional profiling, and \\textit{Application Profile\nGeneralization}, which uses generative models trained on compressed and\naugmented \\textit{Whole Program Path} (WPP) based function profiles to predict\napplication behavior under unseen inputs. Our experiments show that\n\\textit{Phaedrus} can achieve upto $10^7X$ reduction in WPP function profile\nsizes, can predict most frequently executed functions that cover upto 85-99\\%\nof the execution time, along with an average of 13.68\\% (upto 65\\%) reduction\nin application binary size, and an average of 2.8\\% performance improvement\nover the traditional profile-guided optimization."
                },
                "authors": [
                    {
                        "name": "Bodhisatwa Chatterjee"
                    },
                    {
                        "name": "Neeraj Jadhav"
                    },
                    {
                        "name": "Sharjeel Khan"
                    },
                    {
                        "name": "Santosh Pande"
                    }
                ],
                "author_detail": {
                    "name": "Santosh Pande"
                },
                "author": "Santosh Pande",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06994v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06994v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22420v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22420v2",
                "updated": "2025-04-01T05:32:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    5,
                    32,
                    41,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-28T13:32:29Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    13,
                    32,
                    29,
                    4,
                    87,
                    0
                ],
                "title": "Unveiling the Mist over 3D Vision-Language Understanding: Object-centric\n  Evaluation with Chain-of-Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling the Mist over 3D Vision-Language Understanding: Object-centric\n  Evaluation with Chain-of-Analysis"
                },
                "summary": "Existing 3D vision-language (3D-VL) benchmarks fall short in evaluating 3D-VL\nmodels, creating a \"mist\" that obscures rigorous insights into model\ncapabilities and 3D-VL tasks. This mist persists due to three key limitations.\nFirst, flawed test data, like ambiguous referential text in the grounding task,\ncan yield incorrect and unreliable test results. Second, oversimplified metrics\nsuch as simply averaging accuracy per question answering (QA) pair, cannot\nreveal true model capability due to their vulnerability to language variations.\nThird, existing benchmarks isolate the grounding and QA tasks, disregarding the\nunderlying coherence that QA should be based on solid grounding capabilities.\nTo unveil the \"mist\", we propose Beacon3D, a benchmark for 3D-VL grounding and\nQA tasks, delivering a perspective shift in the evaluation of 3D-VL\nunderstanding. Beacon3D features (i) high-quality test data with precise and\nnatural language, (ii) object-centric evaluation with multiple tests per object\nto ensure robustness, and (iii) a novel chain-of-analysis paradigm to address\nlanguage robustness and model performance coherence across grounding and QA.\nOur evaluation of state-of-the-art 3D-VL models on Beacon3D reveals that (i)\nobject-centric evaluation elicits true model performance and particularly weak\ngeneralization in QA; (ii) grounding-QA coherence remains fragile in current\n3D-VL models, and (iii) incorporating large language models (LLMs) to 3D-VL\nmodels, though as a prevalent practice, hinders grounding capabilities and has\nyet to elevate QA capabilities. We hope Beacon3D and our comprehensive analysis\ncould benefit the 3D-VL community towards faithful developments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing 3D vision-language (3D-VL) benchmarks fall short in evaluating 3D-VL\nmodels, creating a \"mist\" that obscures rigorous insights into model\ncapabilities and 3D-VL tasks. This mist persists due to three key limitations.\nFirst, flawed test data, like ambiguous referential text in the grounding task,\ncan yield incorrect and unreliable test results. Second, oversimplified metrics\nsuch as simply averaging accuracy per question answering (QA) pair, cannot\nreveal true model capability due to their vulnerability to language variations.\nThird, existing benchmarks isolate the grounding and QA tasks, disregarding the\nunderlying coherence that QA should be based on solid grounding capabilities.\nTo unveil the \"mist\", we propose Beacon3D, a benchmark for 3D-VL grounding and\nQA tasks, delivering a perspective shift in the evaluation of 3D-VL\nunderstanding. Beacon3D features (i) high-quality test data with precise and\nnatural language, (ii) object-centric evaluation with multiple tests per object\nto ensure robustness, and (iii) a novel chain-of-analysis paradigm to address\nlanguage robustness and model performance coherence across grounding and QA.\nOur evaluation of state-of-the-art 3D-VL models on Beacon3D reveals that (i)\nobject-centric evaluation elicits true model performance and particularly weak\ngeneralization in QA; (ii) grounding-QA coherence remains fragile in current\n3D-VL models, and (iii) incorporating large language models (LLMs) to 3D-VL\nmodels, though as a prevalent practice, hinders grounding capabilities and has\nyet to elevate QA capabilities. We hope Beacon3D and our comprehensive analysis\ncould benefit the 3D-VL community towards faithful developments."
                },
                "authors": [
                    {
                        "name": "Jiangyong Huang"
                    },
                    {
                        "name": "Baoxiong Jia"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Ziyu Zhu"
                    },
                    {
                        "name": "Xiongkun Linghu"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Song-Chun Zhu"
                    },
                    {
                        "name": "Siyuan Huang"
                    }
                ],
                "author_detail": {
                    "name": "Siyuan Huang"
                },
                "author": "Siyuan Huang",
                "arxiv_comment": "CVPR 2025. Project page: https://beacon-3d.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22420v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22420v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01352v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01352v2",
                "updated": "2025-04-01T05:29:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    5,
                    29,
                    0,
                    1,
                    91,
                    0
                ],
                "published": "2024-10-02T09:09:34Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    9,
                    9,
                    34,
                    2,
                    276,
                    0
                ],
                "title": "Mean field equilibrium asset pricing model under partial observation: An\n  exponential quadratic Gaussian approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mean field equilibrium asset pricing model under partial observation: An\n  exponential quadratic Gaussian approach"
                },
                "summary": "This paper studies an asset pricing model in a partially observable market\nwith a large number of heterogeneous agents using the mean field game theory.\nIn this model, we assume that investors can only observe stock prices and must\ninfer the risk premium from these observations when determining trading\nstrategies. We characterize the equilibrium risk premium in such a market\nthrough a solution to the mean field backward stochastic differential equation\n(BSDE). Specifically, the solution to the mean field BSDE can be expressed\nsemi-analytically by employing an exponential quadratic Gaussian framework. We\nthen construct the risk premium process, which cannot be observed directly by\ninvestors, endogenously using the Kalman-Bucy filtering theory. In addition, we\ninclude a simple numerical simulation to visualize the dynamics of our market\nmodel.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper studies an asset pricing model in a partially observable market\nwith a large number of heterogeneous agents using the mean field game theory.\nIn this model, we assume that investors can only observe stock prices and must\ninfer the risk premium from these observations when determining trading\nstrategies. We characterize the equilibrium risk premium in such a market\nthrough a solution to the mean field backward stochastic differential equation\n(BSDE). Specifically, the solution to the mean field BSDE can be expressed\nsemi-analytically by employing an exponential quadratic Gaussian framework. We\nthen construct the risk premium process, which cannot be observed directly by\ninvestors, endogenously using the Kalman-Bucy filtering theory. In addition, we\ninclude a simple numerical simulation to visualize the dynamics of our market\nmodel."
                },
                "authors": [
                    {
                        "name": "Masashi Sekine"
                    }
                ],
                "author_detail": {
                    "name": "Masashi Sekine"
                },
                "author": "Masashi Sekine",
                "arxiv_comment": "Forthcoming in Japan Journal of Industrial and Applied Mathematics.\n  22 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01352v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01352v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.PR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.TR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "49N80, 91B51, 60H10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14781v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14781v5",
                "updated": "2025-04-01T05:09:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    5,
                    9,
                    19,
                    1,
                    91,
                    0
                ],
                "published": "2024-09-23T07:55:35Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    7,
                    55,
                    35,
                    0,
                    267,
                    0
                ],
                "title": "Pretraining Data Detection for Large Language Models: A Divergence-based\n  Calibration Method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pretraining Data Detection for Large Language Models: A Divergence-based\n  Calibration Method"
                },
                "summary": "As the scale of training corpora for large language models (LLMs) grows,\nmodel developers become increasingly reluctant to disclose details on their\ndata. This lack of transparency poses challenges to scientific evaluation and\nethical deployment. Recently, pretraining data detection approaches, which\ninfer whether a given text was part of an LLM's training data through black-box\naccess, have been explored. The Min-K\\% Prob method, which has achieved\nstate-of-the-art results, assumes that a non-training example tends to contain\na few outlier words with low token probabilities. However, the effectiveness\nmay be limited as it tends to misclassify non-training texts that contain many\ncommon words with high probabilities predicted by LLMs. To address this issue,\nwe introduce a divergence-based calibration method, inspired by the\ndivergence-from-randomness concept, to calibrate token probabilities for\npretraining data detection. We compute the cross-entropy (i.e., the divergence)\nbetween the token probability distribution and the token frequency distribution\nto derive a detection score. We have developed a Chinese-language benchmark,\nPatentMIA, to assess the performance of detection approaches for LLMs on\nChinese text. Experimental results on English-language benchmarks and PatentMIA\ndemonstrate that our proposed method significantly outperforms existing\nmethods. Our code and PatentMIA benchmark are available at\nhttps://github.com/zhang-wei-chao/DC-PDD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the scale of training corpora for large language models (LLMs) grows,\nmodel developers become increasingly reluctant to disclose details on their\ndata. This lack of transparency poses challenges to scientific evaluation and\nethical deployment. Recently, pretraining data detection approaches, which\ninfer whether a given text was part of an LLM's training data through black-box\naccess, have been explored. The Min-K\\% Prob method, which has achieved\nstate-of-the-art results, assumes that a non-training example tends to contain\na few outlier words with low token probabilities. However, the effectiveness\nmay be limited as it tends to misclassify non-training texts that contain many\ncommon words with high probabilities predicted by LLMs. To address this issue,\nwe introduce a divergence-based calibration method, inspired by the\ndivergence-from-randomness concept, to calibrate token probabilities for\npretraining data detection. We compute the cross-entropy (i.e., the divergence)\nbetween the token probability distribution and the token frequency distribution\nto derive a detection score. We have developed a Chinese-language benchmark,\nPatentMIA, to assess the performance of detection approaches for LLMs on\nChinese text. Experimental results on English-language benchmarks and PatentMIA\ndemonstrate that our proposed method significantly outperforms existing\nmethods. Our code and PatentMIA benchmark are available at\nhttps://github.com/zhang-wei-chao/DC-PDD."
                },
                "authors": [
                    {
                        "name": "Weichao Zhang"
                    },
                    {
                        "name": "Ruqing Zhang"
                    },
                    {
                        "name": "Jiafeng Guo"
                    },
                    {
                        "name": "Maarten de Rijke"
                    },
                    {
                        "name": "Yixing Fan"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "arxiv_comment": "Accepted by EMNLP 2024 main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14781v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14781v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06559v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06559v2",
                "updated": "2025-04-01T05:04:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    5,
                    4,
                    47,
                    1,
                    91,
                    0
                ],
                "published": "2024-11-10T18:50:51Z",
                "published_parsed": [
                    2024,
                    11,
                    10,
                    18,
                    50,
                    51,
                    6,
                    315,
                    0
                ],
                "title": "Is Your LLM Secretly a World Model of the Internet? Model-Based Planning\n  for Web Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Your LLM Secretly a World Model of the Internet? Model-Based Planning\n  for Web Agents"
                },
                "summary": "Language agents based on large language models (LLMs) have demonstrated great\npromise in automating web-based tasks. Recent work has shown that incorporating\nadvanced planning algorithms, e.g., tree search, is advantageous over reactive\nplanning for web agents. However, unlike simulated sandbox environments,\nreal-world environments such as the web are rife with irreversible actions.\nThis undermines the feasibility of backtracking, a cornerstone of (tree)\nsearch. Overly relying on test-time search also hurts efficiency. We advocate\nmodel-based planning for web agents that employs a world model to simulate and\ndeliberate over the outcome of each candidate action before committing to one.\nWe systematically explore this paradigm by (1) Proposing a model-based planning\nframework, WebDreamer, which employs LLMs to serve as both world models and\nvalue functions; (2) Training specialized LLMs as world models with a scalable\ndata synthesis pipeline. Empirical results demonstrate that WebDreamer achieves\nsubstantial performance improvements over reactive baselines. It is\ncompetitive, while being 4-5 times more efficient, with tree search in sandbox\nenvironments (VisualWebArena) and also works effectively on real-world websites\n(Online-Mind2Web and Mind2Web-Live). Furthermore, our trained world model,\nDreamer-7B, performs comparable to GPT-4o, highlighting the potential of\nspecialized world models for efficient and effective planning in complex web\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language agents based on large language models (LLMs) have demonstrated great\npromise in automating web-based tasks. Recent work has shown that incorporating\nadvanced planning algorithms, e.g., tree search, is advantageous over reactive\nplanning for web agents. However, unlike simulated sandbox environments,\nreal-world environments such as the web are rife with irreversible actions.\nThis undermines the feasibility of backtracking, a cornerstone of (tree)\nsearch. Overly relying on test-time search also hurts efficiency. We advocate\nmodel-based planning for web agents that employs a world model to simulate and\ndeliberate over the outcome of each candidate action before committing to one.\nWe systematically explore this paradigm by (1) Proposing a model-based planning\nframework, WebDreamer, which employs LLMs to serve as both world models and\nvalue functions; (2) Training specialized LLMs as world models with a scalable\ndata synthesis pipeline. Empirical results demonstrate that WebDreamer achieves\nsubstantial performance improvements over reactive baselines. It is\ncompetitive, while being 4-5 times more efficient, with tree search in sandbox\nenvironments (VisualWebArena) and also works effectively on real-world websites\n(Online-Mind2Web and Mind2Web-Live). Furthermore, our trained world model,\nDreamer-7B, performs comparable to GPT-4o, highlighting the potential of\nspecialized world models for efficient and effective planning in complex web\nenvironments."
                },
                "authors": [
                    {
                        "name": "Yu Gu"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Yuting Ning"
                    },
                    {
                        "name": "Boyuan Zheng"
                    },
                    {
                        "name": "Boyu Gou"
                    },
                    {
                        "name": "Tianci Xue"
                    },
                    {
                        "name": "Cheng Chang"
                    },
                    {
                        "name": "Sanjari Srivastava"
                    },
                    {
                        "name": "Yanan Xie"
                    },
                    {
                        "name": "Peng Qi"
                    },
                    {
                        "name": "Huan Sun"
                    },
                    {
                        "name": "Yu Su"
                    }
                ],
                "author_detail": {
                    "name": "Yu Su"
                },
                "author": "Yu Su",
                "arxiv_comment": "22 pages, 11 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06559v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06559v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.04560v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.04560v4",
                "updated": "2025-04-01T05:04:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    5,
                    4,
                    22,
                    1,
                    91,
                    0
                ],
                "published": "2024-01-09T13:56:37Z",
                "published_parsed": [
                    2024,
                    1,
                    9,
                    13,
                    56,
                    37,
                    1,
                    9,
                    0
                ],
                "title": "Phase-shifted remote photoplethysmography for estimating heart rate and\n  blood pressure from facial video",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phase-shifted remote photoplethysmography for estimating heart rate and\n  blood pressure from facial video"
                },
                "summary": "Human health can be critically affected by cardiovascular diseases, such as\nhypertension, arrhythmias, and stroke. Heart rate and blood pressure are\nimportant biometric information for the monitoring of cardiovascular system and\nearly diagnosis of cardiovascular diseases. Existing methods for estimating the\nheart rate are based on electrocardiography and photoplethyomography, which\nrequire contacting the sensor to the skin surface. Moreover, catheter and\ncuff-based methods for measuring blood pressure cause inconvenience and have\nlimited applicability. Therefore, in this thesis, we propose a vision-based\nmethod for estimating the heart rate and blood pressure. This thesis proposes a\n2-stage deep learning framework consisting of a dual remote\nphotoplethysmography network (DRP-Net) and bounded blood pressure network\n(BBP-Net). In the first stage, DRP-Net infers remote photoplethysmography\n(rPPG) signals for the acral and facial regions, and these phase-shifted rPPG\nsignals are utilized to estimate the heart rate. In the second stage, BBP-Net\nintegrates temporal features and analyzes phase discrepancy between the acral\nand facial rPPG signals to estimate SBP and DBP values. To improve the accuracy\nof estimating the heart rate, we employed a data augmentation method based on a\nframe interpolation model. Moreover, we designed BBP-Net to infer blood\npressure within a predefined range by incorporating a scaled sigmoid function.\nOur method resulted in estimating the heart rate with the mean absolute error\n(MAE) of 1.78 BPM, reducing the MAE by 34.31 % compared to the recent method,\non the MMSE-HR dataset. The MAE for estimating the systolic blood pressure\n(SBP) and diastolic blood pressure (DBP) were 10.19 mmHg and 7.09 mmHg. On the\nV4V dataset, the MAE for the heart rate, SBP, and DBP were 3.83 BPM, 13.64\nmmHg, and 9.4 mmHg, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human health can be critically affected by cardiovascular diseases, such as\nhypertension, arrhythmias, and stroke. Heart rate and blood pressure are\nimportant biometric information for the monitoring of cardiovascular system and\nearly diagnosis of cardiovascular diseases. Existing methods for estimating the\nheart rate are based on electrocardiography and photoplethyomography, which\nrequire contacting the sensor to the skin surface. Moreover, catheter and\ncuff-based methods for measuring blood pressure cause inconvenience and have\nlimited applicability. Therefore, in this thesis, we propose a vision-based\nmethod for estimating the heart rate and blood pressure. This thesis proposes a\n2-stage deep learning framework consisting of a dual remote\nphotoplethysmography network (DRP-Net) and bounded blood pressure network\n(BBP-Net). In the first stage, DRP-Net infers remote photoplethysmography\n(rPPG) signals for the acral and facial regions, and these phase-shifted rPPG\nsignals are utilized to estimate the heart rate. In the second stage, BBP-Net\nintegrates temporal features and analyzes phase discrepancy between the acral\nand facial rPPG signals to estimate SBP and DBP values. To improve the accuracy\nof estimating the heart rate, we employed a data augmentation method based on a\nframe interpolation model. Moreover, we designed BBP-Net to infer blood\npressure within a predefined range by incorporating a scaled sigmoid function.\nOur method resulted in estimating the heart rate with the mean absolute error\n(MAE) of 1.78 BPM, reducing the MAE by 34.31 % compared to the recent method,\non the MMSE-HR dataset. The MAE for estimating the systolic blood pressure\n(SBP) and diastolic blood pressure (DBP) were 10.19 mmHg and 7.09 mmHg. On the\nV4V dataset, the MAE for the heart rate, SBP, and DBP were 3.83 BPM, 13.64\nmmHg, and 9.4 mmHg, respectively."
                },
                "authors": [
                    {
                        "name": "Gyutae Hwang"
                    },
                    {
                        "name": "Sang Jun Lee"
                    }
                ],
                "author_detail": {
                    "name": "Sang Jun Lee"
                },
                "author": "Sang Jun Lee",
                "arxiv_comment": "13 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.04560v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.04560v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15207v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15207v3",
                "updated": "2025-04-01T05:01:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    5,
                    1,
                    15,
                    1,
                    91,
                    0
                ],
                "published": "2024-08-27T17:14:21Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    14,
                    21,
                    1,
                    240,
                    0
                ],
                "title": "Understanding the Effectiveness of Coverage Criteria for Large Language\n  Models: A Special Angle from Jailbreak Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the Effectiveness of Coverage Criteria for Large Language\n  Models: A Special Angle from Jailbreak Attacks"
                },
                "summary": "Large language models (LLMs) have revolutionized artificial intelligence, but\ntheir increasing deployment across critical domains has raised concerns about\ntheir abnormal behaviors when faced with malicious attacks. Such vulnerability\nalerts the widespread inadequacy of pre-release testing. In this paper, we\nconduct a comprehensive empirical study to evaluate the effectiveness of\ntraditional coverage criteria in identifying such inadequacies, exemplified by\nthe significant security concern of jailbreak attacks. Our study begins with a\nclustering analysis of the hidden states of LLMs, revealing that the embedded\ncharacteristics effectively distinguish between different query types. We then\nsystematically evaluate the performance of these criteria across three key\ndimensions: criterion level, layer level, and token level. Our research\nuncovers significant differences in neuron coverage when LLMs process normal\nversus jailbreak queries, aligning with our clustering experiments. Leveraging\nthese findings, we propose three practical applications of coverage criteria in\nthe context of LLM security testing. Specifically, we develop a real-time\njailbreak detection mechanism that achieves high accuracy (93.61% on average)\nin classifying queries as normal or jailbreak. Furthermore, we explore the use\nof coverage levels to prioritize test cases, improving testing efficiency by\nfocusing on high-risk interactions and removing redundant tests. Lastly, we\nintroduce a coverage-guided approach for generating jailbreak attack examples,\nenabling systematic refinement of prompts to uncover vulnerabilities. This\nstudy improves our understanding of LLM security testing, enhances their\nsafety, and provides a foundation for developing more robust AI applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have revolutionized artificial intelligence, but\ntheir increasing deployment across critical domains has raised concerns about\ntheir abnormal behaviors when faced with malicious attacks. Such vulnerability\nalerts the widespread inadequacy of pre-release testing. In this paper, we\nconduct a comprehensive empirical study to evaluate the effectiveness of\ntraditional coverage criteria in identifying such inadequacies, exemplified by\nthe significant security concern of jailbreak attacks. Our study begins with a\nclustering analysis of the hidden states of LLMs, revealing that the embedded\ncharacteristics effectively distinguish between different query types. We then\nsystematically evaluate the performance of these criteria across three key\ndimensions: criterion level, layer level, and token level. Our research\nuncovers significant differences in neuron coverage when LLMs process normal\nversus jailbreak queries, aligning with our clustering experiments. Leveraging\nthese findings, we propose three practical applications of coverage criteria in\nthe context of LLM security testing. Specifically, we develop a real-time\njailbreak detection mechanism that achieves high accuracy (93.61% on average)\nin classifying queries as normal or jailbreak. Furthermore, we explore the use\nof coverage levels to prioritize test cases, improving testing efficiency by\nfocusing on high-risk interactions and removing redundant tests. Lastly, we\nintroduce a coverage-guided approach for generating jailbreak attack examples,\nenabling systematic refinement of prompts to uncover vulnerabilities. This\nstudy improves our understanding of LLM security testing, enhances their\nsafety, and provides a foundation for developing more robust AI applications."
                },
                "authors": [
                    {
                        "name": "Shide Zhou"
                    },
                    {
                        "name": "Tianlin Li"
                    },
                    {
                        "name": "Kailong Wang"
                    },
                    {
                        "name": "Yihao Huang"
                    },
                    {
                        "name": "Ling Shi"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Haoyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoyu Wang"
                },
                "author": "Haoyu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15207v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15207v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15426v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15426v2",
                "updated": "2025-04-01T03:53:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    3,
                    53,
                    53,
                    1,
                    91,
                    0
                ],
                "published": "2024-03-13T05:38:39Z",
                "published_parsed": [
                    2024,
                    3,
                    13,
                    5,
                    38,
                    39,
                    2,
                    73,
                    0
                ],
                "title": "CodingTeachLLM: Empowering LLM's Coding Ability via AST Prior Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodingTeachLLM: Empowering LLM's Coding Ability via AST Prior Knowledge"
                },
                "summary": "In this paper, we introduce CodingTeachLLM, a large language model (LLM)\ndesigned for coding teaching. Specially, we aim to enhance the coding ability\nof LLM and lead it to better teaching mode in education context. Thus, we\npropose an end-to-end prior-based three-phases supervised fine-tuned model,\nwhich is proved more competitive than traditional fine-tuning method. More\nspecifically, our model realizes the structural disassembly and incremental\nguided output of educational knowledge. To this end, we robustify data\nclassification of three types via a sampler and overlap estimation neural\nnetwork, and inject the preprocessing datasets into pre-trained model in three\nbatches for LORA fine-tuning. Then, we design a prior module couples system\nprompt, vector databases, and abstract syntax tree task segmentation. Finally,\nthe compression method and regularization constraint are applied to the\nprior-based fine-tuned model, followed by text filter at the output end to\nobtain incremental guided results. Our model represents the first research\neffort to truly embody the tutor role with the features of abundant educational\nknowledge, step-by-step incremental guided outputs and non-disclosure of\nanswers. Extensive experiments report that our model also achieves\nstate-of-the-art in code abilities compared to open-source models, reaching an\nimpressive 75.10% on the HumanEval (@pass 1) benchmark. Additionally, our model\nmaintains strong conversational capabilities, with the 13B quantized version\nachieving scores of 56.34, 50.60, and 45.27 respectively on the MMLU, C-Eval,\nand AGIEval (5 shot) dialogue evaluation benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce CodingTeachLLM, a large language model (LLM)\ndesigned for coding teaching. Specially, we aim to enhance the coding ability\nof LLM and lead it to better teaching mode in education context. Thus, we\npropose an end-to-end prior-based three-phases supervised fine-tuned model,\nwhich is proved more competitive than traditional fine-tuning method. More\nspecifically, our model realizes the structural disassembly and incremental\nguided output of educational knowledge. To this end, we robustify data\nclassification of three types via a sampler and overlap estimation neural\nnetwork, and inject the preprocessing datasets into pre-trained model in three\nbatches for LORA fine-tuning. Then, we design a prior module couples system\nprompt, vector databases, and abstract syntax tree task segmentation. Finally,\nthe compression method and regularization constraint are applied to the\nprior-based fine-tuned model, followed by text filter at the output end to\nobtain incremental guided results. Our model represents the first research\neffort to truly embody the tutor role with the features of abundant educational\nknowledge, step-by-step incremental guided outputs and non-disclosure of\nanswers. Extensive experiments report that our model also achieves\nstate-of-the-art in code abilities compared to open-source models, reaching an\nimpressive 75.10% on the HumanEval (@pass 1) benchmark. Additionally, our model\nmaintains strong conversational capabilities, with the 13B quantized version\nachieving scores of 56.34, 50.60, and 45.27 respectively on the MMLU, C-Eval,\nand AGIEval (5 shot) dialogue evaluation benchmarks."
                },
                "authors": [
                    {
                        "name": "Zhangquan Chen"
                    },
                    {
                        "name": "Chunjiang Liu"
                    },
                    {
                        "name": "Haobin Duan"
                    }
                ],
                "author_detail": {
                    "name": "Haobin Duan"
                },
                "author": "Haobin Duan",
                "arxiv_comment": "9 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15426v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15426v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.17238v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.17238v2",
                "updated": "2025-04-01T03:50:12Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    3,
                    50,
                    12,
                    1,
                    91,
                    0
                ],
                "published": "2024-03-25T22:39:20Z",
                "published_parsed": [
                    2024,
                    3,
                    25,
                    22,
                    39,
                    20,
                    0,
                    85,
                    0
                ],
                "title": "Temporal and Semantic Evaluation Metrics for Foundation Models in\n  Post-Hoc Analysis of Robotic Sub-tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal and Semantic Evaluation Metrics for Foundation Models in\n  Post-Hoc Analysis of Robotic Sub-tasks"
                },
                "summary": "Recent works in Task and Motion Planning (TAMP) show that training control\npolicies on language-supervised robot trajectories with quality labeled data\nmarkedly improves agent task success rates. However, the scarcity of such data\npresents a significant hurdle to extending these methods to general use cases.\nTo address this concern, we present an automated framework to decompose\ntrajectory data into temporally bounded and natural language-based descriptive\nsub-tasks by leveraging recent prompting strategies for Foundation Models (FMs)\nincluding both Large Language Models (LLMs) and Vision Language Models (VLMs).\nOur framework provides both time-based and language-based descriptions for\nlower-level sub-tasks that comprise full trajectories. To rigorously evaluate\nthe quality of our automatic labeling framework, we contribute an algorithm\nSIMILARITY to produce two novel metrics, temporal similarity and semantic\nsimilarity. The metrics measure the temporal alignment and semantic fidelity of\nlanguage descriptions between two sub-task decompositions, namely an FM\nsub-task decomposition prediction and a ground-truth sub-task decomposition. We\npresent scores for temporal similarity and semantic similarity above 90%,\ncompared to 30% of a randomized baseline, for multiple robotic environments,\ndemonstrating the effectiveness of our proposed framework. Our results enable\nbuilding diverse, large-scale, language-supervised datasets for improved\nrobotic TAMP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent works in Task and Motion Planning (TAMP) show that training control\npolicies on language-supervised robot trajectories with quality labeled data\nmarkedly improves agent task success rates. However, the scarcity of such data\npresents a significant hurdle to extending these methods to general use cases.\nTo address this concern, we present an automated framework to decompose\ntrajectory data into temporally bounded and natural language-based descriptive\nsub-tasks by leveraging recent prompting strategies for Foundation Models (FMs)\nincluding both Large Language Models (LLMs) and Vision Language Models (VLMs).\nOur framework provides both time-based and language-based descriptions for\nlower-level sub-tasks that comprise full trajectories. To rigorously evaluate\nthe quality of our automatic labeling framework, we contribute an algorithm\nSIMILARITY to produce two novel metrics, temporal similarity and semantic\nsimilarity. The metrics measure the temporal alignment and semantic fidelity of\nlanguage descriptions between two sub-task decompositions, namely an FM\nsub-task decomposition prediction and a ground-truth sub-task decomposition. We\npresent scores for temporal similarity and semantic similarity above 90%,\ncompared to 30% of a randomized baseline, for multiple robotic environments,\ndemonstrating the effectiveness of our proposed framework. Our results enable\nbuilding diverse, large-scale, language-supervised datasets for improved\nrobotic TAMP."
                },
                "authors": [
                    {
                        "name": "Jonathan Salfity"
                    },
                    {
                        "name": "Selma Wanna"
                    },
                    {
                        "name": "Minkyu Choi"
                    },
                    {
                        "name": "Mitch Pryor"
                    }
                ],
                "author_detail": {
                    "name": "Mitch Pryor"
                },
                "author": "Mitch Pryor",
                "arxiv_comment": "8 pages, 3 figures. IROS 2024 Submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.17238v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.17238v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19867v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19867v2",
                "updated": "2025-04-01T03:31:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    3,
                    31,
                    42,
                    1,
                    91,
                    0
                ],
                "published": "2024-12-27T09:05:48Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    9,
                    5,
                    48,
                    4,
                    362,
                    0
                ],
                "title": "Data-Free Group-Wise Fully Quantized Winograd Convolution via Learnable\n  Scales",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-Free Group-Wise Fully Quantized Winograd Convolution via Learnable\n  Scales"
                },
                "summary": "Despite the revolutionary breakthroughs of large-scale text-to-image\ndiffusion models for complex vision and downstream tasks, their extremely high\ncomputational and storage costs limit their usability. Quantization of\ndiffusion models has been explored in recent works to reduce compute costs and\nmemory bandwidth usage. To further improve inference time, fast convolution\nalgorithms such as Winograd can be used for convolution layers, which account\nfor a significant portion of computations in diffusion models. However, the\nsignificant quality loss of fully quantized Winograd using existing\ncoarser-grained post-training quantization methods, combined with the\ncomplexity and cost of finetuning the Winograd transformation matrices for such\nlarge models to recover quality, makes them unsuitable for large-scale\nfoundation models. Motivated by the presence of a large range of values in\nthem, we investigate the impact of finer-grained group-wise quantization in\nquantizing diffusion models. While group-wise quantization can largely handle\nthe fully quantized Winograd convolution, it struggles to deal with the large\ndistribution imbalance in a sizable portion of the Winograd domain computation.\nTo reduce range differences in the Winograd domain, we propose finetuning only\nthe scale parameters of the Winograd transform matrices without using any\ndomain-specific training data. Because our method does not depend on any\ntraining data, the generalization performance of quantized diffusion models is\nsafely guaranteed. For text-to-image generation task, the 8-bit fully-quantized\ndiffusion model with Winograd provides near-lossless quality (FID and CLIP\nscores) in comparison to the full-precision model. For image classification,\nour method outperforms the state-of-the-art Winograd PTQ method by 1.62% and\n2.56% in top-1 ImageNet accuracy on ResNet18 and ResNet-34, respectively, with\nWinograd F(6, 3).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the revolutionary breakthroughs of large-scale text-to-image\ndiffusion models for complex vision and downstream tasks, their extremely high\ncomputational and storage costs limit their usability. Quantization of\ndiffusion models has been explored in recent works to reduce compute costs and\nmemory bandwidth usage. To further improve inference time, fast convolution\nalgorithms such as Winograd can be used for convolution layers, which account\nfor a significant portion of computations in diffusion models. However, the\nsignificant quality loss of fully quantized Winograd using existing\ncoarser-grained post-training quantization methods, combined with the\ncomplexity and cost of finetuning the Winograd transformation matrices for such\nlarge models to recover quality, makes them unsuitable for large-scale\nfoundation models. Motivated by the presence of a large range of values in\nthem, we investigate the impact of finer-grained group-wise quantization in\nquantizing diffusion models. While group-wise quantization can largely handle\nthe fully quantized Winograd convolution, it struggles to deal with the large\ndistribution imbalance in a sizable portion of the Winograd domain computation.\nTo reduce range differences in the Winograd domain, we propose finetuning only\nthe scale parameters of the Winograd transform matrices without using any\ndomain-specific training data. Because our method does not depend on any\ntraining data, the generalization performance of quantized diffusion models is\nsafely guaranteed. For text-to-image generation task, the 8-bit fully-quantized\ndiffusion model with Winograd provides near-lossless quality (FID and CLIP\nscores) in comparison to the full-precision model. For image classification,\nour method outperforms the state-of-the-art Winograd PTQ method by 1.62% and\n2.56% in top-1 ImageNet accuracy on ResNet18 and ResNet-34, respectively, with\nWinograd F(6, 3)."
                },
                "authors": [
                    {
                        "name": "Shuokai Pan"
                    },
                    {
                        "name": "Gerti Tuzi"
                    },
                    {
                        "name": "Sudarshan Sreeram"
                    },
                    {
                        "name": "Dibakar Gope"
                    }
                ],
                "author_detail": {
                    "name": "Dibakar Gope"
                },
                "author": "Dibakar Gope",
                "arxiv_comment": "Accepted by CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19867v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19867v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01999v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01999v3",
                "updated": "2025-04-01T03:31:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    3,
                    31,
                    38,
                    1,
                    91,
                    0
                ],
                "published": "2024-10-02T20:04:02Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    20,
                    4,
                    2,
                    2,
                    276,
                    0
                ],
                "title": "CodeMMLU: A Multi-Task Benchmark for Assessing Code Understanding\n  Capabilities of CodeLLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeMMLU: A Multi-Task Benchmark for Assessing Code Understanding\n  Capabilities of CodeLLMs"
                },
                "summary": "Recent advances in Code Large Language Models (CodeLLMs) have primarily\nfocused on open-ended code generation, often overlooking the crucial aspect of\ncode understanding and reasoning. To bridge this gap, we introduce CodeMMLU, a\ncomprehensive multiple-choice benchmark designed to evaluate the depth of\nsoftware and code comprehension in LLMs. CodeMMLU includes nearly 20,000\nquestions spanning diverse domains, including code analysis, defect detection,\nand software engineering principles across multiple programming languages.\nUnlike traditional benchmarks that emphasize code generation, CodeMMLU assesses\na model's ability to reason about programs across a wide-range of tasks such as\ncode repair, execution reasoning, and fill-in-the-blank challenges. Our\nextensive evaluation reveals that even state-of-the-art models struggle with\nCodeMMLU, highlighting significant gaps in comprehension beyond generation. By\nemphasizing the essential connection between code understanding and effective\nAI-assisted development, CodeMMLU provides a critical resource for advancing\nmore reliable and capable coding assistants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Code Large Language Models (CodeLLMs) have primarily\nfocused on open-ended code generation, often overlooking the crucial aspect of\ncode understanding and reasoning. To bridge this gap, we introduce CodeMMLU, a\ncomprehensive multiple-choice benchmark designed to evaluate the depth of\nsoftware and code comprehension in LLMs. CodeMMLU includes nearly 20,000\nquestions spanning diverse domains, including code analysis, defect detection,\nand software engineering principles across multiple programming languages.\nUnlike traditional benchmarks that emphasize code generation, CodeMMLU assesses\na model's ability to reason about programs across a wide-range of tasks such as\ncode repair, execution reasoning, and fill-in-the-blank challenges. Our\nextensive evaluation reveals that even state-of-the-art models struggle with\nCodeMMLU, highlighting significant gaps in comprehension beyond generation. By\nemphasizing the essential connection between code understanding and effective\nAI-assisted development, CodeMMLU provides a critical resource for advancing\nmore reliable and capable coding assistants."
                },
                "authors": [
                    {
                        "name": "Dung Nguyen Manh"
                    },
                    {
                        "name": "Thang Phan Chau"
                    },
                    {
                        "name": "Nam Le Hai"
                    },
                    {
                        "name": "Thong T. Doan"
                    },
                    {
                        "name": "Nam V. Nguyen"
                    },
                    {
                        "name": "Quang Pham"
                    },
                    {
                        "name": "Nghi D. Q. Bui"
                    }
                ],
                "author_detail": {
                    "name": "Nghi D. Q. Bui"
                },
                "author": "Nghi D. Q. Bui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01999v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01999v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08185v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08185v2",
                "updated": "2025-04-01T03:19:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    3,
                    19,
                    22,
                    1,
                    91,
                    0
                ],
                "published": "2024-12-11T08:24:15Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    8,
                    24,
                    15,
                    2,
                    346,
                    0
                ],
                "title": "Exploring Multidimensional Checkworthiness: Designing AI-assisted Claim\n  Prioritization for Human Fact-checkers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Multidimensional Checkworthiness: Designing AI-assisted Claim\n  Prioritization for Human Fact-checkers"
                },
                "summary": "Given the massive volume of potentially false claims circulating online,\nclaim prioritization is essential in allocating limited human resources\navailable for fact-checking. In this study, we perceive claim prioritization as\nan information retrieval (IR) task: just as multidimensional IR relevance, with\nmany factors influencing which search results a user deems relevant,\ncheckworthiness is also multi-faceted, subjective, and even personal, with many\nfactors influencing how fact-checkers triage and select which claims to check.\nOur study investigated both the multidimensional nature of checkworthiness and\neffective tool support to assist fact-checkers in claim prioritization.\nMethodologically, we pursued Research through Design combined with mixed-method\nevaluation.\n  Specifically, we developed an AI-assisted claim prioritization prototype as a\nprobe to explore how fact-checkers use multidimensional checkworthy factors to\nprioritize claims, simultaneously probing fact-checker needs and exploring the\ndesign space to meet those needs. With 16 professional fact-checkers\nparticipating in our study, we uncovered a hierarchical prioritization strategy\nfact-checkers implicitly use, revealing an underexplored aspect of their\nworkflow, with actionable design recommendations for improving claim triage\nacross multidimensional checkworthiness and tailoring this process with LLM\nintegration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given the massive volume of potentially false claims circulating online,\nclaim prioritization is essential in allocating limited human resources\navailable for fact-checking. In this study, we perceive claim prioritization as\nan information retrieval (IR) task: just as multidimensional IR relevance, with\nmany factors influencing which search results a user deems relevant,\ncheckworthiness is also multi-faceted, subjective, and even personal, with many\nfactors influencing how fact-checkers triage and select which claims to check.\nOur study investigated both the multidimensional nature of checkworthiness and\neffective tool support to assist fact-checkers in claim prioritization.\nMethodologically, we pursued Research through Design combined with mixed-method\nevaluation.\n  Specifically, we developed an AI-assisted claim prioritization prototype as a\nprobe to explore how fact-checkers use multidimensional checkworthy factors to\nprioritize claims, simultaneously probing fact-checker needs and exploring the\ndesign space to meet those needs. With 16 professional fact-checkers\nparticipating in our study, we uncovered a hierarchical prioritization strategy\nfact-checkers implicitly use, revealing an underexplored aspect of their\nworkflow, with actionable design recommendations for improving claim triage\nacross multidimensional checkworthiness and tailoring this process with LLM\nintegration."
                },
                "authors": [
                    {
                        "name": "Houjiang Liu"
                    },
                    {
                        "name": "Jacek Gwizdka"
                    },
                    {
                        "name": "Matthew Lease"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Lease"
                },
                "author": "Matthew Lease",
                "arxiv_comment": "Accepted at CSCW 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08185v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08185v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24270v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24270v2",
                "updated": "2025-04-01T03:16:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    3,
                    16,
                    38,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-31T16:16:10Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    16,
                    16,
                    10,
                    0,
                    90,
                    0
                ],
                "title": "Visual Acoustic Fields",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Acoustic Fields"
                },
                "summary": "Objects produce different sounds when hit, and humans can intuitively infer\nhow an object might sound based on its appearance and material properties.\nInspired by this intuition, we propose Visual Acoustic Fields, a framework that\nbridges hitting sounds and visual signals within a 3D space using 3D Gaussian\nSplatting (3DGS). Our approach features two key modules: sound generation and\nsound localization. The sound generation module leverages a conditional\ndiffusion model, which takes multiscale features rendered from a\nfeature-augmented 3DGS to generate realistic hitting sounds. Meanwhile, the\nsound localization module enables querying the 3D scene, represented by the\nfeature-augmented 3DGS, to localize hitting positions based on the sound\nsources. To support this framework, we introduce a novel pipeline for\ncollecting scene-level visual-sound sample pairs, achieving alignment between\ncaptured images, impact locations, and corresponding sounds. To the best of our\nknowledge, this is the first dataset to connect visual and acoustic signals in\na 3D context. Extensive experiments on our dataset demonstrate the\neffectiveness of Visual Acoustic Fields in generating plausible impact sounds\nand accurately localizing impact sources. Our project page is at\nhttps://yuelei0428.github.io/projects/Visual-Acoustic-Fields/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Objects produce different sounds when hit, and humans can intuitively infer\nhow an object might sound based on its appearance and material properties.\nInspired by this intuition, we propose Visual Acoustic Fields, a framework that\nbridges hitting sounds and visual signals within a 3D space using 3D Gaussian\nSplatting (3DGS). Our approach features two key modules: sound generation and\nsound localization. The sound generation module leverages a conditional\ndiffusion model, which takes multiscale features rendered from a\nfeature-augmented 3DGS to generate realistic hitting sounds. Meanwhile, the\nsound localization module enables querying the 3D scene, represented by the\nfeature-augmented 3DGS, to localize hitting positions based on the sound\nsources. To support this framework, we introduce a novel pipeline for\ncollecting scene-level visual-sound sample pairs, achieving alignment between\ncaptured images, impact locations, and corresponding sounds. To the best of our\nknowledge, this is the first dataset to connect visual and acoustic signals in\na 3D context. Extensive experiments on our dataset demonstrate the\neffectiveness of Visual Acoustic Fields in generating plausible impact sounds\nand accurately localizing impact sources. Our project page is at\nhttps://yuelei0428.github.io/projects/Visual-Acoustic-Fields/."
                },
                "authors": [
                    {
                        "name": "Yuelei Li"
                    },
                    {
                        "name": "Hyunjin Kim"
                    },
                    {
                        "name": "Fangneng Zhan"
                    },
                    {
                        "name": "Ri-Zhao Qiu"
                    },
                    {
                        "name": "Mazeyu Ji"
                    },
                    {
                        "name": "Xiaojun Shan"
                    },
                    {
                        "name": "Xueyan Zou"
                    },
                    {
                        "name": "Paul Liang"
                    },
                    {
                        "name": "Hanspeter Pfister"
                    },
                    {
                        "name": "Xiaolong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaolong Wang"
                },
                "author": "Xiaolong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24270v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24270v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07272v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07272v3",
                "updated": "2025-04-01T03:14:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    3,
                    14,
                    15,
                    1,
                    91,
                    0
                ],
                "published": "2025-02-11T05:39:49Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    5,
                    39,
                    49,
                    1,
                    42,
                    0
                ],
                "title": "GENERator: A Long-Context Generative Genomic Foundation Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GENERator: A Long-Context Generative Genomic Foundation Model"
                },
                "summary": "Advancements in DNA sequencing technologies have significantly improved our\nability to decode genomic sequences. However, the prediction and interpretation\nof these sequences remain challenging due to the intricate nature of genetic\nmaterial. Large language models (LLMs) have introduced new opportunities for\nbiological sequence analysis. Recent developments in genomic language models\nhave underscored the potential of LLMs in deciphering DNA sequences.\nNonetheless, existing models often face limitations in robustness and\napplication scope, primarily due to constraints in model structure and training\ndata scale. To address these limitations, we present GENERator, a generative\ngenomic foundation model featuring a context length of 98k base pairs (bp) and\n1.2B parameters. Trained on an expansive dataset comprising 386B bp of\neukaryotic DNA, the GENERator demonstrates state-of-the-art performance across\nboth established and newly proposed benchmarks. The model adheres to the\ncentral dogma of molecular biology, accurately generating protein-coding\nsequences that translate into proteins structurally analogous to known\nfamilies. It also shows significant promise in sequence optimization,\nparticularly through the prompt-responsive generation of enhancer sequences\nwith specific activity profiles. These capabilities position the GENERator as a\npivotal tool for genomic research and biotechnological advancement, enhancing\nour ability to interpret and predict complex biological systems and enabling\nprecise genomic interventions. Implementation details and supplementary\nresources are available at https://github.com/GenerTeam/GENERator.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in DNA sequencing technologies have significantly improved our\nability to decode genomic sequences. However, the prediction and interpretation\nof these sequences remain challenging due to the intricate nature of genetic\nmaterial. Large language models (LLMs) have introduced new opportunities for\nbiological sequence analysis. Recent developments in genomic language models\nhave underscored the potential of LLMs in deciphering DNA sequences.\nNonetheless, existing models often face limitations in robustness and\napplication scope, primarily due to constraints in model structure and training\ndata scale. To address these limitations, we present GENERator, a generative\ngenomic foundation model featuring a context length of 98k base pairs (bp) and\n1.2B parameters. Trained on an expansive dataset comprising 386B bp of\neukaryotic DNA, the GENERator demonstrates state-of-the-art performance across\nboth established and newly proposed benchmarks. The model adheres to the\ncentral dogma of molecular biology, accurately generating protein-coding\nsequences that translate into proteins structurally analogous to known\nfamilies. It also shows significant promise in sequence optimization,\nparticularly through the prompt-responsive generation of enhancer sequences\nwith specific activity profiles. These capabilities position the GENERator as a\npivotal tool for genomic research and biotechnological advancement, enhancing\nour ability to interpret and predict complex biological systems and enabling\nprecise genomic interventions. Implementation details and supplementary\nresources are available at https://github.com/GenerTeam/GENERator."
                },
                "authors": [
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Qiuyi Li"
                    },
                    {
                        "name": "Mingyang Li"
                    },
                    {
                        "name": "Kun Fu"
                    },
                    {
                        "name": "Fuli Feng"
                    },
                    {
                        "name": "Jieping Ye"
                    },
                    {
                        "name": "Hui Xiong"
                    },
                    {
                        "name": "Zheng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Wang"
                },
                "author": "Zheng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07272v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07272v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23796v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23796v2",
                "updated": "2025-04-01T02:33:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    2,
                    33,
                    18,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-31T07:19:09Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    7,
                    19,
                    9,
                    0,
                    90,
                    0
                ],
                "title": "On-device Sora: Enabling Training-Free Diffusion-based Text-to-Video\n  Generation for Mobile Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On-device Sora: Enabling Training-Free Diffusion-based Text-to-Video\n  Generation for Mobile Devices"
                },
                "summary": "We present On-device Sora, the first model training-free solution for\ndiffusion-based on-device text-to-video generation that operates efficiently on\nsmartphone-grade devices. To address the challenges of diffusion-based\ntext-to-video generation on computation- and memory-limited mobile devices, the\nproposed On-device Sora applies three novel techniques to pre-trained video\ngenerative models. First, Linear Proportional Leap (LPL) reduces the excessive\ndenoising steps required in video diffusion through an efficient leap-based\napproach. Second, Temporal Dimension Token Merging (TDTM) minimizes intensive\ntoken-processing computation in attention layers by merging consecutive tokens\nalong the temporal dimension. Third, Concurrent Inference with Dynamic Loading\n(CI-DL) dynamically partitions large models into smaller blocks and loads them\ninto memory for concurrent model inference, effectively addressing the\nchallenges of limited device memory. We implement On-device Sora on the iPhone\n15 Pro, and the experimental evaluations show that it is capable of generating\nhigh-quality videos on the device, comparable to those produced by high-end\nGPUs. These results show that On-device Sora enables efficient and high-quality\nvideo generation on resource-constrained mobile devices. We envision the\nproposed On-device Sora as a significant first step toward democratizing\nstate-of-the-art generative technologies, enabling video generation on\ncommodity mobile and embedded devices without resource-intensive re-training\nfor model optimization (compression). The code implementation is available at a\nGitHub repository(https://github.com/eai-lab/On-device-Sora).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present On-device Sora, the first model training-free solution for\ndiffusion-based on-device text-to-video generation that operates efficiently on\nsmartphone-grade devices. To address the challenges of diffusion-based\ntext-to-video generation on computation- and memory-limited mobile devices, the\nproposed On-device Sora applies three novel techniques to pre-trained video\ngenerative models. First, Linear Proportional Leap (LPL) reduces the excessive\ndenoising steps required in video diffusion through an efficient leap-based\napproach. Second, Temporal Dimension Token Merging (TDTM) minimizes intensive\ntoken-processing computation in attention layers by merging consecutive tokens\nalong the temporal dimension. Third, Concurrent Inference with Dynamic Loading\n(CI-DL) dynamically partitions large models into smaller blocks and loads them\ninto memory for concurrent model inference, effectively addressing the\nchallenges of limited device memory. We implement On-device Sora on the iPhone\n15 Pro, and the experimental evaluations show that it is capable of generating\nhigh-quality videos on the device, comparable to those produced by high-end\nGPUs. These results show that On-device Sora enables efficient and high-quality\nvideo generation on resource-constrained mobile devices. We envision the\nproposed On-device Sora as a significant first step toward democratizing\nstate-of-the-art generative technologies, enabling video generation on\ncommodity mobile and embedded devices without resource-intensive re-training\nfor model optimization (compression). The code implementation is available at a\nGitHub repository(https://github.com/eai-lab/On-device-Sora)."
                },
                "authors": [
                    {
                        "name": "Bosung Kim"
                    },
                    {
                        "name": "Kyuhwan Lee"
                    },
                    {
                        "name": "Isu Jeong"
                    },
                    {
                        "name": "Jungmin Cheon"
                    },
                    {
                        "name": "Yeojin Lee"
                    },
                    {
                        "name": "Seulki Lee"
                    }
                ],
                "author_detail": {
                    "name": "Seulki Lee"
                },
                "author": "Seulki Lee",
                "arxiv_comment": "Replicated Submission. arXiv:2502.04363 submitted as second version\n  of the paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23796v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23796v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23427v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23427v2",
                "updated": "2025-04-01T02:24:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    2,
                    24,
                    42,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-30T13:00:52Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    13,
                    0,
                    52,
                    6,
                    89,
                    0
                ],
                "title": "CoRanking: Collaborative Ranking with Small and Large Ranking Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoRanking: Collaborative Ranking with Small and Large Ranking Agents"
                },
                "summary": "Large Language Models (LLMs) have demonstrated superior listwise ranking\nperformance. However, their superior performance often relies on large-scale\nparameters (\\eg, GPT-4) and a repetitive sliding window process, which\nintroduces significant efficiency challenges. In this paper, we propose\n\\textbf{CoRanking}, a novel collaborative ranking framework that combines small\nand large ranking models for efficient and effective ranking. CoRanking first\nemploys a small-size reranker to pre-rank all the candidate passages, bringing\nrelevant ones to the top part of the list (\\eg, top-20). Then, the LLM listwise\nreranker is applied to only rerank these top-ranked passages instead of the\nwhole list, substantially enhancing overall ranking efficiency. Although more\nefficient, previous studies have revealed that the LLM listwise reranker have\nsignificant positional biases on the order of input passages. Directly feed the\ntop-ranked passages from small reranker may result in the sub-optimal\nperformance of LLM listwise reranker. To alleviate this problem, we introduce a\npassage order adjuster trained via reinforcement learning, which reorders the\ntop passages from the small reranker to align with the LLM's preferences of\npassage order. Extensive experiments on three IR benchmarks demonstrate that\nCoRanking significantly improves efficiency (reducing ranking latency by about\n70\\%) while achieving even better effectiveness compared to using only the LLM\nlistwise reranker.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated superior listwise ranking\nperformance. However, their superior performance often relies on large-scale\nparameters (\\eg, GPT-4) and a repetitive sliding window process, which\nintroduces significant efficiency challenges. In this paper, we propose\n\\textbf{CoRanking}, a novel collaborative ranking framework that combines small\nand large ranking models for efficient and effective ranking. CoRanking first\nemploys a small-size reranker to pre-rank all the candidate passages, bringing\nrelevant ones to the top part of the list (\\eg, top-20). Then, the LLM listwise\nreranker is applied to only rerank these top-ranked passages instead of the\nwhole list, substantially enhancing overall ranking efficiency. Although more\nefficient, previous studies have revealed that the LLM listwise reranker have\nsignificant positional biases on the order of input passages. Directly feed the\ntop-ranked passages from small reranker may result in the sub-optimal\nperformance of LLM listwise reranker. To alleviate this problem, we introduce a\npassage order adjuster trained via reinforcement learning, which reorders the\ntop passages from the small reranker to align with the LLM's preferences of\npassage order. Extensive experiments on three IR benchmarks demonstrate that\nCoRanking significantly improves efficiency (reducing ranking latency by about\n70\\%) while achieving even better effectiveness compared to using only the LLM\nlistwise reranker."
                },
                "authors": [
                    {
                        "name": "Wenhan Liu"
                    },
                    {
                        "name": "Xinyu Ma"
                    },
                    {
                        "name": "Yutao Zhu"
                    },
                    {
                        "name": "Lixin Su"
                    },
                    {
                        "name": "Shuaiqiang Wang"
                    },
                    {
                        "name": "Dawei Yin"
                    },
                    {
                        "name": "Zhicheng Dou"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Dou"
                },
                "author": "Zhicheng Dou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23427v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23427v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10459v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10459v3",
                "updated": "2025-04-01T02:23:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    2,
                    23,
                    57,
                    1,
                    91,
                    0
                ],
                "published": "2024-06-15T01:02:48Z",
                "published_parsed": [
                    2024,
                    6,
                    15,
                    1,
                    2,
                    48,
                    5,
                    167,
                    0
                ],
                "title": "CancerLLM: A Large Language Model in Cancer Domain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CancerLLM: A Large Language Model in Cancer Domain"
                },
                "summary": "Medical Large Language Models (LLMs) have demonstrated impressive performance\non a wide variety of medical NLP tasks; however, there still lacks a LLM\nspecifically designed for phenotyping identification and diagnosis in cancer\ndomain. Moreover, these LLMs typically have several billions of parameters,\nmaking them computationally expensive for healthcare systems. Thus, in this\nstudy, we propose CancerLLM, a model with 7 billion parameters and a\nMistral-style architecture, pre-trained on nearly 2.7M clinical notes and over\n515K pathology reports covering 17 cancer types, followed by fine-tuning on two\ncancer-relevant tasks, including cancer phenotypes extraction and cancer\ndiagnosis generation. Our evaluation demonstrated that the CancerLLM achieves\nstate-of-the-art results with F1 score of 91.78% on phenotyping extraction and\n86.81% on disganois generation. It outperformed existing LLMs, with an average\nF1 score improvement of 9.23%. Additionally, the CancerLLM demonstrated its\nefficiency on time and GPU usage, and robustness comparing with other LLMs. We\ndemonstrated that CancerLLM can potentially provide an effective and robust\nsolution to advance clinical research and practice in cancer domain",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical Large Language Models (LLMs) have demonstrated impressive performance\non a wide variety of medical NLP tasks; however, there still lacks a LLM\nspecifically designed for phenotyping identification and diagnosis in cancer\ndomain. Moreover, these LLMs typically have several billions of parameters,\nmaking them computationally expensive for healthcare systems. Thus, in this\nstudy, we propose CancerLLM, a model with 7 billion parameters and a\nMistral-style architecture, pre-trained on nearly 2.7M clinical notes and over\n515K pathology reports covering 17 cancer types, followed by fine-tuning on two\ncancer-relevant tasks, including cancer phenotypes extraction and cancer\ndiagnosis generation. Our evaluation demonstrated that the CancerLLM achieves\nstate-of-the-art results with F1 score of 91.78% on phenotyping extraction and\n86.81% on disganois generation. It outperformed existing LLMs, with an average\nF1 score improvement of 9.23%. Additionally, the CancerLLM demonstrated its\nefficiency on time and GPU usage, and robustness comparing with other LLMs. We\ndemonstrated that CancerLLM can potentially provide an effective and robust\nsolution to advance clinical research and practice in cancer domain"
                },
                "authors": [
                    {
                        "name": "Mingchen Li"
                    },
                    {
                        "name": "Jiatan Huang"
                    },
                    {
                        "name": "Jeremy Yeung"
                    },
                    {
                        "name": "Anne Blaes"
                    },
                    {
                        "name": "Steven Johnson"
                    },
                    {
                        "name": "Hongfang Liu"
                    },
                    {
                        "name": "Hua Xu"
                    },
                    {
                        "name": "Rui Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Zhang"
                },
                "author": "Rui Zhang",
                "arxiv_comment": "new version, add the RAG version of cancerLLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10459v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10459v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04667v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04667v4",
                "updated": "2025-04-01T02:20:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    2,
                    20,
                    6,
                    1,
                    91,
                    0
                ],
                "published": "2024-08-06T16:43:35Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    16,
                    43,
                    35,
                    1,
                    219,
                    0
                ],
                "title": "Non-Determinism of \"Deterministic\" LLM Settings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-Determinism of \"Deterministic\" LLM Settings"
                },
                "summary": "LLM (large language model) practitioners commonly notice that outputs can\nvary for the same inputs under settings expected to be deterministic. Yet the\nquestions of how pervasive this is, and with what impact on results, have not\nto our knowledge been systematically investigated. We investigate\nnon-determinism in five LLMs configured to be deterministic when applied to\neight common tasks in across 10 runs, in both zero-shot and few-shot settings.\nWe see accuracy variations up to 15% across naturally occurring runs with a gap\nof best possible performance to worst possible performance up to 70%. In fact,\nnone of the LLMs consistently delivers repeatable accuracy across all tasks,\nmuch less identical output strings. Sharing preliminary results with insiders\nhas revealed that non-determinism perhaps essential to the efficient use of\ncompute resources via co-mingled data in input buffers so this issue is not\ngoing away anytime soon. To better quantify our observations, we introduce\nmetrics focused on quantifying determinism, TARr@N for the total agreement rate\nat N runs over raw output, and TARa@N for total agreement rate of parsed-out\nanswers. Our code and data are publicly available at\nhttp://github.com/REDACTED.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM (large language model) practitioners commonly notice that outputs can\nvary for the same inputs under settings expected to be deterministic. Yet the\nquestions of how pervasive this is, and with what impact on results, have not\nto our knowledge been systematically investigated. We investigate\nnon-determinism in five LLMs configured to be deterministic when applied to\neight common tasks in across 10 runs, in both zero-shot and few-shot settings.\nWe see accuracy variations up to 15% across naturally occurring runs with a gap\nof best possible performance to worst possible performance up to 70%. In fact,\nnone of the LLMs consistently delivers repeatable accuracy across all tasks,\nmuch less identical output strings. Sharing preliminary results with insiders\nhas revealed that non-determinism perhaps essential to the efficient use of\ncompute resources via co-mingled data in input buffers so this issue is not\ngoing away anytime soon. To better quantify our observations, we introduce\nmetrics focused on quantifying determinism, TARr@N for the total agreement rate\nat N runs over raw output, and TARa@N for total agreement rate of parsed-out\nanswers. Our code and data are publicly available at\nhttp://github.com/REDACTED."
                },
                "authors": [
                    {
                        "name": "Berk Atil"
                    },
                    {
                        "name": "Sarp Aykent"
                    },
                    {
                        "name": "Alexa Chittams"
                    },
                    {
                        "name": "Lisheng Fu"
                    },
                    {
                        "name": "Rebecca J. Passonneau"
                    },
                    {
                        "name": "Evan Radcliffe"
                    },
                    {
                        "name": "Guru Rajan Rajagopal"
                    },
                    {
                        "name": "Adam Sloan"
                    },
                    {
                        "name": "Tomasz Tudrej"
                    },
                    {
                        "name": "Ferhan Ture"
                    },
                    {
                        "name": "Zhe Wu"
                    },
                    {
                        "name": "Lixinyu Xu"
                    },
                    {
                        "name": "Breck Baldwin"
                    }
                ],
                "author_detail": {
                    "name": "Breck Baldwin"
                },
                "author": "Breck Baldwin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04667v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04667v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21788v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21788v3",
                "updated": "2025-04-01T02:12:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    2,
                    12,
                    44,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-12T12:53:43Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    12,
                    53,
                    43,
                    2,
                    71,
                    0
                ],
                "title": "PharMolixFM: All-Atom Foundation Models for Molecular Modeling and\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PharMolixFM: All-Atom Foundation Models for Molecular Modeling and\n  Generation"
                },
                "summary": "Structural biology relies on accurate three-dimensional biomolecular\nstructures to advance our understanding of biological functions, disease\nmechanisms, and therapeutics. While recent advances in deep learning have\nenabled the development of all-atom foundation models for molecular modeling\nand generation, existing approaches face challenges in generalization due to\nthe multi-modal nature of atomic data and the lack of comprehensive analysis of\ntraining and sampling strategies. To address these limitations, we propose\nPharMolixFM, a unified framework for constructing all-atom foundation models\nbased on multi-modal generative techniques. Our framework includes three\nvariants using state-of-the-art multi-modal generative models. By formulating\nmolecular tasks as a generalized denoising process with task-specific priors,\nPharMolixFM achieves robust performance across various structural biology\napplications. Experimental results demonstrate that PharMolixFM-Diff achieves\ncompetitive prediction accuracy in protein-small-molecule docking (83.9% vs.\n90.2% RMSD < 2{\\AA}, given pocket) with significantly improved inference speed.\nMoreover, we explore the empirical inference scaling law by introducing more\nsampling repeats or steps. Our code and model are available at\nhttps://github.com/PharMolix/OpenBioMed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structural biology relies on accurate three-dimensional biomolecular\nstructures to advance our understanding of biological functions, disease\nmechanisms, and therapeutics. While recent advances in deep learning have\nenabled the development of all-atom foundation models for molecular modeling\nand generation, existing approaches face challenges in generalization due to\nthe multi-modal nature of atomic data and the lack of comprehensive analysis of\ntraining and sampling strategies. To address these limitations, we propose\nPharMolixFM, a unified framework for constructing all-atom foundation models\nbased on multi-modal generative techniques. Our framework includes three\nvariants using state-of-the-art multi-modal generative models. By formulating\nmolecular tasks as a generalized denoising process with task-specific priors,\nPharMolixFM achieves robust performance across various structural biology\napplications. Experimental results demonstrate that PharMolixFM-Diff achieves\ncompetitive prediction accuracy in protein-small-molecule docking (83.9% vs.\n90.2% RMSD < 2{\\AA}, given pocket) with significantly improved inference speed.\nMoreover, we explore the empirical inference scaling law by introducing more\nsampling repeats or steps. Our code and model are available at\nhttps://github.com/PharMolix/OpenBioMed."
                },
                "authors": [
                    {
                        "name": "Yizhen Luo"
                    },
                    {
                        "name": "Jiashuo Wang"
                    },
                    {
                        "name": "Siqi Fan"
                    },
                    {
                        "name": "Zaiqing Nie"
                    }
                ],
                "author_detail": {
                    "name": "Zaiqing Nie"
                },
                "author": "Zaiqing Nie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21788v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21788v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.BM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16724v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16724v3",
                "updated": "2025-04-01T01:33:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    1,
                    33,
                    30,
                    1,
                    91,
                    0
                ],
                "published": "2024-11-23T03:40:05Z",
                "published_parsed": [
                    2024,
                    11,
                    23,
                    3,
                    40,
                    5,
                    5,
                    328,
                    0
                ],
                "title": "Devils in Middle Layers of Large Vision-Language Models: Interpreting,\n  Detecting and Mitigating Object Hallucinations via Attention Lens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Devils in Middle Layers of Large Vision-Language Models: Interpreting,\n  Detecting and Mitigating Object Hallucinations via Attention Lens"
                },
                "summary": "Hallucinations in Large Vision-Language Models (LVLMs) significantly\nundermine their reliability, motivating researchers to explore the causes of\nhallucination. However, most studies primarily focus on the language aspect\nrather than the visual. In this paper, we address how LVLMs process visual\ninformation and whether this process causes hallucination. Firstly, we use the\nattention lens to identify the stages at which LVLMs handle visual data,\ndiscovering that the middle layers are crucial. Moreover, we find that these\nlayers can be further divided into two stages: ''visual information\nenrichment'' and ''semantic refinement'' which respectively propagate visual\ndata to object tokens and interpret it through text. By analyzing attention\npatterns during the visual information enrichment stage, we find that real\ntokens consistently receive higher attention weights than hallucinated ones,\nserving as a strong indicator of hallucination. Further examination of\nmulti-head attention maps reveals that hallucination tokens often result from\nheads interacting with inconsistent objects. Based on these insights, we\npropose a simple inference-time method that adjusts visual attention by\nintegrating information across various heads. Extensive experiments demonstrate\nthat this approach effectively mitigates hallucinations in mainstream LVLMs\nwithout additional training costs. Code is available at\nhttps://github.com/ZhangqiJiang07/middle_layers_indicating_hallucinations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucinations in Large Vision-Language Models (LVLMs) significantly\nundermine their reliability, motivating researchers to explore the causes of\nhallucination. However, most studies primarily focus on the language aspect\nrather than the visual. In this paper, we address how LVLMs process visual\ninformation and whether this process causes hallucination. Firstly, we use the\nattention lens to identify the stages at which LVLMs handle visual data,\ndiscovering that the middle layers are crucial. Moreover, we find that these\nlayers can be further divided into two stages: ''visual information\nenrichment'' and ''semantic refinement'' which respectively propagate visual\ndata to object tokens and interpret it through text. By analyzing attention\npatterns during the visual information enrichment stage, we find that real\ntokens consistently receive higher attention weights than hallucinated ones,\nserving as a strong indicator of hallucination. Further examination of\nmulti-head attention maps reveals that hallucination tokens often result from\nheads interacting with inconsistent objects. Based on these insights, we\npropose a simple inference-time method that adjusts visual attention by\nintegrating information across various heads. Extensive experiments demonstrate\nthat this approach effectively mitigates hallucinations in mainstream LVLMs\nwithout additional training costs. Code is available at\nhttps://github.com/ZhangqiJiang07/middle_layers_indicating_hallucinations."
                },
                "authors": [
                    {
                        "name": "Zhangqi Jiang"
                    },
                    {
                        "name": "Junkai Chen"
                    },
                    {
                        "name": "Beier Zhu"
                    },
                    {
                        "name": "Tingjin Luo"
                    },
                    {
                        "name": "Yankun Shen"
                    },
                    {
                        "name": "Xu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xu Yang"
                },
                "author": "Xu Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16724v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16724v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01141v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01141v2",
                "updated": "2025-04-01T00:41:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    0,
                    41,
                    36,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-03T03:48:20Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    3,
                    48,
                    20,
                    0,
                    62,
                    0
                ],
                "title": "How Well do LLMs Compress Their Own Chain-of-Thought? A Token Complexity\n  Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Well do LLMs Compress Their Own Chain-of-Thought? A Token Complexity\n  Approach"
                },
                "summary": "Chain-of-thought prompting has emerged as a powerful technique for enabling\nlarge language models (LLMs) to solve complex reasoning tasks. However, these\nreasoning chains can be verbose, raising concerns about efficiency. In\nresponse, recent works have sought to decrease response lengths through simple\nprompting strategies (e.g. 'be concise'). In this work, we conduct the first\nsystematic study of the relationship between reasoning length and model\nperformance across a diverse range of compression instructions (e.g. 'use 10\nwords or less' or 'remove all punctuation'). In doing so, we discover a\nuniversal tradeoff between reasoning length and accuracy that persists across\neven very distinct reasoning chains. We demonstrate that this tradeoff emerges\nfrom a sharp threshold behavior at the question level: each task has an\nintrinsic 'token complexity' - a minimal number of tokens required for\nsuccessful problem-solving. We show how token complexity enables us to compute\ninformation-theoretic limits on the accuracy-compression tradeoff, and find\nthat prompt-based compression strategies operate far from these theoretical\nlimits. This suggests there may be significant room for improvement and our\nframework provides a benchmark to help researchers evaluate progress in\nreasoning efficiency. Our work also highlights the importance of adaptive\ncompression -- giving shorter responses for easier questions -- and we show\nthat token complexity is a useful tool for measuring this capability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-thought prompting has emerged as a powerful technique for enabling\nlarge language models (LLMs) to solve complex reasoning tasks. However, these\nreasoning chains can be verbose, raising concerns about efficiency. In\nresponse, recent works have sought to decrease response lengths through simple\nprompting strategies (e.g. 'be concise'). In this work, we conduct the first\nsystematic study of the relationship between reasoning length and model\nperformance across a diverse range of compression instructions (e.g. 'use 10\nwords or less' or 'remove all punctuation'). In doing so, we discover a\nuniversal tradeoff between reasoning length and accuracy that persists across\neven very distinct reasoning chains. We demonstrate that this tradeoff emerges\nfrom a sharp threshold behavior at the question level: each task has an\nintrinsic 'token complexity' - a minimal number of tokens required for\nsuccessful problem-solving. We show how token complexity enables us to compute\ninformation-theoretic limits on the accuracy-compression tradeoff, and find\nthat prompt-based compression strategies operate far from these theoretical\nlimits. This suggests there may be significant room for improvement and our\nframework provides a benchmark to help researchers evaluate progress in\nreasoning efficiency. Our work also highlights the importance of adaptive\ncompression -- giving shorter responses for easier questions -- and we show\nthat token complexity is a useful tool for measuring this capability."
                },
                "authors": [
                    {
                        "name": "Ayeong Lee"
                    },
                    {
                        "name": "Ethan Che"
                    },
                    {
                        "name": "Tianyi Peng"
                    }
                ],
                "author_detail": {
                    "name": "Tianyi Peng"
                },
                "author": "Tianyi Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01141v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01141v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.10559v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.10559v2",
                "updated": "2025-04-01T00:40:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    0,
                    40,
                    24,
                    1,
                    91,
                    0
                ],
                "published": "2023-10-16T16:32:35Z",
                "published_parsed": [
                    2023,
                    10,
                    16,
                    16,
                    32,
                    35,
                    0,
                    289,
                    0
                ],
                "title": "Causal Dynamic Variational Autoencoder for Counterfactual Regression in\n  Longitudinal Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Dynamic Variational Autoencoder for Counterfactual Regression in\n  Longitudinal Data"
                },
                "summary": "Estimating treatment effects over time is relevant in many real-world\napplications, such as precision medicine, epidemiology, economy, and marketing.\nMany state-of-the-art methods either assume the observations of all confounders\nor seek to infer the unobserved ones. We take a different perspective by\nassuming unobserved risk factors, i.e., adjustment variables that affect only\nthe sequence of outcomes. Under unconfoundedness, we target the Individual\nTreatment Effect (ITE) estimation with unobserved heterogeneity in the\ntreatment response due to missing risk factors. We address the challenges posed\nby time-varying effects and unobserved adjustment variables. Led by theoretical\nresults over the validity of the learned adjustment variables and\ngeneralization bounds over the treatment effect, we devise Causal DVAE (CDVAE).\nThis model combines a Dynamic Variational Autoencoder (DVAE) framework with a\nweighting strategy using propensity scores to estimate counterfactual\nresponses. The CDVAE model allows for accurate estimation of ITE and captures\nthe underlying heterogeneity in longitudinal data. Evaluations of our model\nshow superior performance over state-of-the-art models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating treatment effects over time is relevant in many real-world\napplications, such as precision medicine, epidemiology, economy, and marketing.\nMany state-of-the-art methods either assume the observations of all confounders\nor seek to infer the unobserved ones. We take a different perspective by\nassuming unobserved risk factors, i.e., adjustment variables that affect only\nthe sequence of outcomes. Under unconfoundedness, we target the Individual\nTreatment Effect (ITE) estimation with unobserved heterogeneity in the\ntreatment response due to missing risk factors. We address the challenges posed\nby time-varying effects and unobserved adjustment variables. Led by theoretical\nresults over the validity of the learned adjustment variables and\ngeneralization bounds over the treatment effect, we devise Causal DVAE (CDVAE).\nThis model combines a Dynamic Variational Autoencoder (DVAE) framework with a\nweighting strategy using propensity scores to estimate counterfactual\nresponses. The CDVAE model allows for accurate estimation of ITE and captures\nthe underlying heterogeneity in longitudinal data. Evaluations of our model\nshow superior performance over state-of-the-art models."
                },
                "authors": [
                    {
                        "name": "Mouad El Bouchattaoui"
                    },
                    {
                        "name": "Myriam Tami"
                    },
                    {
                        "name": "Benoit Lepetit"
                    },
                    {
                        "name": "Paul-Henry Cournède"
                    }
                ],
                "author_detail": {
                    "name": "Paul-Henry Cournède"
                },
                "author": "Paul-Henry Cournède",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.10559v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.10559v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09194v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09194v2",
                "updated": "2025-04-01T00:19:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    0,
                    19,
                    11,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-12T09:38:40Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    9,
                    38,
                    40,
                    2,
                    71,
                    0
                ],
                "title": "Addressing pitfalls in implicit unobserved confounding synthesis using\n  explicit block hierarchical ancestral sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing pitfalls in implicit unobserved confounding synthesis using\n  explicit block hierarchical ancestral sampling"
                },
                "summary": "Unbiased data synthesis is crucial for evaluating causal discovery algorithms\nin the presence of unobserved confounding, given the scarcity of real-world\ndatasets. A common approach, implicit parameterization, encodes unobserved\nconfounding by modifying the off-diagonal entries of the idiosyncratic\ncovariance matrix while preserving positive definiteness. Within this approach,\nwe identify that state-of-the-art protocols have two distinct issues that\nhinder unbiased sampling from the complete space of causal models: first, we\ngive a detailed analysis of use of diagonally dominant constructions restricts\nthe spectrum of partial correlation matrices; and second, the restriction of\npossible graphical structures when sampling bidirected edges, unnecessarily\nruling out valid causal models. To address these limitations, we propose an\nimproved explicit modeling approach for unobserved confounding, leveraging\nblock-hierarchical ancestral generation of ground truth causal graphs.\nAlgorithms for converting the ground truth DAG into ancestral graph is provided\nso that the output of causal discovery algorithms could be compared with. We\ndraw connections between implicit and explicit parameterization, prove that our\napproach fully covers the space of causal models, including those generated by\nthe implicit parameterization, thus enabling more robust evaluation of methods\nfor causal discovery and inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unbiased data synthesis is crucial for evaluating causal discovery algorithms\nin the presence of unobserved confounding, given the scarcity of real-world\ndatasets. A common approach, implicit parameterization, encodes unobserved\nconfounding by modifying the off-diagonal entries of the idiosyncratic\ncovariance matrix while preserving positive definiteness. Within this approach,\nwe identify that state-of-the-art protocols have two distinct issues that\nhinder unbiased sampling from the complete space of causal models: first, we\ngive a detailed analysis of use of diagonally dominant constructions restricts\nthe spectrum of partial correlation matrices; and second, the restriction of\npossible graphical structures when sampling bidirected edges, unnecessarily\nruling out valid causal models. To address these limitations, we propose an\nimproved explicit modeling approach for unobserved confounding, leveraging\nblock-hierarchical ancestral generation of ground truth causal graphs.\nAlgorithms for converting the ground truth DAG into ancestral graph is provided\nso that the output of causal discovery algorithms could be compared with. We\ndraw connections between implicit and explicit parameterization, prove that our\napproach fully covers the space of causal models, including those generated by\nthe implicit parameterization, thus enabling more robust evaluation of methods\nfor causal discovery and inference."
                },
                "authors": [
                    {
                        "name": "Xudong Sun"
                    },
                    {
                        "name": "Alex Markham"
                    },
                    {
                        "name": "Pratik Misra"
                    },
                    {
                        "name": "Carsten Marr"
                    }
                ],
                "author_detail": {
                    "name": "Carsten Marr"
                },
                "author": "Carsten Marr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09194v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09194v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19326v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19326v2",
                "updated": "2025-04-01T00:07:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    0,
                    7,
                    54,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-25T03:43:11Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    3,
                    43,
                    11,
                    1,
                    84,
                    0
                ],
                "title": "Process or Result? Manipulated Ending Tokens Can Mislead Reasoning LLMs\n  to Ignore the Correct Reasoning Steps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Process or Result? Manipulated Ending Tokens Can Mislead Reasoning LLMs\n  to Ignore the Correct Reasoning Steps"
                },
                "summary": "Recent reasoning large language models (LLMs) have demonstrated remarkable\nimprovements in mathematical reasoning capabilities through long\nChain-of-Thought. The reasoning tokens of these models enable self-correction\nwithin reasoning chains, enhancing robustness. This motivates our exploration:\nhow vulnerable are reasoning LLMs to subtle errors in their input reasoning\nchains? We introduce \"Compromising Thought\" (CPT), a vulnerability where models\npresented with reasoning tokens containing manipulated calculation results tend\nto ignore correct reasoning steps and adopt incorrect results instead. Through\nsystematic evaluation across multiple reasoning LLMs, we design three\nincreasingly explicit prompting methods to measure CPT resistance, revealing\nthat models struggle significantly to identify and correct these manipulations.\nNotably, contrary to existing research suggesting structural alterations affect\nmodel performance more than content modifications, we find that local ending\ntoken manipulations have greater impact on reasoning outcomes than structural\nchanges. Moreover, we discover a security vulnerability in DeepSeek-R1 where\ntampered reasoning tokens can trigger complete reasoning cessation. Our work\nenhances understanding of reasoning robustness and highlights security\nconsiderations for reasoning-intensive applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent reasoning large language models (LLMs) have demonstrated remarkable\nimprovements in mathematical reasoning capabilities through long\nChain-of-Thought. The reasoning tokens of these models enable self-correction\nwithin reasoning chains, enhancing robustness. This motivates our exploration:\nhow vulnerable are reasoning LLMs to subtle errors in their input reasoning\nchains? We introduce \"Compromising Thought\" (CPT), a vulnerability where models\npresented with reasoning tokens containing manipulated calculation results tend\nto ignore correct reasoning steps and adopt incorrect results instead. Through\nsystematic evaluation across multiple reasoning LLMs, we design three\nincreasingly explicit prompting methods to measure CPT resistance, revealing\nthat models struggle significantly to identify and correct these manipulations.\nNotably, contrary to existing research suggesting structural alterations affect\nmodel performance more than content modifications, we find that local ending\ntoken manipulations have greater impact on reasoning outcomes than structural\nchanges. Moreover, we discover a security vulnerability in DeepSeek-R1 where\ntampered reasoning tokens can trigger complete reasoning cessation. Our work\nenhances understanding of reasoning robustness and highlights security\nconsiderations for reasoning-intensive applications."
                },
                "authors": [
                    {
                        "name": "Yu Cui"
                    },
                    {
                        "name": "Bryan Hooi"
                    },
                    {
                        "name": "Yujun Cai"
                    },
                    {
                        "name": "Yiwei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yiwei Wang"
                },
                "author": "Yiwei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19326v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19326v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03717v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03717v2",
                "updated": "2025-03-31T23:24:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    23,
                    24,
                    2,
                    0,
                    90,
                    0
                ],
                "published": "2025-02-06T02:07:18Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    2,
                    7,
                    18,
                    3,
                    37,
                    0
                ],
                "title": "Efficiently Generating Expressive Quadruped Behaviors via\n  Language-Guided Preference Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently Generating Expressive Quadruped Behaviors via\n  Language-Guided Preference Learning"
                },
                "summary": "Expressive robotic behavior is essential for the widespread acceptance of\nrobots in social environments. Recent advancements in learned legged locomotion\ncontrollers have enabled more dynamic and versatile robot behaviors. However,\ndetermining the optimal behavior for interactions with different users across\nvaried scenarios remains a challenge. Current methods either rely on natural\nlanguage input, which is efficient but low-resolution, or learn from human\npreferences, which, although high-resolution, is sample inefficient. This paper\nintroduces a novel approach that leverages priors generated by pre-trained LLMs\nalongside the precision of preference learning. Our method, termed\nLanguage-Guided Preference Learning (LGPL), uses LLMs to generate initial\nbehavior samples, which are then refined through preference-based feedback to\nlearn behaviors that closely align with human expectations. Our core insight is\nthat LLMs can guide the sampling process for preference learning, leading to a\nsubstantial improvement in sample efficiency. We demonstrate that LGPL can\nquickly learn accurate and expressive behaviors with as few as four queries,\noutperforming both purely language-parameterized models and traditional\npreference learning approaches. Website with videos:\nhttps://lgpl-gaits.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expressive robotic behavior is essential for the widespread acceptance of\nrobots in social environments. Recent advancements in learned legged locomotion\ncontrollers have enabled more dynamic and versatile robot behaviors. However,\ndetermining the optimal behavior for interactions with different users across\nvaried scenarios remains a challenge. Current methods either rely on natural\nlanguage input, which is efficient but low-resolution, or learn from human\npreferences, which, although high-resolution, is sample inefficient. This paper\nintroduces a novel approach that leverages priors generated by pre-trained LLMs\nalongside the precision of preference learning. Our method, termed\nLanguage-Guided Preference Learning (LGPL), uses LLMs to generate initial\nbehavior samples, which are then refined through preference-based feedback to\nlearn behaviors that closely align with human expectations. Our core insight is\nthat LLMs can guide the sampling process for preference learning, leading to a\nsubstantial improvement in sample efficiency. We demonstrate that LGPL can\nquickly learn accurate and expressive behaviors with as few as four queries,\noutperforming both purely language-parameterized models and traditional\npreference learning approaches. Website with videos:\nhttps://lgpl-gaits.github.io/"
                },
                "authors": [
                    {
                        "name": "Jaden Clark"
                    },
                    {
                        "name": "Joey Hejna"
                    },
                    {
                        "name": "Dorsa Sadigh"
                    }
                ],
                "author_detail": {
                    "name": "Dorsa Sadigh"
                },
                "author": "Dorsa Sadigh",
                "arxiv_comment": "8 pages 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03717v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03717v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02140v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02140v2",
                "updated": "2025-03-31T21:57:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    21,
                    57,
                    6,
                    0,
                    90,
                    0
                ],
                "published": "2024-10-03T01:52:01Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    1,
                    52,
                    1,
                    3,
                    277,
                    0
                ],
                "title": "A Formal Framework for Understanding Length Generalization in\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Formal Framework for Understanding Length Generalization in\n  Transformers"
                },
                "summary": "A major challenge for transformers is generalizing to sequences longer than\nthose observed during training. While previous works have empirically shown\nthat transformers can either succeed or fail at length generalization depending\non the task, theoretical understanding of this phenomenon remains limited. In\nthis work, we introduce a rigorous theoretical framework to analyze length\ngeneralization in causal transformers with learnable absolute positional\nencodings. In particular, we characterize those functions that are identifiable\nin the limit from sufficiently long inputs with absolute positional encodings\nunder an idealized inference scheme using a norm-based regularizer. This\nenables us to prove the possibility of length generalization for a rich family\nof problems. We experimentally validate the theory as a predictor of success\nand failure of length generalization across a range of algorithmic and formal\nlanguage tasks. Our theory not only explains a broad set of empirical\nobservations but also opens the way to provably predicting length\ngeneralization capabilities in transformers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A major challenge for transformers is generalizing to sequences longer than\nthose observed during training. While previous works have empirically shown\nthat transformers can either succeed or fail at length generalization depending\non the task, theoretical understanding of this phenomenon remains limited. In\nthis work, we introduce a rigorous theoretical framework to analyze length\ngeneralization in causal transformers with learnable absolute positional\nencodings. In particular, we characterize those functions that are identifiable\nin the limit from sufficiently long inputs with absolute positional encodings\nunder an idealized inference scheme using a norm-based regularizer. This\nenables us to prove the possibility of length generalization for a rich family\nof problems. We experimentally validate the theory as a predictor of success\nand failure of length generalization across a range of algorithmic and formal\nlanguage tasks. Our theory not only explains a broad set of empirical\nobservations but also opens the way to provably predicting length\ngeneralization capabilities in transformers."
                },
                "authors": [
                    {
                        "name": "Xinting Huang"
                    },
                    {
                        "name": "Andy Yang"
                    },
                    {
                        "name": "Satwik Bhattamishra"
                    },
                    {
                        "name": "Yash Sarrof"
                    },
                    {
                        "name": "Andreas Krebs"
                    },
                    {
                        "name": "Hattie Zhou"
                    },
                    {
                        "name": "Preetum Nakkiran"
                    },
                    {
                        "name": "Michael Hahn"
                    }
                ],
                "author_detail": {
                    "name": "Michael Hahn"
                },
                "author": "Michael Hahn",
                "arxiv_comment": "85 pages, 9 figures, 11 tables. Accepted for publication at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02140v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02140v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03735v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03735v2",
                "updated": "2025-03-31T21:07:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    21,
                    7,
                    49,
                    0,
                    90,
                    0
                ],
                "published": "2024-12-04T22:03:19Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    22,
                    3,
                    19,
                    2,
                    339,
                    0
                ],
                "title": "VidHalluc: Evaluating Temporal Hallucinations in Multimodal Large\n  Language Models for Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VidHalluc: Evaluating Temporal Hallucinations in Multimodal Large\n  Language Models for Video Understanding"
                },
                "summary": "Multimodal large language models (MLLMs) have recently shown significant\nadvancements in video understanding, excelling in content reasoning and\ninstruction-following tasks. However, hallucination, where models generate\ninaccurate or misleading content, remains underexplored in the video domain.\nBuilding on the observation that MLLM visual encoders often fail to distinguish\nvisually different yet semantically similar video pairs, we introduce\nVidHalluc, the largest benchmark designed to examine hallucinations in MLLMs\nfor video understanding. It consists of 5,002 videos, paired to highlight cases\nprone to hallucinations. VidHalluc assesses hallucinations across three\ncritical dimensions: (1) action, (2) temporal sequence, and (3) scene\ntransition. Comprehensive testing shows that most MLLMs are vulnerable to\nhallucinations across these dimensions. Furthermore, we propose DINO-HEAL, a\ntraining-free method that reduces hallucinations by incorporating spatial\nsaliency from DINOv2 to reweight visual features during inference. Our results\nshow that DINO-HEAL consistently improves performance on VidHalluc, achieving\nan average improvement of 3.02% in mitigating hallucinations across all tasks.\nBoth the VidHalluc benchmark and DINO-HEAL code are available at\nhttps://people-robots.github.io/vidhalluc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have recently shown significant\nadvancements in video understanding, excelling in content reasoning and\ninstruction-following tasks. However, hallucination, where models generate\ninaccurate or misleading content, remains underexplored in the video domain.\nBuilding on the observation that MLLM visual encoders often fail to distinguish\nvisually different yet semantically similar video pairs, we introduce\nVidHalluc, the largest benchmark designed to examine hallucinations in MLLMs\nfor video understanding. It consists of 5,002 videos, paired to highlight cases\nprone to hallucinations. VidHalluc assesses hallucinations across three\ncritical dimensions: (1) action, (2) temporal sequence, and (3) scene\ntransition. Comprehensive testing shows that most MLLMs are vulnerable to\nhallucinations across these dimensions. Furthermore, we propose DINO-HEAL, a\ntraining-free method that reduces hallucinations by incorporating spatial\nsaliency from DINOv2 to reweight visual features during inference. Our results\nshow that DINO-HEAL consistently improves performance on VidHalluc, achieving\nan average improvement of 3.02% in mitigating hallucinations across all tasks.\nBoth the VidHalluc benchmark and DINO-HEAL code are available at\nhttps://people-robots.github.io/vidhalluc."
                },
                "authors": [
                    {
                        "name": "Chaoyu Li"
                    },
                    {
                        "name": "Eun Woo Im"
                    },
                    {
                        "name": "Pooyan Fazli"
                    }
                ],
                "author_detail": {
                    "name": "Pooyan Fazli"
                },
                "author": "Pooyan Fazli",
                "arxiv_comment": "CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03735v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03735v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.14558v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.14558v6",
                "updated": "2025-03-31T21:04:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    21,
                    4,
                    11,
                    0,
                    90,
                    0
                ],
                "published": "2023-10-23T04:22:50Z",
                "published_parsed": [
                    2023,
                    10,
                    23,
                    4,
                    22,
                    50,
                    0,
                    296,
                    0
                ],
                "title": "AlpaCare:Instruction-tuned Large Language Models for Medical Application",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlpaCare:Instruction-tuned Large Language Models for Medical Application"
                },
                "summary": "Instruction-finetuning (IFT) has become crucial in aligning Large Language\nModels (LLMs) with diverse human needs and has shown great potential in medical\napplications. However, previous studies mainly fine-tune LLMs on biomedical\ndatasets with limited diversity, which often rely on benchmarks or narrow task\nscopes, and hence significantly limit the effectiveness on their medical\ninstruction-following ability and generalizability. To bridge this gap, we\npropose creating a diverse, machine-generated medical IFT dataset,\nMedInstruct-52k, using GPT-4 and ChatGPT with a high-quality expert-curated\nseed set. We then fine-tune LLaMA-series models on the dataset to develop\nAlpaCare. Despite using a smaller domain-specific dataset than previous medical\nLLMs, AlpaCare not only demonstrates superior performance on medical\napplications, with up to 38.1% absolute gain over best baselines in medical\nfree-form instruction evaluations, but also achieves 6.7% absolute gains\naveraged over multiple general domain benchmarks. Human evaluation further\nshows that AlpaCare consistently outperforms best baselines in terms of both\ncorrectness and helpfulness. We offer public access to our data, model, and\ncodebase in https://github.com/XZhang97666/AlpaCare.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction-finetuning (IFT) has become crucial in aligning Large Language\nModels (LLMs) with diverse human needs and has shown great potential in medical\napplications. However, previous studies mainly fine-tune LLMs on biomedical\ndatasets with limited diversity, which often rely on benchmarks or narrow task\nscopes, and hence significantly limit the effectiveness on their medical\ninstruction-following ability and generalizability. To bridge this gap, we\npropose creating a diverse, machine-generated medical IFT dataset,\nMedInstruct-52k, using GPT-4 and ChatGPT with a high-quality expert-curated\nseed set. We then fine-tune LLaMA-series models on the dataset to develop\nAlpaCare. Despite using a smaller domain-specific dataset than previous medical\nLLMs, AlpaCare not only demonstrates superior performance on medical\napplications, with up to 38.1% absolute gain over best baselines in medical\nfree-form instruction evaluations, but also achieves 6.7% absolute gains\naveraged over multiple general domain benchmarks. Human evaluation further\nshows that AlpaCare consistently outperforms best baselines in terms of both\ncorrectness and helpfulness. We offer public access to our data, model, and\ncodebase in https://github.com/XZhang97666/AlpaCare."
                },
                "authors": [
                    {
                        "name": "Xinlu Zhang"
                    },
                    {
                        "name": "Chenxin Tian"
                    },
                    {
                        "name": "Xianjun Yang"
                    },
                    {
                        "name": "Lichang Chen"
                    },
                    {
                        "name": "Zekun Li"
                    },
                    {
                        "name": "Linda Ruth Petzold"
                    }
                ],
                "author_detail": {
                    "name": "Linda Ruth Petzold"
                },
                "author": "Linda Ruth Petzold",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.14558v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.14558v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12901v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12901v2",
                "updated": "2025-03-31T20:57:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    20,
                    57,
                    10,
                    0,
                    90,
                    0
                ],
                "published": "2024-09-19T16:47:01Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    16,
                    47,
                    1,
                    3,
                    263,
                    0
                ],
                "title": "Chiral superfluid helium-3 in the quasi-two-dimensional limit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chiral superfluid helium-3 in the quasi-two-dimensional limit"
                },
                "summary": "Anisotropic pair breaking close to surfaces favors the chiral A phase of the\nsuperfluid $^3$He over the time-reversal invariant B phase. Confining the\nsuperfluid $^3$He into a cavity of height $D$ of the order of the Cooper pair\nsize characterized by the coherence length $\\xi_0$ - ranging between 16 nm (34\nbar) and 77 nm (0 bar) - extends the surface effects over the whole sample\nvolume, thus allowing stabilization of the A phase at pressures $P$ and\ntemperatures $T$ where otherwise the B phase would be stable. In this Letter,\nthe surfaces of such a confined sample are covered with a superfluid $^4$He\nfilm to create specular quasiparticle scattering boundary conditions,\npreventing the suppression of the superfluid order parameter. We show that the\nchiral A phase is the stable superfluid phase under strong confinement over the\nfull $P$-$T$ phase diagram down to a quasi-two-dimensional limit $D / \\xi_0 =\n1$ , where $D = 80$ nm. The planar phase, which is degenerate with the chiral A\nphase in the weak-coupling limit, is not observed. The gap inferred from\nmeasurements over the wide pressure range from 0.2 to 21.0 bar leads to an\nempirical ansatz for temperature-dependent strong-coupling effects. We discuss\nhow these results pave the way for the realization of the fully gapped\ntwo-dimensional $p_x + ip_y$ superfluid under more extreme confinement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anisotropic pair breaking close to surfaces favors the chiral A phase of the\nsuperfluid $^3$He over the time-reversal invariant B phase. Confining the\nsuperfluid $^3$He into a cavity of height $D$ of the order of the Cooper pair\nsize characterized by the coherence length $\\xi_0$ - ranging between 16 nm (34\nbar) and 77 nm (0 bar) - extends the surface effects over the whole sample\nvolume, thus allowing stabilization of the A phase at pressures $P$ and\ntemperatures $T$ where otherwise the B phase would be stable. In this Letter,\nthe surfaces of such a confined sample are covered with a superfluid $^4$He\nfilm to create specular quasiparticle scattering boundary conditions,\npreventing the suppression of the superfluid order parameter. We show that the\nchiral A phase is the stable superfluid phase under strong confinement over the\nfull $P$-$T$ phase diagram down to a quasi-two-dimensional limit $D / \\xi_0 =\n1$ , where $D = 80$ nm. The planar phase, which is degenerate with the chiral A\nphase in the weak-coupling limit, is not observed. The gap inferred from\nmeasurements over the wide pressure range from 0.2 to 21.0 bar leads to an\nempirical ansatz for temperature-dependent strong-coupling effects. We discuss\nhow these results pave the way for the realization of the fully gapped\ntwo-dimensional $p_x + ip_y$ superfluid under more extreme confinement."
                },
                "authors": [
                    {
                        "name": "Petri J. Heikkinen"
                    },
                    {
                        "name": "Lev V. Levitin"
                    },
                    {
                        "name": "Xavier Rojas"
                    },
                    {
                        "name": "Angadjit Singh"
                    },
                    {
                        "name": "Nathan Eng"
                    },
                    {
                        "name": "Andrew Casey"
                    },
                    {
                        "name": "John Saunders"
                    },
                    {
                        "name": "Anton Vorontsov"
                    },
                    {
                        "name": "Nikolay Zhelev"
                    },
                    {
                        "name": "Abhilash Thanniyil Sebastian"
                    },
                    {
                        "name": "Jeevak M. Parpia"
                    }
                ],
                "author_detail": {
                    "name": "Jeevak M. Parpia"
                },
                "author": "Jeevak M. Parpia",
                "arxiv_doi": "10.1103/PhysRevLett.134.136001",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevLett.134.136001",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.12901v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12901v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages, 4 figures. This is a copy of the version of record of this\n  article, first published in Physical Review Letters, available online at\n  Publisher's website: https://doi.org/10.1103/PhysRevLett.134.136001 The\n  Supplemental Material is available at\n  http://link.aps.org/supplemental/10.1103/PhysRevLett.134.136001",
                "arxiv_journal_ref": "Physical Review Letters 134, 136001 (2025)",
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.quant-gas",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15164v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15164v3",
                "updated": "2025-03-31T20:39:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    20,
                    39,
                    17,
                    0,
                    90,
                    0
                ],
                "published": "2024-10-19T17:28:48Z",
                "published_parsed": [
                    2024,
                    10,
                    19,
                    17,
                    28,
                    48,
                    5,
                    293,
                    0
                ],
                "title": "SPA-Bench: A Comprehensive Benchmark for SmartPhone Agent Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPA-Bench: A Comprehensive Benchmark for SmartPhone Agent Evaluation"
                },
                "summary": "Smartphone agents are increasingly important for helping users control\ndevices efficiently, with (Multimodal) Large Language Model (MLLM)-based\napproaches emerging as key contenders. Fairly comparing these agents is\nessential but challenging, requiring a varied task scope, the integration of\nagents with different implementations, and a generalisable evaluation pipeline\nto assess their strengths and weaknesses. In this paper, we present SPA-Bench,\na comprehensive SmartPhone Agent Benchmark designed to evaluate (M)LLM-based\nagents in an interactive environment that simulates real-world conditions.\nSPA-Bench offers three key contributions: (1) A diverse set of tasks covering\nsystem and third-party apps in both English and Chinese, focusing on features\ncommonly used in daily routines; (2) A plug-and-play framework enabling\nreal-time agent interaction with Android devices, integrating over ten agents\nwith the flexibility to add more; (3) A novel evaluation pipeline that\nautomatically assesses agent performance across multiple dimensions,\nencompassing seven metrics related to task completion and resource consumption.\nOur extensive experiments across tasks and agents reveal challenges like\ninterpreting mobile user interfaces, action grounding, memory retention, and\nexecution costs. We propose future research directions to ease these\ndifficulties, moving closer to real-world smartphone agent applications.\nSPA-Bench is available at https://ai-agents-2030.github.io/SPA-Bench/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smartphone agents are increasingly important for helping users control\ndevices efficiently, with (Multimodal) Large Language Model (MLLM)-based\napproaches emerging as key contenders. Fairly comparing these agents is\nessential but challenging, requiring a varied task scope, the integration of\nagents with different implementations, and a generalisable evaluation pipeline\nto assess their strengths and weaknesses. In this paper, we present SPA-Bench,\na comprehensive SmartPhone Agent Benchmark designed to evaluate (M)LLM-based\nagents in an interactive environment that simulates real-world conditions.\nSPA-Bench offers three key contributions: (1) A diverse set of tasks covering\nsystem and third-party apps in both English and Chinese, focusing on features\ncommonly used in daily routines; (2) A plug-and-play framework enabling\nreal-time agent interaction with Android devices, integrating over ten agents\nwith the flexibility to add more; (3) A novel evaluation pipeline that\nautomatically assesses agent performance across multiple dimensions,\nencompassing seven metrics related to task completion and resource consumption.\nOur extensive experiments across tasks and agents reveal challenges like\ninterpreting mobile user interfaces, action grounding, memory retention, and\nexecution costs. We propose future research directions to ease these\ndifficulties, moving closer to real-world smartphone agent applications.\nSPA-Bench is available at https://ai-agents-2030.github.io/SPA-Bench/."
                },
                "authors": [
                    {
                        "name": "Jingxuan Chen"
                    },
                    {
                        "name": "Derek Yuen"
                    },
                    {
                        "name": "Bin Xie"
                    },
                    {
                        "name": "Yuhao Yang"
                    },
                    {
                        "name": "Gongwei Chen"
                    },
                    {
                        "name": "Zhihao Wu"
                    },
                    {
                        "name": "Li Yixing"
                    },
                    {
                        "name": "Xurui Zhou"
                    },
                    {
                        "name": "Weiwen Liu"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Kaiwen Zhou"
                    },
                    {
                        "name": "Rui Shao"
                    },
                    {
                        "name": "Liqiang Nie"
                    },
                    {
                        "name": "Yasheng Wang"
                    },
                    {
                        "name": "Jianye Hao"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Kun Shao"
                    }
                ],
                "author_detail": {
                    "name": "Kun Shao"
                },
                "author": "Kun Shao",
                "arxiv_comment": "ICLR 2025 Spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15164v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15164v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01100v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01100v3",
                "updated": "2025-03-31T20:37:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    20,
                    37,
                    34,
                    0,
                    90,
                    0
                ],
                "published": "2024-07-01T09:06:57Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    9,
                    6,
                    57,
                    0,
                    183,
                    0
                ],
                "title": "Eliminating Position Bias of Language Models: A Mechanistic Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eliminating Position Bias of Language Models: A Mechanistic Approach"
                },
                "summary": "Position bias has proven to be a prevalent issue of modern language models\n(LMs), where the models prioritize content based on its position within the\ngiven context. This bias often leads to unexpected model failures and hurts\nperformance, robustness, and reliability across various applications. Our\nmechanistic analysis attributes the position bias to two components employed in\nnearly all state-of-the-art LMs: causal attention and relative positional\nencodings. Based on the analyses, we propose to eliminate position bias (e.g.,\ndifferent retrieved documents' orders in QA affect performance) with a\ntraining-free zero-shot approach. Our method changes the causal attention to\nbidirectional attention between documents and utilizes model attention values\nto decide the relative orders of documents instead of using the order provided\nin input prompts, therefore enabling Position-INvariant inferencE (PINE) at the\ndocument level. By eliminating position bias, models achieve better performance\nand reliability in downstream tasks, including LM-as-a-judge,\nretrieval-augmented QA, molecule generation, and math reasoning. Notably, PINE\nis especially useful when adapting LMs for evaluating reasoning pairs: it\nconsistently provides 8 to 10 percentage points performance gains, making\nLlama-3-70B-Instruct perform even better than GPT-4-0125-preview and\nGPT-4o-2024-08-06 on the RewardBench reasoning set.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Position bias has proven to be a prevalent issue of modern language models\n(LMs), where the models prioritize content based on its position within the\ngiven context. This bias often leads to unexpected model failures and hurts\nperformance, robustness, and reliability across various applications. Our\nmechanistic analysis attributes the position bias to two components employed in\nnearly all state-of-the-art LMs: causal attention and relative positional\nencodings. Based on the analyses, we propose to eliminate position bias (e.g.,\ndifferent retrieved documents' orders in QA affect performance) with a\ntraining-free zero-shot approach. Our method changes the causal attention to\nbidirectional attention between documents and utilizes model attention values\nto decide the relative orders of documents instead of using the order provided\nin input prompts, therefore enabling Position-INvariant inferencE (PINE) at the\ndocument level. By eliminating position bias, models achieve better performance\nand reliability in downstream tasks, including LM-as-a-judge,\nretrieval-augmented QA, molecule generation, and math reasoning. Notably, PINE\nis especially useful when adapting LMs for evaluating reasoning pairs: it\nconsistently provides 8 to 10 percentage points performance gains, making\nLlama-3-70B-Instruct perform even better than GPT-4-0125-preview and\nGPT-4o-2024-08-06 on the RewardBench reasoning set."
                },
                "authors": [
                    {
                        "name": "Ziqi Wang"
                    },
                    {
                        "name": "Hanlin Zhang"
                    },
                    {
                        "name": "Xiner Li"
                    },
                    {
                        "name": "Kuan-Hao Huang"
                    },
                    {
                        "name": "Chi Han"
                    },
                    {
                        "name": "Shuiwang Ji"
                    },
                    {
                        "name": "Sham M. Kakade"
                    },
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Heng Ji"
                    }
                ],
                "author_detail": {
                    "name": "Heng Ji"
                },
                "author": "Heng Ji",
                "arxiv_comment": "26 pages, 6 figures, 15 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.01100v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01100v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16698v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16698v2",
                "updated": "2025-03-31T20:33:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    20,
                    33,
                    59,
                    0,
                    90,
                    0
                ],
                "published": "2024-12-21T16:54:28Z",
                "published_parsed": [
                    2024,
                    12,
                    21,
                    16,
                    54,
                    28,
                    5,
                    356,
                    0
                ],
                "title": "Interact with me: Joint Egocentric Forecasting of Intent to Interact,\n  Attitude and Social Actions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interact with me: Joint Egocentric Forecasting of Intent to Interact,\n  Attitude and Social Actions"
                },
                "summary": "For efficient human-agent interaction, an agent should proactively recognize\ntheir target user and prepare for upcoming interactions. We formulate this\nchallenging problem as the novel task of jointly forecasting a person's intent\nto interact with the agent, their attitude towards the agent and the action\nthey will perform, from the agent's (egocentric) perspective. So we propose\n\\emph{SocialEgoNet} - a graph-based spatiotemporal framework that exploits task\ndependencies through a hierarchical multitask learning approach. SocialEgoNet\nuses whole-body skeletons (keypoints from face, hands and body) extracted from\nonly 1 second of video input for high inference speed. For evaluation, we\naugment an existing egocentric human-agent interaction dataset with new class\nlabels and bounding box annotations. Extensive experiments on this augmented\ndataset, named JPL-Social, demonstrate \\emph{real-time} inference and superior\nperformance (average accuracy across all tasks: 83.15\\%) of our model\noutperforming several competitive baselines. The additional annotations and\ncode will be available upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For efficient human-agent interaction, an agent should proactively recognize\ntheir target user and prepare for upcoming interactions. We formulate this\nchallenging problem as the novel task of jointly forecasting a person's intent\nto interact with the agent, their attitude towards the agent and the action\nthey will perform, from the agent's (egocentric) perspective. So we propose\n\\emph{SocialEgoNet} - a graph-based spatiotemporal framework that exploits task\ndependencies through a hierarchical multitask learning approach. SocialEgoNet\nuses whole-body skeletons (keypoints from face, hands and body) extracted from\nonly 1 second of video input for high inference speed. For evaluation, we\naugment an existing egocentric human-agent interaction dataset with new class\nlabels and bounding box annotations. Extensive experiments on this augmented\ndataset, named JPL-Social, demonstrate \\emph{real-time} inference and superior\nperformance (average accuracy across all tasks: 83.15\\%) of our model\noutperforming several competitive baselines. The additional annotations and\ncode will be available upon acceptance."
                },
                "authors": [
                    {
                        "name": "Tongfei Bian"
                    },
                    {
                        "name": "Yiming Ma"
                    },
                    {
                        "name": "Mathieu Chollet"
                    },
                    {
                        "name": "Victor Sanchez"
                    },
                    {
                        "name": "Tanaya Guha"
                    }
                ],
                "author_detail": {
                    "name": "Tanaya Guha"
                },
                "author": "Tanaya Guha",
                "arxiv_comment": "Accepted at ICME, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16698v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16698v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01095v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01095v3",
                "updated": "2025-03-31T20:17:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    20,
                    17,
                    27,
                    0,
                    90,
                    0
                ],
                "published": "2024-12-02T04:10:14Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    4,
                    10,
                    14,
                    0,
                    337,
                    0
                ],
                "title": "VERA: Explainable Video Anomaly Detection via Verbalized Learning of\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VERA: Explainable Video Anomaly Detection via Verbalized Learning of\n  Vision-Language Models"
                },
                "summary": "The rapid advancement of vision-language models (VLMs) has established a new\nparadigm in video anomaly detection (VAD): leveraging VLMs to simultaneously\ndetect anomalies and provide comprehendible explanations for the decisions.\nExisting work in this direction often assumes the complex reasoning required\nfor VAD exceeds the capabilities of pretrained VLMs. Consequently, these\napproaches either incorporate specialized reasoning modules during inference or\nrely on instruction tuning datasets through additional training to adapt VLMs\nfor VAD. However, such strategies often incur substantial computational costs\nor data annotation overhead. To address these challenges in explainable VAD, we\nintroduce a verbalized learning framework named VERA that enables VLMs to\nperform VAD without model parameter modifications. Specifically, VERA\nautomatically decomposes the complex reasoning required for VAD into\nreflections on simpler, more focused guiding questions capturing distinct\nabnormal patterns. It treats these reflective questions as learnable parameters\nand optimizes them through data-driven verbal interactions between learner and\noptimizer VLMs, using coarsely labeled training data. During inference, VERA\nembeds the learned questions into model prompts to guide VLMs in generating\nsegment-level anomaly scores, which are then refined into frame-level scores\nvia the fusion of scene and temporal contexts. Experimental results on\nchallenging benchmarks demonstrate that the learned questions of VERA are\nhighly adaptable, significantly improving both detection performance and\nexplainability of VLMs for VAD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of vision-language models (VLMs) has established a new\nparadigm in video anomaly detection (VAD): leveraging VLMs to simultaneously\ndetect anomalies and provide comprehendible explanations for the decisions.\nExisting work in this direction often assumes the complex reasoning required\nfor VAD exceeds the capabilities of pretrained VLMs. Consequently, these\napproaches either incorporate specialized reasoning modules during inference or\nrely on instruction tuning datasets through additional training to adapt VLMs\nfor VAD. However, such strategies often incur substantial computational costs\nor data annotation overhead. To address these challenges in explainable VAD, we\nintroduce a verbalized learning framework named VERA that enables VLMs to\nperform VAD without model parameter modifications. Specifically, VERA\nautomatically decomposes the complex reasoning required for VAD into\nreflections on simpler, more focused guiding questions capturing distinct\nabnormal patterns. It treats these reflective questions as learnable parameters\nand optimizes them through data-driven verbal interactions between learner and\noptimizer VLMs, using coarsely labeled training data. During inference, VERA\nembeds the learned questions into model prompts to guide VLMs in generating\nsegment-level anomaly scores, which are then refined into frame-level scores\nvia the fusion of scene and temporal contexts. Experimental results on\nchallenging benchmarks demonstrate that the learned questions of VERA are\nhighly adaptable, significantly improving both detection performance and\nexplainability of VLMs for VAD."
                },
                "authors": [
                    {
                        "name": "Muchao Ye"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Pan He"
                    }
                ],
                "author_detail": {
                    "name": "Pan He"
                },
                "author": "Pan He",
                "arxiv_comment": "Accepted in CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01095v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01095v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.13164v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.13164v4",
                "updated": "2025-03-31T20:03:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    20,
                    3,
                    34,
                    0,
                    90,
                    0
                ],
                "published": "2024-03-19T21:31:56Z",
                "published_parsed": [
                    2024,
                    3,
                    19,
                    21,
                    31,
                    56,
                    1,
                    79,
                    0
                ],
                "title": "VL-ICL Bench: The Devil in the Details of Multimodal In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VL-ICL Bench: The Devil in the Details of Multimodal In-Context Learning"
                },
                "summary": "Large language models (LLMs) famously exhibit emergent in-context learning\n(ICL) -- the ability to rapidly adapt to new tasks using few-shot examples\nprovided as a prompt, without updating the model's weights. Built on top of\nLLMs, vision large language models (VLLMs) have advanced significantly in areas\nsuch as recognition, reasoning, and grounding. However, investigations into\n\\emph{multimodal ICL} have predominantly focused on few-shot visual question\nanswering (VQA), and image captioning, which we will show neither exploit the\nstrengths of ICL, nor test its limitations. The broader capabilities and\nlimitations of multimodal ICL remain under-explored. In this study, we\nintroduce a comprehensive benchmark VL-ICL Bench for multimodal in-context\nlearning, encompassing a broad spectrum of tasks that involve both images and\ntext as inputs and outputs, and different types of challenges, from {perception\nto reasoning and long context length}. We evaluate the abilities of\nstate-of-the-art VLLMs against this benchmark suite, revealing their diverse\nstrengths and weaknesses, and showing that even the most advanced models, such\nas GPT-4, find the tasks challenging. By highlighting a range of new ICL tasks,\nand the associated strengths and limitations of existing models, we hope that\nour dataset will inspire future work on enhancing the in-context learning\ncapabilities of VLLMs, as well as inspire new applications that leverage VLLM\nICL. The code and dataset are available at https://github.com/ys-zong/VL-ICL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) famously exhibit emergent in-context learning\n(ICL) -- the ability to rapidly adapt to new tasks using few-shot examples\nprovided as a prompt, without updating the model's weights. Built on top of\nLLMs, vision large language models (VLLMs) have advanced significantly in areas\nsuch as recognition, reasoning, and grounding. However, investigations into\n\\emph{multimodal ICL} have predominantly focused on few-shot visual question\nanswering (VQA), and image captioning, which we will show neither exploit the\nstrengths of ICL, nor test its limitations. The broader capabilities and\nlimitations of multimodal ICL remain under-explored. In this study, we\nintroduce a comprehensive benchmark VL-ICL Bench for multimodal in-context\nlearning, encompassing a broad spectrum of tasks that involve both images and\ntext as inputs and outputs, and different types of challenges, from {perception\nto reasoning and long context length}. We evaluate the abilities of\nstate-of-the-art VLLMs against this benchmark suite, revealing their diverse\nstrengths and weaknesses, and showing that even the most advanced models, such\nas GPT-4, find the tasks challenging. By highlighting a range of new ICL tasks,\nand the associated strengths and limitations of existing models, we hope that\nour dataset will inspire future work on enhancing the in-context learning\ncapabilities of VLLMs, as well as inspire new applications that leverage VLLM\nICL. The code and dataset are available at https://github.com/ys-zong/VL-ICL."
                },
                "authors": [
                    {
                        "name": "Yongshuo Zong"
                    },
                    {
                        "name": "Ondrej Bohdal"
                    },
                    {
                        "name": "Timothy Hospedales"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Hospedales"
                },
                "author": "Timothy Hospedales",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.13164v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.13164v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01860v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01860v2",
                "updated": "2025-03-31T19:39:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    19,
                    39,
                    16,
                    0,
                    90,
                    0
                ],
                "published": "2025-02-03T22:19:28Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    22,
                    19,
                    28,
                    0,
                    34,
                    0
                ],
                "title": "SE Arena: An Interactive Platform for Evaluating Foundation Models in\n  Software Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SE Arena: An Interactive Platform for Evaluating Foundation Models in\n  Software Engineering"
                },
                "summary": "Foundation models (FMs), particularly large language models (LLMs), have\nshown significant promise in various software engineering (SE) tasks, including\ncode generation, debugging, and requirement refinement. Despite these advances,\nexisting evaluation frameworks are insufficient for assessing model performance\nin iterative, context-rich workflows characteristic of SE activities. To\naddress this limitation, we introduce SE Arena, an interactive platform\ndesigned to evaluate SE-focused chatbots. SE Arena provides a transparent,\nopen-source leaderboard, supports multi-round conversational workflows, and\nenables end-to-end model comparisons. Moreover, SE Arena incorporates a new\nfeature called RepoChat, which automatically injects repository-related context\n(e.g., issues, commits, pull requests) into the conversation, further aligning\nevaluations with real-world development processes. This paper outlines the\ndesign and capabilities of SE Arena, emphasizing its potential to advance the\nevaluation and practical application of FMs in software engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation models (FMs), particularly large language models (LLMs), have\nshown significant promise in various software engineering (SE) tasks, including\ncode generation, debugging, and requirement refinement. Despite these advances,\nexisting evaluation frameworks are insufficient for assessing model performance\nin iterative, context-rich workflows characteristic of SE activities. To\naddress this limitation, we introduce SE Arena, an interactive platform\ndesigned to evaluate SE-focused chatbots. SE Arena provides a transparent,\nopen-source leaderboard, supports multi-round conversational workflows, and\nenables end-to-end model comparisons. Moreover, SE Arena incorporates a new\nfeature called RepoChat, which automatically injects repository-related context\n(e.g., issues, commits, pull requests) into the conversation, further aligning\nevaluations with real-world development processes. This paper outlines the\ndesign and capabilities of SE Arena, emphasizing its potential to advance the\nevaluation and practical application of FMs in software engineering."
                },
                "authors": [
                    {
                        "name": "Zhimin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Zhimin Zhao"
                },
                "author": "Zhimin Zhao",
                "arxiv_comment": "Check the arena at\n  https://huggingface.co/spaces/SE-Arena/Software-Engineering-Arena",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01860v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01860v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05506v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05506v2",
                "updated": "2025-03-31T19:23:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    19,
                    23,
                    51,
                    0,
                    90,
                    0
                ],
                "published": "2024-10-07T21:24:22Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    21,
                    24,
                    22,
                    0,
                    281,
                    0
                ],
                "title": "Privacy Vulnerabilities in Marginals-based Synthetic Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy Vulnerabilities in Marginals-based Synthetic Data"
                },
                "summary": "When acting as a privacy-enhancing technology, synthetic data generation\n(SDG) aims to maintain a resemblance to the real data while excluding\npersonally-identifiable information. Many SDG algorithms provide robust\ndifferential privacy (DP) guarantees to this end. However, we show that the\nstrongest class of SDG algorithms--those that preserve \\textit{marginal\nprobabilities}, or similar statistics, from the underlying data--leak\ninformation about individuals that can be recovered more efficiently than\npreviously understood. We demonstrate this by presenting a novel membership\ninference attack, MAMA-MIA, and evaluate it against three seminal DP SDG\nalgorithms: MST, PrivBayes, and Private-GSD. MAMA-MIA leverages knowledge of\nwhich SDG algorithm was used, allowing it to learn information about the hidden\ndata more accurately, and orders-of-magnitude faster, than other leading\nattacks. We use MAMA-MIA to lend insight into existing SDG vulnerabilities. Our\napproach went on to win the first SNAKE (SaNitization Algorithm under attacK\n... $\\varepsilon$) competition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When acting as a privacy-enhancing technology, synthetic data generation\n(SDG) aims to maintain a resemblance to the real data while excluding\npersonally-identifiable information. Many SDG algorithms provide robust\ndifferential privacy (DP) guarantees to this end. However, we show that the\nstrongest class of SDG algorithms--those that preserve \\textit{marginal\nprobabilities}, or similar statistics, from the underlying data--leak\ninformation about individuals that can be recovered more efficiently than\npreviously understood. We demonstrate this by presenting a novel membership\ninference attack, MAMA-MIA, and evaluate it against three seminal DP SDG\nalgorithms: MST, PrivBayes, and Private-GSD. MAMA-MIA leverages knowledge of\nwhich SDG algorithm was used, allowing it to learn information about the hidden\ndata more accurately, and orders-of-magnitude faster, than other leading\nattacks. We use MAMA-MIA to lend insight into existing SDG vulnerabilities. Our\napproach went on to win the first SNAKE (SaNitization Algorithm under attacK\n... $\\varepsilon$) competition."
                },
                "authors": [
                    {
                        "name": "Steven Golob"
                    },
                    {
                        "name": "Sikha Pentyala"
                    },
                    {
                        "name": "Anuar Maratkhan"
                    },
                    {
                        "name": "Martine De Cock"
                    }
                ],
                "author_detail": {
                    "name": "Martine De Cock"
                },
                "author": "Martine De Cock",
                "arxiv_comment": "Accepted at 3rd IEEE Conference on Secure and Trustworthy Machine\n  Learning (SaTML) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05506v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05506v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04912v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04912v2",
                "updated": "2025-03-31T18:16:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    18,
                    16,
                    41,
                    0,
                    90,
                    0
                ],
                "published": "2024-04-04T16:20:06Z",
                "published_parsed": [
                    2024,
                    4,
                    4,
                    16,
                    20,
                    6,
                    3,
                    95,
                    0
                ],
                "title": "GP-MoLFormer: A Foundation Model For Molecular Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GP-MoLFormer: A Foundation Model For Molecular Generation"
                },
                "summary": "Transformer-based models trained on large and general purpose datasets\nconsisting of molecular strings have recently emerged as a powerful tool for\nsuccessfully modeling various structure-property relations. Inspired by this\nsuccess, we extend the paradigm of training chemical language transformers on\nlarge-scale chemical datasets to generative tasks in this work. Specifically,\nwe propose GP-MoLFormer, an autoregressive molecular string generator that is\ntrained on more than 1.1B (billion) chemical SMILES. GP-MoLFormer uses a 46.8M\nparameter transformer decoder model with linear attention and rotary positional\nencodings as the base architecture. GP-MoLFormer's utility is evaluated and\ncompared with that of existing baselines on three different tasks: de novo\ngeneration, scaffold-constrained molecular decoration, and unconstrained\nproperty-guided optimization. While the first two are handled with no\nadditional training, we propose a parameter-efficient fine-tuning method for\nthe last task, which uses property-ordered molecular pairs as input. We call\nthis new approach pair-tuning. Our results show GP-MoLFormer performs better or\ncomparable with baselines across all three tasks, demonstrating its general\nutility for a variety of molecular generation tasks. We further report strong\nmemorization of training data in GP-MoLFormer generations, which has so far\nremained unexplored for chemical language models. Our analyses reveal that\ntraining data memorization and novelty in generations are impacted by the\nquality and scale of the training data; duplication bias in training data can\nenhance memorization at the cost of lowering novelty. We further establish a\nscaling law relating inference compute and novelty in generations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based models trained on large and general purpose datasets\nconsisting of molecular strings have recently emerged as a powerful tool for\nsuccessfully modeling various structure-property relations. Inspired by this\nsuccess, we extend the paradigm of training chemical language transformers on\nlarge-scale chemical datasets to generative tasks in this work. Specifically,\nwe propose GP-MoLFormer, an autoregressive molecular string generator that is\ntrained on more than 1.1B (billion) chemical SMILES. GP-MoLFormer uses a 46.8M\nparameter transformer decoder model with linear attention and rotary positional\nencodings as the base architecture. GP-MoLFormer's utility is evaluated and\ncompared with that of existing baselines on three different tasks: de novo\ngeneration, scaffold-constrained molecular decoration, and unconstrained\nproperty-guided optimization. While the first two are handled with no\nadditional training, we propose a parameter-efficient fine-tuning method for\nthe last task, which uses property-ordered molecular pairs as input. We call\nthis new approach pair-tuning. Our results show GP-MoLFormer performs better or\ncomparable with baselines across all three tasks, demonstrating its general\nutility for a variety of molecular generation tasks. We further report strong\nmemorization of training data in GP-MoLFormer generations, which has so far\nremained unexplored for chemical language models. Our analyses reveal that\ntraining data memorization and novelty in generations are impacted by the\nquality and scale of the training data; duplication bias in training data can\nenhance memorization at the cost of lowering novelty. We further establish a\nscaling law relating inference compute and novelty in generations."
                },
                "authors": [
                    {
                        "name": "Jerret Ross"
                    },
                    {
                        "name": "Brian Belgodere"
                    },
                    {
                        "name": "Samuel C. Hoffman"
                    },
                    {
                        "name": "Vijil Chenthamarakshan"
                    },
                    {
                        "name": "Jiri Navratil"
                    },
                    {
                        "name": "Youssef Mroueh"
                    },
                    {
                        "name": "Payel Das"
                    }
                ],
                "author_detail": {
                    "name": "Payel Das"
                },
                "author": "Payel Das",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.04912v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04912v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.BM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2503.09647v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09647v3",
                "updated": "2025-04-01T17:54:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    54,
                    27,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-12T08:41:36Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    8,
                    41,
                    36,
                    2,
                    71,
                    0
                ],
                "title": "Leveraging LLMS for Top-Down Sector Allocation In Automated Trading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging LLMS for Top-Down Sector Allocation In Automated Trading"
                },
                "summary": "This paper introduces a methodology leveraging Large Language Models (LLMs)\nfor sector-level portfolio allocation through systematic analysis of\nmacroeconomic conditions and market sentiment. Our framework emphasizes\ntop-down sector allocation by processing multiple data streams simultaneously,\nincluding policy documents, economic indicators, and sentiment patterns.\nEmpirical results demonstrate superior risk-adjusted returns compared to\ntraditional cross momentum strategies, achieving a Sharpe ratio of 2.51 and\nportfolio return of 8.79% versus -0.61 and -1.39% respectively. These results\nsuggest that LLM-based systematic macro analysis presents a viable approach for\nenhancing automated portfolio allocation decisions at the sector level.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a methodology leveraging Large Language Models (LLMs)\nfor sector-level portfolio allocation through systematic analysis of\nmacroeconomic conditions and market sentiment. Our framework emphasizes\ntop-down sector allocation by processing multiple data streams simultaneously,\nincluding policy documents, economic indicators, and sentiment patterns.\nEmpirical results demonstrate superior risk-adjusted returns compared to\ntraditional cross momentum strategies, achieving a Sharpe ratio of 2.51 and\nportfolio return of 8.79% versus -0.61 and -1.39% respectively. These results\nsuggest that LLM-based systematic macro analysis presents a viable approach for\nenhancing automated portfolio allocation decisions at the sector level."
                },
                "authors": [
                    {
                        "name": "Ryan Quek Wei Heng"
                    },
                    {
                        "name": "Edoardo Vittori"
                    },
                    {
                        "name": "Keane Ong"
                    },
                    {
                        "name": "Rui Mao"
                    },
                    {
                        "name": "Erik Cambria"
                    },
                    {
                        "name": "Gianmarco Mengaldo"
                    }
                ],
                "author_detail": {
                    "name": "Gianmarco Mengaldo"
                },
                "author": "Gianmarco Mengaldo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09647v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09647v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.PM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10380v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10380v3",
                "updated": "2025-04-01T17:25:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    25,
                    53,
                    1,
                    91,
                    0
                ],
                "published": "2024-07-15T01:21:56Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    1,
                    21,
                    56,
                    0,
                    197,
                    0
                ],
                "title": "NTSEBENCH: Cognitive Reasoning Benchmark for Vision Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NTSEBENCH: Cognitive Reasoning Benchmark for Vision Language Models"
                },
                "summary": "Cognitive textual and visual reasoning tasks, including puzzles, series, and\nanalogies, demand the ability to quickly reason, decipher, and evaluate\npatterns both textually and spatially. Due to extensive training on vast\namounts of human-curated data, LLMs and VLMs excel in common-sense reasoning\ntasks, however still struggle with more complex reasoning that demands deeper\ncognitive understanding. We introduce NTSEBench, a new dataset designed to\nevaluate cognitive multi-modal reasoning and problem-solving skills of large\nmodels. The dataset contains 2728 multiple-choice questions, accompanied by a\ntotal of 4,642 images, categorized into 26 different types. These questions are\ndrawn from the nationwide NTSE examination in India and feature a mix of visual\nand textual general aptitude challenges, designed to assess intelligence and\ncritical thinking skills beyond mere rote learning. We establish baselines on\nthe dataset using state-of-the-art LLMs and VLMs. To facilitate a comparison\nbetween open source and propriety models, we propose four distinct modeling\nstrategies to handle different modalities -- text and images -- in the dataset\ninstances.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive textual and visual reasoning tasks, including puzzles, series, and\nanalogies, demand the ability to quickly reason, decipher, and evaluate\npatterns both textually and spatially. Due to extensive training on vast\namounts of human-curated data, LLMs and VLMs excel in common-sense reasoning\ntasks, however still struggle with more complex reasoning that demands deeper\ncognitive understanding. We introduce NTSEBench, a new dataset designed to\nevaluate cognitive multi-modal reasoning and problem-solving skills of large\nmodels. The dataset contains 2728 multiple-choice questions, accompanied by a\ntotal of 4,642 images, categorized into 26 different types. These questions are\ndrawn from the nationwide NTSE examination in India and feature a mix of visual\nand textual general aptitude challenges, designed to assess intelligence and\ncritical thinking skills beyond mere rote learning. We establish baselines on\nthe dataset using state-of-the-art LLMs and VLMs. To facilitate a comparison\nbetween open source and propriety models, we propose four distinct modeling\nstrategies to handle different modalities -- text and images -- in the dataset\ninstances."
                },
                "authors": [
                    {
                        "name": "Pranshu Pandya"
                    },
                    {
                        "name": "Vatsal Gupta"
                    },
                    {
                        "name": "Agney S Talwarr"
                    },
                    {
                        "name": "Tushar Kataria"
                    },
                    {
                        "name": "Dan Roth"
                    },
                    {
                        "name": "Vivek Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Vivek Gupta"
                },
                "author": "Vivek Gupta",
                "arxiv_comment": "28 pages, 3 figures, 12 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10380v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10380v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17849v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17849v3",
                "updated": "2025-04-01T17:21:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    21,
                    55,
                    1,
                    91,
                    0
                ],
                "published": "2024-11-26T20:06:55Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    20,
                    6,
                    55,
                    1,
                    331,
                    0
                ],
                "title": "GNN 101: Visual Learning of Graph Neural Networks in Your Web Browser",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GNN 101: Visual Learning of Graph Neural Networks in Your Web Browser"
                },
                "summary": "Graph Neural Networks (GNNs) have achieved significant success across various\napplications. However, their complex structures and inner workings can be\nchallenging for non-AI experts to understand. To address this issue, this study\npresents \\name{}, an educational visualization tool for interactive learning of\nGNNs. GNN 101 introduces a set of animated visualizations that seamlessly\nintegrate mathematical formulas with visualizations via multiple levels of\nabstraction, including a model overview, layer operations, and detailed\ncalculations. Users can easily switch between two complementary views: a\nnode-link view that offers an intuitive understanding of the graph data, and a\nmatrix view that provides a space-efficient and comprehensive overview of all\nfeatures and their transformations across layers. GNN 101 was designed and\ndeveloped based on close collaboration with four GNN experts and deployment in\nthree GNN-related courses. We demonstrated the usability and effectiveness of\nGNN 101 via use cases and user studies with both GNN teaching assistants and\nstudents. To ensure broad educational access, GNN 101 is open-source and\navailable directly in web browsers without requiring any installations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) have achieved significant success across various\napplications. However, their complex structures and inner workings can be\nchallenging for non-AI experts to understand. To address this issue, this study\npresents \\name{}, an educational visualization tool for interactive learning of\nGNNs. GNN 101 introduces a set of animated visualizations that seamlessly\nintegrate mathematical formulas with visualizations via multiple levels of\nabstraction, including a model overview, layer operations, and detailed\ncalculations. Users can easily switch between two complementary views: a\nnode-link view that offers an intuitive understanding of the graph data, and a\nmatrix view that provides a space-efficient and comprehensive overview of all\nfeatures and their transformations across layers. GNN 101 was designed and\ndeveloped based on close collaboration with four GNN experts and deployment in\nthree GNN-related courses. We demonstrated the usability and effectiveness of\nGNN 101 via use cases and user studies with both GNN teaching assistants and\nstudents. To ensure broad educational access, GNN 101 is open-source and\navailable directly in web browsers without requiring any installations."
                },
                "authors": [
                    {
                        "name": "Yilin Lu"
                    },
                    {
                        "name": "Chongwei Chen"
                    },
                    {
                        "name": "Yuxin Chen"
                    },
                    {
                        "name": "Kexin Huang"
                    },
                    {
                        "name": "Marinka Zitnik"
                    },
                    {
                        "name": "Qianwen Wang"
                    }
                ],
                "author_detail": {
                    "name": "Qianwen Wang"
                },
                "author": "Qianwen Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17849v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17849v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.06501v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.06501v3",
                "updated": "2025-04-01T16:54:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    16,
                    54,
                    54,
                    1,
                    91,
                    0
                ],
                "published": "2024-07-09T02:06:30Z",
                "published_parsed": [
                    2024,
                    7,
                    9,
                    2,
                    6,
                    30,
                    1,
                    191,
                    0
                ],
                "title": "STORYSUMM: Evaluating Faithfulness in Story Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STORYSUMM: Evaluating Faithfulness in Story Summarization"
                },
                "summary": "Human evaluation has been the gold standard for checking faithfulness in\nabstractive summarization. However, with a challenging source domain like\nnarrative, multiple annotators can agree a summary is faithful, while missing\ndetails that are obvious errors only once pointed out. We therefore introduce a\nnew dataset, STORYSUMM, comprising LLM summaries of short stories with\nlocalized faithfulness labels and error explanations. This benchmark is for\nevaluation methods, testing whether a given method can detect challenging\ninconsistencies. Using this dataset, we first show that any one human\nannotation protocol is likely to miss inconsistencies, and we advocate for\npursuing a range of methods when establishing ground truth for a summarization\ndataset. We finally test recent automatic metrics and find that none of them\nachieve more than 70% balanced accuracy on this task, demonstrating that it is\na challenging benchmark for future work in faithfulness evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human evaluation has been the gold standard for checking faithfulness in\nabstractive summarization. However, with a challenging source domain like\nnarrative, multiple annotators can agree a summary is faithful, while missing\ndetails that are obvious errors only once pointed out. We therefore introduce a\nnew dataset, STORYSUMM, comprising LLM summaries of short stories with\nlocalized faithfulness labels and error explanations. This benchmark is for\nevaluation methods, testing whether a given method can detect challenging\ninconsistencies. Using this dataset, we first show that any one human\nannotation protocol is likely to miss inconsistencies, and we advocate for\npursuing a range of methods when establishing ground truth for a summarization\ndataset. We finally test recent automatic metrics and find that none of them\nachieve more than 70% balanced accuracy on this task, demonstrating that it is\na challenging benchmark for future work in faithfulness evaluation."
                },
                "authors": [
                    {
                        "name": "Melanie Subbiah"
                    },
                    {
                        "name": "Faisal Ladhak"
                    },
                    {
                        "name": "Akankshya Mishra"
                    },
                    {
                        "name": "Griffin Adams"
                    },
                    {
                        "name": "Lydia B. Chilton"
                    },
                    {
                        "name": "Kathleen McKeown"
                    }
                ],
                "author_detail": {
                    "name": "Kathleen McKeown"
                },
                "author": "Kathleen McKeown",
                "arxiv_comment": "EMNLP Main 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.06501v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.06501v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13727v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13727v2",
                "updated": "2025-04-01T16:24:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    16,
                    24,
                    24,
                    1,
                    91,
                    0
                ],
                "published": "2024-10-17T16:33:01Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    16,
                    33,
                    1,
                    3,
                    291,
                    0
                ],
                "title": "LLM-Human Pipeline for Cultural Context Grounding of Conversations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Human Pipeline for Cultural Context Grounding of Conversations"
                },
                "summary": "Conversations often adhere to well-understood social norms that vary across\ncultures. For example, while \"addressing parents by name\" is commonplace in the\nWest, it is rare in most Asian cultures. Adherence or violation of such norms\noften dictates the tenor of conversations. Humans are able to navigate social\nsituations requiring cultural awareness quite adeptly. However, it is a hard\ntask for NLP models.\n  In this paper, we tackle this problem by introducing a \"Cultural Context\nSchema\" for conversations. It comprises (1) conversational information such as\nemotions, dialogue acts, etc., and (2) cultural information such as social\nnorms, violations, etc. We generate ~110k social norm and violation\ndescriptions for ~23k conversations from Chinese culture using LLMs. We refine\nthem using automated verification strategies which are evaluated against\nculturally aware human judgements. We organize these descriptions into\nmeaningful structures we call \"Norm Concepts\", using an interactive\nhuman-in-loop framework. We ground the norm concepts and the descriptions in\nconversations using symbolic annotation. Finally, we use the obtained dataset\nfor downstream tasks such as emotion, sentiment, and dialogue act detection. We\nshow that it significantly improves the empirical performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversations often adhere to well-understood social norms that vary across\ncultures. For example, while \"addressing parents by name\" is commonplace in the\nWest, it is rare in most Asian cultures. Adherence or violation of such norms\noften dictates the tenor of conversations. Humans are able to navigate social\nsituations requiring cultural awareness quite adeptly. However, it is a hard\ntask for NLP models.\n  In this paper, we tackle this problem by introducing a \"Cultural Context\nSchema\" for conversations. It comprises (1) conversational information such as\nemotions, dialogue acts, etc., and (2) cultural information such as social\nnorms, violations, etc. We generate ~110k social norm and violation\ndescriptions for ~23k conversations from Chinese culture using LLMs. We refine\nthem using automated verification strategies which are evaluated against\nculturally aware human judgements. We organize these descriptions into\nmeaningful structures we call \"Norm Concepts\", using an interactive\nhuman-in-loop framework. We ground the norm concepts and the descriptions in\nconversations using symbolic annotation. Finally, we use the obtained dataset\nfor downstream tasks such as emotion, sentiment, and dialogue act detection. We\nshow that it significantly improves the empirical performance."
                },
                "authors": [
                    {
                        "name": "Rajkumar Pujari"
                    },
                    {
                        "name": "Dan Goldwasser"
                    }
                ],
                "author_detail": {
                    "name": "Dan Goldwasser"
                },
                "author": "Dan Goldwasser",
                "arxiv_comment": "Oral at NAACL 2025 Main conference. Albuquerque, USA. Apr 29 - May 4,\n  2025. 19 pages, 9 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13727v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13727v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14642v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14642v2",
                "updated": "2025-04-01T16:18:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    16,
                    18,
                    55,
                    1,
                    91,
                    0
                ],
                "published": "2024-12-19T08:51:16Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    8,
                    51,
                    16,
                    3,
                    354,
                    0
                ],
                "title": "TOMG-Bench: Evaluating LLMs on Text-based Open Molecule Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TOMG-Bench: Evaluating LLMs on Text-based Open Molecule Generation"
                },
                "summary": "In this paper, we propose Text-based Open Molecule Generation Benchmark\n(TOMG-Bench), the first benchmark to evaluate the open-domain molecule\ngeneration capability of LLMs. TOMG-Bench encompasses a dataset of three major\ntasks: molecule editing (MolEdit), molecule optimization (MolOpt), and\ncustomized molecule generation (MolCustom). Each major task further contains\nthree subtasks, while each subtask comprises 5,000 test samples. Given the\ninherent complexity of open molecule generation evaluation, we also developed\nan automated evaluation system that helps measure both the quality and the\naccuracy of the generated molecules. Our comprehensive benchmarking of 25 LLMs\nreveals the current limitations as well as potential areas for improvement in\ntext-guided molecule discovery. Furthermore, we propose OpenMolIns, a\nspecialized instruction tuning dataset established for solving challenges\nraised by TOMG-Bench. Fine-tuned on OpenMolIns, Llama3.1-8B could outperform\nall the open-source general LLMs, even surpassing GPT-3.5-turbo by 46.5\\% on\nTOMG-Bench. Our codes and datasets are available through\nhttps://github.com/phenixace/TOMG-Bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose Text-based Open Molecule Generation Benchmark\n(TOMG-Bench), the first benchmark to evaluate the open-domain molecule\ngeneration capability of LLMs. TOMG-Bench encompasses a dataset of three major\ntasks: molecule editing (MolEdit), molecule optimization (MolOpt), and\ncustomized molecule generation (MolCustom). Each major task further contains\nthree subtasks, while each subtask comprises 5,000 test samples. Given the\ninherent complexity of open molecule generation evaluation, we also developed\nan automated evaluation system that helps measure both the quality and the\naccuracy of the generated molecules. Our comprehensive benchmarking of 25 LLMs\nreveals the current limitations as well as potential areas for improvement in\ntext-guided molecule discovery. Furthermore, we propose OpenMolIns, a\nspecialized instruction tuning dataset established for solving challenges\nraised by TOMG-Bench. Fine-tuned on OpenMolIns, Llama3.1-8B could outperform\nall the open-source general LLMs, even surpassing GPT-3.5-turbo by 46.5\\% on\nTOMG-Bench. Our codes and datasets are available through\nhttps://github.com/phenixace/TOMG-Bench."
                },
                "authors": [
                    {
                        "name": "Jiatong Li"
                    },
                    {
                        "name": "Junxian Li"
                    },
                    {
                        "name": "Yunqing Liu"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "arxiv_comment": "The first benchmark for text-based open molecule generation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14642v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14642v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13548v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13548v5",
                "updated": "2025-04-01T16:15:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    16,
                    15,
                    2,
                    1,
                    91,
                    0
                ],
                "published": "2024-12-18T06:49:46Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    6,
                    49,
                    46,
                    2,
                    353,
                    0
                ],
                "title": "TelePreview: A User-Friendly Teleoperation System with Virtual Arm\n  Assistance for Enhanced Effectiveness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TelePreview: A User-Friendly Teleoperation System with Virtual Arm\n  Assistance for Enhanced Effectiveness"
                },
                "summary": "Teleoperation provides an effective way to collect robot data, which is\ncrucial for learning from demonstrations. In this field, teleoperation faces\nseveral key challenges: user-friendliness for new users, safety assurance, and\ntransferability across different platforms. While collecting real robot\ndexterous manipulation data by teleoperation to train robots has shown\nimpressive results on diverse tasks, due to the morphological differences\nbetween human and robot hands, it is not only hard for new users to understand\nthe action mapping but also raises potential safety concerns during operation.\nTo address these limitations, we introduce TelePreview. This teleoperation\nsystem offers real-time visual feedback on robot actions based on human user\ninputs, with a total hardware cost of less than $1,000. TelePreview allows the\nuser to see a virtual robot that represents the outcome of the user's next\nmovement. By enabling flexible switching between command visualization and\nactual execution, this system helps new users learn how to demonstrate quickly\nand safely. We demonstrate that it outperforms other teleoperation systems\nacross five tasks, emphasize its ease of use, and highlight its straightforward\ndeployment across diverse robotic platforms. We release our code and a\ndeployment document on our website\nhttps://nus-lins-lab.github.io/telepreview-web/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teleoperation provides an effective way to collect robot data, which is\ncrucial for learning from demonstrations. In this field, teleoperation faces\nseveral key challenges: user-friendliness for new users, safety assurance, and\ntransferability across different platforms. While collecting real robot\ndexterous manipulation data by teleoperation to train robots has shown\nimpressive results on diverse tasks, due to the morphological differences\nbetween human and robot hands, it is not only hard for new users to understand\nthe action mapping but also raises potential safety concerns during operation.\nTo address these limitations, we introduce TelePreview. This teleoperation\nsystem offers real-time visual feedback on robot actions based on human user\ninputs, with a total hardware cost of less than $1,000. TelePreview allows the\nuser to see a virtual robot that represents the outcome of the user's next\nmovement. By enabling flexible switching between command visualization and\nactual execution, this system helps new users learn how to demonstrate quickly\nand safely. We demonstrate that it outperforms other teleoperation systems\nacross five tasks, emphasize its ease of use, and highlight its straightforward\ndeployment across diverse robotic platforms. We release our code and a\ndeployment document on our website\nhttps://nus-lins-lab.github.io/telepreview-web/."
                },
                "authors": [
                    {
                        "name": "Jingxiang Guo"
                    },
                    {
                        "name": "Jiayu Luo"
                    },
                    {
                        "name": "Zhenyu Wei"
                    },
                    {
                        "name": "Yiwen Hou"
                    },
                    {
                        "name": "Zhixuan Xu"
                    },
                    {
                        "name": "Xiaoyi Lin"
                    },
                    {
                        "name": "Chongkai Gao"
                    },
                    {
                        "name": "Lin Shao"
                    }
                ],
                "author_detail": {
                    "name": "Lin Shao"
                },
                "author": "Lin Shao",
                "arxiv_comment": "In submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13548v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13548v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14561v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14561v4",
                "updated": "2025-04-01T16:04:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    16,
                    4,
                    53,
                    1,
                    91,
                    0
                ],
                "published": "2024-07-18T17:59:01Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    17,
                    59,
                    1,
                    3,
                    200,
                    0
                ],
                "title": "NNsight and NDIF: Democratizing Access to Open-Weight Foundation Model\n  Internals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NNsight and NDIF: Democratizing Access to Open-Weight Foundation Model\n  Internals"
                },
                "summary": "We introduce NNsight and NDIF, technologies that work in tandem to enable\nscientific study of the representations and computations learned by very large\nneural networks. NNsight is an open-source system that extends PyTorch to\nintroduce deferred remote execution. The National Deep Inference Fabric (NDIF)\nis a scalable inference service that executes NNsight requests, allowing users\nto share GPU resources and pretrained models. These technologies are enabled by\nthe Intervention Graph, an architecture developed to decouple experimental\ndesign from model runtime. Together, this framework provides transparent and\nefficient access to the internals of deep neural networks such as very large\nlanguage models (LLMs) without imposing the cost or complexity of hosting\ncustomized models individually. We conduct a quantitative survey of the machine\nlearning literature that reveals a growing gap in the study of the internals of\nlarge-scale AI. We demonstrate the design and use of our framework to address\nthis gap by enabling a range of research methods on huge models. Finally, we\nconduct benchmarks to compare performance with previous approaches.\n  Code, documentation, and tutorials are available at https://nnsight.net/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce NNsight and NDIF, technologies that work in tandem to enable\nscientific study of the representations and computations learned by very large\nneural networks. NNsight is an open-source system that extends PyTorch to\nintroduce deferred remote execution. The National Deep Inference Fabric (NDIF)\nis a scalable inference service that executes NNsight requests, allowing users\nto share GPU resources and pretrained models. These technologies are enabled by\nthe Intervention Graph, an architecture developed to decouple experimental\ndesign from model runtime. Together, this framework provides transparent and\nefficient access to the internals of deep neural networks such as very large\nlanguage models (LLMs) without imposing the cost or complexity of hosting\ncustomized models individually. We conduct a quantitative survey of the machine\nlearning literature that reveals a growing gap in the study of the internals of\nlarge-scale AI. We demonstrate the design and use of our framework to address\nthis gap by enabling a range of research methods on huge models. Finally, we\nconduct benchmarks to compare performance with previous approaches.\n  Code, documentation, and tutorials are available at https://nnsight.net/."
                },
                "authors": [
                    {
                        "name": "Jaden Fiotto-Kaufman"
                    },
                    {
                        "name": "Alexander R. Loftus"
                    },
                    {
                        "name": "Eric Todd"
                    },
                    {
                        "name": "Jannik Brinkmann"
                    },
                    {
                        "name": "Koyena Pal"
                    },
                    {
                        "name": "Dmitrii Troitskii"
                    },
                    {
                        "name": "Michael Ripa"
                    },
                    {
                        "name": "Adam Belfki"
                    },
                    {
                        "name": "Can Rager"
                    },
                    {
                        "name": "Caden Juang"
                    },
                    {
                        "name": "Aaron Mueller"
                    },
                    {
                        "name": "Samuel Marks"
                    },
                    {
                        "name": "Arnab Sen Sharma"
                    },
                    {
                        "name": "Francesca Lucchetti"
                    },
                    {
                        "name": "Nikhil Prakash"
                    },
                    {
                        "name": "Carla Brodley"
                    },
                    {
                        "name": "Arjun Guha"
                    },
                    {
                        "name": "Jonathan Bell"
                    },
                    {
                        "name": "Byron C. Wallace"
                    },
                    {
                        "name": "David Bau"
                    }
                ],
                "author_detail": {
                    "name": "David Bau"
                },
                "author": "David Bau",
                "arxiv_comment": "Code at https://nnsight.net",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14561v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14561v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.04197v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.04197v3",
                "updated": "2025-04-01T16:02:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    16,
                    2,
                    3,
                    1,
                    91,
                    0
                ],
                "published": "2024-03-07T03:58:28Z",
                "published_parsed": [
                    2024,
                    3,
                    7,
                    3,
                    58,
                    28,
                    3,
                    67,
                    0
                ],
                "title": "Large Language Models are In-Context Molecule Learners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are In-Context Molecule Learners"
                },
                "summary": "Large Language Models (LLMs) have demonstrated exceptional performance in\nbiochemical tasks, especially the molecule caption translation task, which aims\nto bridge the gap between molecules and natural language texts. However,\nprevious methods in adapting LLMs to the molecule-caption translation task\nrequired extra domain-specific pre-training stages, suffered weak alignment\nbetween molecular and textual spaces, or imposed stringent demands on the scale\nof LLMs. To resolve the challenges, we propose In-Context Molecule Adaptation\n(ICMA), as a new paradigm allowing LLMs to learn the molecule-text alignment\nfrom context examples via In-Context Molecule Tuning. Specifically, ICMA\nincorporates the following three stages: Hybrid Context Retrieval,\nPost-retrieval Re-ranking, and In-context Molecule Tuning. Initially, Hybrid\nContext Retrieval utilizes BM25 Caption Retrieval and Molecule Graph Retrieval\nto retrieve similar informative context examples. Additionally, Post-retrieval\nRe-ranking is composed of Sequence Reversal and Random Walk selection to\nfurther improve the quality of retrieval results. Finally, In-Context Molecule\nTuning unlocks the in-context learning and reasoning capability of LLMs with\nthe retrieved examples and adapts the parameters of LLMs for better alignment\nbetween molecules and texts. Experimental results demonstrate that ICMA can\nempower LLMs to achieve state-of-the-art or comparable performance without\nextra training corpora and intricate structures, showing that LLMs are\ninherently in-context molecule learners.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated exceptional performance in\nbiochemical tasks, especially the molecule caption translation task, which aims\nto bridge the gap between molecules and natural language texts. However,\nprevious methods in adapting LLMs to the molecule-caption translation task\nrequired extra domain-specific pre-training stages, suffered weak alignment\nbetween molecular and textual spaces, or imposed stringent demands on the scale\nof LLMs. To resolve the challenges, we propose In-Context Molecule Adaptation\n(ICMA), as a new paradigm allowing LLMs to learn the molecule-text alignment\nfrom context examples via In-Context Molecule Tuning. Specifically, ICMA\nincorporates the following three stages: Hybrid Context Retrieval,\nPost-retrieval Re-ranking, and In-context Molecule Tuning. Initially, Hybrid\nContext Retrieval utilizes BM25 Caption Retrieval and Molecule Graph Retrieval\nto retrieve similar informative context examples. Additionally, Post-retrieval\nRe-ranking is composed of Sequence Reversal and Random Walk selection to\nfurther improve the quality of retrieval results. Finally, In-Context Molecule\nTuning unlocks the in-context learning and reasoning capability of LLMs with\nthe retrieved examples and adapts the parameters of LLMs for better alignment\nbetween molecules and texts. Experimental results demonstrate that ICMA can\nempower LLMs to achieve state-of-the-art or comparable performance without\nextra training corpora and intricate structures, showing that LLMs are\ninherently in-context molecule learners."
                },
                "authors": [
                    {
                        "name": "Jiatong Li"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Zhihao Ding"
                    },
                    {
                        "name": "Wenqi Fan"
                    },
                    {
                        "name": "Yuqiang Li"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "arxiv_comment": "Accepted by IEEE TKDE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.04197v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.04197v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12481v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12481v2",
                "updated": "2025-04-01T15:16:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    15,
                    16,
                    34,
                    1,
                    91,
                    0
                ],
                "published": "2024-07-17T11:06:27Z",
                "published_parsed": [
                    2024,
                    7,
                    17,
                    11,
                    6,
                    27,
                    2,
                    199,
                    0
                ],
                "title": "Krutrim LLM: A Novel Tokenization Strategy for Multilingual Indic\n  Languages with Petabyte-Scale Data Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Krutrim LLM: A Novel Tokenization Strategy for Multilingual Indic\n  Languages with Petabyte-Scale Data Processing"
                },
                "summary": "We present a novel approach to data preparation for developing multilingual\nIndic large language model. Our meticulous data acquisition spans open-source\nand proprietary sources, including Common Crawl, Indic books, news articles,\nand Wikipedia, ensuring a diverse and rich linguistic representation. For each\nIndic language, we design a custom preprocessing pipeline to effectively\neliminate redundant and low-quality text content. Additionally, we perform\ndeduplication on Common Crawl data to address the redundancy present in 70% of\nthe crawled web pages. This study focuses on developing high-quality data,\noptimizing tokenization for our multilingual dataset for Indic large language\nmodels with 3B and 7B parameters, engineered for superior performance in Indic\nlanguages. We introduce a novel multilingual tokenizer training strategy,\ndemonstrating our custom-trained Indic tokenizer outperforms the\nstate-of-the-art OpenAI Tiktoken tokenizer, achieving a superior token-to-word\nratio for Indic languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel approach to data preparation for developing multilingual\nIndic large language model. Our meticulous data acquisition spans open-source\nand proprietary sources, including Common Crawl, Indic books, news articles,\nand Wikipedia, ensuring a diverse and rich linguistic representation. For each\nIndic language, we design a custom preprocessing pipeline to effectively\neliminate redundant and low-quality text content. Additionally, we perform\ndeduplication on Common Crawl data to address the redundancy present in 70% of\nthe crawled web pages. This study focuses on developing high-quality data,\noptimizing tokenization for our multilingual dataset for Indic large language\nmodels with 3B and 7B parameters, engineered for superior performance in Indic\nlanguages. We introduce a novel multilingual tokenizer training strategy,\ndemonstrating our custom-trained Indic tokenizer outperforms the\nstate-of-the-art OpenAI Tiktoken tokenizer, achieving a superior token-to-word\nratio for Indic languages."
                },
                "authors": [
                    {
                        "name": "Rahul Kumar"
                    },
                    {
                        "name": "Shubham Kakde"
                    },
                    {
                        "name": "Divyansh Rajput"
                    },
                    {
                        "name": "Daud Ibrahim"
                    },
                    {
                        "name": "Rishabh Nahata"
                    },
                    {
                        "name": "Pidathala Sowjanya"
                    },
                    {
                        "name": "Deepak Kumarr"
                    },
                    {
                        "name": "Gautam Bhargava"
                    },
                    {
                        "name": "Chandra Khatri"
                    }
                ],
                "author_detail": {
                    "name": "Chandra Khatri"
                },
                "author": "Chandra Khatri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12481v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12481v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15035v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15035v2",
                "updated": "2025-04-01T15:02:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    15,
                    2,
                    40,
                    1,
                    91,
                    0
                ],
                "published": "2024-12-19T16:46:54Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    16,
                    46,
                    54,
                    3,
                    354,
                    0
                ],
                "title": "LLMs Lost in Translation: M-ALERT uncovers Cross-Linguistic Safety Gaps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Lost in Translation: M-ALERT uncovers Cross-Linguistic Safety Gaps"
                },
                "summary": "Building safe Large Language Models (LLMs) across multiple languages is\nessential in ensuring both safe access and linguistic diversity. To this end,\nwe introduce M-ALERT, a multilingual benchmark that evaluates the safety of\nLLMs in five languages: English, French, German, Italian, and Spanish. M-ALERT\nincludes 15k high-quality prompts per language, totaling 75k, following the\ndetailed ALERT taxonomy. Our extensive experiments on 10 state-of-the-art LLMs\nhighlight the importance of language-specific safety analysis, revealing that\nmodels often exhibit significant inconsistencies in safety across languages and\ncategories. For instance, Llama3.2 shows high unsafety in the category\ncrime_tax for Italian but remains safe in other languages. Similar differences\ncan be observed across all models. In contrast, certain categories, such as\nsubstance_cannabis and crime_propaganda, consistently trigger unsafe responses\nacross models and languages. These findings underscore the need for robust\nmultilingual safety practices in LLMs to ensure safe and responsible usage\nacross diverse user communities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building safe Large Language Models (LLMs) across multiple languages is\nessential in ensuring both safe access and linguistic diversity. To this end,\nwe introduce M-ALERT, a multilingual benchmark that evaluates the safety of\nLLMs in five languages: English, French, German, Italian, and Spanish. M-ALERT\nincludes 15k high-quality prompts per language, totaling 75k, following the\ndetailed ALERT taxonomy. Our extensive experiments on 10 state-of-the-art LLMs\nhighlight the importance of language-specific safety analysis, revealing that\nmodels often exhibit significant inconsistencies in safety across languages and\ncategories. For instance, Llama3.2 shows high unsafety in the category\ncrime_tax for Italian but remains safe in other languages. Similar differences\ncan be observed across all models. In contrast, certain categories, such as\nsubstance_cannabis and crime_propaganda, consistently trigger unsafe responses\nacross models and languages. These findings underscore the need for robust\nmultilingual safety practices in LLMs to ensure safe and responsible usage\nacross diverse user communities."
                },
                "authors": [
                    {
                        "name": "Felix Friedrich"
                    },
                    {
                        "name": "Simone Tedeschi"
                    },
                    {
                        "name": "Patrick Schramowski"
                    },
                    {
                        "name": "Manuel Brack"
                    },
                    {
                        "name": "Roberto Navigli"
                    },
                    {
                        "name": "Huu Nguyen"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Kristian Kersting"
                    }
                ],
                "author_detail": {
                    "name": "Kristian Kersting"
                },
                "author": "Kristian Kersting",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15035v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15035v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23829v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23829v2",
                "updated": "2025-04-01T14:48:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    14,
                    48,
                    2,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-31T08:22:49Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    8,
                    22,
                    49,
                    0,
                    90,
                    0
                ],
                "title": "Crossing the Reward Bridge: Expanding RL with Verifiable Rewards Across\n  Diverse Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Crossing the Reward Bridge: Expanding RL with Verifiable Rewards Across\n  Diverse Domains"
                },
                "summary": "Reinforcement learning with verifiable rewards (RLVR) has demonstrated\nsignificant success in enhancing mathematical reasoning and coding performance\nof large language models (LLMs), especially when structured reference answers\nare accessible for verification. However, its extension to broader, less\nstructured domains remains unexplored. In this work, we investigate the\neffectiveness and scalability of RLVR across diverse real-world domains\nincluding medicine, chemistry, psychology, economics, and education, where\nstructured reference answers are typically unavailable. We reveal that binary\nverification judgments on broad-domain tasks exhibit high consistency across\nvarious LLMs provided expert-written reference answers exist. Motivated by this\nfinding, we utilize a generative scoring technique that yields soft,\nmodel-based reward signals to overcome limitations posed by binary\nverifications, especially in free-form, unstructured answer scenarios. We\nfurther demonstrate the feasibility of training cross-domain generative reward\nmodels using relatively small (7B) LLMs without the need for extensive\ndomain-specific annotation. Through comprehensive experiments, our RLVR\nframework establishes clear performance gains, significantly outperforming\nstate-of-the-art open-source aligned models such as Qwen2.5-72B and\nDeepSeek-R1-Distill-Qwen-32B across domains in free-form settings. Our approach\nnotably enhances the robustness, flexibility, and scalability of RLVR,\nrepresenting a substantial step towards practical reinforcement learning\napplications in complex, noisy-label scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning with verifiable rewards (RLVR) has demonstrated\nsignificant success in enhancing mathematical reasoning and coding performance\nof large language models (LLMs), especially when structured reference answers\nare accessible for verification. However, its extension to broader, less\nstructured domains remains unexplored. In this work, we investigate the\neffectiveness and scalability of RLVR across diverse real-world domains\nincluding medicine, chemistry, psychology, economics, and education, where\nstructured reference answers are typically unavailable. We reveal that binary\nverification judgments on broad-domain tasks exhibit high consistency across\nvarious LLMs provided expert-written reference answers exist. Motivated by this\nfinding, we utilize a generative scoring technique that yields soft,\nmodel-based reward signals to overcome limitations posed by binary\nverifications, especially in free-form, unstructured answer scenarios. We\nfurther demonstrate the feasibility of training cross-domain generative reward\nmodels using relatively small (7B) LLMs without the need for extensive\ndomain-specific annotation. Through comprehensive experiments, our RLVR\nframework establishes clear performance gains, significantly outperforming\nstate-of-the-art open-source aligned models such as Qwen2.5-72B and\nDeepSeek-R1-Distill-Qwen-32B across domains in free-form settings. Our approach\nnotably enhances the robustness, flexibility, and scalability of RLVR,\nrepresenting a substantial step towards practical reinforcement learning\napplications in complex, noisy-label scenarios."
                },
                "authors": [
                    {
                        "name": "Yi Su"
                    },
                    {
                        "name": "Dian Yu"
                    },
                    {
                        "name": "Linfeng Song"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Zhaopeng Tu"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Dong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yu"
                },
                "author": "Dong Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23829v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23829v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13543v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13543v2",
                "updated": "2025-04-01T14:45:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    14,
                    45,
                    22,
                    1,
                    91,
                    0
                ],
                "published": "2024-11-20T18:54:32Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    18,
                    54,
                    32,
                    2,
                    325,
                    0
                ],
                "title": "BALROG: Benchmarking Agentic LLM and VLM Reasoning On Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BALROG: Benchmarking Agentic LLM and VLM Reasoning On Games"
                },
                "summary": "Large Language Models (LLMs) and Vision Language Models (VLMs) possess\nextensive knowledge and exhibit promising reasoning abilities, however, they\nstill struggle to perform well in complex, dynamic environments. Real-world\ntasks require handling intricate interactions, advanced spatial reasoning,\nlong-term planning, and continuous exploration of new strategies-areas in which\nwe lack effective methodologies for comprehensively evaluating these\ncapabilities. To address this gap, we introduce BALROG, a novel benchmark\ndesigned to assess the agentic capabilities of LLMs and VLMs through a diverse\nset of challenging games. Our benchmark incorporates a range of existing\nreinforcement learning environments with varying levels of difficulty,\nincluding tasks that are solvable by non-expert humans in seconds to extremely\nchallenging ones that may take years to master (e.g., the NetHack Learning\nEnvironment). We devise fine-grained metrics to measure performance and conduct\nan extensive evaluation of several popular open-source and closed-source LLMs\nand VLMs. Our findings indicate that while current models achieve partial\nsuccess in the easier games, they struggle significantly with more challenging\ntasks. Notably, we observe severe deficiencies in vision-based decision-making,\nas several models perform worse when visual representations of the environments\nare provided. We release BALROG as an open and user-friendly benchmark to\nfacilitate future research and development in the agentic community. Code and\nLeaderboard at balrogai.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) and Vision Language Models (VLMs) possess\nextensive knowledge and exhibit promising reasoning abilities, however, they\nstill struggle to perform well in complex, dynamic environments. Real-world\ntasks require handling intricate interactions, advanced spatial reasoning,\nlong-term planning, and continuous exploration of new strategies-areas in which\nwe lack effective methodologies for comprehensively evaluating these\ncapabilities. To address this gap, we introduce BALROG, a novel benchmark\ndesigned to assess the agentic capabilities of LLMs and VLMs through a diverse\nset of challenging games. Our benchmark incorporates a range of existing\nreinforcement learning environments with varying levels of difficulty,\nincluding tasks that are solvable by non-expert humans in seconds to extremely\nchallenging ones that may take years to master (e.g., the NetHack Learning\nEnvironment). We devise fine-grained metrics to measure performance and conduct\nan extensive evaluation of several popular open-source and closed-source LLMs\nand VLMs. Our findings indicate that while current models achieve partial\nsuccess in the easier games, they struggle significantly with more challenging\ntasks. Notably, we observe severe deficiencies in vision-based decision-making,\nas several models perform worse when visual representations of the environments\nare provided. We release BALROG as an open and user-friendly benchmark to\nfacilitate future research and development in the agentic community. Code and\nLeaderboard at balrogai.com."
                },
                "authors": [
                    {
                        "name": "Davide Paglieri"
                    },
                    {
                        "name": "Bartłomiej Cupiał"
                    },
                    {
                        "name": "Samuel Coward"
                    },
                    {
                        "name": "Ulyana Piterbarg"
                    },
                    {
                        "name": "Maciej Wolczyk"
                    },
                    {
                        "name": "Akbir Khan"
                    },
                    {
                        "name": "Eduardo Pignatelli"
                    },
                    {
                        "name": "Łukasz Kuciński"
                    },
                    {
                        "name": "Lerrel Pinto"
                    },
                    {
                        "name": "Rob Fergus"
                    },
                    {
                        "name": "Jakob Nicolaus Foerster"
                    },
                    {
                        "name": "Jack Parker-Holder"
                    },
                    {
                        "name": "Tim Rocktäschel"
                    }
                ],
                "author_detail": {
                    "name": "Tim Rocktäschel"
                },
                "author": "Tim Rocktäschel",
                "arxiv_comment": "Published as a conference paper at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13543v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13543v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03275v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03275v3",
                "updated": "2025-04-01T14:33:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    14,
                    33,
                    53,
                    1,
                    91,
                    0
                ],
                "published": "2024-04-04T07:59:24Z",
                "published_parsed": [
                    2024,
                    4,
                    4,
                    7,
                    59,
                    24,
                    3,
                    95,
                    0
                ],
                "title": "DELTA: Decomposed Efficient Long-Term Robot Task Planning using Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DELTA: Decomposed Efficient Long-Term Robot Task Planning using Large\n  Language Models"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have sparked a revolution\nacross many research fields. In robotics, the integration of common-sense\nknowledge from LLMs into task and motion planning has drastically advanced the\nfield by unlocking unprecedented levels of context awareness. Despite their\nvast collection of knowledge, large language models may generate infeasible\nplans due to hallucinations or missing domain information. To address these\nchallenges and improve plan feasibility and computational efficiency, we\nintroduce DELTA, a novel LLM-informed task planning approach. By using scene\ngraphs as environment representations within LLMs, DELTA achieves rapid\ngeneration of precise planning problem descriptions. To enhance planning\nperformance, DELTA decomposes long-term task goals with LLMs into an\nautoregressive sequence of sub-goals, enabling automated task planners to\nefficiently solve complex problems. In our extensive evaluation, we show that\nDELTA enables an efficient and fully automatic task planning pipeline,\nachieving higher planning success rates and significantly shorter planning\ntimes compared to the state of the art. Project webpage:\nhttps://delta-llm.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have sparked a revolution\nacross many research fields. In robotics, the integration of common-sense\nknowledge from LLMs into task and motion planning has drastically advanced the\nfield by unlocking unprecedented levels of context awareness. Despite their\nvast collection of knowledge, large language models may generate infeasible\nplans due to hallucinations or missing domain information. To address these\nchallenges and improve plan feasibility and computational efficiency, we\nintroduce DELTA, a novel LLM-informed task planning approach. By using scene\ngraphs as environment representations within LLMs, DELTA achieves rapid\ngeneration of precise planning problem descriptions. To enhance planning\nperformance, DELTA decomposes long-term task goals with LLMs into an\nautoregressive sequence of sub-goals, enabling automated task planners to\nefficiently solve complex problems. In our extensive evaluation, we show that\nDELTA enables an efficient and fully automatic task planning pipeline,\nachieving higher planning success rates and significantly shorter planning\ntimes compared to the state of the art. Project webpage:\nhttps://delta-llm.github.io/"
                },
                "authors": [
                    {
                        "name": "Yuchen Liu"
                    },
                    {
                        "name": "Luigi Palmieri"
                    },
                    {
                        "name": "Sebastian Koch"
                    },
                    {
                        "name": "Ilche Georgievski"
                    },
                    {
                        "name": "Marco Aiello"
                    }
                ],
                "author_detail": {
                    "name": "Marco Aiello"
                },
                "author": "Marco Aiello",
                "arxiv_comment": "Accepted at ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03275v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03275v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19548v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19548v2",
                "updated": "2025-04-01T14:26:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    14,
                    26,
                    43,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-25T11:02:08Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    11,
                    2,
                    8,
                    1,
                    84,
                    0
                ],
                "title": "On-Chain Analysis of Smart Contract Dependency Risks on Ethereum",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On-Chain Analysis of Smart Contract Dependency Risks on Ethereum"
                },
                "summary": "In this paper, we present the first large-scale empirical study of smart\ncontract dependencies, analyzing over 41 million contracts and 11 billion\ninteractions on Ethereum up to December 2024. Our results yield four key\ninsights: (1) 59% of contract transactions involve multiple contracts (median\nof 4 per transaction in 2024) indicating potential smart contract dependency\nrisks; (2) the ecosystem exhibits extreme centralization, with just 11 (0.001%)\ndeployers controlling 20.5 million (50%) of alive contracts, with major risks\nrelated to factory contracts and deployer privileges; (3) three most\ndepended-upon contracts are mutable, meaning large parts of the ecosystem rely\non contracts that can be altered at any time, which is a significant risk, (4)\nactual smart contract protocol dependencies are significantly more complex than\nofficially documented, undermining Ethereum's transparency ethos, and creating\nunnecessary attack surface. Our work provides the first large-scale empirical\nfoundation for understanding smart contract dependency risks, offering crucial\ninsights for developers, users, and security researchers in the blockchain\nspace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present the first large-scale empirical study of smart\ncontract dependencies, analyzing over 41 million contracts and 11 billion\ninteractions on Ethereum up to December 2024. Our results yield four key\ninsights: (1) 59% of contract transactions involve multiple contracts (median\nof 4 per transaction in 2024) indicating potential smart contract dependency\nrisks; (2) the ecosystem exhibits extreme centralization, with just 11 (0.001%)\ndeployers controlling 20.5 million (50%) of alive contracts, with major risks\nrelated to factory contracts and deployer privileges; (3) three most\ndepended-upon contracts are mutable, meaning large parts of the ecosystem rely\non contracts that can be altered at any time, which is a significant risk, (4)\nactual smart contract protocol dependencies are significantly more complex than\nofficially documented, undermining Ethereum's transparency ethos, and creating\nunnecessary attack surface. Our work provides the first large-scale empirical\nfoundation for understanding smart contract dependency risks, offering crucial\ninsights for developers, users, and security researchers in the blockchain\nspace."
                },
                "authors": [
                    {
                        "name": "Monica Jin"
                    },
                    {
                        "name": "Raphina Liu"
                    },
                    {
                        "name": "Martin Monperrus"
                    }
                ],
                "author_detail": {
                    "name": "Martin Monperrus"
                },
                "author": "Martin Monperrus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19548v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19548v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05330v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05330v2",
                "updated": "2025-04-01T14:23:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    14,
                    23,
                    52,
                    1,
                    91,
                    0
                ],
                "published": "2024-12-04T18:46:05Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    18,
                    46,
                    5,
                    2,
                    339,
                    0
                ],
                "title": "Patient-specific prediction of glioblastoma growth via reduced order\n  modeling and neural networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Patient-specific prediction of glioblastoma growth via reduced order\n  modeling and neural networks"
                },
                "summary": "Glioblastoma is among the most aggressive brain tumors in adults,\ncharacterized by patient-specific invasion patterns driven by the underlying\nbrain microstructure. In this work, we present a proof-of-concept for a\nmathematical model of GBL growth, enabling real-time prediction and\npatient-specific parameter identification from longitudinal neuroimaging data.\n  The framework exploits a diffuse-interface mathematical model to describe the\ntumor evolution and a reduced-order modeling strategy, relying on proper\northogonal decomposition, trained on synthetic data derived from\npatient-specific brain anatomies reconstructed from magnetic resonance imaging\nand diffusion tensor imaging. A neural network surrogate learns the inverse\nmapping from tumor evolution to model parameters, achieving significant\ncomputational speed-up while preserving high accuracy.\n  To ensure robustness and interpretability, we perform both global and local\nsensitivity analyses, identifying the key biophysical parameters governing\ntumor dynamics and assessing the stability of the inverse problem solution.\nThese results establish a methodological foundation for future clinical\ndeployment of patient-specific digital twins in neuro-oncology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Glioblastoma is among the most aggressive brain tumors in adults,\ncharacterized by patient-specific invasion patterns driven by the underlying\nbrain microstructure. In this work, we present a proof-of-concept for a\nmathematical model of GBL growth, enabling real-time prediction and\npatient-specific parameter identification from longitudinal neuroimaging data.\n  The framework exploits a diffuse-interface mathematical model to describe the\ntumor evolution and a reduced-order modeling strategy, relying on proper\northogonal decomposition, trained on synthetic data derived from\npatient-specific brain anatomies reconstructed from magnetic resonance imaging\nand diffusion tensor imaging. A neural network surrogate learns the inverse\nmapping from tumor evolution to model parameters, achieving significant\ncomputational speed-up while preserving high accuracy.\n  To ensure robustness and interpretability, we perform both global and local\nsensitivity analyses, identifying the key biophysical parameters governing\ntumor dynamics and assessing the stability of the inverse problem solution.\nThese results establish a methodological foundation for future clinical\ndeployment of patient-specific digital twins in neuro-oncology."
                },
                "authors": [
                    {
                        "name": "D. Cerrone"
                    },
                    {
                        "name": "D. Riccobelli"
                    },
                    {
                        "name": "S. Gazzoni"
                    },
                    {
                        "name": "P. Vitullo"
                    },
                    {
                        "name": "F. Ballarin"
                    },
                    {
                        "name": "J. Falco"
                    },
                    {
                        "name": "F. Acerbi"
                    },
                    {
                        "name": "A. Manzoni"
                    },
                    {
                        "name": "P. Zunino"
                    },
                    {
                        "name": "P. Ciarletta"
                    }
                ],
                "author_detail": {
                    "name": "P. Ciarletta"
                },
                "author": "P. Ciarletta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05330v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05330v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.TO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13275v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13275v2",
                "updated": "2025-04-01T14:21:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    14,
                    21,
                    15,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-17T15:27:02Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    27,
                    2,
                    0,
                    76,
                    0
                ],
                "title": "Knowledge-Aware Iterative Retrieval for Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-Aware Iterative Retrieval for Multi-Agent Systems"
                },
                "summary": "We introduce a novel large language model (LLM)-driven agent framework, which\niteratively refines queries and filters contextual evidence by leveraging\ndynamically evolving knowledge. A defining feature of the system is its\ndecoupling of external sources from an internal knowledge cache that is\nprogressively updated to guide both query generation and evidence selection.\nThis design mitigates bias-reinforcement loops and enables dynamic, trackable\nsearch exploration paths, thereby optimizing the trade-off between exploring\ndiverse information and maintaining accuracy through autonomous agent\ndecision-making. Our approach is evaluated on a broad range of open-domain\nquestion answering benchmarks, including multi-step tasks that mirror\nreal-world scenarios where integrating information from multiple sources is\ncritical, especially given the vulnerabilities of LLMs that lack explicit\nreasoning or planning capabilities. The results show that the proposed system\nnot only outperforms single-step baselines regardless of task difficulty but\nalso, compared to conventional iterative retrieval methods, demonstrates\npronounced advantages in complex tasks through precise evidence-based reasoning\nand enhanced efficiency. The proposed system supports both competitive and\ncollaborative sharing of updated context, enabling multi-agent extension. The\nbenefits of multi-agent configurations become especially prominent as task\ndifficulty increases. The number of convergence steps scales with task\ndifficulty, suggesting cost-effective scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel large language model (LLM)-driven agent framework, which\niteratively refines queries and filters contextual evidence by leveraging\ndynamically evolving knowledge. A defining feature of the system is its\ndecoupling of external sources from an internal knowledge cache that is\nprogressively updated to guide both query generation and evidence selection.\nThis design mitigates bias-reinforcement loops and enables dynamic, trackable\nsearch exploration paths, thereby optimizing the trade-off between exploring\ndiverse information and maintaining accuracy through autonomous agent\ndecision-making. Our approach is evaluated on a broad range of open-domain\nquestion answering benchmarks, including multi-step tasks that mirror\nreal-world scenarios where integrating information from multiple sources is\ncritical, especially given the vulnerabilities of LLMs that lack explicit\nreasoning or planning capabilities. The results show that the proposed system\nnot only outperforms single-step baselines regardless of task difficulty but\nalso, compared to conventional iterative retrieval methods, demonstrates\npronounced advantages in complex tasks through precise evidence-based reasoning\nand enhanced efficiency. The proposed system supports both competitive and\ncollaborative sharing of updated context, enabling multi-agent extension. The\nbenefits of multi-agent configurations become especially prominent as task\ndifficulty increases. The number of convergence steps scales with task\ndifficulty, suggesting cost-effective scalability."
                },
                "authors": [
                    {
                        "name": "Seyoung Song"
                    }
                ],
                "author_detail": {
                    "name": "Seyoung Song"
                },
                "author": "Seyoung Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13275v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13275v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.7; I.2.11; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24115v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24115v2",
                "updated": "2025-04-01T14:04:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    14,
                    4,
                    47,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-31T14:06:17Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    14,
                    6,
                    17,
                    0,
                    90,
                    0
                ],
                "title": "TeleAntiFraud-28k: An Audio-Text Slow-Thinking Dataset for Telecom Fraud\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TeleAntiFraud-28k: An Audio-Text Slow-Thinking Dataset for Telecom Fraud\n  Detection"
                },
                "summary": "The detection of telecom fraud faces significant challenges due to the lack\nof high-quality multimodal training data that integrates audio signals with\nreasoning-oriented textual analysis. To address this gap, we present\nTeleAntiFraud-28k, the first open-source audio-text slow-thinking dataset\nspecifically designed for automated telecom fraud analysis. Our dataset is\nconstructed through three strategies: (1) Privacy-preserved text-truth sample\ngeneration using automatically speech recognition (ASR)-transcribed call\nrecordings (with anonymized original audio), ensuring real-world consistency\nthrough text-to-speech (TTS) model regeneration; (2) Semantic enhancement via\nlarge language model (LLM)-based self-instruction sampling on authentic ASR\noutputs to expand scenario coverage; (3) Multi-agent adversarial synthesis that\nsimulates emerging fraud tactics through predefined communication scenarios and\nfraud typologies. The generated dataset contains 28,511 rigorously processed\nspeech-text pairs, complete with detailed annotations for fraud reasoning. The\ndataset is divided into three tasks: scenario classification, fraud detection,\nfraud type classification. Furthermore, we construct TeleAntiFraud-Bench, a\nstandardized evaluation benchmark comprising proportionally sampled instances\nfrom the dataset, to facilitate systematic testing of model performance on\ntelecom fraud detection tasks. We also contribute a production-optimized\nsupervised fine-tuning (SFT) model trained on hybrid real/synthetic data, while\nopen-sourcing the data processing framework to enable community-driven dataset\nexpansion. This work establishes a foundational framework for multimodal\nanti-fraud research while addressing critical challenges in data privacy and\nscenario diversity. The project will be released at\nhttps://github.com/JimmyMa99/TeleAntiFraud.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The detection of telecom fraud faces significant challenges due to the lack\nof high-quality multimodal training data that integrates audio signals with\nreasoning-oriented textual analysis. To address this gap, we present\nTeleAntiFraud-28k, the first open-source audio-text slow-thinking dataset\nspecifically designed for automated telecom fraud analysis. Our dataset is\nconstructed through three strategies: (1) Privacy-preserved text-truth sample\ngeneration using automatically speech recognition (ASR)-transcribed call\nrecordings (with anonymized original audio), ensuring real-world consistency\nthrough text-to-speech (TTS) model regeneration; (2) Semantic enhancement via\nlarge language model (LLM)-based self-instruction sampling on authentic ASR\noutputs to expand scenario coverage; (3) Multi-agent adversarial synthesis that\nsimulates emerging fraud tactics through predefined communication scenarios and\nfraud typologies. The generated dataset contains 28,511 rigorously processed\nspeech-text pairs, complete with detailed annotations for fraud reasoning. The\ndataset is divided into three tasks: scenario classification, fraud detection,\nfraud type classification. Furthermore, we construct TeleAntiFraud-Bench, a\nstandardized evaluation benchmark comprising proportionally sampled instances\nfrom the dataset, to facilitate systematic testing of model performance on\ntelecom fraud detection tasks. We also contribute a production-optimized\nsupervised fine-tuning (SFT) model trained on hybrid real/synthetic data, while\nopen-sourcing the data processing framework to enable community-driven dataset\nexpansion. This work establishes a foundational framework for multimodal\nanti-fraud research while addressing critical challenges in data privacy and\nscenario diversity. The project will be released at\nhttps://github.com/JimmyMa99/TeleAntiFraud."
                },
                "authors": [
                    {
                        "name": "Zhiming Ma"
                    },
                    {
                        "name": "Peidong Wang"
                    },
                    {
                        "name": "Minhua Huang"
                    },
                    {
                        "name": "Jingpeng Wang"
                    },
                    {
                        "name": "Kai Wu"
                    },
                    {
                        "name": "Xiangzhao Lv"
                    },
                    {
                        "name": "Yachun Pang"
                    },
                    {
                        "name": "Yin Yang"
                    },
                    {
                        "name": "Wenjie Tang"
                    },
                    {
                        "name": "Yuchen Kang"
                    }
                ],
                "author_detail": {
                    "name": "Yuchen Kang"
                },
                "author": "Yuchen Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24115v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24115v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05447v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05447v2",
                "updated": "2025-04-01T14:03:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    14,
                    3,
                    15,
                    1,
                    91,
                    0
                ],
                "published": "2024-12-06T22:05:39Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    22,
                    5,
                    39,
                    4,
                    341,
                    0
                ],
                "title": "TOBUGraph: Knowledge Graph-Based Retrieval for Enhanced LLM Performance\n  Beyond RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TOBUGraph: Knowledge Graph-Based Retrieval for Enhanced LLM Performance\n  Beyond RAG"
                },
                "summary": "Retrieval-Augmented Generation (RAG) is one of the leading and most widely\nused techniques for enhancing LLM retrieval capabilities, but it still faces\nsignificant limitations in commercial use cases. RAG primarily relies on the\nquery-chunk text-to-text similarity in the embedding space for retrieval and\ncan fail to capture deeper semantic relationships across chunks, is highly\nsensitive to chunking strategies, and is prone to hallucinations. To address\nthese challenges, we propose TOBUGraph, a graph-based retrieval framework that\nfirst constructs the knowledge graph from unstructured data dynamically and\nautomatically. Using LLMs, TOBUGraph extracts structured knowledge and diverse\nrelationships among data, going beyond RAG's text-to-text similarity. Retrieval\nis achieved through graph traversal, leveraging the extracted relationships and\nstructures to enhance retrieval accuracy, eliminating the need for chunking\nconfigurations while reducing hallucination. We demonstrate TOBUGraph's\neffectiveness in TOBU, a real-world application in production for personal\nmemory organization and retrieval. Our evaluation using real user data\ndemonstrates that TOBUGraph outperforms multiple RAG implementations in both\nprecision and recall, significantly improving user experience through improved\nretrieval accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) is one of the leading and most widely\nused techniques for enhancing LLM retrieval capabilities, but it still faces\nsignificant limitations in commercial use cases. RAG primarily relies on the\nquery-chunk text-to-text similarity in the embedding space for retrieval and\ncan fail to capture deeper semantic relationships across chunks, is highly\nsensitive to chunking strategies, and is prone to hallucinations. To address\nthese challenges, we propose TOBUGraph, a graph-based retrieval framework that\nfirst constructs the knowledge graph from unstructured data dynamically and\nautomatically. Using LLMs, TOBUGraph extracts structured knowledge and diverse\nrelationships among data, going beyond RAG's text-to-text similarity. Retrieval\nis achieved through graph traversal, leveraging the extracted relationships and\nstructures to enhance retrieval accuracy, eliminating the need for chunking\nconfigurations while reducing hallucination. We demonstrate TOBUGraph's\neffectiveness in TOBU, a real-world application in production for personal\nmemory organization and retrieval. Our evaluation using real user data\ndemonstrates that TOBUGraph outperforms multiple RAG implementations in both\nprecision and recall, significantly improving user experience through improved\nretrieval accuracy."
                },
                "authors": [
                    {
                        "name": "Savini Kashmira"
                    },
                    {
                        "name": "Jayanaka L. Dantanarayana"
                    },
                    {
                        "name": "Joshua Brodsky"
                    },
                    {
                        "name": "Ashish Mahendra"
                    },
                    {
                        "name": "Yiping Kang"
                    },
                    {
                        "name": "Krisztian Flautner"
                    },
                    {
                        "name": "Lingjia Tang"
                    },
                    {
                        "name": "Jason Mars"
                    }
                ],
                "author_detail": {
                    "name": "Jason Mars"
                },
                "author": "Jason Mars",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05447v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05447v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06621v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06621v4",
                "updated": "2025-04-01T12:53:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    12,
                    53,
                    30,
                    1,
                    91,
                    0
                ],
                "published": "2024-08-13T04:18:32Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    4,
                    18,
                    32,
                    1,
                    226,
                    0
                ],
                "title": "Towards Robust and Parameter-Efficient Knowledge Unlearning for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Robust and Parameter-Efficient Knowledge Unlearning for LLMs"
                },
                "summary": "Large Language Models (LLMs) have demonstrated strong reasoning and\nmemorization capabilities via pretraining on massive textual corpora. However,\nthis poses risk of privacy and copyright violations, highlighting the need for\nefficient machine unlearning methods that remove sensitive data without\nretraining from scratch. While Gradient Ascent (GA) is commonly used to unlearn\nby reducing the likelihood of generating unwanted content, it leads to unstable\noptimization and catastrophic forgetting of retrained knowledge. We find that\ncombining GA with low-rank adaptation results in poor trade-offs between\ncomputational cost and generative performance. To address these challenges, we\npropose Low-rank Knowledge Unlearning (LoKU), a novel framework that enables\nrobust and efficient unlearning for LLMs. First, we introduce Inverted Hinge\nLoss, which suppresses unwanted tokens while maintaining fluency by boosting\nthe probability of the next most likely token. Second, we develop a\ndata-adaptive initialization for LoRA adapters via low-rank approximation\nweighted with relative Fisher information, thereby focusing updates on\nparameters critical for removing targeted knowledge. Experiments on the\nTraining Data Extraction Challenge dataset using GPT-Neo models as well as on\nthe TOFU benchmark with Phi-1.5B and Llama2-7B models demonstrate that our\napproach effectively removes sensitive information while maintaining reasoning\nand generative capabilities with minimal impact. Our implementation can be\nfound in https://github.com/csm9493/efficient-llm-unlearning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated strong reasoning and\nmemorization capabilities via pretraining on massive textual corpora. However,\nthis poses risk of privacy and copyright violations, highlighting the need for\nefficient machine unlearning methods that remove sensitive data without\nretraining from scratch. While Gradient Ascent (GA) is commonly used to unlearn\nby reducing the likelihood of generating unwanted content, it leads to unstable\noptimization and catastrophic forgetting of retrained knowledge. We find that\ncombining GA with low-rank adaptation results in poor trade-offs between\ncomputational cost and generative performance. To address these challenges, we\npropose Low-rank Knowledge Unlearning (LoKU), a novel framework that enables\nrobust and efficient unlearning for LLMs. First, we introduce Inverted Hinge\nLoss, which suppresses unwanted tokens while maintaining fluency by boosting\nthe probability of the next most likely token. Second, we develop a\ndata-adaptive initialization for LoRA adapters via low-rank approximation\nweighted with relative Fisher information, thereby focusing updates on\nparameters critical for removing targeted knowledge. Experiments on the\nTraining Data Extraction Challenge dataset using GPT-Neo models as well as on\nthe TOFU benchmark with Phi-1.5B and Llama2-7B models demonstrate that our\napproach effectively removes sensitive information while maintaining reasoning\nand generative capabilities with minimal impact. Our implementation can be\nfound in https://github.com/csm9493/efficient-llm-unlearning."
                },
                "authors": [
                    {
                        "name": "Sungmin Cha"
                    },
                    {
                        "name": "Sungjun Cho"
                    },
                    {
                        "name": "Dasol Hwang"
                    },
                    {
                        "name": "Moontae Lee"
                    }
                ],
                "author_detail": {
                    "name": "Moontae Lee"
                },
                "author": "Moontae Lee",
                "arxiv_comment": "ICLR 2025 camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06621v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06621v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23157v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23157v2",
                "updated": "2025-04-01T12:53:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    12,
                    53,
                    16,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-29T17:29:30Z",
                "published_parsed": [
                    2025,
                    3,
                    29,
                    17,
                    29,
                    30,
                    5,
                    88,
                    0
                ],
                "title": "Reasoning-SQL: Reinforcement Learning with SQL Tailored Partial Rewards\n  for Reasoning-Enhanced Text-to-SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning-SQL: Reinforcement Learning with SQL Tailored Partial Rewards\n  for Reasoning-Enhanced Text-to-SQL"
                },
                "summary": "Text-to-SQL is a challenging task involving multiple reasoning-intensive\nsubtasks, including natural language understanding, database schema\ncomprehension, and precise SQL query formulation. Existing approaches often\nrely on handcrafted reasoning paths with inductive biases that can limit their\noverall effectiveness. Motivated by the recent success of reasoning-enhanced\nmodels such as DeepSeek R1 and OpenAI o1, which effectively leverage\nreward-driven self-exploration to enhance reasoning capabilities and\ngeneralization, we propose a novel set of partial rewards tailored specifically\nfor the Text-to-SQL task. Our reward set includes schema-linking, AI feedback,\nn-gram similarity, and syntax check, explicitly designed to address the reward\nsparsity issue prevalent in reinforcement learning (RL). Leveraging group\nrelative policy optimization (GRPO), our approach explicitly encourages large\nlanguage models (LLMs) to develop intrinsic reasoning skills necessary for\naccurate SQL query generation. With models of different sizes, we demonstrate\nthat RL-only training with our proposed rewards consistently achieves higher\naccuracy and superior generalization compared to supervised fine-tuning (SFT).\nRemarkably, our RL-trained 14B-parameter model significantly outperforms larger\nproprietary models, e.g. o3-mini by 4% and Gemini-1.5-Pro-002 by 3% on the BIRD\nbenchmark. These highlight the efficacy of our proposed RL-training framework\nwith partial rewards for enhancing both accuracy and reasoning capabilities in\nText-to-SQL tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-SQL is a challenging task involving multiple reasoning-intensive\nsubtasks, including natural language understanding, database schema\ncomprehension, and precise SQL query formulation. Existing approaches often\nrely on handcrafted reasoning paths with inductive biases that can limit their\noverall effectiveness. Motivated by the recent success of reasoning-enhanced\nmodels such as DeepSeek R1 and OpenAI o1, which effectively leverage\nreward-driven self-exploration to enhance reasoning capabilities and\ngeneralization, we propose a novel set of partial rewards tailored specifically\nfor the Text-to-SQL task. Our reward set includes schema-linking, AI feedback,\nn-gram similarity, and syntax check, explicitly designed to address the reward\nsparsity issue prevalent in reinforcement learning (RL). Leveraging group\nrelative policy optimization (GRPO), our approach explicitly encourages large\nlanguage models (LLMs) to develop intrinsic reasoning skills necessary for\naccurate SQL query generation. With models of different sizes, we demonstrate\nthat RL-only training with our proposed rewards consistently achieves higher\naccuracy and superior generalization compared to supervised fine-tuning (SFT).\nRemarkably, our RL-trained 14B-parameter model significantly outperforms larger\nproprietary models, e.g. o3-mini by 4% and Gemini-1.5-Pro-002 by 3% on the BIRD\nbenchmark. These highlight the efficacy of our proposed RL-training framework\nwith partial rewards for enhancing both accuracy and reasoning capabilities in\nText-to-SQL tasks."
                },
                "authors": [
                    {
                        "name": "Mohammadreza Pourreza"
                    },
                    {
                        "name": "Shayan Talaei"
                    },
                    {
                        "name": "Ruoxi Sun"
                    },
                    {
                        "name": "Xingchen Wan"
                    },
                    {
                        "name": "Hailong Li"
                    },
                    {
                        "name": "Azalia Mirhoseini"
                    },
                    {
                        "name": "Amin Saberi"
                    },
                    {
                        "name": "Sercan \"O. Arik"
                    }
                ],
                "author_detail": {
                    "name": "Sercan \"O. Arik"
                },
                "author": "Sercan \"O. Arik",
                "arxiv_comment": "Mohammadreza Pourreza and Shayan Talaei contributed equally to this\n  work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23157v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23157v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09078v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09078v5",
                "updated": "2025-04-01T12:48:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    12,
                    48,
                    43,
                    1,
                    91,
                    0
                ],
                "published": "2024-12-12T09:01:18Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    9,
                    1,
                    18,
                    3,
                    347,
                    0
                ],
                "title": "Forest-of-Thought: Scaling Test-Time Compute for Enhancing LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forest-of-Thought: Scaling Test-Time Compute for Enhancing LLM Reasoning"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable abilities across\nvarious language tasks, but solving complex reasoning problems remains a\nsignificant challenge. While existing methods, such as Chain-of-Thought (CoT)\nand Tree-of-Thought (ToT), enhance reasoning by decomposing problems or\nstructuring prompts, they typically perform a single pass of reasoning and may\nfail to revisit flawed paths, compromising accuracy. To address this\nlimitation, we propose a novel reasoning framework called Forest-of-Thought\n(FoT), which integrates multiple reasoning trees to leverage collective\ndecision-making for solving complex logical problems. FoT employs sparse\nactivation strategies to select the most relevant reasoning paths, improving\nboth efficiency and accuracy. Additionally, we introduce a dynamic\nself-correction strategy that enables real-time error correction, along with\nconsensus-guided decision-making strategies to optimize both correctness and\ncomputational resources. Experimental results demonstrate that the FoT\nframework, combined with these strategies, significantly enhances the reasoning\ncapabilities of LLMs, enabling them to solve complex tasks with greater\nprecision and efficiency. Code will be available at\nhttps://github.com/iamhankai/Forest-of-Thought.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable abilities across\nvarious language tasks, but solving complex reasoning problems remains a\nsignificant challenge. While existing methods, such as Chain-of-Thought (CoT)\nand Tree-of-Thought (ToT), enhance reasoning by decomposing problems or\nstructuring prompts, they typically perform a single pass of reasoning and may\nfail to revisit flawed paths, compromising accuracy. To address this\nlimitation, we propose a novel reasoning framework called Forest-of-Thought\n(FoT), which integrates multiple reasoning trees to leverage collective\ndecision-making for solving complex logical problems. FoT employs sparse\nactivation strategies to select the most relevant reasoning paths, improving\nboth efficiency and accuracy. Additionally, we introduce a dynamic\nself-correction strategy that enables real-time error correction, along with\nconsensus-guided decision-making strategies to optimize both correctness and\ncomputational resources. Experimental results demonstrate that the FoT\nframework, combined with these strategies, significantly enhances the reasoning\ncapabilities of LLMs, enabling them to solve complex tasks with greater\nprecision and efficiency. Code will be available at\nhttps://github.com/iamhankai/Forest-of-Thought."
                },
                "authors": [
                    {
                        "name": "Zhenni Bi"
                    },
                    {
                        "name": "Kai Han"
                    },
                    {
                        "name": "Chuanjian Liu"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Yunhe Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yunhe Wang"
                },
                "author": "Yunhe Wang",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09078v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09078v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11923v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11923v2",
                "updated": "2025-04-01T12:45:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    12,
                    45,
                    58,
                    1,
                    91,
                    0
                ],
                "published": "2024-12-16T16:09:35Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    9,
                    35,
                    0,
                    351,
                    0
                ],
                "title": "PICLe: Pseudo-Annotations for In-Context Learning in Low-Resource Named\n  Entity Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PICLe: Pseudo-Annotations for In-Context Learning in Low-Resource Named\n  Entity Detection"
                },
                "summary": "In-context learning (ICL) enables Large Language Models (LLMs) to perform\ntasks using few demonstrations, facilitating task adaptation when labeled\nexamples are hard to obtain. However, ICL is sensitive to the choice of\ndemonstrations, and it remains unclear which demonstration attributes enable\nin-context generalization. In this work, we conduct a perturbation study of\nin-context demonstrations for low-resource Named Entity Detection (NED). Our\nsurprising finding is that in-context demonstrations with partially correct\nannotated entity mentions can be as effective for task transfer as fully\ncorrect demonstrations. Based off our findings, we propose Pseudo-annotated\nIn-Context Learning (PICLe), a framework for in-context learning with noisy,\npseudo-annotated demonstrations. PICLe leverages LLMs to annotate many\ndemonstrations in a zero-shot first pass. We then cluster these synthetic\ndemonstrations, sample specific sets of in-context demonstrations from each\ncluster, and predict entity mentions using each set independently. Finally, we\nuse self-verification to select the final set of entity mentions. We evaluate\nPICLe on five biomedical NED datasets and show that, with zero human\nannotation, PICLe outperforms ICL in low-resource settings where limited gold\nexamples can be used as in-context demonstrations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) enables Large Language Models (LLMs) to perform\ntasks using few demonstrations, facilitating task adaptation when labeled\nexamples are hard to obtain. However, ICL is sensitive to the choice of\ndemonstrations, and it remains unclear which demonstration attributes enable\nin-context generalization. In this work, we conduct a perturbation study of\nin-context demonstrations for low-resource Named Entity Detection (NED). Our\nsurprising finding is that in-context demonstrations with partially correct\nannotated entity mentions can be as effective for task transfer as fully\ncorrect demonstrations. Based off our findings, we propose Pseudo-annotated\nIn-Context Learning (PICLe), a framework for in-context learning with noisy,\npseudo-annotated demonstrations. PICLe leverages LLMs to annotate many\ndemonstrations in a zero-shot first pass. We then cluster these synthetic\ndemonstrations, sample specific sets of in-context demonstrations from each\ncluster, and predict entity mentions using each set independently. Finally, we\nuse self-verification to select the final set of entity mentions. We evaluate\nPICLe on five biomedical NED datasets and show that, with zero human\nannotation, PICLe outperforms ICL in low-resource settings where limited gold\nexamples can be used as in-context demonstrations."
                },
                "authors": [
                    {
                        "name": "Sepideh Mamooler"
                    },
                    {
                        "name": "Syrielle Montariol"
                    },
                    {
                        "name": "Alexander Mathis"
                    },
                    {
                        "name": "Antoine Bosselut"
                    }
                ],
                "author_detail": {
                    "name": "Antoine Bosselut"
                },
                "author": "Antoine Bosselut",
                "arxiv_comment": "In Proceedings of NAACL2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11923v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11923v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22968v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22968v2",
                "updated": "2025-04-01T12:37:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    12,
                    37,
                    16,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-29T04:17:58Z",
                "published_parsed": [
                    2025,
                    3,
                    29,
                    4,
                    17,
                    58,
                    5,
                    88,
                    0
                ],
                "title": "HRET: A Self-Evolving LLM Evaluation Toolkit for Korean",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HRET: A Self-Evolving LLM Evaluation Toolkit for Korean"
                },
                "summary": "Recent advancements in Korean large language models (LLMs) have spurred\nnumerous benchmarks and evaluation methodologies, yet the lack of a\nstandardized evaluation framework has led to inconsistent results and limited\ncomparability. To address this, we introduce HRET Haerae Evaluation Toolkit, an\nopen-source, self-evolving evaluation framework tailored specifically for\nKorean LLMs. HRET unifies diverse evaluation methods, including logit-based\nscoring, exact-match, language-inconsistency penalization, and LLM-as-a-Judge\nassessments. Its modular, registry-based architecture integrates major\nbenchmarks (HAE-RAE Bench, KMMLU, KUDGE, HRM8K) and multiple inference backends\n(vLLM, HuggingFace, OpenAI-compatible endpoints). With automated pipelines for\ncontinuous evolution, HRET provides a robust foundation for reproducible, fair,\nand transparent Korean NLP research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Korean large language models (LLMs) have spurred\nnumerous benchmarks and evaluation methodologies, yet the lack of a\nstandardized evaluation framework has led to inconsistent results and limited\ncomparability. To address this, we introduce HRET Haerae Evaluation Toolkit, an\nopen-source, self-evolving evaluation framework tailored specifically for\nKorean LLMs. HRET unifies diverse evaluation methods, including logit-based\nscoring, exact-match, language-inconsistency penalization, and LLM-as-a-Judge\nassessments. Its modular, registry-based architecture integrates major\nbenchmarks (HAE-RAE Bench, KMMLU, KUDGE, HRM8K) and multiple inference backends\n(vLLM, HuggingFace, OpenAI-compatible endpoints). With automated pipelines for\ncontinuous evolution, HRET provides a robust foundation for reproducible, fair,\nand transparent Korean NLP research."
                },
                "authors": [
                    {
                        "name": "Hanwool Lee"
                    },
                    {
                        "name": "Soo Yong Kim"
                    },
                    {
                        "name": "Dasol Choi"
                    },
                    {
                        "name": "SangWon Baek"
                    },
                    {
                        "name": "Seunghyeok Hong"
                    },
                    {
                        "name": "Ilgyun Jeong"
                    },
                    {
                        "name": "Inseon Hwang"
                    },
                    {
                        "name": "Naeun Lee"
                    },
                    {
                        "name": "Guijin Son"
                    }
                ],
                "author_detail": {
                    "name": "Guijin Son"
                },
                "author": "Guijin Son",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22968v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22968v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16644v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16644v3",
                "updated": "2025-04-01T12:35:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    12,
                    35,
                    25,
                    1,
                    91,
                    0
                ],
                "published": "2024-09-25T05:44:44Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    5,
                    44,
                    44,
                    2,
                    269,
                    0
                ],
                "title": "Enabling Auditory Large Language Models for Automatic Speech Quality\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Auditory Large Language Models for Automatic Speech Quality\n  Evaluation"
                },
                "summary": "Speech quality assessment typically requires evaluating audio from multiple\naspects, such as mean opinion score (MOS) and speaker similarity (SIM) \\etc.,\nwhich can be challenging to cover using one small model designed for a single\ntask. In this paper, we propose leveraging recently introduced auditory large\nlanguage models (LLMs) for automatic speech quality assessment. By employing\ntask-specific prompts, auditory LLMs are finetuned to predict MOS, SIM and A/B\ntesting results, which are commonly used for evaluating text-to-speech systems.\nAdditionally, the finetuned auditory LLM is able to generate natural language\ndescriptions assessing aspects like noisiness, distortion, discontinuity, and\noverall quality, providing more interpretable outputs. Extensive experiments\nhave been performed on the NISQA, BVCC, SOMOS and VoxSim speech quality\ndatasets, using open-source auditory LLMs such as SALMONN, Qwen-Audio, and\nQwen2-Audio. For the natural language descriptions task, a commercial model\nGoogle Gemini 1.5 Pro is also evaluated. The results demonstrate that auditory\nLLMs achieve competitive performance compared to state-of-the-art task-specific\nsmall models in predicting MOS and SIM, while also delivering promising results\nin A/B testing and natural language descriptions. Our data processing scripts\nand finetuned model checkpoints can be found at\nhttps://github.com/bytedance/SALMONN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speech quality assessment typically requires evaluating audio from multiple\naspects, such as mean opinion score (MOS) and speaker similarity (SIM) \\etc.,\nwhich can be challenging to cover using one small model designed for a single\ntask. In this paper, we propose leveraging recently introduced auditory large\nlanguage models (LLMs) for automatic speech quality assessment. By employing\ntask-specific prompts, auditory LLMs are finetuned to predict MOS, SIM and A/B\ntesting results, which are commonly used for evaluating text-to-speech systems.\nAdditionally, the finetuned auditory LLM is able to generate natural language\ndescriptions assessing aspects like noisiness, distortion, discontinuity, and\noverall quality, providing more interpretable outputs. Extensive experiments\nhave been performed on the NISQA, BVCC, SOMOS and VoxSim speech quality\ndatasets, using open-source auditory LLMs such as SALMONN, Qwen-Audio, and\nQwen2-Audio. For the natural language descriptions task, a commercial model\nGoogle Gemini 1.5 Pro is also evaluated. The results demonstrate that auditory\nLLMs achieve competitive performance compared to state-of-the-art task-specific\nsmall models in predicting MOS and SIM, while also delivering promising results\nin A/B testing and natural language descriptions. Our data processing scripts\nand finetuned model checkpoints can be found at\nhttps://github.com/bytedance/SALMONN."
                },
                "authors": [
                    {
                        "name": "Siyin Wang"
                    },
                    {
                        "name": "Wenyi Yu"
                    },
                    {
                        "name": "Yudong Yang"
                    },
                    {
                        "name": "Changli Tang"
                    },
                    {
                        "name": "Yixuan Li"
                    },
                    {
                        "name": "Jimin Zhuang"
                    },
                    {
                        "name": "Xianzhao Chen"
                    },
                    {
                        "name": "Xiaohai Tian"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Guangzhi Sun"
                    },
                    {
                        "name": "Lu Lu"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Chao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Zhang"
                },
                "author": "Chao Zhang",
                "arxiv_comment": "Accepted by ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16644v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16644v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20290v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20290v2",
                "updated": "2025-04-01T12:33:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    12,
                    33,
                    53,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-26T07:32:20Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    7,
                    32,
                    20,
                    2,
                    85,
                    0
                ],
                "title": "QualiSpeech: A Speech Quality Assessment Dataset with Natural Language\n  Reasoning and Descriptions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QualiSpeech: A Speech Quality Assessment Dataset with Natural Language\n  Reasoning and Descriptions"
                },
                "summary": "This paper explores a novel perspective to speech quality assessment by\nleveraging natural language descriptions, offering richer, more nuanced\ninsights than traditional numerical scoring methods. Natural language feedback\nprovides instructive recommendations and detailed evaluations, yet existing\ndatasets lack the comprehensive annotations needed for this approach. To bridge\nthis gap, we introduce QualiSpeech, a comprehensive low-level speech quality\nassessment dataset encompassing 11 key aspects and detailed natural language\ncomments that include reasoning and contextual insights. Additionally, we\npropose the QualiSpeech Benchmark to evaluate the low-level speech\nunderstanding capabilities of auditory large language models (LLMs).\nExperimental results demonstrate that finetuned auditory LLMs can reliably\ngenerate detailed descriptions of noise and distortion, effectively identifying\ntheir types and temporal characteristics. The results further highlight the\npotential for incorporating reasoning to enhance the accuracy and reliability\nof quality assessments. The dataset will be released at\nhttps://huggingface.co/datasets/tsinghua-ee/QualiSpeech.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores a novel perspective to speech quality assessment by\nleveraging natural language descriptions, offering richer, more nuanced\ninsights than traditional numerical scoring methods. Natural language feedback\nprovides instructive recommendations and detailed evaluations, yet existing\ndatasets lack the comprehensive annotations needed for this approach. To bridge\nthis gap, we introduce QualiSpeech, a comprehensive low-level speech quality\nassessment dataset encompassing 11 key aspects and detailed natural language\ncomments that include reasoning and contextual insights. Additionally, we\npropose the QualiSpeech Benchmark to evaluate the low-level speech\nunderstanding capabilities of auditory large language models (LLMs).\nExperimental results demonstrate that finetuned auditory LLMs can reliably\ngenerate detailed descriptions of noise and distortion, effectively identifying\ntheir types and temporal characteristics. The results further highlight the\npotential for incorporating reasoning to enhance the accuracy and reliability\nof quality assessments. The dataset will be released at\nhttps://huggingface.co/datasets/tsinghua-ee/QualiSpeech."
                },
                "authors": [
                    {
                        "name": "Siyin Wang"
                    },
                    {
                        "name": "Wenyi Yu"
                    },
                    {
                        "name": "Xianzhao Chen"
                    },
                    {
                        "name": "Xiaohai Tian"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Lu Lu"
                    },
                    {
                        "name": "Yu Tsao"
                    },
                    {
                        "name": "Junichi Yamagishi"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Chao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Zhang"
                },
                "author": "Chao Zhang",
                "arxiv_comment": "23 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20290v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20290v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12049v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12049v4",
                "updated": "2025-04-01T12:19:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    12,
                    19,
                    49,
                    1,
                    91,
                    0
                ],
                "published": "2024-10-15T20:37:34Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    20,
                    37,
                    34,
                    1,
                    289,
                    0
                ],
                "title": "Sabiá-3 Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sabiá-3 Technical Report"
                },
                "summary": "This report presents Sabi\\'a-3, our new flagship language model, and\nSabiazinho-3, a more cost-effective sibling. The models were trained on a large\nbrazilian-centric corpus. Evaluations across diverse professional and academic\nbenchmarks show a strong performance on Portuguese and Brazil-related tasks.\nSabi\\'a-3 shows large improvements in comparison to our previous best of model,\nSabia-2 Medium, especially in reasoning-intensive tasks. Notably, Sabi\\'a-3's\naverage performance matches frontier LLMs, while it is offered at a three to\nfour times lower cost per token, reinforcing the benefits of domain\nspecialization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This report presents Sabi\\'a-3, our new flagship language model, and\nSabiazinho-3, a more cost-effective sibling. The models were trained on a large\nbrazilian-centric corpus. Evaluations across diverse professional and academic\nbenchmarks show a strong performance on Portuguese and Brazil-related tasks.\nSabi\\'a-3 shows large improvements in comparison to our previous best of model,\nSabia-2 Medium, especially in reasoning-intensive tasks. Notably, Sabi\\'a-3's\naverage performance matches frontier LLMs, while it is offered at a three to\nfour times lower cost per token, reinforcing the benefits of domain\nspecialization."
                },
                "authors": [
                    {
                        "name": "Hugo Abonizio"
                    },
                    {
                        "name": "Thales Sales Almeida"
                    },
                    {
                        "name": "Thiago Laitz"
                    },
                    {
                        "name": "Roseval Malaquias Junior"
                    },
                    {
                        "name": "Giovana Kerche Bonás"
                    },
                    {
                        "name": "Rodrigo Nogueira"
                    },
                    {
                        "name": "Ramon Pires"
                    }
                ],
                "author_detail": {
                    "name": "Ramon Pires"
                },
                "author": "Ramon Pires",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12049v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12049v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13269v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13269v2",
                "updated": "2025-04-01T12:13:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    12,
                    13,
                    46,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-17T15:22:19Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    22,
                    19,
                    0,
                    76,
                    0
                ],
                "title": "DAgent: A Relational Database-Driven Data Analysis Report Generation\n  Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DAgent: A Relational Database-Driven Data Analysis Report Generation\n  Agent"
                },
                "summary": "Relational database-driven data analysis (RDB-DA) report generation, which\naims to generate data analysis reports after querying relational databases, has\nbeen widely applied in fields such as finance and healthcare. Typically, these\ntasks are manually completed by data scientists, making the process very\nlabor-intensive and showing a clear need for automation. Although existing\nmethods (e.g., Table QA or Text-to-SQL) have been proposed to reduce human\ndependency, they cannot handle complex analytical tasks that require multi-step\nreasoning, cross-table associations, and synthesizing insights into reports.\nMoreover, there is no dataset available for developing automatic RDB-DA report\ngeneration. To fill this gap, this paper proposes an LLM agent system for\nRDB-DA report generation tasks, dubbed DAgent; moreover, we construct a\nbenchmark for automatic data analysis report generation, which includes a new\ndataset DA-Dataset and evaluation metrics. DAgent integrates planning, tools,\nand memory modules to decompose natural language questions into logically\nindependent sub-queries, accurately retrieve key information from relational\ndatabases, and generate analytical reports that meet the requirements of\ncompleteness, correctness, and conciseness through multi-step reasoning and\neffective data integration. Experimental analysis on the DA-Dataset\ndemonstrates that DAgent's superiority in retrieval performance and analysis\nreport generation quality, showcasing its strong potential for tackling complex\ndatabase analysis report generation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relational database-driven data analysis (RDB-DA) report generation, which\naims to generate data analysis reports after querying relational databases, has\nbeen widely applied in fields such as finance and healthcare. Typically, these\ntasks are manually completed by data scientists, making the process very\nlabor-intensive and showing a clear need for automation. Although existing\nmethods (e.g., Table QA or Text-to-SQL) have been proposed to reduce human\ndependency, they cannot handle complex analytical tasks that require multi-step\nreasoning, cross-table associations, and synthesizing insights into reports.\nMoreover, there is no dataset available for developing automatic RDB-DA report\ngeneration. To fill this gap, this paper proposes an LLM agent system for\nRDB-DA report generation tasks, dubbed DAgent; moreover, we construct a\nbenchmark for automatic data analysis report generation, which includes a new\ndataset DA-Dataset and evaluation metrics. DAgent integrates planning, tools,\nand memory modules to decompose natural language questions into logically\nindependent sub-queries, accurately retrieve key information from relational\ndatabases, and generate analytical reports that meet the requirements of\ncompleteness, correctness, and conciseness through multi-step reasoning and\neffective data integration. Experimental analysis on the DA-Dataset\ndemonstrates that DAgent's superiority in retrieval performance and analysis\nreport generation quality, showcasing its strong potential for tackling complex\ndatabase analysis report generation tasks."
                },
                "authors": [
                    {
                        "name": "Wenyi Xu"
                    },
                    {
                        "name": "Yuren Mao"
                    },
                    {
                        "name": "Xiaolu Zhang"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Xuemei Dong"
                    },
                    {
                        "name": "Mengfei Zhang"
                    },
                    {
                        "name": "Yunjun Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yunjun Gao"
                },
                "author": "Yunjun Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13269v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13269v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17216v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17216v2",
                "updated": "2025-04-01T10:49:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    10,
                    49,
                    56,
                    1,
                    91,
                    0
                ],
                "published": "2024-06-25T02:05:29Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    2,
                    5,
                    29,
                    1,
                    177,
                    0
                ],
                "title": "Machine Unlearning Fails to Remove Data Poisoning Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine Unlearning Fails to Remove Data Poisoning Attacks"
                },
                "summary": "We revisit the efficacy of several practical methods for approximate machine\nunlearning developed for large-scale deep learning. In addition to complying\nwith data deletion requests, one often-cited potential application for\nunlearning methods is to remove the effects of poisoned data. We experimentally\ndemonstrate that, while existing unlearning methods have been demonstrated to\nbe effective in a number of settings, they fail to remove the effects of data\npoisoning across a variety of types of poisoning attacks (indiscriminate,\ntargeted, and a newly-introduced Gaussian poisoning attack) and models (image\nclassifiers and LLMs); even when granted a relatively large compute budget. In\norder to precisely characterize unlearning efficacy, we introduce new\nevaluation metrics for unlearning based on data poisoning. Our results suggest\nthat a broader perspective, including a wider variety of evaluations, are\nrequired to avoid a false sense of confidence in machine unlearning procedures\nfor deep learning without provable guarantees. Moreover, while unlearning\nmethods show some signs of being useful to efficiently remove poisoned data\nwithout having to retrain, our work suggests that these methods are not yet\n``ready for prime time,'' and currently provide limited benefit over\nretraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We revisit the efficacy of several practical methods for approximate machine\nunlearning developed for large-scale deep learning. In addition to complying\nwith data deletion requests, one often-cited potential application for\nunlearning methods is to remove the effects of poisoned data. We experimentally\ndemonstrate that, while existing unlearning methods have been demonstrated to\nbe effective in a number of settings, they fail to remove the effects of data\npoisoning across a variety of types of poisoning attacks (indiscriminate,\ntargeted, and a newly-introduced Gaussian poisoning attack) and models (image\nclassifiers and LLMs); even when granted a relatively large compute budget. In\norder to precisely characterize unlearning efficacy, we introduce new\nevaluation metrics for unlearning based on data poisoning. Our results suggest\nthat a broader perspective, including a wider variety of evaluations, are\nrequired to avoid a false sense of confidence in machine unlearning procedures\nfor deep learning without provable guarantees. Moreover, while unlearning\nmethods show some signs of being useful to efficiently remove poisoned data\nwithout having to retrain, our work suggests that these methods are not yet\n``ready for prime time,'' and currently provide limited benefit over\nretraining."
                },
                "authors": [
                    {
                        "name": "Martin Pawelczyk"
                    },
                    {
                        "name": "Jimmy Z. Di"
                    },
                    {
                        "name": "Yiwei Lu"
                    },
                    {
                        "name": "Ayush Sekhari"
                    },
                    {
                        "name": "Gautam Kamath"
                    },
                    {
                        "name": "Seth Neel"
                    }
                ],
                "author_detail": {
                    "name": "Seth Neel"
                },
                "author": "Seth Neel",
                "arxiv_comment": "Published at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17216v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17216v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22517v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22517v2",
                "updated": "2025-04-01T10:42:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    10,
                    42,
                    11,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-28T15:21:24Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    15,
                    21,
                    24,
                    4,
                    87,
                    0
                ],
                "title": "Exploiting Mixture-of-Experts Redundancy Unlocks Multimodal Generative\n  Abilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploiting Mixture-of-Experts Redundancy Unlocks Multimodal Generative\n  Abilities"
                },
                "summary": "In this work, we undertake the challenge of augmenting the existing\ngenerative capabilities of pre-trained text-only large language models (LLMs)\nwith multi-modal generation capability while satisfying two core constraints:\nC1 preserving the preservation of original language generative capabilities\nwith negligible performance degradation, and C2 adhering to a small parameter\nbudget to learn the new modality, ensuring scalability and efficiency. In\ncontrast to current approaches that add dedicated modules, thereby\nsignificantly increasing the parameter count, we propose a method that\nleverages the underutilized capacity inherent in deep models. Specifically, we\nexploit the parameter redundancy within Mixture-of-Experts (MoEs) as a source\nof additional capacity for learning a new modality, enabling better parameter\nefficiency (C1). Moreover, we preserve the original language generation\ncapabilities by applying low-rank adaptation exclusively to the tokens of the\nnew modality (C2). Furthermore, we introduce a novel parameter initialization\nscheme based on the Gromov-Wasserstein distance to improve convergence and\ntraining stability. Through an extensive analysis of the routing mechanism, we\nuncover the emergence of modality-specific pathways and decreased redundancy\nwithin the experts that can efficiently unlock multi-modal generative\ncapabilities. Overall, our method can be seamlessly applied to a wide range of\ncontemporary LLMs, providing a new pathway for transitioning from uni-modal to\nmulti-modal architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we undertake the challenge of augmenting the existing\ngenerative capabilities of pre-trained text-only large language models (LLMs)\nwith multi-modal generation capability while satisfying two core constraints:\nC1 preserving the preservation of original language generative capabilities\nwith negligible performance degradation, and C2 adhering to a small parameter\nbudget to learn the new modality, ensuring scalability and efficiency. In\ncontrast to current approaches that add dedicated modules, thereby\nsignificantly increasing the parameter count, we propose a method that\nleverages the underutilized capacity inherent in deep models. Specifically, we\nexploit the parameter redundancy within Mixture-of-Experts (MoEs) as a source\nof additional capacity for learning a new modality, enabling better parameter\nefficiency (C1). Moreover, we preserve the original language generation\ncapabilities by applying low-rank adaptation exclusively to the tokens of the\nnew modality (C2). Furthermore, we introduce a novel parameter initialization\nscheme based on the Gromov-Wasserstein distance to improve convergence and\ntraining stability. Through an extensive analysis of the routing mechanism, we\nuncover the emergence of modality-specific pathways and decreased redundancy\nwithin the experts that can efficiently unlock multi-modal generative\ncapabilities. Overall, our method can be seamlessly applied to a wide range of\ncontemporary LLMs, providing a new pathway for transitioning from uni-modal to\nmulti-modal architectures."
                },
                "authors": [
                    {
                        "name": "Raman Dutt"
                    },
                    {
                        "name": "Harleen Hanspal"
                    },
                    {
                        "name": "Guoxuan Xia"
                    },
                    {
                        "name": "Petru-Daniel Tudosiu"
                    },
                    {
                        "name": "Alexander Black"
                    },
                    {
                        "name": "Yongxin Yang"
                    },
                    {
                        "name": "Steven McDonagh"
                    },
                    {
                        "name": "Sarah Parisot"
                    }
                ],
                "author_detail": {
                    "name": "Sarah Parisot"
                },
                "author": "Sarah Parisot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22517v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22517v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23859v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23859v2",
                "updated": "2025-04-01T10:23:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    10,
                    23,
                    7,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-31T09:06:23Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    9,
                    6,
                    23,
                    0,
                    90,
                    0
                ],
                "title": "Evaluating small vision-language models as AI assistants for radio\n  astronomical source analysis tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating small vision-language models as AI assistants for radio\n  astronomical source analysis tasks"
                },
                "summary": "The advent of next-generation radio telescopes is set to transform radio\nastronomy by producing massive data volumes that challenge traditional\nprocessing methods. Deep learning techniques have shown strong potential in\nautomating radio analysis tasks, yet are often constrained by the limited\navailability of large annotated datasets. Recent progress in self-supervised\nlearning has led to foundational radio vision models, but adapting them for new\ntasks typically requires coding expertise, limiting their accessibility to a\nbroader astronomical community. Text-based AI interfaces offer a promising\nalternative by enabling task-specific queries and example-driven learning. In\nthis context, Large Language Models (LLMs), with their remarkable zero-shot\ncapabilities, are increasingly used in scientific domains. However, deploying\nlarge-scale models remains resource-intensive, and there is a growing demand\nfor AI systems that can reason over both visual and textual data in\nastronomical analysis. This study explores small-scale Vision-Language Models\n(VLMs) as AI assistants for radio astronomy, combining LLM capabilities with\nvision transformers. We fine-tuned the LLaVA VLM on a dataset of 59k radio\nimages from multiple surveys, enriched with 38k image-caption pairs from the\nliterature. The fine-tuned models show clear improvements over base models in\nradio-specific tasks, achieving ~30% F1-score gains in extended source\ndetection, but they underperform pure vision models and exhibit ~20% drop on\ngeneral multimodal tasks. Inclusion of caption data and LoRA fine-tuning\nenhances instruction-following and helps recover ~10% accuracy on standard\nbenchmarks. This work lays the foundation for future advancements in radio\nVLMs, highlighting their potential and limitations, such as the need for better\nmultimodal alignment, higher-quality datasets, and mitigation of catastrophic\nforgetting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of next-generation radio telescopes is set to transform radio\nastronomy by producing massive data volumes that challenge traditional\nprocessing methods. Deep learning techniques have shown strong potential in\nautomating radio analysis tasks, yet are often constrained by the limited\navailability of large annotated datasets. Recent progress in self-supervised\nlearning has led to foundational radio vision models, but adapting them for new\ntasks typically requires coding expertise, limiting their accessibility to a\nbroader astronomical community. Text-based AI interfaces offer a promising\nalternative by enabling task-specific queries and example-driven learning. In\nthis context, Large Language Models (LLMs), with their remarkable zero-shot\ncapabilities, are increasingly used in scientific domains. However, deploying\nlarge-scale models remains resource-intensive, and there is a growing demand\nfor AI systems that can reason over both visual and textual data in\nastronomical analysis. This study explores small-scale Vision-Language Models\n(VLMs) as AI assistants for radio astronomy, combining LLM capabilities with\nvision transformers. We fine-tuned the LLaVA VLM on a dataset of 59k radio\nimages from multiple surveys, enriched with 38k image-caption pairs from the\nliterature. The fine-tuned models show clear improvements over base models in\nradio-specific tasks, achieving ~30% F1-score gains in extended source\ndetection, but they underperform pure vision models and exhibit ~20% drop on\ngeneral multimodal tasks. Inclusion of caption data and LoRA fine-tuning\nenhances instruction-following and helps recover ~10% accuracy on standard\nbenchmarks. This work lays the foundation for future advancements in radio\nVLMs, highlighting their potential and limitations, such as the need for better\nmultimodal alignment, higher-quality datasets, and mitigation of catastrophic\nforgetting."
                },
                "authors": [
                    {
                        "name": "S. Riggi"
                    },
                    {
                        "name": "T. Cecconello"
                    },
                    {
                        "name": "A. Pilzer"
                    },
                    {
                        "name": "S. Palazzo"
                    },
                    {
                        "name": "N. Gupta"
                    },
                    {
                        "name": "A. M. Hopkins"
                    },
                    {
                        "name": "C. Trigilio"
                    },
                    {
                        "name": "G. Umana"
                    }
                ],
                "author_detail": {
                    "name": "G. Umana"
                },
                "author": "G. Umana",
                "arxiv_comment": "17 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23859v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23859v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01503v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01503v3",
                "updated": "2025-04-01T10:22:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    10,
                    22,
                    26,
                    1,
                    91,
                    0
                ],
                "published": "2024-11-03T09:49:12Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    9,
                    49,
                    12,
                    6,
                    308,
                    0
                ],
                "title": "A Highly Scalable LLM Clusters with Optical Interconnect",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Highly Scalable LLM Clusters with Optical Interconnect"
                },
                "summary": "We propose \\emph{LumosCore} to build high-bandwidth and large-scale data\ncenter networks for LLM jobs. By replacing the core-layer electrical packet\nswitches by optical circuit switches, \\emph{LumosCore} could achieves $2\\times$\nincrease in bandwidth or $8\\times$ increase in network size. We offer the\ndetailed design of \\emph{LumosCore} at both deployment stage and running stage.\nAt deployment stage, we propose Interleaved Wiring, which is compatible with\nall possible logical topologies. At running stage, we design polynomial-time\nalgorithms for GPU placement, logical topology generating and OCS\nreconfiguration to minimize network contention and reduce impact to scheduled\njobs. We evaluate \\emph{LumosCore} using both testbed experiments and\nlarge-scale simulation. Compared to traditional hybrid optical/electrical\narchitectures, \\emph{LumosCore} increases the end-to-end training throughput by\nup to 39.5\\% on a 128-node testbed. Compared to the state-of-art Clos\narchitectures, \\emph{LumosCore} reduces the average job completion time by up\nto 34.1\\% in a 16k simulation platform.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose \\emph{LumosCore} to build high-bandwidth and large-scale data\ncenter networks for LLM jobs. By replacing the core-layer electrical packet\nswitches by optical circuit switches, \\emph{LumosCore} could achieves $2\\times$\nincrease in bandwidth or $8\\times$ increase in network size. We offer the\ndetailed design of \\emph{LumosCore} at both deployment stage and running stage.\nAt deployment stage, we propose Interleaved Wiring, which is compatible with\nall possible logical topologies. At running stage, we design polynomial-time\nalgorithms for GPU placement, logical topology generating and OCS\nreconfiguration to minimize network contention and reduce impact to scheduled\njobs. We evaluate \\emph{LumosCore} using both testbed experiments and\nlarge-scale simulation. Compared to traditional hybrid optical/electrical\narchitectures, \\emph{LumosCore} increases the end-to-end training throughput by\nup to 39.5\\% on a 128-node testbed. Compared to the state-of-art Clos\narchitectures, \\emph{LumosCore} reduces the average job completion time by up\nto 34.1\\% in a 16k simulation platform."
                },
                "authors": [
                    {
                        "name": "Xinchi Han"
                    },
                    {
                        "name": "Yongxi Lv"
                    },
                    {
                        "name": "Shizhen Zhao"
                    },
                    {
                        "name": "Zhuotao Liu"
                    },
                    {
                        "name": "Ximeng Liu"
                    },
                    {
                        "name": "Xinbing Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinbing Wang"
                },
                "author": "Xinbing Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01503v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01503v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.08035v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.08035v2",
                "updated": "2025-04-01T10:19:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    10,
                    19,
                    16,
                    1,
                    91,
                    0
                ],
                "published": "2024-07-10T20:32:50Z",
                "published_parsed": [
                    2024,
                    7,
                    10,
                    20,
                    32,
                    50,
                    2,
                    192,
                    0
                ],
                "title": "FsPONER: Few-shot Prompt Optimization for Named Entity Recognition in\n  Domain-specific Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FsPONER: Few-shot Prompt Optimization for Named Entity Recognition in\n  Domain-specific Scenarios"
                },
                "summary": "Large Language Models (LLMs) have provided a new pathway for Named Entity\nRecognition (NER) tasks. Compared with fine-tuning, LLM-powered prompting\nmethods avoid the need for training, conserve substantial computational\nresources, and rely on minimal annotated data. Previous studies have achieved\ncomparable performance to fully supervised BERT-based fine-tuning approaches on\ngeneral NER benchmarks. However, none of the previous approaches has\ninvestigated the efficiency of LLM-based few-shot learning in domain-specific\nscenarios. To address this gap, we introduce FsPONER, a novel approach for\noptimizing few-shot prompts, and evaluate its performance on domain-specific\nNER datasets, with a focus on industrial manufacturing and maintenance, while\nusing multiple LLMs -- GPT-4-32K, GPT-3.5-Turbo, LLaMA 2-chat, and Vicuna.\nFsPONER consists of three few-shot selection methods based on random sampling,\nTF-IDF vectors, and a combination of both. We compare these methods with a\ngeneral-purpose GPT-NER method as the number of few-shot examples increases and\nevaluate their optimal NER performance against fine-tuned BERT and LLaMA\n2-chat. In the considered real-world scenarios with data scarcity, FsPONER with\nTF-IDF surpasses fine-tuned models by approximately 10% in F1 score.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have provided a new pathway for Named Entity\nRecognition (NER) tasks. Compared with fine-tuning, LLM-powered prompting\nmethods avoid the need for training, conserve substantial computational\nresources, and rely on minimal annotated data. Previous studies have achieved\ncomparable performance to fully supervised BERT-based fine-tuning approaches on\ngeneral NER benchmarks. However, none of the previous approaches has\ninvestigated the efficiency of LLM-based few-shot learning in domain-specific\nscenarios. To address this gap, we introduce FsPONER, a novel approach for\noptimizing few-shot prompts, and evaluate its performance on domain-specific\nNER datasets, with a focus on industrial manufacturing and maintenance, while\nusing multiple LLMs -- GPT-4-32K, GPT-3.5-Turbo, LLaMA 2-chat, and Vicuna.\nFsPONER consists of three few-shot selection methods based on random sampling,\nTF-IDF vectors, and a combination of both. We compare these methods with a\ngeneral-purpose GPT-NER method as the number of few-shot examples increases and\nevaluate their optimal NER performance against fine-tuned BERT and LLaMA\n2-chat. In the considered real-world scenarios with data scarcity, FsPONER with\nTF-IDF surpasses fine-tuned models by approximately 10% in F1 score."
                },
                "authors": [
                    {
                        "name": "Yongjian Tang"
                    },
                    {
                        "name": "Rakebul Hasan"
                    },
                    {
                        "name": "Thomas Runkler"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Runkler"
                },
                "author": "Thomas Runkler",
                "arxiv_comment": "accepted in the main track at the 27th European Conference on\n  Artificial Intelligence (ECAI-2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.08035v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.08035v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01487v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01487v3",
                "updated": "2025-04-01T09:34:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    9,
                    34,
                    45,
                    1,
                    91,
                    0
                ],
                "published": "2024-12-02T13:39:29Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    13,
                    39,
                    29,
                    0,
                    337,
                    0
                ],
                "title": "FastRM: An efficient and automatic explainability framework for\n  multimodal generative models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastRM: An efficient and automatic explainability framework for\n  multimodal generative models"
                },
                "summary": "Large Vision Language Models (LVLMs) have demonstrated remarkable reasoning\ncapabilities over textual and visual inputs. However, these models remain prone\nto generating misinformation. Identifying and mitigating ungrounded responses\nis crucial for developing trustworthy AI. Traditional explainability methods\nsuch as gradient-based relevancy maps, offer insight into the decision process\nof models, but are often computationally expensive and unsuitable for real-time\noutput validation. In this work, we introduce FastRM, an efficient method for\npredicting explainable Relevancy Maps of LVLMs. Furthermore, FastRM provides\nboth quantitative and qualitative assessment of model confidence. Experimental\nresults demonstrate that FastRM achieves a 99.8% reduction in computation time\nand a 44.4% reduction in memory footprint compared to traditional relevancy map\ngeneration. FastRM allows explainable AI to be more practical and scalable,\nthereby promoting its deployment in real-world applications and enabling users\nto more effectively evaluate the reliability of model outputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision Language Models (LVLMs) have demonstrated remarkable reasoning\ncapabilities over textual and visual inputs. However, these models remain prone\nto generating misinformation. Identifying and mitigating ungrounded responses\nis crucial for developing trustworthy AI. Traditional explainability methods\nsuch as gradient-based relevancy maps, offer insight into the decision process\nof models, but are often computationally expensive and unsuitable for real-time\noutput validation. In this work, we introduce FastRM, an efficient method for\npredicting explainable Relevancy Maps of LVLMs. Furthermore, FastRM provides\nboth quantitative and qualitative assessment of model confidence. Experimental\nresults demonstrate that FastRM achieves a 99.8% reduction in computation time\nand a 44.4% reduction in memory footprint compared to traditional relevancy map\ngeneration. FastRM allows explainable AI to be more practical and scalable,\nthereby promoting its deployment in real-world applications and enabling users\nto more effectively evaluate the reliability of model outputs."
                },
                "authors": [
                    {
                        "name": "Gabriela Ben-Melech Stan"
                    },
                    {
                        "name": "Estelle Aflalo"
                    },
                    {
                        "name": "Man Luo"
                    },
                    {
                        "name": "Shachar Rosenman"
                    },
                    {
                        "name": "Tiep Le"
                    },
                    {
                        "name": "Sayak Paul"
                    },
                    {
                        "name": "Shao-Yen Tseng"
                    },
                    {
                        "name": "Vasudev Lal"
                    }
                ],
                "author_detail": {
                    "name": "Vasudev Lal"
                },
                "author": "Vasudev Lal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01487v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01487v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00596v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00596v2",
                "updated": "2025-04-01T09:33:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    9,
                    33,
                    55,
                    1,
                    91,
                    0
                ],
                "published": "2024-11-30T22:02:12Z",
                "published_parsed": [
                    2024,
                    11,
                    30,
                    22,
                    2,
                    12,
                    5,
                    335,
                    0
                ],
                "title": "PhyT2V: LLM-Guided Iterative Self-Refinement for Physics-Grounded\n  Text-to-Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhyT2V: LLM-Guided Iterative Self-Refinement for Physics-Grounded\n  Text-to-Video Generation"
                },
                "summary": "Text-to-video (T2V) generation has been recently enabled by transformer-based\ndiffusion models, but current T2V models lack capabilities in adhering to the\nreal-world common knowledge and physical rules, due to their limited\nunderstanding of physical realism and deficiency in temporal modeling. Existing\nsolutions are either data-driven or require extra model inputs, but cannot be\ngeneralizable to out-of-distribution domains. In this paper, we present PhyT2V,\na new data-independent T2V technique that expands the current T2V model's\ncapability of video generation to out-of-distribution domains, by enabling\nchain-of-thought and step-back reasoning in T2V prompting. Our experiments show\nthat PhyT2V improves existing T2V models' adherence to real-world physical\nrules by 2.3x, and achieves 35% improvement compared to T2V prompt enhancers.\nThe source codes are available at: https://github.com/pittisl/PhyT2V.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-video (T2V) generation has been recently enabled by transformer-based\ndiffusion models, but current T2V models lack capabilities in adhering to the\nreal-world common knowledge and physical rules, due to their limited\nunderstanding of physical realism and deficiency in temporal modeling. Existing\nsolutions are either data-driven or require extra model inputs, but cannot be\ngeneralizable to out-of-distribution domains. In this paper, we present PhyT2V,\na new data-independent T2V technique that expands the current T2V model's\ncapability of video generation to out-of-distribution domains, by enabling\nchain-of-thought and step-back reasoning in T2V prompting. Our experiments show\nthat PhyT2V improves existing T2V models' adherence to real-world physical\nrules by 2.3x, and achieves 35% improvement compared to T2V prompt enhancers.\nThe source codes are available at: https://github.com/pittisl/PhyT2V."
                },
                "authors": [
                    {
                        "name": "Qiyao Xue"
                    },
                    {
                        "name": "Xiangyu Yin"
                    },
                    {
                        "name": "Boyuan Yang"
                    },
                    {
                        "name": "Wei Gao"
                    }
                ],
                "author_detail": {
                    "name": "Wei Gao"
                },
                "author": "Wei Gao",
                "arxiv_comment": "28 pages",
                "arxiv_journal_ref": "in Proceedings of the IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition (CVPR), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00596v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00596v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17003v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17003v3",
                "updated": "2025-04-01T09:33:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    9,
                    33,
                    19,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-21T10:09:16Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    10,
                    9,
                    16,
                    4,
                    80,
                    0
                ],
                "title": "A Survey on Personalized Alignment -- The Missing Piece for Large\n  Language Models in Real-World Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Personalized Alignment -- The Missing Piece for Large\n  Language Models in Real-World Applications"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities, yet\ntheir transition to real-world applications reveals a critical limitation: the\ninability to adapt to individual preferences while maintaining alignment with\nuniversal human values. Current alignment techniques adopt a one-size-fits-all\napproach that fails to accommodate users' diverse backgrounds and needs. This\npaper presents the first comprehensive survey of personalized alignment-a\nparadigm that enables LLMs to adapt their behavior within ethical boundaries\nbased on individual preferences. We propose a unified framework comprising\npreference memory management, personalized generation, and feedback-based\nalignment, systematically analyzing implementation approaches and evaluating\ntheir effectiveness across various scenarios. By examining current techniques,\npotential risks, and future challenges, this survey provides a structured\nfoundation for developing more adaptable and ethically-aligned LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities, yet\ntheir transition to real-world applications reveals a critical limitation: the\ninability to adapt to individual preferences while maintaining alignment with\nuniversal human values. Current alignment techniques adopt a one-size-fits-all\napproach that fails to accommodate users' diverse backgrounds and needs. This\npaper presents the first comprehensive survey of personalized alignment-a\nparadigm that enables LLMs to adapt their behavior within ethical boundaries\nbased on individual preferences. We propose a unified framework comprising\npreference memory management, personalized generation, and feedback-based\nalignment, systematically analyzing implementation approaches and evaluating\ntheir effectiveness across various scenarios. By examining current techniques,\npotential risks, and future challenges, this survey provides a structured\nfoundation for developing more adaptable and ethically-aligned LLMs."
                },
                "authors": [
                    {
                        "name": "Jian Guan"
                    },
                    {
                        "name": "Junfei Wu"
                    },
                    {
                        "name": "Jia-Nan Li"
                    },
                    {
                        "name": "Chuanqi Cheng"
                    },
                    {
                        "name": "Wei Wu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wu"
                },
                "author": "Wei Wu",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17003v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17003v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18800v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18800v2",
                "updated": "2025-04-01T09:07:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    9,
                    7,
                    49,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-24T15:46:18Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    46,
                    18,
                    0,
                    83,
                    0
                ],
                "title": "Development of portable cosmic-ray muon detector array for muography",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development of portable cosmic-ray muon detector array for muography"
                },
                "summary": "As the multidisciplinary applications of cosmic-ray muons expand to\nlarge-scale and wide-area scenarios, the construction of cosmic-ray muon\ndetector arrays has become a key solution to overcome the hardware limitations\nof individual detector. For muography, the array-based detector design enables\nfast-scanning of large target objects, allowing for rapid identification of\ndensity variation regions, which can improve the efficiency of tomography. This\npaper integrates scintillator detector technology with Internet of things (IoT)\ntechnology, proposing a novel array networking model for nationwide deployment.\nThe model enables long-distance data collection and distribution, laying the\nfoundation for future multidisciplinary applications such as muography and\nother fields.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the multidisciplinary applications of cosmic-ray muons expand to\nlarge-scale and wide-area scenarios, the construction of cosmic-ray muon\ndetector arrays has become a key solution to overcome the hardware limitations\nof individual detector. For muography, the array-based detector design enables\nfast-scanning of large target objects, allowing for rapid identification of\ndensity variation regions, which can improve the efficiency of tomography. This\npaper integrates scintillator detector technology with Internet of things (IoT)\ntechnology, proposing a novel array networking model for nationwide deployment.\nThe model enables long-distance data collection and distribution, laying the\nfoundation for future multidisciplinary applications such as muography and\nother fields."
                },
                "authors": [
                    {
                        "name": "Yunsong Ning"
                    },
                    {
                        "name": "Yi Yuan"
                    },
                    {
                        "name": "Tao Yu"
                    },
                    {
                        "name": "Hongyu Chen"
                    },
                    {
                        "name": "Chengyan Xie"
                    },
                    {
                        "name": "Hui Jiang"
                    },
                    {
                        "name": "Hesheng Liu"
                    },
                    {
                        "name": "Guihao Lu"
                    },
                    {
                        "name": "Mingchen Sun"
                    },
                    {
                        "name": "Yu Chen"
                    },
                    {
                        "name": "Jian Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Tang"
                },
                "author": "Jian Tang",
                "arxiv_comment": "7 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18800v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18800v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16855v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16855v2",
                "updated": "2025-04-01T08:48:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    8,
                    48,
                    4,
                    1,
                    91,
                    0
                ],
                "published": "2024-12-22T04:40:24Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    4,
                    40,
                    24,
                    6,
                    357,
                    0
                ],
                "title": "GME: Improving Universal Multimodal Retrieval by Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GME: Improving Universal Multimodal Retrieval by Multimodal LLMs"
                },
                "summary": "Universal Multimodal Retrieval (UMR) aims to enable search across various\nmodalities using a unified model, where queries and candidates can consist of\npure text, images, or a combination of both. Previous work has attempted to\nadopt multimodal large language models (MLLMs) to realize UMR using only text\ndata. However, our preliminary experiments demonstrate that more diverse\nmultimodal training data can further unlock the potential of MLLMs. Despite its\neffectiveness, the existing multimodal training data is highly imbalanced in\nterms of modality, which motivates us to develop a training data synthesis\npipeline and construct a large-scale, high-quality fused-modal training\ndataset. Based on the synthetic training data, we develop the General\nMultimodal Embedder (GME), an MLLM-based dense retriever designed for UMR.\nFurthermore, we construct a comprehensive UMR Benchmark (UMRB) to evaluate the\neffectiveness of our approach. Experimental results show that our method\nachieves state-of-the-art performance among existing UMR methods. Last, we\nprovide in-depth analyses of model scaling and training strategies, and perform\nablation studies on both the model and synthetic data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Universal Multimodal Retrieval (UMR) aims to enable search across various\nmodalities using a unified model, where queries and candidates can consist of\npure text, images, or a combination of both. Previous work has attempted to\nadopt multimodal large language models (MLLMs) to realize UMR using only text\ndata. However, our preliminary experiments demonstrate that more diverse\nmultimodal training data can further unlock the potential of MLLMs. Despite its\neffectiveness, the existing multimodal training data is highly imbalanced in\nterms of modality, which motivates us to develop a training data synthesis\npipeline and construct a large-scale, high-quality fused-modal training\ndataset. Based on the synthetic training data, we develop the General\nMultimodal Embedder (GME), an MLLM-based dense retriever designed for UMR.\nFurthermore, we construct a comprehensive UMR Benchmark (UMRB) to evaluate the\neffectiveness of our approach. Experimental results show that our method\nachieves state-of-the-art performance among existing UMR methods. Last, we\nprovide in-depth analyses of model scaling and training strategies, and perform\nablation studies on both the model and synthetic data."
                },
                "authors": [
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Yanzhao Zhang"
                    },
                    {
                        "name": "Wen Xie"
                    },
                    {
                        "name": "Mingxin Li"
                    },
                    {
                        "name": "Ziqi Dai"
                    },
                    {
                        "name": "Dingkun Long"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Meishan Zhang"
                    },
                    {
                        "name": "Wenjie Li"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "Accepted to CVPR 2025, models at\n  https://huggingface.co/Alibaba-NLP/gme-Qwen2-VL-2B-Instruct",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16855v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16855v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15877v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15877v4",
                "updated": "2025-04-01T08:36:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    8,
                    36,
                    44,
                    1,
                    91,
                    0
                ],
                "published": "2024-06-22T15:52:04Z",
                "published_parsed": [
                    2024,
                    6,
                    22,
                    15,
                    52,
                    4,
                    5,
                    174,
                    0
                ],
                "title": "BigCodeBench: Benchmarking Code Generation with Diverse Function Calls\n  and Complex Instructions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BigCodeBench: Benchmarking Code Generation with Diverse Function Calls\n  and Complex Instructions"
                },
                "summary": "Task automation has been greatly empowered by the recent advances in Large\nLanguage Models (LLMs) via Python code, where the tasks ranging from software\nengineering development to general-purpose reasoning. While current benchmarks\nhave shown that LLMs can solve tasks using programs like human developers, the\nmajority of their evaluations are limited to short and self-contained\nalgorithmic tasks or standalone function calls. Solving challenging and\npractical tasks requires the capability of utilizing diverse function calls as\ntools to efficiently implement functionalities like data analysis and web\ndevelopment. In addition, using multiple tools to solve a task needs\ncompositional reasoning by accurately understanding complex instructions.\nFulfilling both of these characteristics can pose a great challenge for LLMs.To\nassess how well LLMs can solve challenging and practical tasks via programs, we\nintroduce BigCodeBench, a benchmark that challenges LLMs to invoke multiple\nfunction calls as tools from 139 libraries and 7 domains for 1,140 fine-grained\ntasks. To evaluate LLMs rigorously, each task encompasses 5.6 test cases with\nan average branch coverage of 99%. In addition, we propose a\nnatural-language-oriented variant of BigCodeBench, BigCodeBench-Instruct, that\nautomatically transforms the original docstrings into short instructions only\nwith essential information. Our extensive evaluation of 60 LLMs shows that LLMs\nare not yet capable of following complex instructions to use function calls\nprecisely, with scores up to 60%, significantly lower than the human\nperformance of 97%. The results underscore the need for further advancements in\nthis area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task automation has been greatly empowered by the recent advances in Large\nLanguage Models (LLMs) via Python code, where the tasks ranging from software\nengineering development to general-purpose reasoning. While current benchmarks\nhave shown that LLMs can solve tasks using programs like human developers, the\nmajority of their evaluations are limited to short and self-contained\nalgorithmic tasks or standalone function calls. Solving challenging and\npractical tasks requires the capability of utilizing diverse function calls as\ntools to efficiently implement functionalities like data analysis and web\ndevelopment. In addition, using multiple tools to solve a task needs\ncompositional reasoning by accurately understanding complex instructions.\nFulfilling both of these characteristics can pose a great challenge for LLMs.To\nassess how well LLMs can solve challenging and practical tasks via programs, we\nintroduce BigCodeBench, a benchmark that challenges LLMs to invoke multiple\nfunction calls as tools from 139 libraries and 7 domains for 1,140 fine-grained\ntasks. To evaluate LLMs rigorously, each task encompasses 5.6 test cases with\nan average branch coverage of 99%. In addition, we propose a\nnatural-language-oriented variant of BigCodeBench, BigCodeBench-Instruct, that\nautomatically transforms the original docstrings into short instructions only\nwith essential information. Our extensive evaluation of 60 LLMs shows that LLMs\nare not yet capable of following complex instructions to use function calls\nprecisely, with scores up to 60%, significantly lower than the human\nperformance of 97%. The results underscore the need for further advancements in\nthis area."
                },
                "authors": [
                    {
                        "name": "Terry Yue Zhuo"
                    },
                    {
                        "name": "Minh Chien Vu"
                    },
                    {
                        "name": "Jenny Chim"
                    },
                    {
                        "name": "Han Hu"
                    },
                    {
                        "name": "Wenhao Yu"
                    },
                    {
                        "name": "Ratnadira Widyasari"
                    },
                    {
                        "name": "Imam Nur Bani Yusuf"
                    },
                    {
                        "name": "Haolan Zhan"
                    },
                    {
                        "name": "Junda He"
                    },
                    {
                        "name": "Indraneil Paul"
                    },
                    {
                        "name": "Simon Brunner"
                    },
                    {
                        "name": "Chen Gong"
                    },
                    {
                        "name": "Thong Hoang"
                    },
                    {
                        "name": "Armel Randy Zebaze"
                    },
                    {
                        "name": "Xiaoheng Hong"
                    },
                    {
                        "name": "Wen-Ding Li"
                    },
                    {
                        "name": "Jean Kaddour"
                    },
                    {
                        "name": "Ming Xu"
                    },
                    {
                        "name": "Zhihan Zhang"
                    },
                    {
                        "name": "Prateek Yadav"
                    },
                    {
                        "name": "Naman Jain"
                    },
                    {
                        "name": "Alex Gu"
                    },
                    {
                        "name": "Zhoujun Cheng"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Zijian Wang"
                    },
                    {
                        "name": "Binyuan Hui"
                    },
                    {
                        "name": "Niklas Muennighoff"
                    },
                    {
                        "name": "David Lo"
                    },
                    {
                        "name": "Daniel Fried"
                    },
                    {
                        "name": "Xiaoning Du"
                    },
                    {
                        "name": "Harm de Vries"
                    },
                    {
                        "name": "Leandro Von Werra"
                    }
                ],
                "author_detail": {
                    "name": "Leandro Von Werra"
                },
                "author": "Leandro Von Werra",
                "arxiv_comment": "Accpeted at ICLR 2025 (Oral), built with love by the BigCode\n  community :)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15877v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15877v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19178v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19178v2",
                "updated": "2025-04-01T08:18:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    8,
                    18,
                    34,
                    1,
                    91,
                    0
                ],
                "published": "2025-02-26T14:34:00Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    34,
                    0,
                    2,
                    57,
                    0
                ],
                "title": "UQABench: Evaluating User Embedding for Prompting LLMs in Personalized\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UQABench: Evaluating User Embedding for Prompting LLMs in Personalized\n  Question Answering"
                },
                "summary": "Large language models (LLMs) achieve remarkable success in natural language\nprocessing (NLP). In practical scenarios like recommendations, as users\nincreasingly seek personalized experiences, it becomes crucial to incorporate\nuser interaction history into the context of LLMs to enhance personalization.\nHowever, from a practical utility perspective, user interactions' extensive\nlength and noise present challenges when used directly as text prompts. A\npromising solution is to compress and distill interactions into compact\nembeddings, serving as soft prompts to assist LLMs in generating personalized\nresponses. Although this approach brings efficiency, a critical concern\nemerges: Can user embeddings adequately capture valuable information and prompt\nLLMs? To address this concern, we propose \\name, a benchmark designed to\nevaluate the effectiveness of user embeddings in prompting LLMs for\npersonalization. We establish a fair and standardized evaluation process,\nencompassing pre-training, fine-tuning, and evaluation stages. To thoroughly\nevaluate user embeddings, we design three dimensions of tasks: sequence\nunderstanding, action prediction, and interest perception. These evaluation\ntasks cover the industry's demands in traditional recommendation tasks, such as\nimproving prediction accuracy, and its aspirations for LLM-based methods, such\nas accurately understanding user interests and enhancing the user experience.\nWe conduct extensive experiments on various state-of-the-art methods for\nmodeling user embeddings. Additionally, we reveal the scaling laws of\nleveraging user embeddings to prompt LLMs. The benchmark is available online.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) achieve remarkable success in natural language\nprocessing (NLP). In practical scenarios like recommendations, as users\nincreasingly seek personalized experiences, it becomes crucial to incorporate\nuser interaction history into the context of LLMs to enhance personalization.\nHowever, from a practical utility perspective, user interactions' extensive\nlength and noise present challenges when used directly as text prompts. A\npromising solution is to compress and distill interactions into compact\nembeddings, serving as soft prompts to assist LLMs in generating personalized\nresponses. Although this approach brings efficiency, a critical concern\nemerges: Can user embeddings adequately capture valuable information and prompt\nLLMs? To address this concern, we propose \\name, a benchmark designed to\nevaluate the effectiveness of user embeddings in prompting LLMs for\npersonalization. We establish a fair and standardized evaluation process,\nencompassing pre-training, fine-tuning, and evaluation stages. To thoroughly\nevaluate user embeddings, we design three dimensions of tasks: sequence\nunderstanding, action prediction, and interest perception. These evaluation\ntasks cover the industry's demands in traditional recommendation tasks, such as\nimproving prediction accuracy, and its aspirations for LLM-based methods, such\nas accurately understanding user interests and enhancing the user experience.\nWe conduct extensive experiments on various state-of-the-art methods for\nmodeling user embeddings. Additionally, we reveal the scaling laws of\nleveraging user embeddings to prompt LLMs. The benchmark is available online."
                },
                "authors": [
                    {
                        "name": "Langming Liu"
                    },
                    {
                        "name": "Shilei Liu"
                    },
                    {
                        "name": "Yujin Yuan"
                    },
                    {
                        "name": "Yizhen Zhang"
                    },
                    {
                        "name": "Bencheng Yan"
                    },
                    {
                        "name": "Zhiyuan Zeng"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Jiaqi Liu"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Pengjie Wang"
                    },
                    {
                        "name": "Jian Xu"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "arxiv_comment": "10 pages, 3 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19178v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19178v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.11910v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.11910v2",
                "updated": "2025-04-01T07:37:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    7,
                    37,
                    55,
                    1,
                    91,
                    0
                ],
                "published": "2024-02-19T07:50:54Z",
                "published_parsed": [
                    2024,
                    2,
                    19,
                    7,
                    50,
                    54,
                    0,
                    50,
                    0
                ],
                "title": "Enhancing Large Language Models for Text-to-Testcase Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Large Language Models for Text-to-Testcase Generation"
                },
                "summary": "Context: Test-driven development (TDD) is a widely employed software\ndevelopment practice that involves developing test cases based on requirements\nprior to writing the code. Although various methods for automated test case\ngeneration have been proposed, they are not specifically tailored for TDD,\nwhere requirements instead of code serve as input. Objective: In this paper, we\nintroduce a text-to-testcase generation approach based on a large language\nmodel (GPT-3.5) that is fine-tuned on our curated dataset with an effective\nprompt design. Method: Our approach involves enhancing the capabilities of\nbasic GPT-3.5 for text-to-testcase generation task that is fine-tuned on our\ncurated dataset with an effective prompting design. We evaluated the\neffectiveness of our approach using a span of five large-scale open-source\nsoftware projects. Results: Our approach generated 7k test cases for open\nsource projects, achieving 78.5% syntactic correctness, 67.09% requirement\nalignment, and 61.7% code coverage, which substantially outperforms all other\nLLMs (basic GPT-3.5, Bloom, and CodeT5). In addition, our ablation study\ndemonstrates the substantial performance improvement of the fine-tuning and\nprompting components of the GPT-3.5 model. Conclusions: These findings lead us\nto conclude that fine-tuning and prompting should be considered in the future\nwhen building a language model for the text-to-testcase generation task",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context: Test-driven development (TDD) is a widely employed software\ndevelopment practice that involves developing test cases based on requirements\nprior to writing the code. Although various methods for automated test case\ngeneration have been proposed, they are not specifically tailored for TDD,\nwhere requirements instead of code serve as input. Objective: In this paper, we\nintroduce a text-to-testcase generation approach based on a large language\nmodel (GPT-3.5) that is fine-tuned on our curated dataset with an effective\nprompt design. Method: Our approach involves enhancing the capabilities of\nbasic GPT-3.5 for text-to-testcase generation task that is fine-tuned on our\ncurated dataset with an effective prompting design. We evaluated the\neffectiveness of our approach using a span of five large-scale open-source\nsoftware projects. Results: Our approach generated 7k test cases for open\nsource projects, achieving 78.5% syntactic correctness, 67.09% requirement\nalignment, and 61.7% code coverage, which substantially outperforms all other\nLLMs (basic GPT-3.5, Bloom, and CodeT5). In addition, our ablation study\ndemonstrates the substantial performance improvement of the fine-tuning and\nprompting components of the GPT-3.5 model. Conclusions: These findings lead us\nto conclude that fine-tuning and prompting should be considered in the future\nwhen building a language model for the text-to-testcase generation task"
                },
                "authors": [
                    {
                        "name": "Saranya Alagarsamy"
                    },
                    {
                        "name": "Chakkrit Tantithamthavorn"
                    },
                    {
                        "name": "Wannita Takerngsaksiri"
                    },
                    {
                        "name": "Chetan Arora"
                    },
                    {
                        "name": "Aldeida Aleti"
                    }
                ],
                "author_detail": {
                    "name": "Aldeida Aleti"
                },
                "author": "Aldeida Aleti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.11910v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.11910v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.02057v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.02057v2",
                "updated": "2025-04-01T07:33:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    7,
                    33,
                    46,
                    1,
                    91,
                    0
                ],
                "published": "2023-09-05T08:58:46Z",
                "published_parsed": [
                    2023,
                    9,
                    5,
                    8,
                    58,
                    46,
                    1,
                    248,
                    0
                ],
                "title": "Robust Recommender System: A Survey and Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Recommender System: A Survey and Future Directions"
                },
                "summary": "With the rapid growth of information, recommender systems have become\nintegral for providing personalized suggestions and overcoming information\noverload. However, their practical deployment often encounters ``dirty'' data,\nwhere noise or malicious information can lead to abnormal recommendations.\nResearch on improving recommender systems' robustness against such dirty data\nhas thus gained significant attention. This survey provides a comprehensive\nreview of recent work on recommender systems' robustness. We first present a\ntaxonomy to organize current techniques for withstanding malicious attacks and\nnatural noise. We then explore state-of-the-art methods in each category,\nincluding fraudster detection, adversarial training, certifiable robust\ntraining for defending against malicious attacks, and regularization,\npurification, self-supervised learning for defending against malicious attacks.\nAdditionally, we summarize evaluation metrics and commonly used datasets for\nassessing robustness. We discuss robustness across varying recommendation\nscenarios and its interplay with other properties like accuracy,\ninterpretability, privacy, and fairness. Finally, we delve into open issues and\nfuture research directions in this emerging field. Our goal is to provide\nreaders with a comprehensive understanding of robust recommender systems and to\nidentify key pathways for future research and development. To facilitate\nongoing exploration, we maintain a continuously updated GitHub repository with\nrelated research: https://github.com/Kaike-Zhang/Robust-Recommender-System.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid growth of information, recommender systems have become\nintegral for providing personalized suggestions and overcoming information\noverload. However, their practical deployment often encounters ``dirty'' data,\nwhere noise or malicious information can lead to abnormal recommendations.\nResearch on improving recommender systems' robustness against such dirty data\nhas thus gained significant attention. This survey provides a comprehensive\nreview of recent work on recommender systems' robustness. We first present a\ntaxonomy to organize current techniques for withstanding malicious attacks and\nnatural noise. We then explore state-of-the-art methods in each category,\nincluding fraudster detection, adversarial training, certifiable robust\ntraining for defending against malicious attacks, and regularization,\npurification, self-supervised learning for defending against malicious attacks.\nAdditionally, we summarize evaluation metrics and commonly used datasets for\nassessing robustness. We discuss robustness across varying recommendation\nscenarios and its interplay with other properties like accuracy,\ninterpretability, privacy, and fairness. Finally, we delve into open issues and\nfuture research directions in this emerging field. Our goal is to provide\nreaders with a comprehensive understanding of robust recommender systems and to\nidentify key pathways for future research and development. To facilitate\nongoing exploration, we maintain a continuously updated GitHub repository with\nrelated research: https://github.com/Kaike-Zhang/Robust-Recommender-System."
                },
                "authors": [
                    {
                        "name": "Kaike Zhang"
                    },
                    {
                        "name": "Qi Cao"
                    },
                    {
                        "name": "Fei Sun"
                    },
                    {
                        "name": "Yunfan Wu"
                    },
                    {
                        "name": "Shuchang Tao"
                    },
                    {
                        "name": "Huawei Shen"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.02057v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.02057v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13208v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13208v2",
                "updated": "2025-04-01T07:04:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    7,
                    4,
                    25,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-17T14:20:48Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    14,
                    20,
                    48,
                    0,
                    76,
                    0
                ],
                "title": "Improving Complex Reasoning with Dynamic Prompt Corruption: A soft\n  prompt Optimization Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Complex Reasoning with Dynamic Prompt Corruption: A soft\n  prompt Optimization Approach"
                },
                "summary": "Prompt-tuning (PT) for large language models (LLMs) can facilitate the\nperformance on various conventional NLP tasks with significantly fewer\ntrainable parameters. However, our investigation reveals that PT provides\nlimited improvement and may even degrade the primitive performance of LLMs on\ncomplex reasoning tasks. Such a phenomenon suggests that soft prompts can\npositively impact certain instances while negatively affecting others,\nparticularly during the later phases of reasoning. To address these challenges,\nWe first identify an information accumulation within the soft prompts. Through\ndetailed analysis, we demonstrate that this phenomenon is often accompanied by\nerroneous information flow patterns in the deeper layers of the model, which\nultimately lead to incorrect reasoning outcomes. we propose a novel method\ncalled Dynamic Prompt Corruption (DPC) to take better advantage of soft prompts\nin complex reasoning tasks, which dynamically adjusts the influence of soft\nprompts based on their impact on the reasoning process. Specifically, DPC\nconsists of two stages: Dynamic Trigger and Dynamic Corruption. First, Dynamic\nTrigger measures the impact of soft prompts, identifying whether beneficial or\ndetrimental. Then, Dynamic Corruption mitigates the negative effects of soft\nprompts by selectively masking key tokens that interfere with the reasoning\nprocess. We validate the proposed approach through extensive experiments on\nvarious LLMs and reasoning tasks, including GSM8K, MATH, and AQuA. Experimental\nresults demonstrate that DPC can consistently enhance the performance of PT,\nachieving 4%-8% accuracy gains compared to vanilla prompt tuning, highlighting\nthe effectiveness of our approach and its potential to enhance complex\nreasoning in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt-tuning (PT) for large language models (LLMs) can facilitate the\nperformance on various conventional NLP tasks with significantly fewer\ntrainable parameters. However, our investigation reveals that PT provides\nlimited improvement and may even degrade the primitive performance of LLMs on\ncomplex reasoning tasks. Such a phenomenon suggests that soft prompts can\npositively impact certain instances while negatively affecting others,\nparticularly during the later phases of reasoning. To address these challenges,\nWe first identify an information accumulation within the soft prompts. Through\ndetailed analysis, we demonstrate that this phenomenon is often accompanied by\nerroneous information flow patterns in the deeper layers of the model, which\nultimately lead to incorrect reasoning outcomes. we propose a novel method\ncalled Dynamic Prompt Corruption (DPC) to take better advantage of soft prompts\nin complex reasoning tasks, which dynamically adjusts the influence of soft\nprompts based on their impact on the reasoning process. Specifically, DPC\nconsists of two stages: Dynamic Trigger and Dynamic Corruption. First, Dynamic\nTrigger measures the impact of soft prompts, identifying whether beneficial or\ndetrimental. Then, Dynamic Corruption mitigates the negative effects of soft\nprompts by selectively masking key tokens that interfere with the reasoning\nprocess. We validate the proposed approach through extensive experiments on\nvarious LLMs and reasoning tasks, including GSM8K, MATH, and AQuA. Experimental\nresults demonstrate that DPC can consistently enhance the performance of PT,\nachieving 4%-8% accuracy gains compared to vanilla prompt tuning, highlighting\nthe effectiveness of our approach and its potential to enhance complex\nreasoning in LLMs."
                },
                "authors": [
                    {
                        "name": "Sinan Fan"
                    },
                    {
                        "name": "Liang Xie"
                    },
                    {
                        "name": "Chen Shen"
                    },
                    {
                        "name": "Ge Teng"
                    },
                    {
                        "name": "Xiaosong Yuan"
                    },
                    {
                        "name": "Xiaofeng Zhang"
                    },
                    {
                        "name": "Chenxi Huang"
                    },
                    {
                        "name": "Wenxiao Wang"
                    },
                    {
                        "name": "Xiaofei He"
                    },
                    {
                        "name": "Jieping Ye"
                    }
                ],
                "author_detail": {
                    "name": "Jieping Ye"
                },
                "author": "Jieping Ye",
                "arxiv_comment": "Accepted by ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13208v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13208v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23811v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23811v2",
                "updated": "2025-04-01T06:56:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    6,
                    56,
                    57,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-31T07:44:26Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    7,
                    44,
                    26,
                    0,
                    90,
                    0
                ],
                "title": "Did ChatGPT or Copilot use alter the style of internet news headlines? A\n  time series regression analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Did ChatGPT or Copilot use alter the style of internet news headlines? A\n  time series regression analysis"
                },
                "summary": "The release of advanced Large Language Models (LLMs) such as ChatGPT and\nCopilot is changing the way text is created and may influence the content that\nwe find on the web. This study investigated whether the release of these two\npopular LLMs coincided with a change in writing style in headlines and links on\nworldwide news websites. 175 NLP features were obtained for each text in a\ndataset of 451 million headlines/links. An interrupted time series analysis was\napplied for each of the 175 NLP features to evaluate whether there were any\nstatistically significant sustained changes after the release dates of ChatGPT\nand/or Copilot. There were a total of 44 features that did not appear to have\nany significant sustained change after the release of ChatGPT/Copilot. A total\nof 91 other features did show significant change with ChatGPT and/or Copilot\nalthough significance with earlier control LLM release dates (GPT-1/2/3,\nGopher) removed them from consideration. This initial analysis suggests these\nlanguage models may have had a limited impact on the style of individual news\nheadlines/links, with respect to only some NLP measures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The release of advanced Large Language Models (LLMs) such as ChatGPT and\nCopilot is changing the way text is created and may influence the content that\nwe find on the web. This study investigated whether the release of these two\npopular LLMs coincided with a change in writing style in headlines and links on\nworldwide news websites. 175 NLP features were obtained for each text in a\ndataset of 451 million headlines/links. An interrupted time series analysis was\napplied for each of the 175 NLP features to evaluate whether there were any\nstatistically significant sustained changes after the release dates of ChatGPT\nand/or Copilot. There were a total of 44 features that did not appear to have\nany significant sustained change after the release of ChatGPT/Copilot. A total\nof 91 other features did show significant change with ChatGPT and/or Copilot\nalthough significance with earlier control LLM release dates (GPT-1/2/3,\nGopher) removed them from consideration. This initial analysis suggests these\nlanguage models may have had a limited impact on the style of individual news\nheadlines/links, with respect to only some NLP measures."
                },
                "authors": [
                    {
                        "name": "Chris Brogly"
                    },
                    {
                        "name": "Connor McElroy"
                    }
                ],
                "author_detail": {
                    "name": "Connor McElroy"
                },
                "author": "Connor McElroy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23811v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23811v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18942v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18942v2",
                "updated": "2025-04-01T06:52:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    6,
                    52,
                    58,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-24T17:59:04Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    59,
                    4,
                    0,
                    83,
                    0
                ],
                "title": "Video-T1: Test-Time Scaling for Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-T1: Test-Time Scaling for Video Generation"
                },
                "summary": "With the scale capability of increasing training data, model size, and\ncomputational cost, video generation has achieved impressive results in digital\ncreation, enabling users to express creativity across various domains.\nRecently, researchers in Large Language Models (LLMs) have expanded the scaling\nto test-time, which can significantly improve LLM performance by using more\ninference-time computation. Instead of scaling up video foundation models\nthrough expensive training costs, we explore the power of Test-Time Scaling\n(TTS) in video generation, aiming to answer the question: if a video generation\nmodel is allowed to use non-trivial amount of inference-time compute, how much\ncan it improve generation quality given a challenging text prompt. In this\nwork, we reinterpret the test-time scaling of video generation as a searching\nproblem to sample better trajectories from Gaussian noise space to the target\nvideo distribution. Specifically, we build the search space with test-time\nverifiers to provide feedback and heuristic algorithms to guide searching\nprocess. Given a text prompt, we first explore an intuitive linear search\nstrategy by increasing noise candidates at inference time. As full-step\ndenoising all frames simultaneously requires heavy test-time computation costs,\nwe further design a more efficient TTS method for video generation called\nTree-of-Frames (ToF) that adaptively expands and prunes video branches in an\nautoregressive manner. Extensive experiments on text-conditioned video\ngeneration benchmarks demonstrate that increasing test-time compute\nconsistently leads to significant improvements in the quality of videos.\nProject page: https://liuff19.github.io/Video-T1",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the scale capability of increasing training data, model size, and\ncomputational cost, video generation has achieved impressive results in digital\ncreation, enabling users to express creativity across various domains.\nRecently, researchers in Large Language Models (LLMs) have expanded the scaling\nto test-time, which can significantly improve LLM performance by using more\ninference-time computation. Instead of scaling up video foundation models\nthrough expensive training costs, we explore the power of Test-Time Scaling\n(TTS) in video generation, aiming to answer the question: if a video generation\nmodel is allowed to use non-trivial amount of inference-time compute, how much\ncan it improve generation quality given a challenging text prompt. In this\nwork, we reinterpret the test-time scaling of video generation as a searching\nproblem to sample better trajectories from Gaussian noise space to the target\nvideo distribution. Specifically, we build the search space with test-time\nverifiers to provide feedback and heuristic algorithms to guide searching\nprocess. Given a text prompt, we first explore an intuitive linear search\nstrategy by increasing noise candidates at inference time. As full-step\ndenoising all frames simultaneously requires heavy test-time computation costs,\nwe further design a more efficient TTS method for video generation called\nTree-of-Frames (ToF) that adaptively expands and prunes video branches in an\nautoregressive manner. Extensive experiments on text-conditioned video\ngeneration benchmarks demonstrate that increasing test-time compute\nconsistently leads to significant improvements in the quality of videos.\nProject page: https://liuff19.github.io/Video-T1"
                },
                "authors": [
                    {
                        "name": "Fangfu Liu"
                    },
                    {
                        "name": "Hanyang Wang"
                    },
                    {
                        "name": "Yimo Cai"
                    },
                    {
                        "name": "Kaiyan Zhang"
                    },
                    {
                        "name": "Xiaohang Zhan"
                    },
                    {
                        "name": "Yueqi Duan"
                    }
                ],
                "author_detail": {
                    "name": "Yueqi Duan"
                },
                "author": "Yueqi Duan",
                "arxiv_comment": "Project page: https://liuff19.github.io/Video-T1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18942v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18942v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07298v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07298v2",
                "updated": "2025-04-01T06:30:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    6,
                    30,
                    16,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-10T13:18:05Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    13,
                    18,
                    5,
                    0,
                    69,
                    0
                ],
                "title": "ALLVB: All-in-One Long Video Understanding Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ALLVB: All-in-One Long Video Understanding Benchmark"
                },
                "summary": "From image to video understanding, the capabilities of Multi-modal LLMs\n(MLLMs) are increasingly powerful. However, most existing video understanding\nbenchmarks are relatively short, which makes them inadequate for effectively\nevaluating the long-sequence modeling capabilities of MLLMs. This highlights\nthe urgent need for a comprehensive and integrated long video understanding\nbenchmark to assess the ability of MLLMs thoroughly. To this end, we propose\nALLVB (ALL-in-One Long Video Understanding Benchmark). ALLVB's main\ncontributions include: 1) It integrates 9 major video understanding tasks.\nThese tasks are converted into video QA formats, allowing a single benchmark to\nevaluate 9 different video understanding capabilities of MLLMs, highlighting\nthe versatility, comprehensiveness, and challenging nature of ALLVB. 2) A fully\nautomated annotation pipeline using GPT-4o is designed, requiring only human\nquality control, which facilitates the maintenance and expansion of the\nbenchmark. 3) It contains 1,376 videos across 16 categories, averaging nearly 2\nhours each, with a total of 252k QAs. To the best of our knowledge, it is the\nlargest long video understanding benchmark in terms of the number of videos,\naverage duration, and number of QAs. We have tested various mainstream MLLMs on\nALLVB, and the results indicate that even the most advanced commercial models\nhave significant room for improvement. This reflects the benchmark's\nchallenging nature and demonstrates the substantial potential for development\nin long video understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From image to video understanding, the capabilities of Multi-modal LLMs\n(MLLMs) are increasingly powerful. However, most existing video understanding\nbenchmarks are relatively short, which makes them inadequate for effectively\nevaluating the long-sequence modeling capabilities of MLLMs. This highlights\nthe urgent need for a comprehensive and integrated long video understanding\nbenchmark to assess the ability of MLLMs thoroughly. To this end, we propose\nALLVB (ALL-in-One Long Video Understanding Benchmark). ALLVB's main\ncontributions include: 1) It integrates 9 major video understanding tasks.\nThese tasks are converted into video QA formats, allowing a single benchmark to\nevaluate 9 different video understanding capabilities of MLLMs, highlighting\nthe versatility, comprehensiveness, and challenging nature of ALLVB. 2) A fully\nautomated annotation pipeline using GPT-4o is designed, requiring only human\nquality control, which facilitates the maintenance and expansion of the\nbenchmark. 3) It contains 1,376 videos across 16 categories, averaging nearly 2\nhours each, with a total of 252k QAs. To the best of our knowledge, it is the\nlargest long video understanding benchmark in terms of the number of videos,\naverage duration, and number of QAs. We have tested various mainstream MLLMs on\nALLVB, and the results indicate that even the most advanced commercial models\nhave significant room for improvement. This reflects the benchmark's\nchallenging nature and demonstrates the substantial potential for development\nin long video understanding."
                },
                "authors": [
                    {
                        "name": "Xichen Tan"
                    },
                    {
                        "name": "Yuanjing Luo"
                    },
                    {
                        "name": "Yunfan Ye"
                    },
                    {
                        "name": "Fang Liu"
                    },
                    {
                        "name": "Zhiping Cai"
                    }
                ],
                "author_detail": {
                    "name": "Zhiping Cai"
                },
                "author": "Zhiping Cai",
                "arxiv_comment": "AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07298v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07298v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20197v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20197v2",
                "updated": "2025-04-01T06:06:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    6,
                    6,
                    38,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-26T03:44:03Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    3,
                    44,
                    3,
                    2,
                    85,
                    0
                ],
                "title": "Enhancing the Robustness of LLM-Generated Code: Empirical Study and\n  Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing the Robustness of LLM-Generated Code: Empirical Study and\n  Framework"
                },
                "summary": "Ensuring the robustness of code generated by large language models (LLMs) is\ncrucial for real-world reliability. However, existing evaluations predominantly\nfocus on correctness, often neglecting key robustness concerns such as missing\ninput validation and insufficient error handling. In this paper, we present the\nfirst empirical study on the robustness of LLM-generated code. We introduce\nnovel robustness metrics and analyze four state-of-the-art code LLMs, revealing\nthat, on average, 43.1% of their generated code is less robust than\nhuman-written counterparts. Notably, over 90% of robustness deficiencies stem\nfrom missing conditional checks, with 70% of these omissions occurring in the\nfirst line of code. Additionally, in 69% of cases where a conditional statement\nis necessary but absent, the \"if\" token still ranks third or higher in the\nmodel's predicted token probabilities, indicating an implicit recognition of\ncontrol structures. Building on these findings, we propose RobGen, a framework\ndesigned to enhance code robustness without requiring model retraining. RobGen\nleverages two model-agnostic techniques: RobGen-Adj, which dynamically adjusts\ntoken probabilities during decoding to encourage the inclusion of control\nstructures, and RobGen-Ins, which improves generated code by inserting missing\nconditionals after generation. Experimental results demonstrate that RobGen\nreduces the proportion of less robust model-generated code by 20.0%,\nsignificantly enhancing code reliability across diverse tasks. As a lightweight\nand adaptable solution, RobGen effectively mitigates robustness challenges in\nLLM-generated code. All code and data are available at\nhttps://github.com/SYSUSELab/RobGen.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring the robustness of code generated by large language models (LLMs) is\ncrucial for real-world reliability. However, existing evaluations predominantly\nfocus on correctness, often neglecting key robustness concerns such as missing\ninput validation and insufficient error handling. In this paper, we present the\nfirst empirical study on the robustness of LLM-generated code. We introduce\nnovel robustness metrics and analyze four state-of-the-art code LLMs, revealing\nthat, on average, 43.1% of their generated code is less robust than\nhuman-written counterparts. Notably, over 90% of robustness deficiencies stem\nfrom missing conditional checks, with 70% of these omissions occurring in the\nfirst line of code. Additionally, in 69% of cases where a conditional statement\nis necessary but absent, the \"if\" token still ranks third or higher in the\nmodel's predicted token probabilities, indicating an implicit recognition of\ncontrol structures. Building on these findings, we propose RobGen, a framework\ndesigned to enhance code robustness without requiring model retraining. RobGen\nleverages two model-agnostic techniques: RobGen-Adj, which dynamically adjusts\ntoken probabilities during decoding to encourage the inclusion of control\nstructures, and RobGen-Ins, which improves generated code by inserting missing\nconditionals after generation. Experimental results demonstrate that RobGen\nreduces the proportion of less robust model-generated code by 20.0%,\nsignificantly enhancing code reliability across diverse tasks. As a lightweight\nand adaptable solution, RobGen effectively mitigates robustness challenges in\nLLM-generated code. All code and data are available at\nhttps://github.com/SYSUSELab/RobGen."
                },
                "authors": [
                    {
                        "name": "Zike Li"
                    },
                    {
                        "name": "Mingwei Liu"
                    },
                    {
                        "name": "Anji Li"
                    },
                    {
                        "name": "Kaifeng He"
                    },
                    {
                        "name": "Yanlin Wang"
                    },
                    {
                        "name": "Xin Peng"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20197v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20197v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06994v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06994v2",
                "updated": "2025-04-01T05:33:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    5,
                    33,
                    5,
                    1,
                    91,
                    0
                ],
                "published": "2024-12-09T21:01:45Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    21,
                    1,
                    45,
                    0,
                    344,
                    0
                ],
                "title": "Phaedrus: Predicting Dynamic Application Behavior with Lightweight\n  Generative Models and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phaedrus: Predicting Dynamic Application Behavior with Lightweight\n  Generative Models and LLMs"
                },
                "summary": "Application profiling is an indispensable technique for many software\ndevelopment tasks, such as code and memory layout optimizations, where\noptimization decisions are tailored to specific program profiles.\nUnfortunately, modern applications codebases exhibit highly variant behavior\nacross different inputs, creating challenges for conventional profiling\napproaches that rely on a single representative execution instance. In this\npaper, we propose \\textbf{Phaedrus}, a new \\textit{compiler-assisted deep\nlearning framework} designed to predict dynamic program behaviors across varied\nexecution instances, specifically focusing on dynamic function call\nprediction.Such predicted call sequences are then used for producing optimized\ncode pertinent to a given input.\n  Traditional profile-guided optimization methods struggle with the\ninput-dependent variability of modern applications, where profiling on\ndifferent inputs yields divergent application behaviors. To address this,\nPhaedrus proposes two new approaches: \\textit{Application Behavior Synthesis},\na profile-less approach where Large Language Models (LLMs) directly infer\ndynamic functions based on source code \\& static compiler analysis, bypassing\nthe need for traditional profiling, and \\textit{Application Profile\nGeneralization}, which uses generative models trained on compressed and\naugmented \\textit{Whole Program Path} (WPP) based function profiles to predict\napplication behavior under unseen inputs. Our experiments show that\n\\textit{Phaedrus} can achieve upto $10^7X$ reduction in WPP function profile\nsizes, can predict most frequently executed functions that cover upto 85-99\\%\nof the execution time, along with an average of 13.68\\% (upto 65\\%) reduction\nin application binary size, and an average of 2.8\\% performance improvement\nover the traditional profile-guided optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application profiling is an indispensable technique for many software\ndevelopment tasks, such as code and memory layout optimizations, where\noptimization decisions are tailored to specific program profiles.\nUnfortunately, modern applications codebases exhibit highly variant behavior\nacross different inputs, creating challenges for conventional profiling\napproaches that rely on a single representative execution instance. In this\npaper, we propose \\textbf{Phaedrus}, a new \\textit{compiler-assisted deep\nlearning framework} designed to predict dynamic program behaviors across varied\nexecution instances, specifically focusing on dynamic function call\nprediction.Such predicted call sequences are then used for producing optimized\ncode pertinent to a given input.\n  Traditional profile-guided optimization methods struggle with the\ninput-dependent variability of modern applications, where profiling on\ndifferent inputs yields divergent application behaviors. To address this,\nPhaedrus proposes two new approaches: \\textit{Application Behavior Synthesis},\na profile-less approach where Large Language Models (LLMs) directly infer\ndynamic functions based on source code \\& static compiler analysis, bypassing\nthe need for traditional profiling, and \\textit{Application Profile\nGeneralization}, which uses generative models trained on compressed and\naugmented \\textit{Whole Program Path} (WPP) based function profiles to predict\napplication behavior under unseen inputs. Our experiments show that\n\\textit{Phaedrus} can achieve upto $10^7X$ reduction in WPP function profile\nsizes, can predict most frequently executed functions that cover upto 85-99\\%\nof the execution time, along with an average of 13.68\\% (upto 65\\%) reduction\nin application binary size, and an average of 2.8\\% performance improvement\nover the traditional profile-guided optimization."
                },
                "authors": [
                    {
                        "name": "Bodhisatwa Chatterjee"
                    },
                    {
                        "name": "Neeraj Jadhav"
                    },
                    {
                        "name": "Sharjeel Khan"
                    },
                    {
                        "name": "Santosh Pande"
                    }
                ],
                "author_detail": {
                    "name": "Santosh Pande"
                },
                "author": "Santosh Pande",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06994v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06994v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22420v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22420v2",
                "updated": "2025-04-01T05:32:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    5,
                    32,
                    41,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-28T13:32:29Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    13,
                    32,
                    29,
                    4,
                    87,
                    0
                ],
                "title": "Unveiling the Mist over 3D Vision-Language Understanding: Object-centric\n  Evaluation with Chain-of-Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling the Mist over 3D Vision-Language Understanding: Object-centric\n  Evaluation with Chain-of-Analysis"
                },
                "summary": "Existing 3D vision-language (3D-VL) benchmarks fall short in evaluating 3D-VL\nmodels, creating a \"mist\" that obscures rigorous insights into model\ncapabilities and 3D-VL tasks. This mist persists due to three key limitations.\nFirst, flawed test data, like ambiguous referential text in the grounding task,\ncan yield incorrect and unreliable test results. Second, oversimplified metrics\nsuch as simply averaging accuracy per question answering (QA) pair, cannot\nreveal true model capability due to their vulnerability to language variations.\nThird, existing benchmarks isolate the grounding and QA tasks, disregarding the\nunderlying coherence that QA should be based on solid grounding capabilities.\nTo unveil the \"mist\", we propose Beacon3D, a benchmark for 3D-VL grounding and\nQA tasks, delivering a perspective shift in the evaluation of 3D-VL\nunderstanding. Beacon3D features (i) high-quality test data with precise and\nnatural language, (ii) object-centric evaluation with multiple tests per object\nto ensure robustness, and (iii) a novel chain-of-analysis paradigm to address\nlanguage robustness and model performance coherence across grounding and QA.\nOur evaluation of state-of-the-art 3D-VL models on Beacon3D reveals that (i)\nobject-centric evaluation elicits true model performance and particularly weak\ngeneralization in QA; (ii) grounding-QA coherence remains fragile in current\n3D-VL models, and (iii) incorporating large language models (LLMs) to 3D-VL\nmodels, though as a prevalent practice, hinders grounding capabilities and has\nyet to elevate QA capabilities. We hope Beacon3D and our comprehensive analysis\ncould benefit the 3D-VL community towards faithful developments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing 3D vision-language (3D-VL) benchmarks fall short in evaluating 3D-VL\nmodels, creating a \"mist\" that obscures rigorous insights into model\ncapabilities and 3D-VL tasks. This mist persists due to three key limitations.\nFirst, flawed test data, like ambiguous referential text in the grounding task,\ncan yield incorrect and unreliable test results. Second, oversimplified metrics\nsuch as simply averaging accuracy per question answering (QA) pair, cannot\nreveal true model capability due to their vulnerability to language variations.\nThird, existing benchmarks isolate the grounding and QA tasks, disregarding the\nunderlying coherence that QA should be based on solid grounding capabilities.\nTo unveil the \"mist\", we propose Beacon3D, a benchmark for 3D-VL grounding and\nQA tasks, delivering a perspective shift in the evaluation of 3D-VL\nunderstanding. Beacon3D features (i) high-quality test data with precise and\nnatural language, (ii) object-centric evaluation with multiple tests per object\nto ensure robustness, and (iii) a novel chain-of-analysis paradigm to address\nlanguage robustness and model performance coherence across grounding and QA.\nOur evaluation of state-of-the-art 3D-VL models on Beacon3D reveals that (i)\nobject-centric evaluation elicits true model performance and particularly weak\ngeneralization in QA; (ii) grounding-QA coherence remains fragile in current\n3D-VL models, and (iii) incorporating large language models (LLMs) to 3D-VL\nmodels, though as a prevalent practice, hinders grounding capabilities and has\nyet to elevate QA capabilities. We hope Beacon3D and our comprehensive analysis\ncould benefit the 3D-VL community towards faithful developments."
                },
                "authors": [
                    {
                        "name": "Jiangyong Huang"
                    },
                    {
                        "name": "Baoxiong Jia"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Ziyu Zhu"
                    },
                    {
                        "name": "Xiongkun Linghu"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Song-Chun Zhu"
                    },
                    {
                        "name": "Siyuan Huang"
                    }
                ],
                "author_detail": {
                    "name": "Siyuan Huang"
                },
                "author": "Siyuan Huang",
                "arxiv_comment": "CVPR 2025. Project page: https://beacon-3d.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22420v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22420v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14781v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14781v5",
                "updated": "2025-04-01T05:09:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    5,
                    9,
                    19,
                    1,
                    91,
                    0
                ],
                "published": "2024-09-23T07:55:35Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    7,
                    55,
                    35,
                    0,
                    267,
                    0
                ],
                "title": "Pretraining Data Detection for Large Language Models: A Divergence-based\n  Calibration Method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pretraining Data Detection for Large Language Models: A Divergence-based\n  Calibration Method"
                },
                "summary": "As the scale of training corpora for large language models (LLMs) grows,\nmodel developers become increasingly reluctant to disclose details on their\ndata. This lack of transparency poses challenges to scientific evaluation and\nethical deployment. Recently, pretraining data detection approaches, which\ninfer whether a given text was part of an LLM's training data through black-box\naccess, have been explored. The Min-K\\% Prob method, which has achieved\nstate-of-the-art results, assumes that a non-training example tends to contain\na few outlier words with low token probabilities. However, the effectiveness\nmay be limited as it tends to misclassify non-training texts that contain many\ncommon words with high probabilities predicted by LLMs. To address this issue,\nwe introduce a divergence-based calibration method, inspired by the\ndivergence-from-randomness concept, to calibrate token probabilities for\npretraining data detection. We compute the cross-entropy (i.e., the divergence)\nbetween the token probability distribution and the token frequency distribution\nto derive a detection score. We have developed a Chinese-language benchmark,\nPatentMIA, to assess the performance of detection approaches for LLMs on\nChinese text. Experimental results on English-language benchmarks and PatentMIA\ndemonstrate that our proposed method significantly outperforms existing\nmethods. Our code and PatentMIA benchmark are available at\nhttps://github.com/zhang-wei-chao/DC-PDD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the scale of training corpora for large language models (LLMs) grows,\nmodel developers become increasingly reluctant to disclose details on their\ndata. This lack of transparency poses challenges to scientific evaluation and\nethical deployment. Recently, pretraining data detection approaches, which\ninfer whether a given text was part of an LLM's training data through black-box\naccess, have been explored. The Min-K\\% Prob method, which has achieved\nstate-of-the-art results, assumes that a non-training example tends to contain\na few outlier words with low token probabilities. However, the effectiveness\nmay be limited as it tends to misclassify non-training texts that contain many\ncommon words with high probabilities predicted by LLMs. To address this issue,\nwe introduce a divergence-based calibration method, inspired by the\ndivergence-from-randomness concept, to calibrate token probabilities for\npretraining data detection. We compute the cross-entropy (i.e., the divergence)\nbetween the token probability distribution and the token frequency distribution\nto derive a detection score. We have developed a Chinese-language benchmark,\nPatentMIA, to assess the performance of detection approaches for LLMs on\nChinese text. Experimental results on English-language benchmarks and PatentMIA\ndemonstrate that our proposed method significantly outperforms existing\nmethods. Our code and PatentMIA benchmark are available at\nhttps://github.com/zhang-wei-chao/DC-PDD."
                },
                "authors": [
                    {
                        "name": "Weichao Zhang"
                    },
                    {
                        "name": "Ruqing Zhang"
                    },
                    {
                        "name": "Jiafeng Guo"
                    },
                    {
                        "name": "Maarten de Rijke"
                    },
                    {
                        "name": "Yixing Fan"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "arxiv_comment": "Accepted by EMNLP 2024 main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14781v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14781v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06559v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06559v2",
                "updated": "2025-04-01T05:04:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    5,
                    4,
                    47,
                    1,
                    91,
                    0
                ],
                "published": "2024-11-10T18:50:51Z",
                "published_parsed": [
                    2024,
                    11,
                    10,
                    18,
                    50,
                    51,
                    6,
                    315,
                    0
                ],
                "title": "Is Your LLM Secretly a World Model of the Internet? Model-Based Planning\n  for Web Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Your LLM Secretly a World Model of the Internet? Model-Based Planning\n  for Web Agents"
                },
                "summary": "Language agents based on large language models (LLMs) have demonstrated great\npromise in automating web-based tasks. Recent work has shown that incorporating\nadvanced planning algorithms, e.g., tree search, is advantageous over reactive\nplanning for web agents. However, unlike simulated sandbox environments,\nreal-world environments such as the web are rife with irreversible actions.\nThis undermines the feasibility of backtracking, a cornerstone of (tree)\nsearch. Overly relying on test-time search also hurts efficiency. We advocate\nmodel-based planning for web agents that employs a world model to simulate and\ndeliberate over the outcome of each candidate action before committing to one.\nWe systematically explore this paradigm by (1) Proposing a model-based planning\nframework, WebDreamer, which employs LLMs to serve as both world models and\nvalue functions; (2) Training specialized LLMs as world models with a scalable\ndata synthesis pipeline. Empirical results demonstrate that WebDreamer achieves\nsubstantial performance improvements over reactive baselines. It is\ncompetitive, while being 4-5 times more efficient, with tree search in sandbox\nenvironments (VisualWebArena) and also works effectively on real-world websites\n(Online-Mind2Web and Mind2Web-Live). Furthermore, our trained world model,\nDreamer-7B, performs comparable to GPT-4o, highlighting the potential of\nspecialized world models for efficient and effective planning in complex web\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language agents based on large language models (LLMs) have demonstrated great\npromise in automating web-based tasks. Recent work has shown that incorporating\nadvanced planning algorithms, e.g., tree search, is advantageous over reactive\nplanning for web agents. However, unlike simulated sandbox environments,\nreal-world environments such as the web are rife with irreversible actions.\nThis undermines the feasibility of backtracking, a cornerstone of (tree)\nsearch. Overly relying on test-time search also hurts efficiency. We advocate\nmodel-based planning for web agents that employs a world model to simulate and\ndeliberate over the outcome of each candidate action before committing to one.\nWe systematically explore this paradigm by (1) Proposing a model-based planning\nframework, WebDreamer, which employs LLMs to serve as both world models and\nvalue functions; (2) Training specialized LLMs as world models with a scalable\ndata synthesis pipeline. Empirical results demonstrate that WebDreamer achieves\nsubstantial performance improvements over reactive baselines. It is\ncompetitive, while being 4-5 times more efficient, with tree search in sandbox\nenvironments (VisualWebArena) and also works effectively on real-world websites\n(Online-Mind2Web and Mind2Web-Live). Furthermore, our trained world model,\nDreamer-7B, performs comparable to GPT-4o, highlighting the potential of\nspecialized world models for efficient and effective planning in complex web\nenvironments."
                },
                "authors": [
                    {
                        "name": "Yu Gu"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Yuting Ning"
                    },
                    {
                        "name": "Boyuan Zheng"
                    },
                    {
                        "name": "Boyu Gou"
                    },
                    {
                        "name": "Tianci Xue"
                    },
                    {
                        "name": "Cheng Chang"
                    },
                    {
                        "name": "Sanjari Srivastava"
                    },
                    {
                        "name": "Yanan Xie"
                    },
                    {
                        "name": "Peng Qi"
                    },
                    {
                        "name": "Huan Sun"
                    },
                    {
                        "name": "Yu Su"
                    }
                ],
                "author_detail": {
                    "name": "Yu Su"
                },
                "author": "Yu Su",
                "arxiv_comment": "22 pages, 11 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06559v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06559v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15207v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15207v3",
                "updated": "2025-04-01T05:01:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    5,
                    1,
                    15,
                    1,
                    91,
                    0
                ],
                "published": "2024-08-27T17:14:21Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    14,
                    21,
                    1,
                    240,
                    0
                ],
                "title": "Understanding the Effectiveness of Coverage Criteria for Large Language\n  Models: A Special Angle from Jailbreak Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the Effectiveness of Coverage Criteria for Large Language\n  Models: A Special Angle from Jailbreak Attacks"
                },
                "summary": "Large language models (LLMs) have revolutionized artificial intelligence, but\ntheir increasing deployment across critical domains has raised concerns about\ntheir abnormal behaviors when faced with malicious attacks. Such vulnerability\nalerts the widespread inadequacy of pre-release testing. In this paper, we\nconduct a comprehensive empirical study to evaluate the effectiveness of\ntraditional coverage criteria in identifying such inadequacies, exemplified by\nthe significant security concern of jailbreak attacks. Our study begins with a\nclustering analysis of the hidden states of LLMs, revealing that the embedded\ncharacteristics effectively distinguish between different query types. We then\nsystematically evaluate the performance of these criteria across three key\ndimensions: criterion level, layer level, and token level. Our research\nuncovers significant differences in neuron coverage when LLMs process normal\nversus jailbreak queries, aligning with our clustering experiments. Leveraging\nthese findings, we propose three practical applications of coverage criteria in\nthe context of LLM security testing. Specifically, we develop a real-time\njailbreak detection mechanism that achieves high accuracy (93.61% on average)\nin classifying queries as normal or jailbreak. Furthermore, we explore the use\nof coverage levels to prioritize test cases, improving testing efficiency by\nfocusing on high-risk interactions and removing redundant tests. Lastly, we\nintroduce a coverage-guided approach for generating jailbreak attack examples,\nenabling systematic refinement of prompts to uncover vulnerabilities. This\nstudy improves our understanding of LLM security testing, enhances their\nsafety, and provides a foundation for developing more robust AI applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have revolutionized artificial intelligence, but\ntheir increasing deployment across critical domains has raised concerns about\ntheir abnormal behaviors when faced with malicious attacks. Such vulnerability\nalerts the widespread inadequacy of pre-release testing. In this paper, we\nconduct a comprehensive empirical study to evaluate the effectiveness of\ntraditional coverage criteria in identifying such inadequacies, exemplified by\nthe significant security concern of jailbreak attacks. Our study begins with a\nclustering analysis of the hidden states of LLMs, revealing that the embedded\ncharacteristics effectively distinguish between different query types. We then\nsystematically evaluate the performance of these criteria across three key\ndimensions: criterion level, layer level, and token level. Our research\nuncovers significant differences in neuron coverage when LLMs process normal\nversus jailbreak queries, aligning with our clustering experiments. Leveraging\nthese findings, we propose three practical applications of coverage criteria in\nthe context of LLM security testing. Specifically, we develop a real-time\njailbreak detection mechanism that achieves high accuracy (93.61% on average)\nin classifying queries as normal or jailbreak. Furthermore, we explore the use\nof coverage levels to prioritize test cases, improving testing efficiency by\nfocusing on high-risk interactions and removing redundant tests. Lastly, we\nintroduce a coverage-guided approach for generating jailbreak attack examples,\nenabling systematic refinement of prompts to uncover vulnerabilities. This\nstudy improves our understanding of LLM security testing, enhances their\nsafety, and provides a foundation for developing more robust AI applications."
                },
                "authors": [
                    {
                        "name": "Shide Zhou"
                    },
                    {
                        "name": "Tianlin Li"
                    },
                    {
                        "name": "Kailong Wang"
                    },
                    {
                        "name": "Yihao Huang"
                    },
                    {
                        "name": "Ling Shi"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Haoyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoyu Wang"
                },
                "author": "Haoyu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15207v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15207v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06494v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06494v2",
                "updated": "2025-04-01T04:26:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    4,
                    26,
                    56,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-09T07:36:40Z",
                "published_parsed": [
                    2025,
                    3,
                    9,
                    7,
                    36,
                    40,
                    6,
                    68,
                    0
                ],
                "title": "UAV-Assisted Coverage Hole Detection Using Reinforcement Learning in\n  Urban Cellular Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UAV-Assisted Coverage Hole Detection Using Reinforcement Learning in\n  Urban Cellular Networks"
                },
                "summary": "Deployment of cellular networks in urban areas requires addressing various\nchallenges. For example, high-rise buildings with varying geometrical shapes\nand heights contribute to signal attenuation, reflection, diffraction, and\nscattering effects. This creates a high possibility of coverage holes (CHs)\nwithin the proximity of the buildings. Detecting these CHs is critical for\nnetwork operators to ensure quality of service, as customers in these areas may\nexperience weak or no signal reception. To address this challenge, we propose\nan approach using an autonomous vehicle, such as an unmanned aerial vehicle\n(UAV), to detect CHs, for minimizing drive test efforts and reducing human\nlabor. The UAV leverages reinforcement learning (RL) to find CHs using stored\nlocal building maps, its current location, and measured signal strengths. As\nthe UAV moves, it dynamically updates its knowledge of the signal environment\nand its direction to a nearby CH while avoiding collisions with buildings. We\ncreated a wide range of testing scenarios using building maps from\nOpenStreetMap and signal strength data generated by NVIDIA Sionna raytracing\nsimulations. The results show that the RL-based approach outperforms\nnon-machine learning, geometry-based methods in detecting CHs in urban areas.\nAdditionally, even with a limited number of UAV measurements, the method\nachieves performance close to theoretical upper bounds that assume complete\nknowledge of all signal strengths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deployment of cellular networks in urban areas requires addressing various\nchallenges. For example, high-rise buildings with varying geometrical shapes\nand heights contribute to signal attenuation, reflection, diffraction, and\nscattering effects. This creates a high possibility of coverage holes (CHs)\nwithin the proximity of the buildings. Detecting these CHs is critical for\nnetwork operators to ensure quality of service, as customers in these areas may\nexperience weak or no signal reception. To address this challenge, we propose\nan approach using an autonomous vehicle, such as an unmanned aerial vehicle\n(UAV), to detect CHs, for minimizing drive test efforts and reducing human\nlabor. The UAV leverages reinforcement learning (RL) to find CHs using stored\nlocal building maps, its current location, and measured signal strengths. As\nthe UAV moves, it dynamically updates its knowledge of the signal environment\nand its direction to a nearby CH while avoiding collisions with buildings. We\ncreated a wide range of testing scenarios using building maps from\nOpenStreetMap and signal strength data generated by NVIDIA Sionna raytracing\nsimulations. The results show that the RL-based approach outperforms\nnon-machine learning, geometry-based methods in detecting CHs in urban areas.\nAdditionally, even with a limited number of UAV measurements, the method\nachieves performance close to theoretical upper bounds that assume complete\nknowledge of all signal strengths."
                },
                "authors": [
                    {
                        "name": "Mushfiqur Rahman"
                    },
                    {
                        "name": "Ismail Guvenc"
                    },
                    {
                        "name": "David Ramirez"
                    },
                    {
                        "name": "Chau-Wai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Chau-Wai Wong"
                },
                "author": "Chau-Wai Wong",
                "arxiv_comment": "Accepted at the ICC 2025 Workshop on 6G Connected Robotics for\n  Collaborative Control, Sensing, and Communication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06494v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06494v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15426v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15426v2",
                "updated": "2025-04-01T03:53:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    3,
                    53,
                    53,
                    1,
                    91,
                    0
                ],
                "published": "2024-03-13T05:38:39Z",
                "published_parsed": [
                    2024,
                    3,
                    13,
                    5,
                    38,
                    39,
                    2,
                    73,
                    0
                ],
                "title": "CodingTeachLLM: Empowering LLM's Coding Ability via AST Prior Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodingTeachLLM: Empowering LLM's Coding Ability via AST Prior Knowledge"
                },
                "summary": "In this paper, we introduce CodingTeachLLM, a large language model (LLM)\ndesigned for coding teaching. Specially, we aim to enhance the coding ability\nof LLM and lead it to better teaching mode in education context. Thus, we\npropose an end-to-end prior-based three-phases supervised fine-tuned model,\nwhich is proved more competitive than traditional fine-tuning method. More\nspecifically, our model realizes the structural disassembly and incremental\nguided output of educational knowledge. To this end, we robustify data\nclassification of three types via a sampler and overlap estimation neural\nnetwork, and inject the preprocessing datasets into pre-trained model in three\nbatches for LORA fine-tuning. Then, we design a prior module couples system\nprompt, vector databases, and abstract syntax tree task segmentation. Finally,\nthe compression method and regularization constraint are applied to the\nprior-based fine-tuned model, followed by text filter at the output end to\nobtain incremental guided results. Our model represents the first research\neffort to truly embody the tutor role with the features of abundant educational\nknowledge, step-by-step incremental guided outputs and non-disclosure of\nanswers. Extensive experiments report that our model also achieves\nstate-of-the-art in code abilities compared to open-source models, reaching an\nimpressive 75.10% on the HumanEval (@pass 1) benchmark. Additionally, our model\nmaintains strong conversational capabilities, with the 13B quantized version\nachieving scores of 56.34, 50.60, and 45.27 respectively on the MMLU, C-Eval,\nand AGIEval (5 shot) dialogue evaluation benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce CodingTeachLLM, a large language model (LLM)\ndesigned for coding teaching. Specially, we aim to enhance the coding ability\nof LLM and lead it to better teaching mode in education context. Thus, we\npropose an end-to-end prior-based three-phases supervised fine-tuned model,\nwhich is proved more competitive than traditional fine-tuning method. More\nspecifically, our model realizes the structural disassembly and incremental\nguided output of educational knowledge. To this end, we robustify data\nclassification of three types via a sampler and overlap estimation neural\nnetwork, and inject the preprocessing datasets into pre-trained model in three\nbatches for LORA fine-tuning. Then, we design a prior module couples system\nprompt, vector databases, and abstract syntax tree task segmentation. Finally,\nthe compression method and regularization constraint are applied to the\nprior-based fine-tuned model, followed by text filter at the output end to\nobtain incremental guided results. Our model represents the first research\neffort to truly embody the tutor role with the features of abundant educational\nknowledge, step-by-step incremental guided outputs and non-disclosure of\nanswers. Extensive experiments report that our model also achieves\nstate-of-the-art in code abilities compared to open-source models, reaching an\nimpressive 75.10% on the HumanEval (@pass 1) benchmark. Additionally, our model\nmaintains strong conversational capabilities, with the 13B quantized version\nachieving scores of 56.34, 50.60, and 45.27 respectively on the MMLU, C-Eval,\nand AGIEval (5 shot) dialogue evaluation benchmarks."
                },
                "authors": [
                    {
                        "name": "Zhangquan Chen"
                    },
                    {
                        "name": "Chunjiang Liu"
                    },
                    {
                        "name": "Haobin Duan"
                    }
                ],
                "author_detail": {
                    "name": "Haobin Duan"
                },
                "author": "Haobin Duan",
                "arxiv_comment": "9 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15426v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15426v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.17238v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.17238v2",
                "updated": "2025-04-01T03:50:12Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    3,
                    50,
                    12,
                    1,
                    91,
                    0
                ],
                "published": "2024-03-25T22:39:20Z",
                "published_parsed": [
                    2024,
                    3,
                    25,
                    22,
                    39,
                    20,
                    0,
                    85,
                    0
                ],
                "title": "Temporal and Semantic Evaluation Metrics for Foundation Models in\n  Post-Hoc Analysis of Robotic Sub-tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal and Semantic Evaluation Metrics for Foundation Models in\n  Post-Hoc Analysis of Robotic Sub-tasks"
                },
                "summary": "Recent works in Task and Motion Planning (TAMP) show that training control\npolicies on language-supervised robot trajectories with quality labeled data\nmarkedly improves agent task success rates. However, the scarcity of such data\npresents a significant hurdle to extending these methods to general use cases.\nTo address this concern, we present an automated framework to decompose\ntrajectory data into temporally bounded and natural language-based descriptive\nsub-tasks by leveraging recent prompting strategies for Foundation Models (FMs)\nincluding both Large Language Models (LLMs) and Vision Language Models (VLMs).\nOur framework provides both time-based and language-based descriptions for\nlower-level sub-tasks that comprise full trajectories. To rigorously evaluate\nthe quality of our automatic labeling framework, we contribute an algorithm\nSIMILARITY to produce two novel metrics, temporal similarity and semantic\nsimilarity. The metrics measure the temporal alignment and semantic fidelity of\nlanguage descriptions between two sub-task decompositions, namely an FM\nsub-task decomposition prediction and a ground-truth sub-task decomposition. We\npresent scores for temporal similarity and semantic similarity above 90%,\ncompared to 30% of a randomized baseline, for multiple robotic environments,\ndemonstrating the effectiveness of our proposed framework. Our results enable\nbuilding diverse, large-scale, language-supervised datasets for improved\nrobotic TAMP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent works in Task and Motion Planning (TAMP) show that training control\npolicies on language-supervised robot trajectories with quality labeled data\nmarkedly improves agent task success rates. However, the scarcity of such data\npresents a significant hurdle to extending these methods to general use cases.\nTo address this concern, we present an automated framework to decompose\ntrajectory data into temporally bounded and natural language-based descriptive\nsub-tasks by leveraging recent prompting strategies for Foundation Models (FMs)\nincluding both Large Language Models (LLMs) and Vision Language Models (VLMs).\nOur framework provides both time-based and language-based descriptions for\nlower-level sub-tasks that comprise full trajectories. To rigorously evaluate\nthe quality of our automatic labeling framework, we contribute an algorithm\nSIMILARITY to produce two novel metrics, temporal similarity and semantic\nsimilarity. The metrics measure the temporal alignment and semantic fidelity of\nlanguage descriptions between two sub-task decompositions, namely an FM\nsub-task decomposition prediction and a ground-truth sub-task decomposition. We\npresent scores for temporal similarity and semantic similarity above 90%,\ncompared to 30% of a randomized baseline, for multiple robotic environments,\ndemonstrating the effectiveness of our proposed framework. Our results enable\nbuilding diverse, large-scale, language-supervised datasets for improved\nrobotic TAMP."
                },
                "authors": [
                    {
                        "name": "Jonathan Salfity"
                    },
                    {
                        "name": "Selma Wanna"
                    },
                    {
                        "name": "Minkyu Choi"
                    },
                    {
                        "name": "Mitch Pryor"
                    }
                ],
                "author_detail": {
                    "name": "Mitch Pryor"
                },
                "author": "Mitch Pryor",
                "arxiv_comment": "8 pages, 3 figures. IROS 2024 Submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.17238v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.17238v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11381v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11381v2",
                "updated": "2025-04-01T03:44:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    3,
                    44,
                    0,
                    1,
                    91,
                    0
                ],
                "published": "2025-02-17T02:53:08Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    2,
                    53,
                    8,
                    0,
                    48,
                    0
                ],
                "title": "Without Paired Labeled Data: An End-to-End Self-Supervised Paradigm for\n  UAV-View Geo-Localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Without Paired Labeled Data: An End-to-End Self-Supervised Paradigm for\n  UAV-View Geo-Localization"
                },
                "summary": "UAV-View Geo-Localization (UVGL) aims to achieve accurate localization of\nunmanned aerial vehicles (UAVs) by retrieving the most relevant GPS-tagged\nsatellite images. However, existing methods heavily rely on pre-paired\nUAV-satellite images for supervised learning. Such dependency not only incurs\nhigh annotation costs but also severely limits scalability and practical\ndeployment in open-world UVGL scenarios. To address these limitations, we\npropose an end-to-end self-supervised UVGL method. Our method leverages a\nshallow backbone network to extract initial features, employs clustering to\ngenerate pseudo labels, and adopts a dual-path contrastive learning\narchitecture to learn discriminative intra-view representations. Furthermore,\nour method incorporates two core modules, the dynamic hierarchical memory\nlearning module and the information consistency evolution learning module. The\ndynamic hierarchical memory learning module combines short-term and long-term\nmemory to enhance intra-view feature consistency and discriminability.\nMeanwhile, the information consistency evolution learning module leverages a\nneighborhood-driven dynamic constraint mechanism to systematically capture\nimplicit cross-view semantic correlations, thereby improving cross-view feature\nalignment. To further stabilize and strengthen the self-supervised training\nprocess, a pseudo-label enhancement strategy is introduced, which refines the\nquality of pseudo supervision. Our method ultimately constructs a unified\ncross-view feature representation space under self-supervised settings.\nExtensive experiments on three public benchmark datasets demonstrate that the\nproposed method consistently outperforms existing self-supervised methods and\neven surpasses several state-of-the-art supervised methods. Our code is\navailable at https://github.com/ISChenawei/DMNIL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UAV-View Geo-Localization (UVGL) aims to achieve accurate localization of\nunmanned aerial vehicles (UAVs) by retrieving the most relevant GPS-tagged\nsatellite images. However, existing methods heavily rely on pre-paired\nUAV-satellite images for supervised learning. Such dependency not only incurs\nhigh annotation costs but also severely limits scalability and practical\ndeployment in open-world UVGL scenarios. To address these limitations, we\npropose an end-to-end self-supervised UVGL method. Our method leverages a\nshallow backbone network to extract initial features, employs clustering to\ngenerate pseudo labels, and adopts a dual-path contrastive learning\narchitecture to learn discriminative intra-view representations. Furthermore,\nour method incorporates two core modules, the dynamic hierarchical memory\nlearning module and the information consistency evolution learning module. The\ndynamic hierarchical memory learning module combines short-term and long-term\nmemory to enhance intra-view feature consistency and discriminability.\nMeanwhile, the information consistency evolution learning module leverages a\nneighborhood-driven dynamic constraint mechanism to systematically capture\nimplicit cross-view semantic correlations, thereby improving cross-view feature\nalignment. To further stabilize and strengthen the self-supervised training\nprocess, a pseudo-label enhancement strategy is introduced, which refines the\nquality of pseudo supervision. Our method ultimately constructs a unified\ncross-view feature representation space under self-supervised settings.\nExtensive experiments on three public benchmark datasets demonstrate that the\nproposed method consistently outperforms existing self-supervised methods and\neven surpasses several state-of-the-art supervised methods. Our code is\navailable at https://github.com/ISChenawei/DMNIL."
                },
                "authors": [
                    {
                        "name": "Zhongwei Chen"
                    },
                    {
                        "name": "Zhao-Xu Yang"
                    },
                    {
                        "name": "Hai-Jun Rong"
                    }
                ],
                "author_detail": {
                    "name": "Hai-Jun Rong"
                },
                "author": "Hai-Jun Rong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11381v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11381v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01999v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01999v3",
                "updated": "2025-04-01T03:31:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    3,
                    31,
                    38,
                    1,
                    91,
                    0
                ],
                "published": "2024-10-02T20:04:02Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    20,
                    4,
                    2,
                    2,
                    276,
                    0
                ],
                "title": "CodeMMLU: A Multi-Task Benchmark for Assessing Code Understanding\n  Capabilities of CodeLLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeMMLU: A Multi-Task Benchmark for Assessing Code Understanding\n  Capabilities of CodeLLMs"
                },
                "summary": "Recent advances in Code Large Language Models (CodeLLMs) have primarily\nfocused on open-ended code generation, often overlooking the crucial aspect of\ncode understanding and reasoning. To bridge this gap, we introduce CodeMMLU, a\ncomprehensive multiple-choice benchmark designed to evaluate the depth of\nsoftware and code comprehension in LLMs. CodeMMLU includes nearly 20,000\nquestions spanning diverse domains, including code analysis, defect detection,\nand software engineering principles across multiple programming languages.\nUnlike traditional benchmarks that emphasize code generation, CodeMMLU assesses\na model's ability to reason about programs across a wide-range of tasks such as\ncode repair, execution reasoning, and fill-in-the-blank challenges. Our\nextensive evaluation reveals that even state-of-the-art models struggle with\nCodeMMLU, highlighting significant gaps in comprehension beyond generation. By\nemphasizing the essential connection between code understanding and effective\nAI-assisted development, CodeMMLU provides a critical resource for advancing\nmore reliable and capable coding assistants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Code Large Language Models (CodeLLMs) have primarily\nfocused on open-ended code generation, often overlooking the crucial aspect of\ncode understanding and reasoning. To bridge this gap, we introduce CodeMMLU, a\ncomprehensive multiple-choice benchmark designed to evaluate the depth of\nsoftware and code comprehension in LLMs. CodeMMLU includes nearly 20,000\nquestions spanning diverse domains, including code analysis, defect detection,\nand software engineering principles across multiple programming languages.\nUnlike traditional benchmarks that emphasize code generation, CodeMMLU assesses\na model's ability to reason about programs across a wide-range of tasks such as\ncode repair, execution reasoning, and fill-in-the-blank challenges. Our\nextensive evaluation reveals that even state-of-the-art models struggle with\nCodeMMLU, highlighting significant gaps in comprehension beyond generation. By\nemphasizing the essential connection between code understanding and effective\nAI-assisted development, CodeMMLU provides a critical resource for advancing\nmore reliable and capable coding assistants."
                },
                "authors": [
                    {
                        "name": "Dung Nguyen Manh"
                    },
                    {
                        "name": "Thang Phan Chau"
                    },
                    {
                        "name": "Nam Le Hai"
                    },
                    {
                        "name": "Thong T. Doan"
                    },
                    {
                        "name": "Nam V. Nguyen"
                    },
                    {
                        "name": "Quang Pham"
                    },
                    {
                        "name": "Nghi D. Q. Bui"
                    }
                ],
                "author_detail": {
                    "name": "Nghi D. Q. Bui"
                },
                "author": "Nghi D. Q. Bui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01999v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01999v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08185v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08185v2",
                "updated": "2025-04-01T03:19:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    3,
                    19,
                    22,
                    1,
                    91,
                    0
                ],
                "published": "2024-12-11T08:24:15Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    8,
                    24,
                    15,
                    2,
                    346,
                    0
                ],
                "title": "Exploring Multidimensional Checkworthiness: Designing AI-assisted Claim\n  Prioritization for Human Fact-checkers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Multidimensional Checkworthiness: Designing AI-assisted Claim\n  Prioritization for Human Fact-checkers"
                },
                "summary": "Given the massive volume of potentially false claims circulating online,\nclaim prioritization is essential in allocating limited human resources\navailable for fact-checking. In this study, we perceive claim prioritization as\nan information retrieval (IR) task: just as multidimensional IR relevance, with\nmany factors influencing which search results a user deems relevant,\ncheckworthiness is also multi-faceted, subjective, and even personal, with many\nfactors influencing how fact-checkers triage and select which claims to check.\nOur study investigated both the multidimensional nature of checkworthiness and\neffective tool support to assist fact-checkers in claim prioritization.\nMethodologically, we pursued Research through Design combined with mixed-method\nevaluation.\n  Specifically, we developed an AI-assisted claim prioritization prototype as a\nprobe to explore how fact-checkers use multidimensional checkworthy factors to\nprioritize claims, simultaneously probing fact-checker needs and exploring the\ndesign space to meet those needs. With 16 professional fact-checkers\nparticipating in our study, we uncovered a hierarchical prioritization strategy\nfact-checkers implicitly use, revealing an underexplored aspect of their\nworkflow, with actionable design recommendations for improving claim triage\nacross multidimensional checkworthiness and tailoring this process with LLM\nintegration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given the massive volume of potentially false claims circulating online,\nclaim prioritization is essential in allocating limited human resources\navailable for fact-checking. In this study, we perceive claim prioritization as\nan information retrieval (IR) task: just as multidimensional IR relevance, with\nmany factors influencing which search results a user deems relevant,\ncheckworthiness is also multi-faceted, subjective, and even personal, with many\nfactors influencing how fact-checkers triage and select which claims to check.\nOur study investigated both the multidimensional nature of checkworthiness and\neffective tool support to assist fact-checkers in claim prioritization.\nMethodologically, we pursued Research through Design combined with mixed-method\nevaluation.\n  Specifically, we developed an AI-assisted claim prioritization prototype as a\nprobe to explore how fact-checkers use multidimensional checkworthy factors to\nprioritize claims, simultaneously probing fact-checker needs and exploring the\ndesign space to meet those needs. With 16 professional fact-checkers\nparticipating in our study, we uncovered a hierarchical prioritization strategy\nfact-checkers implicitly use, revealing an underexplored aspect of their\nworkflow, with actionable design recommendations for improving claim triage\nacross multidimensional checkworthiness and tailoring this process with LLM\nintegration."
                },
                "authors": [
                    {
                        "name": "Houjiang Liu"
                    },
                    {
                        "name": "Jacek Gwizdka"
                    },
                    {
                        "name": "Matthew Lease"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Lease"
                },
                "author": "Matthew Lease",
                "arxiv_comment": "Accepted at CSCW 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08185v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08185v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07272v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07272v3",
                "updated": "2025-04-01T03:14:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    3,
                    14,
                    15,
                    1,
                    91,
                    0
                ],
                "published": "2025-02-11T05:39:49Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    5,
                    39,
                    49,
                    1,
                    42,
                    0
                ],
                "title": "GENERator: A Long-Context Generative Genomic Foundation Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GENERator: A Long-Context Generative Genomic Foundation Model"
                },
                "summary": "Advancements in DNA sequencing technologies have significantly improved our\nability to decode genomic sequences. However, the prediction and interpretation\nof these sequences remain challenging due to the intricate nature of genetic\nmaterial. Large language models (LLMs) have introduced new opportunities for\nbiological sequence analysis. Recent developments in genomic language models\nhave underscored the potential of LLMs in deciphering DNA sequences.\nNonetheless, existing models often face limitations in robustness and\napplication scope, primarily due to constraints in model structure and training\ndata scale. To address these limitations, we present GENERator, a generative\ngenomic foundation model featuring a context length of 98k base pairs (bp) and\n1.2B parameters. Trained on an expansive dataset comprising 386B bp of\neukaryotic DNA, the GENERator demonstrates state-of-the-art performance across\nboth established and newly proposed benchmarks. The model adheres to the\ncentral dogma of molecular biology, accurately generating protein-coding\nsequences that translate into proteins structurally analogous to known\nfamilies. It also shows significant promise in sequence optimization,\nparticularly through the prompt-responsive generation of enhancer sequences\nwith specific activity profiles. These capabilities position the GENERator as a\npivotal tool for genomic research and biotechnological advancement, enhancing\nour ability to interpret and predict complex biological systems and enabling\nprecise genomic interventions. Implementation details and supplementary\nresources are available at https://github.com/GenerTeam/GENERator.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in DNA sequencing technologies have significantly improved our\nability to decode genomic sequences. However, the prediction and interpretation\nof these sequences remain challenging due to the intricate nature of genetic\nmaterial. Large language models (LLMs) have introduced new opportunities for\nbiological sequence analysis. Recent developments in genomic language models\nhave underscored the potential of LLMs in deciphering DNA sequences.\nNonetheless, existing models often face limitations in robustness and\napplication scope, primarily due to constraints in model structure and training\ndata scale. To address these limitations, we present GENERator, a generative\ngenomic foundation model featuring a context length of 98k base pairs (bp) and\n1.2B parameters. Trained on an expansive dataset comprising 386B bp of\neukaryotic DNA, the GENERator demonstrates state-of-the-art performance across\nboth established and newly proposed benchmarks. The model adheres to the\ncentral dogma of molecular biology, accurately generating protein-coding\nsequences that translate into proteins structurally analogous to known\nfamilies. It also shows significant promise in sequence optimization,\nparticularly through the prompt-responsive generation of enhancer sequences\nwith specific activity profiles. These capabilities position the GENERator as a\npivotal tool for genomic research and biotechnological advancement, enhancing\nour ability to interpret and predict complex biological systems and enabling\nprecise genomic interventions. Implementation details and supplementary\nresources are available at https://github.com/GenerTeam/GENERator."
                },
                "authors": [
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Qiuyi Li"
                    },
                    {
                        "name": "Mingyang Li"
                    },
                    {
                        "name": "Kun Fu"
                    },
                    {
                        "name": "Fuli Feng"
                    },
                    {
                        "name": "Jieping Ye"
                    },
                    {
                        "name": "Hui Xiong"
                    },
                    {
                        "name": "Zheng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Wang"
                },
                "author": "Zheng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07272v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07272v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23427v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23427v2",
                "updated": "2025-04-01T02:24:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    2,
                    24,
                    42,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-30T13:00:52Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    13,
                    0,
                    52,
                    6,
                    89,
                    0
                ],
                "title": "CoRanking: Collaborative Ranking with Small and Large Ranking Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoRanking: Collaborative Ranking with Small and Large Ranking Agents"
                },
                "summary": "Large Language Models (LLMs) have demonstrated superior listwise ranking\nperformance. However, their superior performance often relies on large-scale\nparameters (\\eg, GPT-4) and a repetitive sliding window process, which\nintroduces significant efficiency challenges. In this paper, we propose\n\\textbf{CoRanking}, a novel collaborative ranking framework that combines small\nand large ranking models for efficient and effective ranking. CoRanking first\nemploys a small-size reranker to pre-rank all the candidate passages, bringing\nrelevant ones to the top part of the list (\\eg, top-20). Then, the LLM listwise\nreranker is applied to only rerank these top-ranked passages instead of the\nwhole list, substantially enhancing overall ranking efficiency. Although more\nefficient, previous studies have revealed that the LLM listwise reranker have\nsignificant positional biases on the order of input passages. Directly feed the\ntop-ranked passages from small reranker may result in the sub-optimal\nperformance of LLM listwise reranker. To alleviate this problem, we introduce a\npassage order adjuster trained via reinforcement learning, which reorders the\ntop passages from the small reranker to align with the LLM's preferences of\npassage order. Extensive experiments on three IR benchmarks demonstrate that\nCoRanking significantly improves efficiency (reducing ranking latency by about\n70\\%) while achieving even better effectiveness compared to using only the LLM\nlistwise reranker.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated superior listwise ranking\nperformance. However, their superior performance often relies on large-scale\nparameters (\\eg, GPT-4) and a repetitive sliding window process, which\nintroduces significant efficiency challenges. In this paper, we propose\n\\textbf{CoRanking}, a novel collaborative ranking framework that combines small\nand large ranking models for efficient and effective ranking. CoRanking first\nemploys a small-size reranker to pre-rank all the candidate passages, bringing\nrelevant ones to the top part of the list (\\eg, top-20). Then, the LLM listwise\nreranker is applied to only rerank these top-ranked passages instead of the\nwhole list, substantially enhancing overall ranking efficiency. Although more\nefficient, previous studies have revealed that the LLM listwise reranker have\nsignificant positional biases on the order of input passages. Directly feed the\ntop-ranked passages from small reranker may result in the sub-optimal\nperformance of LLM listwise reranker. To alleviate this problem, we introduce a\npassage order adjuster trained via reinforcement learning, which reorders the\ntop passages from the small reranker to align with the LLM's preferences of\npassage order. Extensive experiments on three IR benchmarks demonstrate that\nCoRanking significantly improves efficiency (reducing ranking latency by about\n70\\%) while achieving even better effectiveness compared to using only the LLM\nlistwise reranker."
                },
                "authors": [
                    {
                        "name": "Wenhan Liu"
                    },
                    {
                        "name": "Xinyu Ma"
                    },
                    {
                        "name": "Yutao Zhu"
                    },
                    {
                        "name": "Lixin Su"
                    },
                    {
                        "name": "Shuaiqiang Wang"
                    },
                    {
                        "name": "Dawei Yin"
                    },
                    {
                        "name": "Zhicheng Dou"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Dou"
                },
                "author": "Zhicheng Dou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23427v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23427v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10459v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10459v3",
                "updated": "2025-04-01T02:23:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    2,
                    23,
                    57,
                    1,
                    91,
                    0
                ],
                "published": "2024-06-15T01:02:48Z",
                "published_parsed": [
                    2024,
                    6,
                    15,
                    1,
                    2,
                    48,
                    5,
                    167,
                    0
                ],
                "title": "CancerLLM: A Large Language Model in Cancer Domain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CancerLLM: A Large Language Model in Cancer Domain"
                },
                "summary": "Medical Large Language Models (LLMs) have demonstrated impressive performance\non a wide variety of medical NLP tasks; however, there still lacks a LLM\nspecifically designed for phenotyping identification and diagnosis in cancer\ndomain. Moreover, these LLMs typically have several billions of parameters,\nmaking them computationally expensive for healthcare systems. Thus, in this\nstudy, we propose CancerLLM, a model with 7 billion parameters and a\nMistral-style architecture, pre-trained on nearly 2.7M clinical notes and over\n515K pathology reports covering 17 cancer types, followed by fine-tuning on two\ncancer-relevant tasks, including cancer phenotypes extraction and cancer\ndiagnosis generation. Our evaluation demonstrated that the CancerLLM achieves\nstate-of-the-art results with F1 score of 91.78% on phenotyping extraction and\n86.81% on disganois generation. It outperformed existing LLMs, with an average\nF1 score improvement of 9.23%. Additionally, the CancerLLM demonstrated its\nefficiency on time and GPU usage, and robustness comparing with other LLMs. We\ndemonstrated that CancerLLM can potentially provide an effective and robust\nsolution to advance clinical research and practice in cancer domain",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical Large Language Models (LLMs) have demonstrated impressive performance\non a wide variety of medical NLP tasks; however, there still lacks a LLM\nspecifically designed for phenotyping identification and diagnosis in cancer\ndomain. Moreover, these LLMs typically have several billions of parameters,\nmaking them computationally expensive for healthcare systems. Thus, in this\nstudy, we propose CancerLLM, a model with 7 billion parameters and a\nMistral-style architecture, pre-trained on nearly 2.7M clinical notes and over\n515K pathology reports covering 17 cancer types, followed by fine-tuning on two\ncancer-relevant tasks, including cancer phenotypes extraction and cancer\ndiagnosis generation. Our evaluation demonstrated that the CancerLLM achieves\nstate-of-the-art results with F1 score of 91.78% on phenotyping extraction and\n86.81% on disganois generation. It outperformed existing LLMs, with an average\nF1 score improvement of 9.23%. Additionally, the CancerLLM demonstrated its\nefficiency on time and GPU usage, and robustness comparing with other LLMs. We\ndemonstrated that CancerLLM can potentially provide an effective and robust\nsolution to advance clinical research and practice in cancer domain"
                },
                "authors": [
                    {
                        "name": "Mingchen Li"
                    },
                    {
                        "name": "Jiatan Huang"
                    },
                    {
                        "name": "Jeremy Yeung"
                    },
                    {
                        "name": "Anne Blaes"
                    },
                    {
                        "name": "Steven Johnson"
                    },
                    {
                        "name": "Hongfang Liu"
                    },
                    {
                        "name": "Hua Xu"
                    },
                    {
                        "name": "Rui Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Zhang"
                },
                "author": "Rui Zhang",
                "arxiv_comment": "new version, add the RAG version of cancerLLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10459v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10459v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04667v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04667v4",
                "updated": "2025-04-01T02:20:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    2,
                    20,
                    6,
                    1,
                    91,
                    0
                ],
                "published": "2024-08-06T16:43:35Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    16,
                    43,
                    35,
                    1,
                    219,
                    0
                ],
                "title": "Non-Determinism of \"Deterministic\" LLM Settings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-Determinism of \"Deterministic\" LLM Settings"
                },
                "summary": "LLM (large language model) practitioners commonly notice that outputs can\nvary for the same inputs under settings expected to be deterministic. Yet the\nquestions of how pervasive this is, and with what impact on results, have not\nto our knowledge been systematically investigated. We investigate\nnon-determinism in five LLMs configured to be deterministic when applied to\neight common tasks in across 10 runs, in both zero-shot and few-shot settings.\nWe see accuracy variations up to 15% across naturally occurring runs with a gap\nof best possible performance to worst possible performance up to 70%. In fact,\nnone of the LLMs consistently delivers repeatable accuracy across all tasks,\nmuch less identical output strings. Sharing preliminary results with insiders\nhas revealed that non-determinism perhaps essential to the efficient use of\ncompute resources via co-mingled data in input buffers so this issue is not\ngoing away anytime soon. To better quantify our observations, we introduce\nmetrics focused on quantifying determinism, TARr@N for the total agreement rate\nat N runs over raw output, and TARa@N for total agreement rate of parsed-out\nanswers. Our code and data are publicly available at\nhttp://github.com/REDACTED.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM (large language model) practitioners commonly notice that outputs can\nvary for the same inputs under settings expected to be deterministic. Yet the\nquestions of how pervasive this is, and with what impact on results, have not\nto our knowledge been systematically investigated. We investigate\nnon-determinism in five LLMs configured to be deterministic when applied to\neight common tasks in across 10 runs, in both zero-shot and few-shot settings.\nWe see accuracy variations up to 15% across naturally occurring runs with a gap\nof best possible performance to worst possible performance up to 70%. In fact,\nnone of the LLMs consistently delivers repeatable accuracy across all tasks,\nmuch less identical output strings. Sharing preliminary results with insiders\nhas revealed that non-determinism perhaps essential to the efficient use of\ncompute resources via co-mingled data in input buffers so this issue is not\ngoing away anytime soon. To better quantify our observations, we introduce\nmetrics focused on quantifying determinism, TARr@N for the total agreement rate\nat N runs over raw output, and TARa@N for total agreement rate of parsed-out\nanswers. Our code and data are publicly available at\nhttp://github.com/REDACTED."
                },
                "authors": [
                    {
                        "name": "Berk Atil"
                    },
                    {
                        "name": "Sarp Aykent"
                    },
                    {
                        "name": "Alexa Chittams"
                    },
                    {
                        "name": "Lisheng Fu"
                    },
                    {
                        "name": "Rebecca J. Passonneau"
                    },
                    {
                        "name": "Evan Radcliffe"
                    },
                    {
                        "name": "Guru Rajan Rajagopal"
                    },
                    {
                        "name": "Adam Sloan"
                    },
                    {
                        "name": "Tomasz Tudrej"
                    },
                    {
                        "name": "Ferhan Ture"
                    },
                    {
                        "name": "Zhe Wu"
                    },
                    {
                        "name": "Lixinyu Xu"
                    },
                    {
                        "name": "Breck Baldwin"
                    }
                ],
                "author_detail": {
                    "name": "Breck Baldwin"
                },
                "author": "Breck Baldwin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04667v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04667v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23764v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23764v2",
                "updated": "2025-04-01T02:13:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    2,
                    13,
                    23,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-31T06:28:41Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    6,
                    28,
                    41,
                    0,
                    90,
                    0
                ],
                "title": "WaveFormer: A 3D Transformer with Wavelet-Driven Feature Representation\n  for Efficient Medical Image Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WaveFormer: A 3D Transformer with Wavelet-Driven Feature Representation\n  for Efficient Medical Image Segmentation"
                },
                "summary": "Transformer-based architectures have advanced medical image analysis by\neffectively modeling long-range dependencies, yet they often struggle in 3D\nsettings due to substantial memory overhead and insufficient capture of\nfine-grained local features. We address these limitations with WaveFormer, a\nnovel 3D-transformer that: i) leverages the fundamental frequency-domain\nproperties of features for contextual representation, and ii) is inspired by\nthe top-down mechanism of the human visual recognition system, making it a\nbiologically motivated architecture. By employing discrete wavelet\ntransformations (DWT) at multiple scales, WaveFormer preserves both global\ncontext and high-frequency details while replacing heavy upsampling layers with\nefficient wavelet-based summarization and reconstruction. This significantly\nreduces the number of parameters, which is critical for real-world deployment\nwhere computational resources and training times are constrained. Furthermore,\nthe model is generic and easily adaptable to diverse applications. Evaluations\non BraTS2023, FLARE2021, and KiTS2023 demonstrate performance on par with\nstate-of-the-art methods while offering substantially lower computational\ncomplexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based architectures have advanced medical image analysis by\neffectively modeling long-range dependencies, yet they often struggle in 3D\nsettings due to substantial memory overhead and insufficient capture of\nfine-grained local features. We address these limitations with WaveFormer, a\nnovel 3D-transformer that: i) leverages the fundamental frequency-domain\nproperties of features for contextual representation, and ii) is inspired by\nthe top-down mechanism of the human visual recognition system, making it a\nbiologically motivated architecture. By employing discrete wavelet\ntransformations (DWT) at multiple scales, WaveFormer preserves both global\ncontext and high-frequency details while replacing heavy upsampling layers with\nefficient wavelet-based summarization and reconstruction. This significantly\nreduces the number of parameters, which is critical for real-world deployment\nwhere computational resources and training times are constrained. Furthermore,\nthe model is generic and easily adaptable to diverse applications. Evaluations\non BraTS2023, FLARE2021, and KiTS2023 demonstrate performance on par with\nstate-of-the-art methods while offering substantially lower computational\ncomplexity."
                },
                "authors": [
                    {
                        "name": "Md Mahfuz Al Hasan"
                    },
                    {
                        "name": "Mahdi Zaman"
                    },
                    {
                        "name": "Abdul Jawad"
                    },
                    {
                        "name": "Alberto Santamaria-Pang"
                    },
                    {
                        "name": "Ho Hin Lee"
                    },
                    {
                        "name": "Ivan Tarapov"
                    },
                    {
                        "name": "Kyle See"
                    },
                    {
                        "name": "Md Shah Imran"
                    },
                    {
                        "name": "Antika Roy"
                    },
                    {
                        "name": "Yaser Pourmohammadi Fallah"
                    },
                    {
                        "name": "Navid Asadizanjani"
                    },
                    {
                        "name": "Reza Forghani"
                    }
                ],
                "author_detail": {
                    "name": "Reza Forghani"
                },
                "author": "Reza Forghani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23764v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23764v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01141v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01141v2",
                "updated": "2025-04-01T00:41:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    0,
                    41,
                    36,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-03T03:48:20Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    3,
                    48,
                    20,
                    0,
                    62,
                    0
                ],
                "title": "How Well do LLMs Compress Their Own Chain-of-Thought? A Token Complexity\n  Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Well do LLMs Compress Their Own Chain-of-Thought? A Token Complexity\n  Approach"
                },
                "summary": "Chain-of-thought prompting has emerged as a powerful technique for enabling\nlarge language models (LLMs) to solve complex reasoning tasks. However, these\nreasoning chains can be verbose, raising concerns about efficiency. In\nresponse, recent works have sought to decrease response lengths through simple\nprompting strategies (e.g. 'be concise'). In this work, we conduct the first\nsystematic study of the relationship between reasoning length and model\nperformance across a diverse range of compression instructions (e.g. 'use 10\nwords or less' or 'remove all punctuation'). In doing so, we discover a\nuniversal tradeoff between reasoning length and accuracy that persists across\neven very distinct reasoning chains. We demonstrate that this tradeoff emerges\nfrom a sharp threshold behavior at the question level: each task has an\nintrinsic 'token complexity' - a minimal number of tokens required for\nsuccessful problem-solving. We show how token complexity enables us to compute\ninformation-theoretic limits on the accuracy-compression tradeoff, and find\nthat prompt-based compression strategies operate far from these theoretical\nlimits. This suggests there may be significant room for improvement and our\nframework provides a benchmark to help researchers evaluate progress in\nreasoning efficiency. Our work also highlights the importance of adaptive\ncompression -- giving shorter responses for easier questions -- and we show\nthat token complexity is a useful tool for measuring this capability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-thought prompting has emerged as a powerful technique for enabling\nlarge language models (LLMs) to solve complex reasoning tasks. However, these\nreasoning chains can be verbose, raising concerns about efficiency. In\nresponse, recent works have sought to decrease response lengths through simple\nprompting strategies (e.g. 'be concise'). In this work, we conduct the first\nsystematic study of the relationship between reasoning length and model\nperformance across a diverse range of compression instructions (e.g. 'use 10\nwords or less' or 'remove all punctuation'). In doing so, we discover a\nuniversal tradeoff between reasoning length and accuracy that persists across\neven very distinct reasoning chains. We demonstrate that this tradeoff emerges\nfrom a sharp threshold behavior at the question level: each task has an\nintrinsic 'token complexity' - a minimal number of tokens required for\nsuccessful problem-solving. We show how token complexity enables us to compute\ninformation-theoretic limits on the accuracy-compression tradeoff, and find\nthat prompt-based compression strategies operate far from these theoretical\nlimits. This suggests there may be significant room for improvement and our\nframework provides a benchmark to help researchers evaluate progress in\nreasoning efficiency. Our work also highlights the importance of adaptive\ncompression -- giving shorter responses for easier questions -- and we show\nthat token complexity is a useful tool for measuring this capability."
                },
                "authors": [
                    {
                        "name": "Ayeong Lee"
                    },
                    {
                        "name": "Ethan Che"
                    },
                    {
                        "name": "Tianyi Peng"
                    }
                ],
                "author_detail": {
                    "name": "Tianyi Peng"
                },
                "author": "Tianyi Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01141v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01141v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19326v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19326v2",
                "updated": "2025-04-01T00:07:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    0,
                    7,
                    54,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-25T03:43:11Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    3,
                    43,
                    11,
                    1,
                    84,
                    0
                ],
                "title": "Process or Result? Manipulated Ending Tokens Can Mislead Reasoning LLMs\n  to Ignore the Correct Reasoning Steps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Process or Result? Manipulated Ending Tokens Can Mislead Reasoning LLMs\n  to Ignore the Correct Reasoning Steps"
                },
                "summary": "Recent reasoning large language models (LLMs) have demonstrated remarkable\nimprovements in mathematical reasoning capabilities through long\nChain-of-Thought. The reasoning tokens of these models enable self-correction\nwithin reasoning chains, enhancing robustness. This motivates our exploration:\nhow vulnerable are reasoning LLMs to subtle errors in their input reasoning\nchains? We introduce \"Compromising Thought\" (CPT), a vulnerability where models\npresented with reasoning tokens containing manipulated calculation results tend\nto ignore correct reasoning steps and adopt incorrect results instead. Through\nsystematic evaluation across multiple reasoning LLMs, we design three\nincreasingly explicit prompting methods to measure CPT resistance, revealing\nthat models struggle significantly to identify and correct these manipulations.\nNotably, contrary to existing research suggesting structural alterations affect\nmodel performance more than content modifications, we find that local ending\ntoken manipulations have greater impact on reasoning outcomes than structural\nchanges. Moreover, we discover a security vulnerability in DeepSeek-R1 where\ntampered reasoning tokens can trigger complete reasoning cessation. Our work\nenhances understanding of reasoning robustness and highlights security\nconsiderations for reasoning-intensive applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent reasoning large language models (LLMs) have demonstrated remarkable\nimprovements in mathematical reasoning capabilities through long\nChain-of-Thought. The reasoning tokens of these models enable self-correction\nwithin reasoning chains, enhancing robustness. This motivates our exploration:\nhow vulnerable are reasoning LLMs to subtle errors in their input reasoning\nchains? We introduce \"Compromising Thought\" (CPT), a vulnerability where models\npresented with reasoning tokens containing manipulated calculation results tend\nto ignore correct reasoning steps and adopt incorrect results instead. Through\nsystematic evaluation across multiple reasoning LLMs, we design three\nincreasingly explicit prompting methods to measure CPT resistance, revealing\nthat models struggle significantly to identify and correct these manipulations.\nNotably, contrary to existing research suggesting structural alterations affect\nmodel performance more than content modifications, we find that local ending\ntoken manipulations have greater impact on reasoning outcomes than structural\nchanges. Moreover, we discover a security vulnerability in DeepSeek-R1 where\ntampered reasoning tokens can trigger complete reasoning cessation. Our work\nenhances understanding of reasoning robustness and highlights security\nconsiderations for reasoning-intensive applications."
                },
                "authors": [
                    {
                        "name": "Yu Cui"
                    },
                    {
                        "name": "Bryan Hooi"
                    },
                    {
                        "name": "Yujun Cai"
                    },
                    {
                        "name": "Yiwei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yiwei Wang"
                },
                "author": "Yiwei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19326v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19326v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03717v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03717v2",
                "updated": "2025-03-31T23:24:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    23,
                    24,
                    2,
                    0,
                    90,
                    0
                ],
                "published": "2025-02-06T02:07:18Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    2,
                    7,
                    18,
                    3,
                    37,
                    0
                ],
                "title": "Efficiently Generating Expressive Quadruped Behaviors via\n  Language-Guided Preference Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently Generating Expressive Quadruped Behaviors via\n  Language-Guided Preference Learning"
                },
                "summary": "Expressive robotic behavior is essential for the widespread acceptance of\nrobots in social environments. Recent advancements in learned legged locomotion\ncontrollers have enabled more dynamic and versatile robot behaviors. However,\ndetermining the optimal behavior for interactions with different users across\nvaried scenarios remains a challenge. Current methods either rely on natural\nlanguage input, which is efficient but low-resolution, or learn from human\npreferences, which, although high-resolution, is sample inefficient. This paper\nintroduces a novel approach that leverages priors generated by pre-trained LLMs\nalongside the precision of preference learning. Our method, termed\nLanguage-Guided Preference Learning (LGPL), uses LLMs to generate initial\nbehavior samples, which are then refined through preference-based feedback to\nlearn behaviors that closely align with human expectations. Our core insight is\nthat LLMs can guide the sampling process for preference learning, leading to a\nsubstantial improvement in sample efficiency. We demonstrate that LGPL can\nquickly learn accurate and expressive behaviors with as few as four queries,\noutperforming both purely language-parameterized models and traditional\npreference learning approaches. Website with videos:\nhttps://lgpl-gaits.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expressive robotic behavior is essential for the widespread acceptance of\nrobots in social environments. Recent advancements in learned legged locomotion\ncontrollers have enabled more dynamic and versatile robot behaviors. However,\ndetermining the optimal behavior for interactions with different users across\nvaried scenarios remains a challenge. Current methods either rely on natural\nlanguage input, which is efficient but low-resolution, or learn from human\npreferences, which, although high-resolution, is sample inefficient. This paper\nintroduces a novel approach that leverages priors generated by pre-trained LLMs\nalongside the precision of preference learning. Our method, termed\nLanguage-Guided Preference Learning (LGPL), uses LLMs to generate initial\nbehavior samples, which are then refined through preference-based feedback to\nlearn behaviors that closely align with human expectations. Our core insight is\nthat LLMs can guide the sampling process for preference learning, leading to a\nsubstantial improvement in sample efficiency. We demonstrate that LGPL can\nquickly learn accurate and expressive behaviors with as few as four queries,\noutperforming both purely language-parameterized models and traditional\npreference learning approaches. Website with videos:\nhttps://lgpl-gaits.github.io/"
                },
                "authors": [
                    {
                        "name": "Jaden Clark"
                    },
                    {
                        "name": "Joey Hejna"
                    },
                    {
                        "name": "Dorsa Sadigh"
                    }
                ],
                "author_detail": {
                    "name": "Dorsa Sadigh"
                },
                "author": "Dorsa Sadigh",
                "arxiv_comment": "8 pages 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03717v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03717v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12226v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12226v2",
                "updated": "2025-03-31T22:32:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    22,
                    32,
                    58,
                    0,
                    90,
                    0
                ],
                "published": "2025-02-17T15:26:16Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    15,
                    26,
                    16,
                    0,
                    48,
                    0
                ],
                "title": "On Creating a Causally Grounded Usable Rating Method for Assessing the\n  Robustness of Foundation Models Supporting Time Series",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Creating a Causally Grounded Usable Rating Method for Assessing the\n  Robustness of Foundation Models Supporting Time Series"
                },
                "summary": "Foundation Models (FMs) have improved time series forecasting in various\nsectors, such as finance, but their vulnerability to input disturbances can\nhinder their adoption by stakeholders, such as investors and analysts. To\naddress this, we propose a causally grounded rating framework to study the\nrobustness of Foundational Models for Time Series (FMTS) with respect to input\nperturbations. We evaluate our approach to the stock price prediction problem,\na well-studied problem with easily accessible public data, evaluating six\nstate-of-the-art (some multi-modal) FMTS across six prominent stocks spanning\nthree industries. The ratings proposed by our framework effectively assess the\nrobustness of FMTS and also offer actionable insights for model selection and\ndeployment. Within the scope of our study, we find that (1) multi-modal FMTS\nexhibit better robustness and accuracy compared to their uni-modal versions\nand, (2) FMTS pre-trained on time series forecasting task exhibit better\nrobustness and forecasting accuracy compared to general-purpose FMTS\npre-trained across diverse settings. Further, to validate our framework's\nusability, we conduct a user study showcasing FMTS prediction errors along with\nour computed ratings. The study confirmed that our ratings reduced the\ndifficulty for users in comparing the robustness of different systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation Models (FMs) have improved time series forecasting in various\nsectors, such as finance, but their vulnerability to input disturbances can\nhinder their adoption by stakeholders, such as investors and analysts. To\naddress this, we propose a causally grounded rating framework to study the\nrobustness of Foundational Models for Time Series (FMTS) with respect to input\nperturbations. We evaluate our approach to the stock price prediction problem,\na well-studied problem with easily accessible public data, evaluating six\nstate-of-the-art (some multi-modal) FMTS across six prominent stocks spanning\nthree industries. The ratings proposed by our framework effectively assess the\nrobustness of FMTS and also offer actionable insights for model selection and\ndeployment. Within the scope of our study, we find that (1) multi-modal FMTS\nexhibit better robustness and accuracy compared to their uni-modal versions\nand, (2) FMTS pre-trained on time series forecasting task exhibit better\nrobustness and forecasting accuracy compared to general-purpose FMTS\npre-trained across diverse settings. Further, to validate our framework's\nusability, we conduct a user study showcasing FMTS prediction errors along with\nour computed ratings. The study confirmed that our ratings reduced the\ndifficulty for users in comparing the robustness of different systems."
                },
                "authors": [
                    {
                        "name": "Kausik Lakkaraju"
                    },
                    {
                        "name": "Rachneet Kaur"
                    },
                    {
                        "name": "Parisa Zehtabi"
                    },
                    {
                        "name": "Sunandita Patra"
                    },
                    {
                        "name": "Siva Likitha Valluru"
                    },
                    {
                        "name": "Zhen Zeng"
                    },
                    {
                        "name": "Biplav Srivastava"
                    },
                    {
                        "name": "Marco Valtorta"
                    }
                ],
                "author_detail": {
                    "name": "Marco Valtorta"
                },
                "author": "Marco Valtorta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12226v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12226v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.14558v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.14558v6",
                "updated": "2025-03-31T21:04:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    21,
                    4,
                    11,
                    0,
                    90,
                    0
                ],
                "published": "2023-10-23T04:22:50Z",
                "published_parsed": [
                    2023,
                    10,
                    23,
                    4,
                    22,
                    50,
                    0,
                    296,
                    0
                ],
                "title": "AlpaCare:Instruction-tuned Large Language Models for Medical Application",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlpaCare:Instruction-tuned Large Language Models for Medical Application"
                },
                "summary": "Instruction-finetuning (IFT) has become crucial in aligning Large Language\nModels (LLMs) with diverse human needs and has shown great potential in medical\napplications. However, previous studies mainly fine-tune LLMs on biomedical\ndatasets with limited diversity, which often rely on benchmarks or narrow task\nscopes, and hence significantly limit the effectiveness on their medical\ninstruction-following ability and generalizability. To bridge this gap, we\npropose creating a diverse, machine-generated medical IFT dataset,\nMedInstruct-52k, using GPT-4 and ChatGPT with a high-quality expert-curated\nseed set. We then fine-tune LLaMA-series models on the dataset to develop\nAlpaCare. Despite using a smaller domain-specific dataset than previous medical\nLLMs, AlpaCare not only demonstrates superior performance on medical\napplications, with up to 38.1% absolute gain over best baselines in medical\nfree-form instruction evaluations, but also achieves 6.7% absolute gains\naveraged over multiple general domain benchmarks. Human evaluation further\nshows that AlpaCare consistently outperforms best baselines in terms of both\ncorrectness and helpfulness. We offer public access to our data, model, and\ncodebase in https://github.com/XZhang97666/AlpaCare.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction-finetuning (IFT) has become crucial in aligning Large Language\nModels (LLMs) with diverse human needs and has shown great potential in medical\napplications. However, previous studies mainly fine-tune LLMs on biomedical\ndatasets with limited diversity, which often rely on benchmarks or narrow task\nscopes, and hence significantly limit the effectiveness on their medical\ninstruction-following ability and generalizability. To bridge this gap, we\npropose creating a diverse, machine-generated medical IFT dataset,\nMedInstruct-52k, using GPT-4 and ChatGPT with a high-quality expert-curated\nseed set. We then fine-tune LLaMA-series models on the dataset to develop\nAlpaCare. Despite using a smaller domain-specific dataset than previous medical\nLLMs, AlpaCare not only demonstrates superior performance on medical\napplications, with up to 38.1% absolute gain over best baselines in medical\nfree-form instruction evaluations, but also achieves 6.7% absolute gains\naveraged over multiple general domain benchmarks. Human evaluation further\nshows that AlpaCare consistently outperforms best baselines in terms of both\ncorrectness and helpfulness. We offer public access to our data, model, and\ncodebase in https://github.com/XZhang97666/AlpaCare."
                },
                "authors": [
                    {
                        "name": "Xinlu Zhang"
                    },
                    {
                        "name": "Chenxin Tian"
                    },
                    {
                        "name": "Xianjun Yang"
                    },
                    {
                        "name": "Lichang Chen"
                    },
                    {
                        "name": "Zekun Li"
                    },
                    {
                        "name": "Linda Ruth Petzold"
                    }
                ],
                "author_detail": {
                    "name": "Linda Ruth Petzold"
                },
                "author": "Linda Ruth Petzold",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.14558v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.14558v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15164v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15164v3",
                "updated": "2025-03-31T20:39:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    20,
                    39,
                    17,
                    0,
                    90,
                    0
                ],
                "published": "2024-10-19T17:28:48Z",
                "published_parsed": [
                    2024,
                    10,
                    19,
                    17,
                    28,
                    48,
                    5,
                    293,
                    0
                ],
                "title": "SPA-Bench: A Comprehensive Benchmark for SmartPhone Agent Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPA-Bench: A Comprehensive Benchmark for SmartPhone Agent Evaluation"
                },
                "summary": "Smartphone agents are increasingly important for helping users control\ndevices efficiently, with (Multimodal) Large Language Model (MLLM)-based\napproaches emerging as key contenders. Fairly comparing these agents is\nessential but challenging, requiring a varied task scope, the integration of\nagents with different implementations, and a generalisable evaluation pipeline\nto assess their strengths and weaknesses. In this paper, we present SPA-Bench,\na comprehensive SmartPhone Agent Benchmark designed to evaluate (M)LLM-based\nagents in an interactive environment that simulates real-world conditions.\nSPA-Bench offers three key contributions: (1) A diverse set of tasks covering\nsystem and third-party apps in both English and Chinese, focusing on features\ncommonly used in daily routines; (2) A plug-and-play framework enabling\nreal-time agent interaction with Android devices, integrating over ten agents\nwith the flexibility to add more; (3) A novel evaluation pipeline that\nautomatically assesses agent performance across multiple dimensions,\nencompassing seven metrics related to task completion and resource consumption.\nOur extensive experiments across tasks and agents reveal challenges like\ninterpreting mobile user interfaces, action grounding, memory retention, and\nexecution costs. We propose future research directions to ease these\ndifficulties, moving closer to real-world smartphone agent applications.\nSPA-Bench is available at https://ai-agents-2030.github.io/SPA-Bench/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smartphone agents are increasingly important for helping users control\ndevices efficiently, with (Multimodal) Large Language Model (MLLM)-based\napproaches emerging as key contenders. Fairly comparing these agents is\nessential but challenging, requiring a varied task scope, the integration of\nagents with different implementations, and a generalisable evaluation pipeline\nto assess their strengths and weaknesses. In this paper, we present SPA-Bench,\na comprehensive SmartPhone Agent Benchmark designed to evaluate (M)LLM-based\nagents in an interactive environment that simulates real-world conditions.\nSPA-Bench offers three key contributions: (1) A diverse set of tasks covering\nsystem and third-party apps in both English and Chinese, focusing on features\ncommonly used in daily routines; (2) A plug-and-play framework enabling\nreal-time agent interaction with Android devices, integrating over ten agents\nwith the flexibility to add more; (3) A novel evaluation pipeline that\nautomatically assesses agent performance across multiple dimensions,\nencompassing seven metrics related to task completion and resource consumption.\nOur extensive experiments across tasks and agents reveal challenges like\ninterpreting mobile user interfaces, action grounding, memory retention, and\nexecution costs. We propose future research directions to ease these\ndifficulties, moving closer to real-world smartphone agent applications.\nSPA-Bench is available at https://ai-agents-2030.github.io/SPA-Bench/."
                },
                "authors": [
                    {
                        "name": "Jingxuan Chen"
                    },
                    {
                        "name": "Derek Yuen"
                    },
                    {
                        "name": "Bin Xie"
                    },
                    {
                        "name": "Yuhao Yang"
                    },
                    {
                        "name": "Gongwei Chen"
                    },
                    {
                        "name": "Zhihao Wu"
                    },
                    {
                        "name": "Li Yixing"
                    },
                    {
                        "name": "Xurui Zhou"
                    },
                    {
                        "name": "Weiwen Liu"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Kaiwen Zhou"
                    },
                    {
                        "name": "Rui Shao"
                    },
                    {
                        "name": "Liqiang Nie"
                    },
                    {
                        "name": "Yasheng Wang"
                    },
                    {
                        "name": "Jianye Hao"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Kun Shao"
                    }
                ],
                "author_detail": {
                    "name": "Kun Shao"
                },
                "author": "Kun Shao",
                "arxiv_comment": "ICLR 2025 Spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15164v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15164v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.13164v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.13164v4",
                "updated": "2025-03-31T20:03:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    20,
                    3,
                    34,
                    0,
                    90,
                    0
                ],
                "published": "2024-03-19T21:31:56Z",
                "published_parsed": [
                    2024,
                    3,
                    19,
                    21,
                    31,
                    56,
                    1,
                    79,
                    0
                ],
                "title": "VL-ICL Bench: The Devil in the Details of Multimodal In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VL-ICL Bench: The Devil in the Details of Multimodal In-Context Learning"
                },
                "summary": "Large language models (LLMs) famously exhibit emergent in-context learning\n(ICL) -- the ability to rapidly adapt to new tasks using few-shot examples\nprovided as a prompt, without updating the model's weights. Built on top of\nLLMs, vision large language models (VLLMs) have advanced significantly in areas\nsuch as recognition, reasoning, and grounding. However, investigations into\n\\emph{multimodal ICL} have predominantly focused on few-shot visual question\nanswering (VQA), and image captioning, which we will show neither exploit the\nstrengths of ICL, nor test its limitations. The broader capabilities and\nlimitations of multimodal ICL remain under-explored. In this study, we\nintroduce a comprehensive benchmark VL-ICL Bench for multimodal in-context\nlearning, encompassing a broad spectrum of tasks that involve both images and\ntext as inputs and outputs, and different types of challenges, from {perception\nto reasoning and long context length}. We evaluate the abilities of\nstate-of-the-art VLLMs against this benchmark suite, revealing their diverse\nstrengths and weaknesses, and showing that even the most advanced models, such\nas GPT-4, find the tasks challenging. By highlighting a range of new ICL tasks,\nand the associated strengths and limitations of existing models, we hope that\nour dataset will inspire future work on enhancing the in-context learning\ncapabilities of VLLMs, as well as inspire new applications that leverage VLLM\nICL. The code and dataset are available at https://github.com/ys-zong/VL-ICL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) famously exhibit emergent in-context learning\n(ICL) -- the ability to rapidly adapt to new tasks using few-shot examples\nprovided as a prompt, without updating the model's weights. Built on top of\nLLMs, vision large language models (VLLMs) have advanced significantly in areas\nsuch as recognition, reasoning, and grounding. However, investigations into\n\\emph{multimodal ICL} have predominantly focused on few-shot visual question\nanswering (VQA), and image captioning, which we will show neither exploit the\nstrengths of ICL, nor test its limitations. The broader capabilities and\nlimitations of multimodal ICL remain under-explored. In this study, we\nintroduce a comprehensive benchmark VL-ICL Bench for multimodal in-context\nlearning, encompassing a broad spectrum of tasks that involve both images and\ntext as inputs and outputs, and different types of challenges, from {perception\nto reasoning and long context length}. We evaluate the abilities of\nstate-of-the-art VLLMs against this benchmark suite, revealing their diverse\nstrengths and weaknesses, and showing that even the most advanced models, such\nas GPT-4, find the tasks challenging. By highlighting a range of new ICL tasks,\nand the associated strengths and limitations of existing models, we hope that\nour dataset will inspire future work on enhancing the in-context learning\ncapabilities of VLLMs, as well as inspire new applications that leverage VLLM\nICL. The code and dataset are available at https://github.com/ys-zong/VL-ICL."
                },
                "authors": [
                    {
                        "name": "Yongshuo Zong"
                    },
                    {
                        "name": "Ondrej Bohdal"
                    },
                    {
                        "name": "Timothy Hospedales"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Hospedales"
                },
                "author": "Timothy Hospedales",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.13164v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.13164v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20794v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20794v2",
                "updated": "2025-03-31T19:44:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    19,
                    44,
                    35,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-21T10:05:04Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    10,
                    5,
                    4,
                    4,
                    80,
                    0
                ],
                "title": "Can Zero-Shot Commercial APIs Deliver Regulatory-Grade Clinical Text\n  DeIdentification?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Zero-Shot Commercial APIs Deliver Regulatory-Grade Clinical Text\n  DeIdentification?"
                },
                "summary": "We evaluate the performance of four leading solutions for de-identification\nof unstructured medical text - Azure Health Data Services, AWS Comprehend\nMedical, OpenAI GPT-4o, and John Snow Labs - on a ground truth dataset of 48\nclinical documents annotated by medical experts. The analysis, conducted at\nboth entity-level and token-level, suggests that John Snow Labs' Medical\nLanguage Models solution achieves the highest accuracy, with a 96% F1-score in\nprotected health information (PHI) detection, outperforming Azure (91%), AWS\n(83%), and GPT-4o (79%). John Snow Labs is not only the only solution which\nachieves regulatory-grade accuracy (surpassing that of human experts) but is\nalso the most cost-effective solution: It is over 80% cheaper compared to Azure\nand GPT-4o, and is the only solution not priced by token. Its fixed-cost local\ndeployment model avoids the escalating per-request fees of cloud-based\nservices, making it a scalable and economical choice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We evaluate the performance of four leading solutions for de-identification\nof unstructured medical text - Azure Health Data Services, AWS Comprehend\nMedical, OpenAI GPT-4o, and John Snow Labs - on a ground truth dataset of 48\nclinical documents annotated by medical experts. The analysis, conducted at\nboth entity-level and token-level, suggests that John Snow Labs' Medical\nLanguage Models solution achieves the highest accuracy, with a 96% F1-score in\nprotected health information (PHI) detection, outperforming Azure (91%), AWS\n(83%), and GPT-4o (79%). John Snow Labs is not only the only solution which\nachieves regulatory-grade accuracy (surpassing that of human experts) but is\nalso the most cost-effective solution: It is over 80% cheaper compared to Azure\nand GPT-4o, and is the only solution not priced by token. Its fixed-cost local\ndeployment model avoids the escalating per-request fees of cloud-based\nservices, making it a scalable and economical choice."
                },
                "authors": [
                    {
                        "name": "Veysel Kocaman"
                    },
                    {
                        "name": "Muhammed Santas"
                    },
                    {
                        "name": "Yigit Gul"
                    },
                    {
                        "name": "Mehmet Butgul"
                    },
                    {
                        "name": "David Talby"
                    }
                ],
                "author_detail": {
                    "name": "David Talby"
                },
                "author": "David Talby",
                "arxiv_comment": "14 pages, accepted at Text2Story Workshop at ECIR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20794v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20794v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3; F.2.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01860v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01860v2",
                "updated": "2025-03-31T19:39:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    19,
                    39,
                    16,
                    0,
                    90,
                    0
                ],
                "published": "2025-02-03T22:19:28Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    22,
                    19,
                    28,
                    0,
                    34,
                    0
                ],
                "title": "SE Arena: An Interactive Platform for Evaluating Foundation Models in\n  Software Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SE Arena: An Interactive Platform for Evaluating Foundation Models in\n  Software Engineering"
                },
                "summary": "Foundation models (FMs), particularly large language models (LLMs), have\nshown significant promise in various software engineering (SE) tasks, including\ncode generation, debugging, and requirement refinement. Despite these advances,\nexisting evaluation frameworks are insufficient for assessing model performance\nin iterative, context-rich workflows characteristic of SE activities. To\naddress this limitation, we introduce SE Arena, an interactive platform\ndesigned to evaluate SE-focused chatbots. SE Arena provides a transparent,\nopen-source leaderboard, supports multi-round conversational workflows, and\nenables end-to-end model comparisons. Moreover, SE Arena incorporates a new\nfeature called RepoChat, which automatically injects repository-related context\n(e.g., issues, commits, pull requests) into the conversation, further aligning\nevaluations with real-world development processes. This paper outlines the\ndesign and capabilities of SE Arena, emphasizing its potential to advance the\nevaluation and practical application of FMs in software engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation models (FMs), particularly large language models (LLMs), have\nshown significant promise in various software engineering (SE) tasks, including\ncode generation, debugging, and requirement refinement. Despite these advances,\nexisting evaluation frameworks are insufficient for assessing model performance\nin iterative, context-rich workflows characteristic of SE activities. To\naddress this limitation, we introduce SE Arena, an interactive platform\ndesigned to evaluate SE-focused chatbots. SE Arena provides a transparent,\nopen-source leaderboard, supports multi-round conversational workflows, and\nenables end-to-end model comparisons. Moreover, SE Arena incorporates a new\nfeature called RepoChat, which automatically injects repository-related context\n(e.g., issues, commits, pull requests) into the conversation, further aligning\nevaluations with real-world development processes. This paper outlines the\ndesign and capabilities of SE Arena, emphasizing its potential to advance the\nevaluation and practical application of FMs in software engineering."
                },
                "authors": [
                    {
                        "name": "Zhimin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Zhimin Zhao"
                },
                "author": "Zhimin Zhao",
                "arxiv_comment": "Check the arena at\n  https://huggingface.co/spaces/SE-Arena/Software-Engineering-Arena",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01860v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01860v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.17956v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.17956v3",
                "updated": "2025-03-31T17:58:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    58,
                    31,
                    0,
                    90,
                    0
                ],
                "published": "2024-05-28T08:35:48Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    8,
                    35,
                    48,
                    1,
                    149,
                    0
                ],
                "title": "Unified Preference Optimization: Language Model Alignment Beyond the\n  Preference Frontier",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unified Preference Optimization: Language Model Alignment Beyond the\n  Preference Frontier"
                },
                "summary": "For aligning large language models (LLMs), prior work has leveraged\nreinforcement learning via human feedback (RLHF) or variations of direct\npreference optimization (DPO). While DPO offers a simpler framework based on\nmaximum likelihood estimation, it compromises on the ability to easily tune\nlanguage models to maximize auxiliary, non-preferential objectives according to\nthe LLM designer's preferences (e.g., tuning lexical style or minimizing\nspecific kinds of harmful content). Critically, these designer objectives may\nnot be amply human-labeled or represented in available data, align with user\npreferences, or even be able to be captured tractably by binary preference\npairs. To leverage the simplicity and performance of DPO with the generality of\nRL, we propose a unified approach. Based on a simple decomposition of\npreference and auxiliary objectives, we allow for tuning LLMs to optimize user\nand designer preferences without any additional specialized or preference data,\ncomputational cost, stability ``tweaks'', or training instability. The proposed\nmethod, Unified Preference Optimization, shows the ability to effectively\ngeneralize to user preferences and auxiliary objectives, while preserving or\nsurpassing alignment performance on challenging benchmarks across a range of\nmodel sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For aligning large language models (LLMs), prior work has leveraged\nreinforcement learning via human feedback (RLHF) or variations of direct\npreference optimization (DPO). While DPO offers a simpler framework based on\nmaximum likelihood estimation, it compromises on the ability to easily tune\nlanguage models to maximize auxiliary, non-preferential objectives according to\nthe LLM designer's preferences (e.g., tuning lexical style or minimizing\nspecific kinds of harmful content). Critically, these designer objectives may\nnot be amply human-labeled or represented in available data, align with user\npreferences, or even be able to be captured tractably by binary preference\npairs. To leverage the simplicity and performance of DPO with the generality of\nRL, we propose a unified approach. Based on a simple decomposition of\npreference and auxiliary objectives, we allow for tuning LLMs to optimize user\nand designer preferences without any additional specialized or preference data,\ncomputational cost, stability ``tweaks'', or training instability. The proposed\nmethod, Unified Preference Optimization, shows the ability to effectively\ngeneralize to user preferences and auxiliary objectives, while preserving or\nsurpassing alignment performance on challenging benchmarks across a range of\nmodel sizes."
                },
                "authors": [
                    {
                        "name": "Anirudhan Badrinath"
                    },
                    {
                        "name": "Prabhat Agarwal"
                    },
                    {
                        "name": "Jiajing Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jiajing Xu"
                },
                "author": "Jiajing Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.17956v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.17956v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24377v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24377v1",
                "updated": "2025-03-31T17:58:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    58,
                    7,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T17:58:07Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    58,
                    7,
                    0,
                    90,
                    0
                ],
                "title": "Harnessing the Reasoning Economy: A Survey of Efficient Reasoning for\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing the Reasoning Economy: A Survey of Efficient Reasoning for\n  Large Language Models"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have significantly\nenhanced their ability to perform complex reasoning tasks, transitioning from\nfast and intuitive thinking (System 1) to slow and deep reasoning (System 2).\nWhile System 2 reasoning improves task accuracy, it often incurs substantial\ncomputational costs due to its slow thinking nature and inefficient or\nunnecessary reasoning behaviors. In contrast, System 1 reasoning is\ncomputationally efficient but leads to suboptimal performance. Consequently, it\nis critical to balance the trade-off between performance (benefits) and\ncomputational costs (budgets), giving rise to the concept of reasoning economy.\nIn this survey, we provide a comprehensive analysis of reasoning economy in\nboth the post-training and test-time inference stages of LLMs, encompassing i)\nthe cause of reasoning inefficiency, ii) behavior analysis of different\nreasoning patterns, and iii) potential solutions to achieve reasoning economy.\nBy offering actionable insights and highlighting open challenges, we aim to\nshed light on strategies for improving the reasoning economy of LLMs, thereby\nserving as a valuable resource for advancing research in this evolving area. We\nalso provide a public repository to continually track developments in this\nfast-evolving field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have significantly\nenhanced their ability to perform complex reasoning tasks, transitioning from\nfast and intuitive thinking (System 1) to slow and deep reasoning (System 2).\nWhile System 2 reasoning improves task accuracy, it often incurs substantial\ncomputational costs due to its slow thinking nature and inefficient or\nunnecessary reasoning behaviors. In contrast, System 1 reasoning is\ncomputationally efficient but leads to suboptimal performance. Consequently, it\nis critical to balance the trade-off between performance (benefits) and\ncomputational costs (budgets), giving rise to the concept of reasoning economy.\nIn this survey, we provide a comprehensive analysis of reasoning economy in\nboth the post-training and test-time inference stages of LLMs, encompassing i)\nthe cause of reasoning inefficiency, ii) behavior analysis of different\nreasoning patterns, and iii) potential solutions to achieve reasoning economy.\nBy offering actionable insights and highlighting open challenges, we aim to\nshed light on strategies for improving the reasoning economy of LLMs, thereby\nserving as a valuable resource for advancing research in this evolving area. We\nalso provide a public repository to continually track developments in this\nfast-evolving field."
                },
                "authors": [
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Hongru Wang"
                    },
                    {
                        "name": "Boyang Xue"
                    },
                    {
                        "name": "Jianhui Pang"
                    },
                    {
                        "name": "Shudong Liu"
                    },
                    {
                        "name": "Yi Chen"
                    },
                    {
                        "name": "Jiahao Qiu"
                    },
                    {
                        "name": "Derek Fai Wong"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Kam-Fai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kam-Fai Wong"
                },
                "author": "Kam-Fai Wong",
                "arxiv_comment": "In Progress; Paper list Repo:\n  https://github.com/DevoAllen/Awesome-Reasoning-Economy-Papers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24377v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24377v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22232v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22232v2",
                "updated": "2025-03-31T17:56:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    56,
                    29,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-28T08:27:47Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    8,
                    27,
                    47,
                    4,
                    87,
                    0
                ],
                "title": "Privacy-Preserving Secure Neighbor Discovery for Wireless Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy-Preserving Secure Neighbor Discovery for Wireless Networks"
                },
                "summary": "Traditional Neighbor Discovery (ND) and Secure Neighbor Discovery (SND) are\nkey elements for network functionality. SND is a hard problem, satisfying not\nonly typical security properties (authentication, integrity) but also\nverification of direct communication, which involves distance estimation based\non time measurements and device coordinates. Defeating relay attacks, also\nknown as \"wormholes\", leading to stealthy Byzantine links and significant\ndegradation of communication and adversarial control, is key in many wireless\nnetworked systems. However, SND is not concerned with privacy; it necessitates\nrevealing the identity and location of the device(s) participating in the\nprotocol execution. This can be a deterrent for deployment, especially\ninvolving user-held devices in the emerging Internet of Things (IoT) enabled\nsmart environments. To address this challenge, we present a novel\nPrivacy-Preserving Secure Neighbor Discovery (PP-SND) protocol, enabling\ndevices to perform SND without revealing their actual identities and locations,\neffectively decoupling discovery from the exposure of sensitive information. We\nuse Homomorphic Encryption (HE) for computing device distances without\nrevealing their actual coordinates, as well as employing a pseudonymous device\nauthentication to hide identities while preserving communication integrity.\nPP-SND provides SND [1] along with pseudonymity, confidentiality, and\nunlinkability. Our presentation here is not specific to one wireless\ntechnology, and we assess the performance of the protocols (cryptographic\noverhead) on a Raspberry Pi 4 and provide a security and privacy analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional Neighbor Discovery (ND) and Secure Neighbor Discovery (SND) are\nkey elements for network functionality. SND is a hard problem, satisfying not\nonly typical security properties (authentication, integrity) but also\nverification of direct communication, which involves distance estimation based\non time measurements and device coordinates. Defeating relay attacks, also\nknown as \"wormholes\", leading to stealthy Byzantine links and significant\ndegradation of communication and adversarial control, is key in many wireless\nnetworked systems. However, SND is not concerned with privacy; it necessitates\nrevealing the identity and location of the device(s) participating in the\nprotocol execution. This can be a deterrent for deployment, especially\ninvolving user-held devices in the emerging Internet of Things (IoT) enabled\nsmart environments. To address this challenge, we present a novel\nPrivacy-Preserving Secure Neighbor Discovery (PP-SND) protocol, enabling\ndevices to perform SND without revealing their actual identities and locations,\neffectively decoupling discovery from the exposure of sensitive information. We\nuse Homomorphic Encryption (HE) for computing device distances without\nrevealing their actual coordinates, as well as employing a pseudonymous device\nauthentication to hide identities while preserving communication integrity.\nPP-SND provides SND [1] along with pseudonymity, confidentiality, and\nunlinkability. Our presentation here is not specific to one wireless\ntechnology, and we assess the performance of the protocols (cryptographic\noverhead) on a Raspberry Pi 4 and provide a security and privacy analysis."
                },
                "authors": [
                    {
                        "name": "Ahmed Mohamed Hussain"
                    },
                    {
                        "name": "Panos Papadimitratos"
                    }
                ],
                "author_detail": {
                    "name": "Panos Papadimitratos"
                },
                "author": "Panos Papadimitratos",
                "arxiv_comment": "10 pages, 6 figures. Author's version; accepted and presented at the\n  IEEE 23rd International Conference on Trust, Security and Privacy in\n  Computing and Communications (TrustCom) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22232v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22232v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21080v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21080v3",
                "updated": "2025-03-31T17:55:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    55,
                    35,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-27T01:41:34Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    1,
                    41,
                    34,
                    3,
                    86,
                    0
                ],
                "title": "EQ-Negotiator: An Emotion-Reasoning LLM Agent in Credit Dialogues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EQ-Negotiator: An Emotion-Reasoning LLM Agent in Credit Dialogues"
                },
                "summary": "While large language model (LLM)-based chatbots have been applied for\neffective engagement in credit dialogues, their capacity for dynamic emotional\nexpression remains limited. Current agents primarily rely on passive empathy\nrather than affective reasoning. For instance, when faced with persistent\nclient negativity, the agent should employ strategic emotional adaptation by\nexpressing measured anger to discourage counterproductive behavior and guide\nthe conversation toward resolution. This context-aware emotional modulation is\nessential for imitating the nuanced decision-making of human negotiators. This\npaper introduces an EQ-negotiator that combines emotion sensing from\npre-trained language models (PLMs) with emotional reasoning based on Game\nTheory and Hidden Markov Models. It takes into account both the current and\nhistorical emotions of the client to better manage and address negative\nemotions during interactions. By fine-tuning pre-trained language models (PLMs)\non public emotion datasets and validating them on the credit dialogue datasets,\nour approach enables LLM-based agents to effectively capture shifts in client\nemotions and dynamically adjust their response tone based on our emotion\ndecision policies in real-world financial negotiations. This EQ-negotiator can\nalso help credit agencies foster positive client relationships, enhancing\nsatisfaction in credit services.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language model (LLM)-based chatbots have been applied for\neffective engagement in credit dialogues, their capacity for dynamic emotional\nexpression remains limited. Current agents primarily rely on passive empathy\nrather than affective reasoning. For instance, when faced with persistent\nclient negativity, the agent should employ strategic emotional adaptation by\nexpressing measured anger to discourage counterproductive behavior and guide\nthe conversation toward resolution. This context-aware emotional modulation is\nessential for imitating the nuanced decision-making of human negotiators. This\npaper introduces an EQ-negotiator that combines emotion sensing from\npre-trained language models (PLMs) with emotional reasoning based on Game\nTheory and Hidden Markov Models. It takes into account both the current and\nhistorical emotions of the client to better manage and address negative\nemotions during interactions. By fine-tuning pre-trained language models (PLMs)\non public emotion datasets and validating them on the credit dialogue datasets,\nour approach enables LLM-based agents to effectively capture shifts in client\nemotions and dynamically adjust their response tone based on our emotion\ndecision policies in real-world financial negotiations. This EQ-negotiator can\nalso help credit agencies foster positive client relationships, enhancing\nsatisfaction in credit services."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Yunbo Long"
                    }
                ],
                "author_detail": {
                    "name": "Yunbo Long"
                },
                "author": "Yunbo Long",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21080v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21080v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24376v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24376v1",
                "updated": "2025-03-31T17:55:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    55,
                    23,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T17:55:23Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    55,
                    23,
                    0,
                    90,
                    0
                ],
                "title": "Exploring the Effect of Reinforcement Learning on Video Understanding:\n  Insights from SEED-Bench-R1",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Effect of Reinforcement Learning on Video Understanding:\n  Insights from SEED-Bench-R1"
                },
                "summary": "Recent advancements in Chain of Thought (COT) generation have significantly\nimproved the reasoning capabilities of Large Language Models (LLMs), with\nreinforcement learning (RL) emerging as an effective post-training approach.\nMultimodal Large Language Models (MLLMs) inherit this reasoning potential but\nremain underexplored in tasks requiring both perception and logical reasoning.\nTo address this, we introduce SEED-Bench-R1, a benchmark designed to\nsystematically evaluate post-training methods for MLLMs in video understanding.\nIt includes intricate real-world videos and complex everyday planning tasks in\nthe format of multiple-choice questions, requiring sophisticated perception and\nreasoning. SEED-Bench-R1 assesses generalization through a three-level\nhierarchy: in-distribution, cross-environment, and cross-environment-task\nscenarios, equipped with a large-scale training dataset with easily verifiable\nground-truth answers. Using Qwen2-VL-Instruct-7B as a base model, we compare RL\nwith supervised fine-tuning (SFT), demonstrating RL's data efficiency and\nsuperior performance on both in-distribution and out-of-distribution tasks,\neven outperforming SFT on general video understanding benchmarks like\nLongVideoBench. Our detailed analysis reveals that RL enhances visual\nperception but often produces less logically coherent reasoning chains. We\nidentify key limitations such as inconsistent reasoning and overlooked visual\ncues, and suggest future improvements in base model reasoning, reward modeling,\nand RL robustness against noisy signals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Chain of Thought (COT) generation have significantly\nimproved the reasoning capabilities of Large Language Models (LLMs), with\nreinforcement learning (RL) emerging as an effective post-training approach.\nMultimodal Large Language Models (MLLMs) inherit this reasoning potential but\nremain underexplored in tasks requiring both perception and logical reasoning.\nTo address this, we introduce SEED-Bench-R1, a benchmark designed to\nsystematically evaluate post-training methods for MLLMs in video understanding.\nIt includes intricate real-world videos and complex everyday planning tasks in\nthe format of multiple-choice questions, requiring sophisticated perception and\nreasoning. SEED-Bench-R1 assesses generalization through a three-level\nhierarchy: in-distribution, cross-environment, and cross-environment-task\nscenarios, equipped with a large-scale training dataset with easily verifiable\nground-truth answers. Using Qwen2-VL-Instruct-7B as a base model, we compare RL\nwith supervised fine-tuning (SFT), demonstrating RL's data efficiency and\nsuperior performance on both in-distribution and out-of-distribution tasks,\neven outperforming SFT on general video understanding benchmarks like\nLongVideoBench. Our detailed analysis reveals that RL enhances visual\nperception but often produces less logically coherent reasoning chains. We\nidentify key limitations such as inconsistent reasoning and overlooked visual\ncues, and suggest future improvements in base model reasoning, reward modeling,\nand RL robustness against noisy signals."
                },
                "authors": [
                    {
                        "name": "Yi Chen"
                    },
                    {
                        "name": "Yuying Ge"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Yixiao Ge"
                    },
                    {
                        "name": "Lu Qiu"
                    },
                    {
                        "name": "Ying Shan"
                    },
                    {
                        "name": "Xihui Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xihui Liu"
                },
                "author": "Xihui Liu",
                "arxiv_comment": "Technical Report (In Progress); Code released at:\n  https://github.com/TencentARC/SEED-Bench-R1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24376v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24376v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24370v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24370v1",
                "updated": "2025-03-31T17:50:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    50,
                    13,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T17:50:13Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    50,
                    13,
                    0,
                    90,
                    0
                ],
                "title": "Effectively Controlling Reasoning Models through Thinking Intervention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effectively Controlling Reasoning Models through Thinking Intervention"
                },
                "summary": "Reasoning-enhanced large language models (LLMs) explicitly generate\nintermediate reasoning steps prior to generating final answers, helping the\nmodel excel in complex problem-solving. In this paper, we demonstrate that this\nemerging generation framework offers a unique opportunity for more fine-grained\ncontrol over model behavior. We propose Thinking Intervention, a novel paradigm\ndesigned to explicitly guide the internal reasoning processes of LLMs by\nstrategically inserting or revising specific thinking tokens. We conduct\ncomprehensive evaluations across multiple tasks, including instruction\nfollowing on IFEval, instruction hierarchy on SEP, and safety alignment on\nXSTest and SORRY-Bench. Our results demonstrate that Thinking Intervention\nsignificantly outperforms baseline prompting approaches, achieving up to 6.7%\naccuracy gains in instruction-following scenarios, 15.4% improvements in\nreasoning about instruction hierarchies, and a 40.0% increase in refusal rates\nfor unsafe prompts using open-source DeepSeek R1 models. Overall, our work\nopens a promising new research avenue for controlling reasoning LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning-enhanced large language models (LLMs) explicitly generate\nintermediate reasoning steps prior to generating final answers, helping the\nmodel excel in complex problem-solving. In this paper, we demonstrate that this\nemerging generation framework offers a unique opportunity for more fine-grained\ncontrol over model behavior. We propose Thinking Intervention, a novel paradigm\ndesigned to explicitly guide the internal reasoning processes of LLMs by\nstrategically inserting or revising specific thinking tokens. We conduct\ncomprehensive evaluations across multiple tasks, including instruction\nfollowing on IFEval, instruction hierarchy on SEP, and safety alignment on\nXSTest and SORRY-Bench. Our results demonstrate that Thinking Intervention\nsignificantly outperforms baseline prompting approaches, achieving up to 6.7%\naccuracy gains in instruction-following scenarios, 15.4% improvements in\nreasoning about instruction hierarchies, and a 40.0% increase in refusal rates\nfor unsafe prompts using open-source DeepSeek R1 models. Overall, our work\nopens a promising new research avenue for controlling reasoning LLMs."
                },
                "authors": [
                    {
                        "name": "Tong Wu"
                    },
                    {
                        "name": "Chong Xiang"
                    },
                    {
                        "name": "Jiachen T. Wang"
                    },
                    {
                        "name": "Prateek Mittal"
                    }
                ],
                "author_detail": {
                    "name": "Prateek Mittal"
                },
                "author": "Prateek Mittal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24370v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24370v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24358v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24358v1",
                "updated": "2025-03-31T17:37:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    37,
                    32,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T17:37:32Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    37,
                    32,
                    0,
                    90,
                    0
                ],
                "title": "SQuat: Subspace-orthogonal KV Cache Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SQuat: Subspace-orthogonal KV Cache Quantization"
                },
                "summary": "The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from\npreviously generated tokens. It reduces redundant computation at the cost of\nincreased memory usage. To mitigate this overhead, existing approaches compress\nKV tensors into lower-bit representations; however, quantization errors can\naccumulate as more tokens are generated, potentially resulting in undesired\noutputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache\nquantization). It first constructs a subspace spanned by query tensors to\ncapture the most critical task-related information. During key tensor\nquantization, it enforces that the difference between the (de)quantized and\noriginal keys remains orthogonal to this subspace, minimizing the impact of\nquantization errors on the attention mechanism's outputs. SQuat requires no\nmodel fine-tuning, no additional calibration dataset for offline learning, and\nis grounded in a theoretical framework we develop. Through numerical\nexperiments, we show that our method reduces peak memory by 2.17 to 2.82,\nimproves throughput by 2.45 to 3.60, and achieves more favorable benchmark\nscores than existing KV cache quantization algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from\npreviously generated tokens. It reduces redundant computation at the cost of\nincreased memory usage. To mitigate this overhead, existing approaches compress\nKV tensors into lower-bit representations; however, quantization errors can\naccumulate as more tokens are generated, potentially resulting in undesired\noutputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache\nquantization). It first constructs a subspace spanned by query tensors to\ncapture the most critical task-related information. During key tensor\nquantization, it enforces that the difference between the (de)quantized and\noriginal keys remains orthogonal to this subspace, minimizing the impact of\nquantization errors on the attention mechanism's outputs. SQuat requires no\nmodel fine-tuning, no additional calibration dataset for offline learning, and\nis grounded in a theoretical framework we develop. Through numerical\nexperiments, we show that our method reduces peak memory by 2.17 to 2.82,\nimproves throughput by 2.45 to 3.60, and achieves more favorable benchmark\nscores than existing KV cache quantization algorithms."
                },
                "authors": [
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Ligong Han"
                    },
                    {
                        "name": "Kai Xu"
                    },
                    {
                        "name": "Akash Srivastava"
                    }
                ],
                "author_detail": {
                    "name": "Akash Srivastava"
                },
                "author": "Akash Srivastava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24358v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24358v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12952v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12952v2",
                "updated": "2025-03-31T17:36:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    36,
                    36,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-17T09:06:03Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    9,
                    6,
                    3,
                    0,
                    76,
                    0
                ],
                "title": "Performance Analysis and Industry Deployment of Post-Quantum\n  Cryptography Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Analysis and Industry Deployment of Post-Quantum\n  Cryptography Algorithms"
                },
                "summary": "As quantum computing advances, modern cryptographic standards face an\nexistential threat, necessitating a transition to post-quantum cryptography\n(PQC). The National Institute of Standards and Technology (NIST) has selected\nCRYSTALS-Kyber and CRYSTALS-Dilithium as standardized PQC algorithms for secure\nkey exchange and digital signatures, respectively. This study conducts a\ncomprehensive performance analysis of these algorithms by benchmarking\nexecution times across cryptographic operations such as key generation,\nencapsulation, decapsulation, signing, and verification. Additionally, the\nimpact of AVX2 optimizations is evaluated to assess hardware acceleration\nbenefits. Our findings demonstrate that Kyber and Dilithium achieve efficient\nexecution times, outperforming classical cryptographic schemes such as RSA and\nECDSA at equivalent security levels. Beyond technical performance, the\nreal-world deployment of PQC introduces challenges in telecommunications\nnetworks, where large-scale infrastructure upgrades, interoperability with\nlegacy systems, and regulatory constraints must be addressed. This paper\nexamines the feasibility of PQC adoption in telecom environments, highlighting\nkey transition challenges, security risks, and implementation strategies.\nThrough industry case studies, we illustrate how telecom operators are\nintegrating PQC into 5G authentication, subscriber identity protection, and\nsecure communications. Our analysis provides insights into the computational\ntrade-offs, deployment considerations, and standardization efforts shaping the\nfuture of quantum-safe cryptographic infrastructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As quantum computing advances, modern cryptographic standards face an\nexistential threat, necessitating a transition to post-quantum cryptography\n(PQC). The National Institute of Standards and Technology (NIST) has selected\nCRYSTALS-Kyber and CRYSTALS-Dilithium as standardized PQC algorithms for secure\nkey exchange and digital signatures, respectively. This study conducts a\ncomprehensive performance analysis of these algorithms by benchmarking\nexecution times across cryptographic operations such as key generation,\nencapsulation, decapsulation, signing, and verification. Additionally, the\nimpact of AVX2 optimizations is evaluated to assess hardware acceleration\nbenefits. Our findings demonstrate that Kyber and Dilithium achieve efficient\nexecution times, outperforming classical cryptographic schemes such as RSA and\nECDSA at equivalent security levels. Beyond technical performance, the\nreal-world deployment of PQC introduces challenges in telecommunications\nnetworks, where large-scale infrastructure upgrades, interoperability with\nlegacy systems, and regulatory constraints must be addressed. This paper\nexamines the feasibility of PQC adoption in telecom environments, highlighting\nkey transition challenges, security risks, and implementation strategies.\nThrough industry case studies, we illustrate how telecom operators are\nintegrating PQC into 5G authentication, subscriber identity protection, and\nsecure communications. Our analysis provides insights into the computational\ntrade-offs, deployment considerations, and standardization efforts shaping the\nfuture of quantum-safe cryptographic infrastructure."
                },
                "authors": [
                    {
                        "name": "Elif Dicle Demir"
                    },
                    {
                        "name": "Buse Bilgin"
                    },
                    {
                        "name": "Mehmet Cengiz Onbasli"
                    }
                ],
                "author_detail": {
                    "name": "Mehmet Cengiz Onbasli"
                },
                "author": "Mehmet Cengiz Onbasli",
                "arxiv_comment": "6 pages, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12952v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12952v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "E.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24354v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24354v1",
                "updated": "2025-03-31T17:34:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    34,
                    59,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T17:34:59Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    34,
                    59,
                    0,
                    90,
                    0
                ],
                "title": "ORAL: Prompting Your Large-Scale LoRAs via Conditional Recurrent\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ORAL: Prompting Your Large-Scale LoRAs via Conditional Recurrent\n  Diffusion"
                },
                "summary": "Parameter generation has emerged as a novel paradigm for neural network\ndevelopment, offering an alternative to traditional neural network training by\nsynthesizing high-quality model weights directly. In the context of Low-Rank\nAdaptation (LoRA) for evolving ($\\textit{i.e.}$, constantly updated) large\nlanguage models (LLMs), this approach promises efficient adaptation without\ncostly retraining. However, existing methods face critical limitations in\nsimultaneously achieving scalability and controllability. In this paper, we\nintroduce $\\texttt{ORAL}$, a novel $\\textbf{conditional recurrent diffusion}$\nframework that addresses these challenges. $\\texttt{ORAL}$ incorporates a novel\nconditioning mechanism that integrates model architecture and textual task\nspecifications, enabling the generation of task-specific LoRA parameters that\ncan seamlessly transfer across evolving foundation models. Our approach\nsuccessfully scales to billions-of-parameter LLMs and maintains\ncontrollability. Through extensive experiments across seven language tasks,\nfour vision tasks, and three multimodal tasks using five pre-trained LLMs, we\ndemonstrate that $\\texttt{ORAL}$ generates high-quality LoRA parameters that\nachieve comparable or superior performance to vanilla trained counterparts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter generation has emerged as a novel paradigm for neural network\ndevelopment, offering an alternative to traditional neural network training by\nsynthesizing high-quality model weights directly. In the context of Low-Rank\nAdaptation (LoRA) for evolving ($\\textit{i.e.}$, constantly updated) large\nlanguage models (LLMs), this approach promises efficient adaptation without\ncostly retraining. However, existing methods face critical limitations in\nsimultaneously achieving scalability and controllability. In this paper, we\nintroduce $\\texttt{ORAL}$, a novel $\\textbf{conditional recurrent diffusion}$\nframework that addresses these challenges. $\\texttt{ORAL}$ incorporates a novel\nconditioning mechanism that integrates model architecture and textual task\nspecifications, enabling the generation of task-specific LoRA parameters that\ncan seamlessly transfer across evolving foundation models. Our approach\nsuccessfully scales to billions-of-parameter LLMs and maintains\ncontrollability. Through extensive experiments across seven language tasks,\nfour vision tasks, and three multimodal tasks using five pre-trained LLMs, we\ndemonstrate that $\\texttt{ORAL}$ generates high-quality LoRA parameters that\nachieve comparable or superior performance to vanilla trained counterparts."
                },
                "authors": [
                    {
                        "name": "Rana Muhammad Shahroz Khan"
                    },
                    {
                        "name": "Dongwen Tang"
                    },
                    {
                        "name": "Pingzhi Li"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24354v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24354v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24347v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24347v1",
                "updated": "2025-03-31T17:32:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    32,
                    18,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T17:32:18Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    32,
                    18,
                    0,
                    90,
                    0
                ],
                "title": "Entanglement Distribution in Lossy Quantum Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entanglement Distribution in Lossy Quantum Networks"
                },
                "summary": "Entanglement distribution is essential for unlocking the potential of\ndistributed quantum information processing. We consider an $N$-partite network\nwhere entanglement is distributed via a central source over lossy channels, and\nnetwork participants cooperate to establish entanglement between any two chosen\nparties under local operations and classical communication (LOCC) constraints.\nWe develop a general mathematical framework to assess the optimal average\nbipartite entanglement shared in a lossy distribution, and introduce a\ntractable lower bound by optimizing over a subset of single-parameter LOCC\ntransformations. Our results show that probabilistically extracting Bell pairs\nfrom W states is more advantageous than deterministically extracting them from\nGHZ-like states in lossy networks, with this advantage increasing with network\nsize. We further extend our analysis analytically, proving that W states remain\nmore effective in large-scale networks. These findings offer valuable insights\ninto the practical deployment of near-term networks, revealing a fundamental\ntrade-off between deterministic entanglement distribution protocols and\nloss-sensitive resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entanglement distribution is essential for unlocking the potential of\ndistributed quantum information processing. We consider an $N$-partite network\nwhere entanglement is distributed via a central source over lossy channels, and\nnetwork participants cooperate to establish entanglement between any two chosen\nparties under local operations and classical communication (LOCC) constraints.\nWe develop a general mathematical framework to assess the optimal average\nbipartite entanglement shared in a lossy distribution, and introduce a\ntractable lower bound by optimizing over a subset of single-parameter LOCC\ntransformations. Our results show that probabilistically extracting Bell pairs\nfrom W states is more advantageous than deterministically extracting them from\nGHZ-like states in lossy networks, with this advantage increasing with network\nsize. We further extend our analysis analytically, proving that W states remain\nmore effective in large-scale networks. These findings offer valuable insights\ninto the practical deployment of near-term networks, revealing a fundamental\ntrade-off between deterministic entanglement distribution protocols and\nloss-sensitive resources."
                },
                "authors": [
                    {
                        "name": "Leonardo Oleynik"
                    },
                    {
                        "name": "Junaid ur Rehman"
                    },
                    {
                        "name": "Seid Koudia"
                    },
                    {
                        "name": "Symeon Chatzinotas"
                    }
                ],
                "author_detail": {
                    "name": "Symeon Chatzinotas"
                },
                "author": "Symeon Chatzinotas",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24347v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24347v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24310v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24310v1",
                "updated": "2025-03-31T16:56:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    16,
                    56,
                    52,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T16:56:52Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    16,
                    56,
                    52,
                    0,
                    90,
                    0
                ],
                "title": "BEATS: Bias Evaluation and Assessment Test Suite for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BEATS: Bias Evaluation and Assessment Test Suite for Large Language\n  Models"
                },
                "summary": "In this research, we introduce BEATS, a novel framework for evaluating Bias,\nEthics, Fairness, and Factuality in Large Language Models (LLMs). Building upon\nthe BEATS framework, we present a bias benchmark for LLMs that measure\nperformance across 29 distinct metrics. These metrics span a broad range of\ncharacteristics, including demographic, cognitive, and social biases, as well\nas measures of ethical reasoning, group fairness, and factuality related\nmisinformation risk. These metrics enable a quantitative assessment of the\nextent to which LLM generated responses may perpetuate societal prejudices that\nreinforce or expand systemic inequities. To achieve a high score on this\nbenchmark a LLM must show very equitable behavior in their responses, making it\na rigorous standard for responsible AI evaluation. Empirical results based on\ndata from our experiment show that, 37.65\\% of outputs generated by industry\nleading models contained some form of bias, highlighting a substantial risk of\nusing these models in critical decision making systems. BEATS framework and\nbenchmark offer a scalable and statistically rigorous methodology to benchmark\nLLMs, diagnose factors driving biases, and develop mitigation strategies. With\nthe BEATS framework, our goal is to help the development of more socially\nresponsible and ethically aligned AI models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this research, we introduce BEATS, a novel framework for evaluating Bias,\nEthics, Fairness, and Factuality in Large Language Models (LLMs). Building upon\nthe BEATS framework, we present a bias benchmark for LLMs that measure\nperformance across 29 distinct metrics. These metrics span a broad range of\ncharacteristics, including demographic, cognitive, and social biases, as well\nas measures of ethical reasoning, group fairness, and factuality related\nmisinformation risk. These metrics enable a quantitative assessment of the\nextent to which LLM generated responses may perpetuate societal prejudices that\nreinforce or expand systemic inequities. To achieve a high score on this\nbenchmark a LLM must show very equitable behavior in their responses, making it\na rigorous standard for responsible AI evaluation. Empirical results based on\ndata from our experiment show that, 37.65\\% of outputs generated by industry\nleading models contained some form of bias, highlighting a substantial risk of\nusing these models in critical decision making systems. BEATS framework and\nbenchmark offer a scalable and statistically rigorous methodology to benchmark\nLLMs, diagnose factors driving biases, and develop mitigation strategies. With\nthe BEATS framework, our goal is to help the development of more socially\nresponsible and ethically aligned AI models."
                },
                "authors": [
                    {
                        "name": "Alok Abhishek"
                    },
                    {
                        "name": "Lisa Erickson"
                    },
                    {
                        "name": "Tushar Bandopadhyay"
                    }
                ],
                "author_detail": {
                    "name": "Tushar Bandopadhyay"
                },
                "author": "Tushar Bandopadhyay",
                "arxiv_comment": "32 pages, 33 figures, preprint version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24310v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24310v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T01 (Primary), 68T50 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24307v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24307v1",
                "updated": "2025-03-31T16:54:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    16,
                    54,
                    4,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T16:54:04Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    16,
                    54,
                    4,
                    0,
                    90,
                    0
                ],
                "title": "A Systematic Evaluation of LLM Strategies for Mental Health Text\n  Analysis: Fine-tuning vs. Prompt Engineering vs. RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Systematic Evaluation of LLM Strategies for Mental Health Text\n  Analysis: Fine-tuning vs. Prompt Engineering vs. RAG"
                },
                "summary": "This study presents a systematic comparison of three approaches for the\nanalysis of mental health text using large language models (LLMs): prompt\nengineering, retrieval augmented generation (RAG), and fine-tuning. Using LLaMA\n3, we evaluate these approaches on emotion classification and mental health\ncondition detection tasks across two datasets. Fine-tuning achieves the highest\naccuracy (91% for emotion classification, 80% for mental health conditions) but\nrequires substantial computational resources and large training sets, while\nprompt engineering and RAG offer more flexible deployment with moderate\nperformance (40-68% accuracy). Our findings provide practical insights for\nimplementing LLM-based solutions in mental health applications, highlighting\nthe trade-offs between accuracy, computational requirements, and deployment\nflexibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study presents a systematic comparison of three approaches for the\nanalysis of mental health text using large language models (LLMs): prompt\nengineering, retrieval augmented generation (RAG), and fine-tuning. Using LLaMA\n3, we evaluate these approaches on emotion classification and mental health\ncondition detection tasks across two datasets. Fine-tuning achieves the highest\naccuracy (91% for emotion classification, 80% for mental health conditions) but\nrequires substantial computational resources and large training sets, while\nprompt engineering and RAG offer more flexible deployment with moderate\nperformance (40-68% accuracy). Our findings provide practical insights for\nimplementing LLM-based solutions in mental health applications, highlighting\nthe trade-offs between accuracy, computational requirements, and deployment\nflexibility."
                },
                "authors": [
                    {
                        "name": "Arshia Kermani"
                    },
                    {
                        "name": "Veronica Perez-Rosas"
                    },
                    {
                        "name": "Vangelis Metsis"
                    }
                ],
                "author_detail": {
                    "name": "Vangelis Metsis"
                },
                "author": "Vangelis Metsis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24307v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24307v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.14738v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.14738v4",
                "updated": "2025-03-31T16:44:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    16,
                    44,
                    27,
                    0,
                    90,
                    0
                ],
                "published": "2024-04-23T04:42:01Z",
                "published_parsed": [
                    2024,
                    4,
                    23,
                    4,
                    42,
                    1,
                    1,
                    114,
                    0
                ],
                "title": "Uncrewed Vehicles in 6G Networks: A Unifying Treatment of Problems,\n  Formulations, and Tools",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncrewed Vehicles in 6G Networks: A Unifying Treatment of Problems,\n  Formulations, and Tools"
                },
                "summary": "Uncrewed Vehicles (UVs) functioning as autonomous agents are anticipated to\nplay a crucial role in the 6th Generation of wireless networks. Their seamless\nintegration, cost-effectiveness, and the additional controllability through\nmotion planning make them an attractive deployment option for a wide range of\napplications, both as assets in the network (e.g., mobile base stations) and as\nconsumers of network services (e.g., autonomous delivery systems). However,\ndespite their potential, the convergence of UVs and wireless systems brings\nforth numerous challenges that require attention from both academia and\nindustry. This paper then aims to offer a comprehensive overview encompassing\nthe transformative possibilities as well as the significant challenges\nassociated with UV-assisted next-generation wireless communications.\nConsidering the diverse landscape of possible application scenarios, problem\nformulations, and mathematical tools related to UV-assisted wireless systems,\nthe underlying core theme of this paper is the unification of the problem\nspace, providing a structured framework to understand the use cases, problem\nformulations, and necessary mathematical tools. Overall, the paper sets forth a\nclear understanding of how uncrewed vehicles can be integrated in the 6G\necosystem, paving the way towards harnessing the full potential at this\nintersection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncrewed Vehicles (UVs) functioning as autonomous agents are anticipated to\nplay a crucial role in the 6th Generation of wireless networks. Their seamless\nintegration, cost-effectiveness, and the additional controllability through\nmotion planning make them an attractive deployment option for a wide range of\napplications, both as assets in the network (e.g., mobile base stations) and as\nconsumers of network services (e.g., autonomous delivery systems). However,\ndespite their potential, the convergence of UVs and wireless systems brings\nforth numerous challenges that require attention from both academia and\nindustry. This paper then aims to offer a comprehensive overview encompassing\nthe transformative possibilities as well as the significant challenges\nassociated with UV-assisted next-generation wireless communications.\nConsidering the diverse landscape of possible application scenarios, problem\nformulations, and mathematical tools related to UV-assisted wireless systems,\nthe underlying core theme of this paper is the unification of the problem\nspace, providing a structured framework to understand the use cases, problem\nformulations, and necessary mathematical tools. Overall, the paper sets forth a\nclear understanding of how uncrewed vehicles can be integrated in the 6G\necosystem, paving the way towards harnessing the full potential at this\nintersection."
                },
                "authors": [
                    {
                        "name": "Winston Hurst"
                    },
                    {
                        "name": "Spilios Evmorfos"
                    },
                    {
                        "name": "Athina Petropulu"
                    },
                    {
                        "name": "Yasamin Mostofi"
                    }
                ],
                "author_detail": {
                    "name": "Yasamin Mostofi"
                },
                "author": "Yasamin Mostofi",
                "arxiv_doi": "10.1109/JPROC.2025.3541949",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/JPROC.2025.3541949",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.14738v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.14738v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24293v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24293v1",
                "updated": "2025-03-31T16:41:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    16,
                    41,
                    16,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T16:41:16Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    16,
                    41,
                    16,
                    0,
                    90,
                    0
                ],
                "title": "Is analogy enough to draw novel adjective-noun inferences?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is analogy enough to draw novel adjective-noun inferences?"
                },
                "summary": "Recent work (Ross et al., 2025, 2024) has argued that the ability of humans\nand LLMs respectively to generalize to novel adjective-noun combinations shows\nthat they each have access to a compositional mechanism to determine the\nphrase's meaning and derive inferences. We study whether these inferences can\ninstead be derived by analogy to known inferences, without need for\ncomposition. We investigate this by (1) building a model of analogical\nreasoning using similarity over lexical items, and (2) asking human\nparticipants to reason by analogy. While we find that this strategy works well\nfor a large proportion of the dataset of Ross et al. (2025), there are novel\ncombinations for which both humans and LLMs derive convergent inferences but\nwhich are not well handled by analogy. We thus conclude that the mechanism\nhumans and LLMs use to generalize in these cases cannot be fully reduced to\nanalogy, and likely involves composition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work (Ross et al., 2025, 2024) has argued that the ability of humans\nand LLMs respectively to generalize to novel adjective-noun combinations shows\nthat they each have access to a compositional mechanism to determine the\nphrase's meaning and derive inferences. We study whether these inferences can\ninstead be derived by analogy to known inferences, without need for\ncomposition. We investigate this by (1) building a model of analogical\nreasoning using similarity over lexical items, and (2) asking human\nparticipants to reason by analogy. While we find that this strategy works well\nfor a large proportion of the dataset of Ross et al. (2025), there are novel\ncombinations for which both humans and LLMs derive convergent inferences but\nwhich are not well handled by analogy. We thus conclude that the mechanism\nhumans and LLMs use to generalize in these cases cannot be fully reduced to\nanalogy, and likely involves composition."
                },
                "authors": [
                    {
                        "name": "Hayley Ross"
                    },
                    {
                        "name": "Kathryn Davidson"
                    },
                    {
                        "name": "Najoung Kim"
                    }
                ],
                "author_detail": {
                    "name": "Najoung Kim"
                },
                "author": "Najoung Kim",
                "arxiv_comment": "8 pages (16 pages with appendix). Submitted to SCiL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24293v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24293v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24289v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24289v1",
                "updated": "2025-03-31T16:36:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    16,
                    36,
                    0,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T16:36:00Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    16,
                    36,
                    0,
                    0,
                    90,
                    0
                ],
                "title": "Rec-R1: Bridging Generative Large Language Models and User-Centric\n  Recommendation Systems via Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rec-R1: Bridging Generative Large Language Models and User-Centric\n  Recommendation Systems via Reinforcement Learning"
                },
                "summary": "We propose Rec-R1, a general reinforcement learning framework that bridges\nlarge language models (LLMs) with recommendation systems through closed-loop\noptimization. Unlike prompting and supervised fine-tuning (SFT), Rec-R1\ndirectly optimizes LLM generation using feedback from a fixed black-box\nrecommendation model, without relying on synthetic SFT data from proprietary\nmodels such as GPT-4o. This avoids the substantial cost and effort required for\ndata distillation. To verify the effectiveness of Rec-R1, we evaluate it on two\nrepresentative tasks: product search and sequential recommendation.\nExperimental results demonstrate that Rec-R1 not only consistently outperforms\nprompting- and SFT-based methods, but also achieves significant gains over\nstrong discriminative baselines, even when used with simple retrievers such as\nBM25. Moreover, Rec-R1 preserves the general-purpose capabilities of the LLM,\nunlike SFT, which often impairs instruction-following and reasoning. These\nfindings suggest Rec-R1 as a promising foundation for continual task-specific\nadaptation without catastrophic forgetting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Rec-R1, a general reinforcement learning framework that bridges\nlarge language models (LLMs) with recommendation systems through closed-loop\noptimization. Unlike prompting and supervised fine-tuning (SFT), Rec-R1\ndirectly optimizes LLM generation using feedback from a fixed black-box\nrecommendation model, without relying on synthetic SFT data from proprietary\nmodels such as GPT-4o. This avoids the substantial cost and effort required for\ndata distillation. To verify the effectiveness of Rec-R1, we evaluate it on two\nrepresentative tasks: product search and sequential recommendation.\nExperimental results demonstrate that Rec-R1 not only consistently outperforms\nprompting- and SFT-based methods, but also achieves significant gains over\nstrong discriminative baselines, even when used with simple retrievers such as\nBM25. Moreover, Rec-R1 preserves the general-purpose capabilities of the LLM,\nunlike SFT, which often impairs instruction-following and reasoning. These\nfindings suggest Rec-R1 as a promising foundation for continual task-specific\nadaptation without catastrophic forgetting."
                },
                "authors": [
                    {
                        "name": "Jiacheng Lin"
                    },
                    {
                        "name": "Tian Wang"
                    },
                    {
                        "name": "Kun Qian"
                    }
                ],
                "author_detail": {
                    "name": "Kun Qian"
                },
                "author": "Kun Qian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24289v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24289v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03962v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03962v5",
                "updated": "2025-03-31T16:35:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    16,
                    35,
                    0,
                    0,
                    90,
                    0
                ],
                "published": "2024-11-06T14:51:02Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    14,
                    51,
                    2,
                    2,
                    311,
                    0
                ],
                "title": "How Does A Text Preprocessing Pipeline Affect Ontology Syntactic\n  Matching?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Does A Text Preprocessing Pipeline Affect Ontology Syntactic\n  Matching?"
                },
                "summary": "The classic text preprocessing pipeline, comprising Tokenisation,\nNormalisation, Stop Words Removal, and Stemming/Lemmatisation, has been\nimplemented in many systems for syntactic ontology matching (OM). However, the\nlack of standardisation in text preprocessing creates diversity in mapping\nresults. In this paper we investigate the effect of the text preprocessing\npipeline on syntactic OM in 8 Ontology Alignment Evaluation Initiative (OAEI)\ntracks with 49 distinct alignments. We find that Phase 1 text preprocessing\n(Tokenisation and Normalisation) is more effective than Phase 2 text\npreprocessing (Stop Words Removal and Stemming/Lemmatisation). To repair the\nunwanted false mappings caused by Phase 2 text preprocessing, we propose a\nnovel context-based pipeline repair approach that employs a post hoc check to\nfind common words that cause false mappings. These words are stored in a\nreserved word set and applied in text preprocessing. The experimental results\nshow that our approach improves the matching correctness and the overall\nmatching performance. We then consider the broader integration of the classic\ntext preprocessing pipeline with modern large language models (LLMs) for OM. We\nrecommend that (1) the text preprocessing pipeline be injected via function\ncalling into LLMs to avoid the tendency towards unstable true mappings produced\nby LLM prompting; or (2) LLMs be used to repair non-existent and\ncounter-intuitive false mappings generated by the text preprocessing pipeline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The classic text preprocessing pipeline, comprising Tokenisation,\nNormalisation, Stop Words Removal, and Stemming/Lemmatisation, has been\nimplemented in many systems for syntactic ontology matching (OM). However, the\nlack of standardisation in text preprocessing creates diversity in mapping\nresults. In this paper we investigate the effect of the text preprocessing\npipeline on syntactic OM in 8 Ontology Alignment Evaluation Initiative (OAEI)\ntracks with 49 distinct alignments. We find that Phase 1 text preprocessing\n(Tokenisation and Normalisation) is more effective than Phase 2 text\npreprocessing (Stop Words Removal and Stemming/Lemmatisation). To repair the\nunwanted false mappings caused by Phase 2 text preprocessing, we propose a\nnovel context-based pipeline repair approach that employs a post hoc check to\nfind common words that cause false mappings. These words are stored in a\nreserved word set and applied in text preprocessing. The experimental results\nshow that our approach improves the matching correctness and the overall\nmatching performance. We then consider the broader integration of the classic\ntext preprocessing pipeline with modern large language models (LLMs) for OM. We\nrecommend that (1) the text preprocessing pipeline be injected via function\ncalling into LLMs to avoid the tendency towards unstable true mappings produced\nby LLM prompting; or (2) LLMs be used to repair non-existent and\ncounter-intuitive false mappings generated by the text preprocessing pipeline."
                },
                "authors": [
                    {
                        "name": "Zhangcheng Qiang"
                    },
                    {
                        "name": "Kerry Taylor"
                    },
                    {
                        "name": "Weiqing Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weiqing Wang"
                },
                "author": "Weiqing Wang",
                "arxiv_comment": "12 pages, 11 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03962v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03962v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22164v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22164v2",
                "updated": "2025-03-31T16:26:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    16,
                    26,
                    42,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-28T06:02:53Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    6,
                    2,
                    53,
                    4,
                    87,
                    0
                ],
                "title": "PharmAgents: Building a Virtual Pharma with Large Language Model Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PharmAgents: Building a Virtual Pharma with Large Language Model Agents"
                },
                "summary": "The discovery of novel small molecule drugs remains a critical scientific\nchallenge with far-reaching implications for treating diseases and advancing\nhuman health. Traditional drug development--especially for small molecule\ntherapeutics--is a highly complex, resource-intensive, and time-consuming\nprocess that requires multidisciplinary collaboration. Recent breakthroughs in\nartificial intelligence (AI), particularly the rise of large language models\n(LLMs), present a transformative opportunity to streamline and accelerate this\nprocess. In this paper, we introduce PharmAgents, a virtual pharmaceutical\necosystem driven by LLM-based multi-agent collaboration. PharmAgents simulates\nthe full drug discovery workflow--from target discovery to preclinical\nevaluation--by integrating explainable, LLM-driven agents equipped with\nspecialized machine learning models and computational tools. Through structured\nknowledge exchange and automated optimization, PharmAgents identifies potential\ntherapeutic targets, discovers promising lead compounds, enhances binding\naffinity and key molecular properties, and performs in silico analyses of\ntoxicity and synthetic feasibility. Additionally, the system supports\ninterpretability, agent interaction, and self-evolvement, enabling it to refine\nfuture drug designs based on prior experience. By showcasing the potential of\nLLM-powered multi-agent systems in drug discovery, this work establishes a new\nparadigm for autonomous, explainable, and scalable pharmaceutical research,\nwith future extensions toward comprehensive drug lifecycle management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The discovery of novel small molecule drugs remains a critical scientific\nchallenge with far-reaching implications for treating diseases and advancing\nhuman health. Traditional drug development--especially for small molecule\ntherapeutics--is a highly complex, resource-intensive, and time-consuming\nprocess that requires multidisciplinary collaboration. Recent breakthroughs in\nartificial intelligence (AI), particularly the rise of large language models\n(LLMs), present a transformative opportunity to streamline and accelerate this\nprocess. In this paper, we introduce PharmAgents, a virtual pharmaceutical\necosystem driven by LLM-based multi-agent collaboration. PharmAgents simulates\nthe full drug discovery workflow--from target discovery to preclinical\nevaluation--by integrating explainable, LLM-driven agents equipped with\nspecialized machine learning models and computational tools. Through structured\nknowledge exchange and automated optimization, PharmAgents identifies potential\ntherapeutic targets, discovers promising lead compounds, enhances binding\naffinity and key molecular properties, and performs in silico analyses of\ntoxicity and synthetic feasibility. Additionally, the system supports\ninterpretability, agent interaction, and self-evolvement, enabling it to refine\nfuture drug designs based on prior experience. By showcasing the potential of\nLLM-powered multi-agent systems in drug discovery, this work establishes a new\nparadigm for autonomous, explainable, and scalable pharmaceutical research,\nwith future extensions toward comprehensive drug lifecycle management."
                },
                "authors": [
                    {
                        "name": "Bowen Gao"
                    },
                    {
                        "name": "Yanwen Huang"
                    },
                    {
                        "name": "Yiqiao Liu"
                    },
                    {
                        "name": "Wenxuan Xie"
                    },
                    {
                        "name": "Wei-Ying Ma"
                    },
                    {
                        "name": "Ya-Qin Zhang"
                    },
                    {
                        "name": "Yanyan Lan"
                    }
                ],
                "author_detail": {
                    "name": "Yanyan Lan"
                },
                "author": "Yanyan Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22164v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22164v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.BM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24277v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24277v1",
                "updated": "2025-03-31T16:22:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    16,
                    22,
                    11,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T16:22:11Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    16,
                    22,
                    11,
                    0,
                    90,
                    0
                ],
                "title": "Evaluating and Designing Sparse Autoencoders by Approximating\n  Quasi-Orthogonality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating and Designing Sparse Autoencoders by Approximating\n  Quasi-Orthogonality"
                },
                "summary": "Sparse autoencoders (SAEs) have emerged as a workhorse of modern mechanistic\ninterpretability, but leading SAE approaches with top-$k$ style activation\nfunctions lack theoretical grounding for selecting the hyperparameter $k$. SAEs\nare based on the linear representation hypothesis (LRH), which assumes that the\nrepresentations of large language models (LLMs) are linearly encoded, and the\nsuperposition hypothesis (SH), which states that there can be more features in\nthe model than its dimensionality. We show that, based on the formal\ndefinitions of the LRH and SH, the magnitude of sparse feature vectors (the\nlatent representations learned by SAEs of the dense embeddings of LLMs) can be\napproximated using their corresponding dense vector with a closed-form error\nbound. To visualize this, we propose the ZF plot, which reveals a previously\nunknown relationship between LLM hidden embeddings and SAE feature vectors,\nallowing us to make the first empirical measurement of the extent to which\nfeature vectors of pre-trained SAEs are over- or under-activated for a given\ninput. Correspondingly, we introduce Approximate Feature Activation (AFA),\nwhich approximates the magnitude of the ground-truth sparse feature vector, and\npropose a new evaluation metric derived from AFA to assess the alignment\nbetween inputs and activations. We also leverage AFA to introduce a novel SAE\narchitecture, the top-AFA SAE, leading to SAEs that: (a) are more in line with\ntheoretical justifications; and (b) obviate the need to tune SAE sparsity\nhyperparameters. Finally, we empirically demonstrate that top-AFA SAEs achieve\nreconstruction loss comparable to that of state-of-the-art top-k SAEs, without\nrequiring the hyperparameter $k$ to be tuned. Our code is available at:\nhttps://github.com/SewoongLee/top-afa-sae.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse autoencoders (SAEs) have emerged as a workhorse of modern mechanistic\ninterpretability, but leading SAE approaches with top-$k$ style activation\nfunctions lack theoretical grounding for selecting the hyperparameter $k$. SAEs\nare based on the linear representation hypothesis (LRH), which assumes that the\nrepresentations of large language models (LLMs) are linearly encoded, and the\nsuperposition hypothesis (SH), which states that there can be more features in\nthe model than its dimensionality. We show that, based on the formal\ndefinitions of the LRH and SH, the magnitude of sparse feature vectors (the\nlatent representations learned by SAEs of the dense embeddings of LLMs) can be\napproximated using their corresponding dense vector with a closed-form error\nbound. To visualize this, we propose the ZF plot, which reveals a previously\nunknown relationship between LLM hidden embeddings and SAE feature vectors,\nallowing us to make the first empirical measurement of the extent to which\nfeature vectors of pre-trained SAEs are over- or under-activated for a given\ninput. Correspondingly, we introduce Approximate Feature Activation (AFA),\nwhich approximates the magnitude of the ground-truth sparse feature vector, and\npropose a new evaluation metric derived from AFA to assess the alignment\nbetween inputs and activations. We also leverage AFA to introduce a novel SAE\narchitecture, the top-AFA SAE, leading to SAEs that: (a) are more in line with\ntheoretical justifications; and (b) obviate the need to tune SAE sparsity\nhyperparameters. Finally, we empirically demonstrate that top-AFA SAEs achieve\nreconstruction loss comparable to that of state-of-the-art top-k SAEs, without\nrequiring the hyperparameter $k$ to be tuned. Our code is available at:\nhttps://github.com/SewoongLee/top-afa-sae."
                },
                "authors": [
                    {
                        "name": "Sewoong Lee"
                    },
                    {
                        "name": "Adam Davies"
                    },
                    {
                        "name": "Marc E. Canby"
                    },
                    {
                        "name": "Julia Hockenmaier"
                    }
                ],
                "author_detail": {
                    "name": "Julia Hockenmaier"
                },
                "author": "Julia Hockenmaier",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24277v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24277v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24262v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24262v1",
                "updated": "2025-03-31T16:08:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    16,
                    8,
                    11,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T16:08:11Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    16,
                    8,
                    11,
                    0,
                    90,
                    0
                ],
                "title": "New Statistical Framework for Extreme Error Probability in High-Stakes\n  Domains for Reliable Machine Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "New Statistical Framework for Extreme Error Probability in High-Stakes\n  Domains for Reliable Machine Learning"
                },
                "summary": "Machine learning is vital in high-stakes domains, yet conventional validation\nmethods rely on averaging metrics like mean squared error (MSE) or mean\nabsolute error (MAE), which fail to quantify extreme errors. Worst-case\nprediction failures can have substantial consequences, but current frameworks\nlack statistical foundations for assessing their probability. In this work a\nnew statistical framework, based on Extreme Value Theory (EVT), is presented\nthat provides a rigorous approach to estimating worst-case failures. Applying\nEVT to synthetic and real-world datasets, this method is shown to enable robust\nestimation of catastrophic failure probabilities, overcoming the fundamental\nlimitations of standard cross-validation. This work establishes EVT as a\nfundamental tool for assessing model reliability, ensuring safer AI deployment\nin new technologies where uncertainty quantification is central to\ndecision-making or scientific analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning is vital in high-stakes domains, yet conventional validation\nmethods rely on averaging metrics like mean squared error (MSE) or mean\nabsolute error (MAE), which fail to quantify extreme errors. Worst-case\nprediction failures can have substantial consequences, but current frameworks\nlack statistical foundations for assessing their probability. In this work a\nnew statistical framework, based on Extreme Value Theory (EVT), is presented\nthat provides a rigorous approach to estimating worst-case failures. Applying\nEVT to synthetic and real-world datasets, this method is shown to enable robust\nestimation of catastrophic failure probabilities, overcoming the fundamental\nlimitations of standard cross-validation. This work establishes EVT as a\nfundamental tool for assessing model reliability, ensuring safer AI deployment\nin new technologies where uncertainty quantification is central to\ndecision-making or scientific analysis."
                },
                "authors": [
                    {
                        "name": "Umberto Michelucci"
                    },
                    {
                        "name": "Francesca Venturini"
                    }
                ],
                "author_detail": {
                    "name": "Francesca Venturini"
                },
                "author": "Francesca Venturini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24262v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24262v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09433v2",
                "updated": "2025-03-31T16:07:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    16,
                    7,
                    10,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-12T14:30:05Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    14,
                    30,
                    5,
                    2,
                    71,
                    0
                ],
                "title": "CASTLE: Benchmarking Dataset for Static Code Analyzers and LLMs towards\n  CWE Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CASTLE: Benchmarking Dataset for Static Code Analyzers and LLMs towards\n  CWE Detection"
                },
                "summary": "Identifying vulnerabilities in source code is crucial, especially in critical\nsoftware components. Existing methods such as static analysis, dynamic\nanalysis, formal verification, and recently Large Language Models are widely\nused to detect security flaws. This paper introduces CASTLE (CWE Automated\nSecurity Testing and Low-Level Evaluation), a benchmarking framework for\nevaluating the vulnerability detection capabilities of different methods. We\nassess 13 static analysis tools, 10 LLMs, and 2 formal verification tools using\na hand-crafted dataset of 250 micro-benchmark programs covering 25 common CWEs.\nWe propose the CASTLE Score, a novel evaluation metric to ensure fair\ncomparison. Our results reveal key differences: ESBMC (a formal verification\ntool) minimizes false positives but struggles with vulnerabilities beyond model\nchecking, such as weak cryptography or SQL injection. Static analyzers suffer\nfrom high false positives, increasing manual validation efforts for developers.\nLLMs perform exceptionally well in the CASTLE dataset when identifying\nvulnerabilities in small code snippets. However, their accuracy declines, and\nhallucinations increase as the code size grows. These results suggest that LLMs\ncould play a pivotal role in future security solutions, particularly within\ncode completion frameworks, where they can provide real-time guidance to\nprevent vulnerabilities. The dataset is accessible at\nhttps://github.com/CASTLE-Benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying vulnerabilities in source code is crucial, especially in critical\nsoftware components. Existing methods such as static analysis, dynamic\nanalysis, formal verification, and recently Large Language Models are widely\nused to detect security flaws. This paper introduces CASTLE (CWE Automated\nSecurity Testing and Low-Level Evaluation), a benchmarking framework for\nevaluating the vulnerability detection capabilities of different methods. We\nassess 13 static analysis tools, 10 LLMs, and 2 formal verification tools using\na hand-crafted dataset of 250 micro-benchmark programs covering 25 common CWEs.\nWe propose the CASTLE Score, a novel evaluation metric to ensure fair\ncomparison. Our results reveal key differences: ESBMC (a formal verification\ntool) minimizes false positives but struggles with vulnerabilities beyond model\nchecking, such as weak cryptography or SQL injection. Static analyzers suffer\nfrom high false positives, increasing manual validation efforts for developers.\nLLMs perform exceptionally well in the CASTLE dataset when identifying\nvulnerabilities in small code snippets. However, their accuracy declines, and\nhallucinations increase as the code size grows. These results suggest that LLMs\ncould play a pivotal role in future security solutions, particularly within\ncode completion frameworks, where they can provide real-time guidance to\nprevent vulnerabilities. The dataset is accessible at\nhttps://github.com/CASTLE-Benchmark."
                },
                "authors": [
                    {
                        "name": "Richard A. Dubniczky"
                    },
                    {
                        "name": "Krisztofer Zoltán Horvát"
                    },
                    {
                        "name": "Tamás Bisztray"
                    },
                    {
                        "name": "Mohamed Amine Ferrag"
                    },
                    {
                        "name": "Lucas C. Cordeiro"
                    },
                    {
                        "name": "Norbert Tihanyi"
                    }
                ],
                "author_detail": {
                    "name": "Norbert Tihanyi"
                },
                "author": "Norbert Tihanyi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24245v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24245v1",
                "updated": "2025-03-31T15:58:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    15,
                    58,
                    8,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T15:58:08Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    15,
                    58,
                    8,
                    0,
                    90,
                    0
                ],
                "title": "Enhancing Large Language Models (LLMs) for Telecommunications using\n  Knowledge Graphs and Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Large Language Models (LLMs) for Telecommunications using\n  Knowledge Graphs and Retrieval-Augmented Generation"
                },
                "summary": "Large language models (LLMs) have made significant progress in\ngeneral-purpose natural language processing tasks. However, LLMs are still\nfacing challenges when applied to domain-specific areas like\ntelecommunications, which demands specialized expertise and adaptability to\nevolving standards. This paper presents a novel framework that combines\nknowledge graph (KG) and retrieval-augmented generation (RAG) techniques to\nenhance LLM performance in the telecom domain. The framework leverages a KG to\ncapture structured, domain-specific information about network protocols,\nstandards, and other telecom-related entities, comprehensively representing\ntheir relationships. By integrating KG with RAG, LLMs can dynamically access\nand utilize the most relevant and up-to-date knowledge during response\ngeneration. This hybrid approach bridges the gap between structured knowledge\nrepresentation and the generative capabilities of LLMs, significantly enhancing\naccuracy, adaptability, and domain-specific comprehension. Our results\ndemonstrate the effectiveness of the KG-RAG framework in addressing complex\ntechnical queries with precision. The proposed KG-RAG model attained an\naccuracy of 88% for question answering tasks on a frequently used\ntelecom-specific dataset, compared to 82% for the RAG-only and 48% for the\nLLM-only approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have made significant progress in\ngeneral-purpose natural language processing tasks. However, LLMs are still\nfacing challenges when applied to domain-specific areas like\ntelecommunications, which demands specialized expertise and adaptability to\nevolving standards. This paper presents a novel framework that combines\nknowledge graph (KG) and retrieval-augmented generation (RAG) techniques to\nenhance LLM performance in the telecom domain. The framework leverages a KG to\ncapture structured, domain-specific information about network protocols,\nstandards, and other telecom-related entities, comprehensively representing\ntheir relationships. By integrating KG with RAG, LLMs can dynamically access\nand utilize the most relevant and up-to-date knowledge during response\ngeneration. This hybrid approach bridges the gap between structured knowledge\nrepresentation and the generative capabilities of LLMs, significantly enhancing\naccuracy, adaptability, and domain-specific comprehension. Our results\ndemonstrate the effectiveness of the KG-RAG framework in addressing complex\ntechnical queries with precision. The proposed KG-RAG model attained an\naccuracy of 88% for question answering tasks on a frequently used\ntelecom-specific dataset, compared to 82% for the RAG-only and 48% for the\nLLM-only approaches."
                },
                "authors": [
                    {
                        "name": "Dun Yuan"
                    },
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Xue Liu"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Yan Xin"
                    },
                    {
                        "name": "Jianzhong"
                    },
                    {
                        "name": "Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhang"
                },
                "arxiv_affiliation": "Charlie",
                "author": "Zhang",
                "arxiv_comment": "This work has been accepted to ICC 2025 IEEE International Conference\n  on Communications. copyright 2025 IEEE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24245v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24245v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24235v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24235v1",
                "updated": "2025-03-31T15:46:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    15,
                    46,
                    15,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T15:46:15Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    15,
                    46,
                    15,
                    0,
                    90,
                    0
                ],
                "title": "What, How, Where, and How Well? A Survey on Test-Time Scaling in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What, How, Where, and How Well? A Survey on Test-Time Scaling in Large\n  Language Models"
                },
                "summary": "As enthusiasm for scaling computation (data and parameters) in the\npretraining era gradually diminished, test-time scaling (TTS), also referred to\nas ``test-time computing'' has emerged as a prominent research focus. Recent\nstudies demonstrate that TTS can further elicit the problem-solving\ncapabilities of large language models (LLMs), enabling significant\nbreakthroughs not only in specialized reasoning tasks, such as mathematics and\ncoding, but also in general tasks like open-ended Q&A. However, despite the\nexplosion of recent efforts in this area, there remains an urgent need for a\ncomprehensive survey offering a systemic understanding. To fill this gap, we\npropose a unified, multidimensional framework structured along four core\ndimensions of TTS research: what to scale, how to scale, where to scale, and\nhow well to scale. Building upon this taxonomy, we conduct an extensive review\nof methods, application scenarios, and assessment aspects, and present an\norganized decomposition that highlights the unique functional roles of\nindividual techniques within the broader TTS landscape. From this analysis, we\ndistill the major developmental trajectories of TTS to date and offer hands-on\nguidelines for practical deployment. Furthermore, we identify several open\nchallenges and offer insights into promising future directions, including\nfurther scaling, clarifying the functional essence of techniques, generalizing\nto more tasks, and more attributions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As enthusiasm for scaling computation (data and parameters) in the\npretraining era gradually diminished, test-time scaling (TTS), also referred to\nas ``test-time computing'' has emerged as a prominent research focus. Recent\nstudies demonstrate that TTS can further elicit the problem-solving\ncapabilities of large language models (LLMs), enabling significant\nbreakthroughs not only in specialized reasoning tasks, such as mathematics and\ncoding, but also in general tasks like open-ended Q&A. However, despite the\nexplosion of recent efforts in this area, there remains an urgent need for a\ncomprehensive survey offering a systemic understanding. To fill this gap, we\npropose a unified, multidimensional framework structured along four core\ndimensions of TTS research: what to scale, how to scale, where to scale, and\nhow well to scale. Building upon this taxonomy, we conduct an extensive review\nof methods, application scenarios, and assessment aspects, and present an\norganized decomposition that highlights the unique functional roles of\nindividual techniques within the broader TTS landscape. From this analysis, we\ndistill the major developmental trajectories of TTS to date and offer hands-on\nguidelines for practical deployment. Furthermore, we identify several open\nchallenges and offer insights into promising future directions, including\nfurther scaling, clarifying the functional essence of techniques, generalizing\nto more tasks, and more attributions."
                },
                "authors": [
                    {
                        "name": "Qiyuan Zhang"
                    },
                    {
                        "name": "Fuyuan Lyu"
                    },
                    {
                        "name": "Zexu Sun"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Weixu Zhang"
                    },
                    {
                        "name": "Zhihan Guo"
                    },
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Irwin King"
                    },
                    {
                        "name": "Xue Liu"
                    },
                    {
                        "name": "Chen Ma"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ma"
                },
                "author": "Chen Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24235v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24235v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24228v1",
                "updated": "2025-03-31T15:41:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    15,
                    41,
                    51,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T15:41:51Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    15,
                    41,
                    51,
                    0,
                    90,
                    0
                ],
                "title": "PAARS: Persona Aligned Agentic Retail Shoppers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PAARS: Persona Aligned Agentic Retail Shoppers"
                },
                "summary": "In e-commerce, behavioral data is collected for decision making which can be\ncostly and slow. Simulation with LLM powered agents is emerging as a promising\nalternative for representing human population behavior. However, LLMs are known\nto exhibit certain biases, such as brand bias, review rating bias and limited\nrepresentation of certain groups in the population, hence they need to be\ncarefully benchmarked and aligned to user behavior. Ultimately, our goal is to\nsynthesise an agent population and verify that it collectively approximates a\nreal sample of humans. To this end, we propose a framework that: (i) creates\nsynthetic shopping agents by automatically mining personas from anonymised\nhistorical shopping data, (ii) equips agents with retail-specific tools to\nsynthesise shopping sessions and (iii) introduces a novel alignment suite\nmeasuring distributional differences between humans and shopping agents at the\ngroup (i.e. population) level rather than the traditional \"individual\" level.\nExperimental results demonstrate that using personas improves performance on\nthe alignment suite, though a gap remains to human behaviour. We showcase an\ninitial application of our framework for automated agentic A/B testing and\ncompare the findings to human results. Finally, we discuss applications,\nlimitations and challenges setting the stage for impactful future work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In e-commerce, behavioral data is collected for decision making which can be\ncostly and slow. Simulation with LLM powered agents is emerging as a promising\nalternative for representing human population behavior. However, LLMs are known\nto exhibit certain biases, such as brand bias, review rating bias and limited\nrepresentation of certain groups in the population, hence they need to be\ncarefully benchmarked and aligned to user behavior. Ultimately, our goal is to\nsynthesise an agent population and verify that it collectively approximates a\nreal sample of humans. To this end, we propose a framework that: (i) creates\nsynthetic shopping agents by automatically mining personas from anonymised\nhistorical shopping data, (ii) equips agents with retail-specific tools to\nsynthesise shopping sessions and (iii) introduces a novel alignment suite\nmeasuring distributional differences between humans and shopping agents at the\ngroup (i.e. population) level rather than the traditional \"individual\" level.\nExperimental results demonstrate that using personas improves performance on\nthe alignment suite, though a gap remains to human behaviour. We showcase an\ninitial application of our framework for automated agentic A/B testing and\ncompare the findings to human results. Finally, we discuss applications,\nlimitations and challenges setting the stage for impactful future work."
                },
                "authors": [
                    {
                        "name": "Saab Mansour"
                    },
                    {
                        "name": "Leonardo Perelli"
                    },
                    {
                        "name": "Lorenzo Mainetti"
                    },
                    {
                        "name": "George Davidson"
                    },
                    {
                        "name": "Stefano D'Amato"
                    }
                ],
                "author_detail": {
                    "name": "Stefano D'Amato"
                },
                "author": "Stefano D'Amato",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05804v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05804v3",
                "updated": "2025-03-31T15:30:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    15,
                    30,
                    45,
                    0,
                    90,
                    0
                ],
                "published": "2024-10-08T08:36:12Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    8,
                    36,
                    12,
                    1,
                    282,
                    0
                ],
                "title": "CASA: Class-Agnostic Shared Attributes in Vision-Language Models for\n  Efficient Incremental Object Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CASA: Class-Agnostic Shared Attributes in Vision-Language Models for\n  Efficient Incremental Object Detection"
                },
                "summary": "Incremental object detection is fundamentally challenged by catastrophic\nforgetting. A major factor contributing to this issue is background shift,\nwhere background categories in sequential tasks may overlap with either\npreviously learned or future unseen classes. To address this, we propose a\nnovel method called Class-Agnostic Shared Attribute Base (CASA) that encourages\nthe model to learn category-agnostic attributes shared across incremental\nclasses. Our approach leverages an LLM to generate candidate textual\nattributes, selects the most relevant ones based on the current training data,\nand records their importance in an assignment matrix. For subsequent tasks, the\nretained attributes are frozen, and new attributes are selected from the\nremaining candidates, ensuring both knowledge retention and adaptability.\nExtensive experiments on the COCO dataset demonstrate the state-of-the-art\nperformance of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incremental object detection is fundamentally challenged by catastrophic\nforgetting. A major factor contributing to this issue is background shift,\nwhere background categories in sequential tasks may overlap with either\npreviously learned or future unseen classes. To address this, we propose a\nnovel method called Class-Agnostic Shared Attribute Base (CASA) that encourages\nthe model to learn category-agnostic attributes shared across incremental\nclasses. Our approach leverages an LLM to generate candidate textual\nattributes, selects the most relevant ones based on the current training data,\nand records their importance in an assignment matrix. For subsequent tasks, the\nretained attributes are frozen, and new attributes are selected from the\nremaining candidates, ensuring both knowledge retention and adaptability.\nExtensive experiments on the COCO dataset demonstrate the state-of-the-art\nperformance of our method."
                },
                "authors": [
                    {
                        "name": "Mingyi Guo"
                    },
                    {
                        "name": "Yuyang Liu"
                    },
                    {
                        "name": "Zhiyuan Yan"
                    },
                    {
                        "name": "Zongying Lin"
                    },
                    {
                        "name": "Peixi Peng"
                    },
                    {
                        "name": "Yonghong Tian"
                    }
                ],
                "author_detail": {
                    "name": "Yonghong Tian"
                },
                "author": "Yonghong Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05804v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05804v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18296v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18296v2",
                "updated": "2025-03-31T15:29:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    15,
                    29,
                    24,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-24T03:02:04Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    3,
                    2,
                    4,
                    0,
                    83,
                    0
                ],
                "title": "Surgical Action Planning with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surgical Action Planning with Large Language Models"
                },
                "summary": "In robot-assisted minimally invasive surgery, we introduce the Surgical\nAction Planning (SAP) task, which generates future action plans from visual\ninputs to address the absence of intraoperative predictive planning in current\nintelligent applications. SAP shows great potential for enhancing\nintraoperative guidance and automating procedures. However, it faces challenges\nsuch as understanding instrument-action relationships and tracking surgical\nprogress. Large Language Models (LLMs) show promise in understanding surgical\nvideo content but remain underexplored for predictive decision-making in SAP,\nas they focus mainly on retrospective analysis. Challenges like data privacy,\ncomputational demands, and modality-specific constraints further highlight\nsignificant research gaps. To tackle these challenges, we introduce LLM-SAP, a\nLarge Language Models-based Surgical Action Planning framework that predicts\nfuture actions and generates text responses by interpreting natural language\nprompts of surgical goals. The text responses potentially support surgical\neducation, intraoperative decision-making, procedure documentation, and skill\nanalysis. LLM-SAP integrates two novel modules: the Near-History Focus Memory\nModule (NHF-MM) for modeling historical states and the prompts factory for\naction planning. We evaluate LLM-SAP on our constructed CholecT50-SAP dataset\nusing models like Qwen2.5 and Qwen2-VL, demonstrating its effectiveness in\nnext-action prediction. Pre-trained LLMs are tested in a zero-shot setting, and\nsupervised fine-tuning (SFT) with LoRA is implemented. Our experiments show\nthat Qwen2.5-72B-SFT surpasses Qwen2.5-72B with a 19.3% higher accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In robot-assisted minimally invasive surgery, we introduce the Surgical\nAction Planning (SAP) task, which generates future action plans from visual\ninputs to address the absence of intraoperative predictive planning in current\nintelligent applications. SAP shows great potential for enhancing\nintraoperative guidance and automating procedures. However, it faces challenges\nsuch as understanding instrument-action relationships and tracking surgical\nprogress. Large Language Models (LLMs) show promise in understanding surgical\nvideo content but remain underexplored for predictive decision-making in SAP,\nas they focus mainly on retrospective analysis. Challenges like data privacy,\ncomputational demands, and modality-specific constraints further highlight\nsignificant research gaps. To tackle these challenges, we introduce LLM-SAP, a\nLarge Language Models-based Surgical Action Planning framework that predicts\nfuture actions and generates text responses by interpreting natural language\nprompts of surgical goals. The text responses potentially support surgical\neducation, intraoperative decision-making, procedure documentation, and skill\nanalysis. LLM-SAP integrates two novel modules: the Near-History Focus Memory\nModule (NHF-MM) for modeling historical states and the prompts factory for\naction planning. We evaluate LLM-SAP on our constructed CholecT50-SAP dataset\nusing models like Qwen2.5 and Qwen2-VL, demonstrating its effectiveness in\nnext-action prediction. Pre-trained LLMs are tested in a zero-shot setting, and\nsupervised fine-tuning (SFT) with LoRA is implemented. Our experiments show\nthat Qwen2.5-72B-SFT surpasses Qwen2.5-72B with a 19.3% higher accuracy."
                },
                "authors": [
                    {
                        "name": "Mengya Xu"
                    },
                    {
                        "name": "Zhongzhen Huang"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Xiaofan Zhang"
                    },
                    {
                        "name": "Qi Dou"
                    }
                ],
                "author_detail": {
                    "name": "Qi Dou"
                },
                "author": "Qi Dou",
                "arxiv_comment": "10 pages,4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18296v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18296v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24206v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24206v1",
                "updated": "2025-03-31T15:24:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    15,
                    24,
                    5,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T15:24:05Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    15,
                    24,
                    5,
                    0,
                    90,
                    0
                ],
                "title": "Synthetic News Generation for Fake News Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic News Generation for Fake News Classification"
                },
                "summary": "This study explores the generation and evaluation of synthetic fake news\nthrough fact based manipulations using large language models (LLMs). We\nintroduce a novel methodology that extracts key facts from real articles,\nmodifies them, and regenerates content to simulate fake news while maintaining\ncoherence. To assess the quality of the generated content, we propose a set of\nevaluation metrics coherence, dissimilarity, and correctness. The research also\ninvestigates the application of synthetic data in fake news classification,\ncomparing traditional machine learning models with transformer based models\nsuch as BERT. Our experiments demonstrate that transformer models, especially\nBERT, effectively leverage synthetic data for fake news detection, showing\nimprovements with smaller proportions of synthetic data. Additionally, we find\nthat fact verification features, which focus on identifying factual\ninconsistencies, provide the most promising results in distinguishing synthetic\nfake news. The study highlights the potential of synthetic data to enhance fake\nnews detection systems, offering valuable insights for future research and\nsuggesting that targeted improvements in synthetic data generation can further\nstrengthen detection models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores the generation and evaluation of synthetic fake news\nthrough fact based manipulations using large language models (LLMs). We\nintroduce a novel methodology that extracts key facts from real articles,\nmodifies them, and regenerates content to simulate fake news while maintaining\ncoherence. To assess the quality of the generated content, we propose a set of\nevaluation metrics coherence, dissimilarity, and correctness. The research also\ninvestigates the application of synthetic data in fake news classification,\ncomparing traditional machine learning models with transformer based models\nsuch as BERT. Our experiments demonstrate that transformer models, especially\nBERT, effectively leverage synthetic data for fake news detection, showing\nimprovements with smaller proportions of synthetic data. Additionally, we find\nthat fact verification features, which focus on identifying factual\ninconsistencies, provide the most promising results in distinguishing synthetic\nfake news. The study highlights the potential of synthetic data to enhance fake\nnews detection systems, offering valuable insights for future research and\nsuggesting that targeted improvements in synthetic data generation can further\nstrengthen detection models."
                },
                "authors": [
                    {
                        "name": "Abdul Sittar"
                    },
                    {
                        "name": "Luka Golob"
                    },
                    {
                        "name": "Mateja Smiljanic"
                    }
                ],
                "author_detail": {
                    "name": "Mateja Smiljanic"
                },
                "author": "Mateja Smiljanic",
                "arxiv_comment": "13 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24206v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24206v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24198v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24198v1",
                "updated": "2025-03-31T15:16:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    15,
                    16,
                    31,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T15:16:31Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    15,
                    16,
                    31,
                    0,
                    90,
                    0
                ],
                "title": "TwT: Thinking without Tokens by Habitual Reasoning Distillation with\n  Multi-Teachers' Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TwT: Thinking without Tokens by Habitual Reasoning Distillation with\n  Multi-Teachers' Guidance"
                },
                "summary": "Large Language Models (LLMs) have made significant strides in problem-solving\nby incorporating reasoning processes. However, this enhanced reasoning\ncapability results in an increased number of output tokens during inference,\nleading to higher computational costs. To address this challenge, we propose\nTwT (Thinking without Tokens), a method that reduces inference-time costs\nthrough habitual reasoning distillation with multi-teachers' guidance, while\nmaintaining high performance. Our approach introduces a Habitual Reasoning\nDistillation method, which internalizes explicit reasoning into the model's\nhabitual behavior through a Teacher-Guided compression strategy inspired by\nhuman cognition. Additionally, we propose Dual-Criteria Rejection Sampling\n(DCRS), a technique that generates a high-quality and diverse distillation\ndataset using multiple teacher models, making our method suitable for\nunsupervised scenarios. Experimental results demonstrate that TwT effectively\nreduces inference costs while preserving superior performance, achieving up to\na 13.6% improvement in accuracy with fewer output tokens compared to other\ndistillation methods, offering a highly practical solution for efficient LLM\ndeployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have made significant strides in problem-solving\nby incorporating reasoning processes. However, this enhanced reasoning\ncapability results in an increased number of output tokens during inference,\nleading to higher computational costs. To address this challenge, we propose\nTwT (Thinking without Tokens), a method that reduces inference-time costs\nthrough habitual reasoning distillation with multi-teachers' guidance, while\nmaintaining high performance. Our approach introduces a Habitual Reasoning\nDistillation method, which internalizes explicit reasoning into the model's\nhabitual behavior through a Teacher-Guided compression strategy inspired by\nhuman cognition. Additionally, we propose Dual-Criteria Rejection Sampling\n(DCRS), a technique that generates a high-quality and diverse distillation\ndataset using multiple teacher models, making our method suitable for\nunsupervised scenarios. Experimental results demonstrate that TwT effectively\nreduces inference costs while preserving superior performance, achieving up to\na 13.6% improvement in accuracy with fewer output tokens compared to other\ndistillation methods, offering a highly practical solution for efficient LLM\ndeployment."
                },
                "authors": [
                    {
                        "name": "Jingxian Xu"
                    },
                    {
                        "name": "Mengyu Zhou"
                    },
                    {
                        "name": "Weichang Liu"
                    },
                    {
                        "name": "Hanbing Liu"
                    },
                    {
                        "name": "Shi Han"
                    },
                    {
                        "name": "Dongmei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Dongmei Zhang"
                },
                "author": "Dongmei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24198v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24198v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24193v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24193v1",
                "updated": "2025-03-31T15:09:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    15,
                    9,
                    19,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T15:09:19Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    15,
                    9,
                    19,
                    0,
                    90,
                    0
                ],
                "title": "Text2Tracks: Prompt-based Music Recommendation via Generative Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text2Tracks: Prompt-based Music Recommendation via Generative Retrieval"
                },
                "summary": "In recent years, Large Language Models (LLMs) have enabled users to provide\nhighly specific music recommendation requests using natural language prompts\n(e.g. \"Can you recommend some old classics for slow dancing?\"). In this setup,\nthe recommended tracks are predicted by the LLM in an autoregressive way, i.e.\nthe LLM generates the track titles one token at a time. While intuitive, this\napproach has several limitation. First, it is based on a general purpose\ntokenization that is optimized for words rather than for track titles. Second,\nit necessitates an additional entity resolution layer that matches the track\ntitle to the actual track identifier. Third, the number of decoding steps\nscales linearly with the length of the track title, slowing down inference. In\nthis paper, we propose to address the task of prompt-based music recommendation\nas a generative retrieval task. Within this setting, we introduce novel,\neffective, and efficient representations of track identifiers that\nsignificantly outperform commonly used strategies. We introduce Text2Tracks, a\ngenerative retrieval model that learns a mapping from a user's music\nrecommendation prompt to the relevant track IDs directly. Through an offline\nevaluation on a dataset of playlists with language inputs, we find that (1) the\nstrategy to create IDs for music tracks is the most important factor for the\neffectiveness of Text2Tracks and semantic IDs significantly outperform commonly\nused strategies that rely on song titles as identifiers (2) provided with the\nright choice of track identifiers, Text2Tracks outperforms sparse and dense\nretrieval solutions trained to retrieve tracks from language prompts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Large Language Models (LLMs) have enabled users to provide\nhighly specific music recommendation requests using natural language prompts\n(e.g. \"Can you recommend some old classics for slow dancing?\"). In this setup,\nthe recommended tracks are predicted by the LLM in an autoregressive way, i.e.\nthe LLM generates the track titles one token at a time. While intuitive, this\napproach has several limitation. First, it is based on a general purpose\ntokenization that is optimized for words rather than for track titles. Second,\nit necessitates an additional entity resolution layer that matches the track\ntitle to the actual track identifier. Third, the number of decoding steps\nscales linearly with the length of the track title, slowing down inference. In\nthis paper, we propose to address the task of prompt-based music recommendation\nas a generative retrieval task. Within this setting, we introduce novel,\neffective, and efficient representations of track identifiers that\nsignificantly outperform commonly used strategies. We introduce Text2Tracks, a\ngenerative retrieval model that learns a mapping from a user's music\nrecommendation prompt to the relevant track IDs directly. Through an offline\nevaluation on a dataset of playlists with language inputs, we find that (1) the\nstrategy to create IDs for music tracks is the most important factor for the\neffectiveness of Text2Tracks and semantic IDs significantly outperform commonly\nused strategies that rely on song titles as identifiers (2) provided with the\nright choice of track identifiers, Text2Tracks outperforms sparse and dense\nretrieval solutions trained to retrieve tracks from language prompts."
                },
                "authors": [
                    {
                        "name": "Enrico Palumbo"
                    },
                    {
                        "name": "Gustavo Penha"
                    },
                    {
                        "name": "Andreas Damianou"
                    },
                    {
                        "name": "José Luis Redondo García"
                    },
                    {
                        "name": "Timothy Christopher Heath"
                    },
                    {
                        "name": "Alice Wang"
                    },
                    {
                        "name": "Hugues Bouchard"
                    },
                    {
                        "name": "Mounia Lalmas"
                    }
                ],
                "author_detail": {
                    "name": "Mounia Lalmas"
                },
                "author": "Mounia Lalmas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24193v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24193v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]