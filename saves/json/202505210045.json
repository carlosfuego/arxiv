[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2505.10951v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10951v2",
                "updated": "2025-05-19T17:51:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    51,
                    26,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-16T07:39:41Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    7,
                    39,
                    41,
                    4,
                    136,
                    0
                ],
                "title": "SubGCache: Accelerating Graph-based RAG with Subgraph-level KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SubGCache: Accelerating Graph-based RAG with Subgraph-level KV Cache"
                },
                "summary": "Graph-based retrieval-augmented generation (RAG) enables large language\nmodels (LLMs) to incorporate structured knowledge via graph retrieval as\ncontextual input, enhancing more accurate and context-aware reasoning. We\nobserve that for different queries, it could retrieve similar subgraphs as\nprompts, and thus we propose SubGCache, which aims to reduce inference latency\nby reusing computation across queries with similar structural prompts (i.e.,\nsubgraphs). Specifically, SubGCache clusters queries based on subgraph\nembeddings, constructs a representative subgraph for each cluster, and\npre-computes the key-value (KV) cache of the representative subgraph. For each\nquery with its retrieved subgraph within a cluster, it reuses the pre-computed\nKV cache of the representative subgraph of the cluster without computing the KV\ntensors again for saving computation. Experiments on two new datasets across\nmultiple LLM backbones and graph-based RAG frameworks demonstrate that\nSubGCache consistently reduces inference latency with comparable and even\nimproved generation quality, achieving up to 6.68$\\times$ reduction in\ntime-to-first-token (TTFT).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-based retrieval-augmented generation (RAG) enables large language\nmodels (LLMs) to incorporate structured knowledge via graph retrieval as\ncontextual input, enhancing more accurate and context-aware reasoning. We\nobserve that for different queries, it could retrieve similar subgraphs as\nprompts, and thus we propose SubGCache, which aims to reduce inference latency\nby reusing computation across queries with similar structural prompts (i.e.,\nsubgraphs). Specifically, SubGCache clusters queries based on subgraph\nembeddings, constructs a representative subgraph for each cluster, and\npre-computes the key-value (KV) cache of the representative subgraph. For each\nquery with its retrieved subgraph within a cluster, it reuses the pre-computed\nKV cache of the representative subgraph of the cluster without computing the KV\ntensors again for saving computation. Experiments on two new datasets across\nmultiple LLM backbones and graph-based RAG frameworks demonstrate that\nSubGCache consistently reduces inference latency with comparable and even\nimproved generation quality, achieving up to 6.68$\\times$ reduction in\ntime-to-first-token (TTFT)."
                },
                "authors": [
                    {
                        "name": "Qiuyu Zhu"
                    },
                    {
                        "name": "Liang Zhang"
                    },
                    {
                        "name": "Qianxiong Xu"
                    },
                    {
                        "name": "Cheng Long"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "author": "Jie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10951v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10951v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13140v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13140v1",
                "updated": "2025-05-19T14:09:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    9,
                    45,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T14:09:45Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    9,
                    45,
                    0,
                    139,
                    0
                ],
                "title": "CacheFlow: Fast Human Motion Prediction by Cached Normalizing Flow",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheFlow: Fast Human Motion Prediction by Cached Normalizing Flow"
                },
                "summary": "Many density estimation techniques for 3D human motion prediction require a\nsignificant amount of inference time, often exceeding the duration of the\npredicted time horizon. To address the need for faster density estimation for\n3D human motion prediction, we introduce a novel flow-based method for human\nmotion prediction called CacheFlow. Unlike previous conditional generative\nmodels that suffer from time efficiency, CacheFlow takes advantage of an\nunconditional flow-based generative model that transforms a Gaussian mixture\ninto the density of future motions. The results of the computation of the\nflow-based generative model can be precomputed and cached. Then, for\nconditional prediction, we seek a mapping from historical trajectories to\nsamples in the Gaussian mixture. This mapping can be done by a much more\nlightweight model, thus saving significant computation overhead compared to a\ntypical conditional flow model. In such a two-stage fashion and by caching\nresults from the slow flow model computation, we build our CacheFlow without\nloss of prediction accuracy and model expressiveness. This inference process is\ncompleted in approximately one millisecond, making it 4 times faster than\nprevious VAE methods and 30 times faster than previous diffusion-based methods\non standard benchmarks such as Human3.6M and AMASS datasets. Furthermore, our\nmethod demonstrates improved density estimation accuracy and comparable\nprediction accuracy to a SOTA method on Human3.6M. Our code and models will be\npublicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many density estimation techniques for 3D human motion prediction require a\nsignificant amount of inference time, often exceeding the duration of the\npredicted time horizon. To address the need for faster density estimation for\n3D human motion prediction, we introduce a novel flow-based method for human\nmotion prediction called CacheFlow. Unlike previous conditional generative\nmodels that suffer from time efficiency, CacheFlow takes advantage of an\nunconditional flow-based generative model that transforms a Gaussian mixture\ninto the density of future motions. The results of the computation of the\nflow-based generative model can be precomputed and cached. Then, for\nconditional prediction, we seek a mapping from historical trajectories to\nsamples in the Gaussian mixture. This mapping can be done by a much more\nlightweight model, thus saving significant computation overhead compared to a\ntypical conditional flow model. In such a two-stage fashion and by caching\nresults from the slow flow model computation, we build our CacheFlow without\nloss of prediction accuracy and model expressiveness. This inference process is\ncompleted in approximately one millisecond, making it 4 times faster than\nprevious VAE methods and 30 times faster than previous diffusion-based methods\non standard benchmarks such as Human3.6M and AMASS datasets. Furthermore, our\nmethod demonstrates improved density estimation accuracy and comparable\nprediction accuracy to a SOTA method on Human3.6M. Our code and models will be\npublicly available."
                },
                "authors": [
                    {
                        "name": "Takahiro Maeda"
                    },
                    {
                        "name": "Jinkun Cao"
                    },
                    {
                        "name": "Norimichi Ukita"
                    },
                    {
                        "name": "Kris Kitani"
                    }
                ],
                "author_detail": {
                    "name": "Kris Kitani"
                },
                "author": "Kris Kitani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13140v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13140v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13109v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13109v1",
                "updated": "2025-05-19T13:36:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    36,
                    45,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T13:36:45Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    36,
                    45,
                    0,
                    139,
                    0
                ],
                "title": "FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference"
                },
                "summary": "Large language models (LLMs) have been widely deployed with rapidly expanding\ncontext windows to support increasingly demanding applications. However, long\ncontexts pose significant deployment challenges, primarily due to the KV cache\nwhose size grows proportionally with context length. While KV cache compression\nmethods are proposed to address this issue, KV dropping methods incur\nconsiderable accuracy loss, and KV retrieval methods suffer from significant\nefficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization\nframework to enhance KV retrieval efficiency while preserving accuracy. On the\nalgorithm side, FreeKV introduces speculative retrieval to shift the KV\nselection and recall processes out of the critical path, combined with\nfine-grained correction to ensure accuracy. On the system side, FreeKV employs\nhybrid KV layouts across CPU and GPU memory to eliminate fragmented data\ntransfers, and leverages double-buffered streamed recall to further improve\nefficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy\nacross various scenarios and models, delivering up to 13$\\times$ speedup\ncompared to SOTA KV retrieval methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been widely deployed with rapidly expanding\ncontext windows to support increasingly demanding applications. However, long\ncontexts pose significant deployment challenges, primarily due to the KV cache\nwhose size grows proportionally with context length. While KV cache compression\nmethods are proposed to address this issue, KV dropping methods incur\nconsiderable accuracy loss, and KV retrieval methods suffer from significant\nefficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization\nframework to enhance KV retrieval efficiency while preserving accuracy. On the\nalgorithm side, FreeKV introduces speculative retrieval to shift the KV\nselection and recall processes out of the critical path, combined with\nfine-grained correction to ensure accuracy. On the system side, FreeKV employs\nhybrid KV layouts across CPU and GPU memory to eliminate fragmented data\ntransfers, and leverages double-buffered streamed recall to further improve\nefficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy\nacross various scenarios and models, delivering up to 13$\\times$ speedup\ncompared to SOTA KV retrieval methods."
                },
                "authors": [
                    {
                        "name": "Guangda Liu"
                    },
                    {
                        "name": "Chengwei Li"
                    },
                    {
                        "name": "Zhenyu Ning"
                    },
                    {
                        "name": "Jing Lin"
                    },
                    {
                        "name": "Yiwu Yao"
                    },
                    {
                        "name": "Danning Ke"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Jieru Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jieru Zhao"
                },
                "author": "Jieru Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13109v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13109v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13094v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13094v1",
                "updated": "2025-05-19T13:25:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    25,
                    51,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T13:25:51Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    25,
                    51,
                    0,
                    139,
                    0
                ],
                "title": "Time-Frequency-Based Attention Cache Memory Model for Real-Time Speech\n  Separation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-Frequency-Based Attention Cache Memory Model for Real-Time Speech\n  Separation"
                },
                "summary": "Existing causal speech separation models often underperform compared to\nnon-causal models due to difficulties in retaining historical information. To\naddress this, we propose the Time-Frequency Attention Cache Memory (TFACM)\nmodel, which effectively captures spatio-temporal relationships through an\nattention mechanism and cache memory (CM) for historical information storage.\nIn TFACM, an LSTM layer captures frequency-relative positions, while causal\nmodeling is applied to the time dimension using local and global\nrepresentations. The CM module stores past information, and the causal\nattention refinement (CAR) module further enhances time-based feature\nrepresentations for finer granularity. Experimental results showed that TFACM\nachieveed comparable performance to the SOTA TF-GridNet-Causal model, with\nsignificantly lower complexity and fewer trainable parameters. For more\ndetails, visit the project page: https://cslikai.cn/TFACM/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing causal speech separation models often underperform compared to\nnon-causal models due to difficulties in retaining historical information. To\naddress this, we propose the Time-Frequency Attention Cache Memory (TFACM)\nmodel, which effectively captures spatio-temporal relationships through an\nattention mechanism and cache memory (CM) for historical information storage.\nIn TFACM, an LSTM layer captures frequency-relative positions, while causal\nmodeling is applied to the time dimension using local and global\nrepresentations. The CM module stores past information, and the causal\nattention refinement (CAR) module further enhances time-based feature\nrepresentations for finer granularity. Experimental results showed that TFACM\nachieveed comparable performance to the SOTA TF-GridNet-Causal model, with\nsignificantly lower complexity and fewer trainable parameters. For more\ndetails, visit the project page: https://cslikai.cn/TFACM/."
                },
                "authors": [
                    {
                        "name": "Guo Chen"
                    },
                    {
                        "name": "Kai Li"
                    },
                    {
                        "name": "Runxuan Yang"
                    },
                    {
                        "name": "Xiaolin Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaolin Hu"
                },
                "author": "Xiaolin Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13094v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13094v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13002v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13002v2",
                "updated": "2025-05-20T07:34:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    7,
                    34,
                    45,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-19T11:41:21Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    11,
                    41,
                    21,
                    0,
                    139,
                    0
                ],
                "title": "PIM-malloc: A Fast and Scalable Dynamic Memory Allocator for\n  Processing-In-Memory (PIM) Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIM-malloc: A Fast and Scalable Dynamic Memory Allocator for\n  Processing-In-Memory (PIM) Architectures"
                },
                "summary": "Dynamic memory allocation is essential in modern programming but remains\nunder-supported in current PIM devices. In this work, we first conduct a design\nspace exploration of PIM memory allocators, examining optimal metadata\nplacement and management strategies. Building on these insights, we propose\nPIM-malloc, a fast and scalable allocator for real PIM hardware, improving\nallocation performance by $66\\times$. We further enhance this design with a\nlightweight, per-PIM core hardware cache for dynamic allocation, achieving an\nadditional $31\\%$ performance gain. Finally, we demonstrate the effectiveness\nof PIM-malloc using a dynamic graph update workload, achieving a $28\\times$\nthroughput increase.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic memory allocation is essential in modern programming but remains\nunder-supported in current PIM devices. In this work, we first conduct a design\nspace exploration of PIM memory allocators, examining optimal metadata\nplacement and management strategies. Building on these insights, we propose\nPIM-malloc, a fast and scalable allocator for real PIM hardware, improving\nallocation performance by $66\\times$. We further enhance this design with a\nlightweight, per-PIM core hardware cache for dynamic allocation, achieving an\nadditional $31\\%$ performance gain. Finally, we demonstrate the effectiveness\nof PIM-malloc using a dynamic graph update workload, achieving a $28\\times$\nthroughput increase."
                },
                "authors": [
                    {
                        "name": "Dongjae Lee"
                    },
                    {
                        "name": "Bongjoon Hyun"
                    },
                    {
                        "name": "Youngjin Kwon"
                    },
                    {
                        "name": "Minsoo Rhu"
                    }
                ],
                "author_detail": {
                    "name": "Minsoo Rhu"
                },
                "author": "Minsoo Rhu",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13002v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13002v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12946v1",
                "updated": "2025-05-19T10:34:54Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    10,
                    34,
                    54,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T10:34:54Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    10,
                    34,
                    54,
                    0,
                    139,
                    0
                ],
                "title": "6G-Enabled Smart Railways",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "6G-Enabled Smart Railways"
                },
                "summary": "Smart railways integrate advanced information technologies into railway\noperating systems to improve efficiency and reliability. Although the\ndevelopment of 5G has enhanced railway services, future smart railways require\nultra-high speeds, ultra-low latency, ultra-high security, full coverage, and\nultra-high positioning accuracy, which 5G cannot fully meet. Therefore, 6G is\nenvisioned to provide green and efficient all-day operations, strong\ninformation security, fully automatic driving, and low-cost intelligent\nmaintenance. To achieve these requirements, we propose an integrated network\narchitecture leveraging communications, computing, edge intelligence, and\ncaching in railway systems. We have conducted in-depth investigations on key\nenabling technologies for reliable transmissions and wireless coverage. For\nhigh-speed mobile scenarios, we propose an AI-enabled cross-domain channel\nmodeling and orthogonal time-frequency space-time spread multiple access\nmechanism to alleviate the conflict between limited spectrum availability and\nmassive user access. The roles of blockchain, edge intelligence, and privacy\ntechnologies in endogenously secure rail communications are also evaluated. We\nfurther explore the application of emerging paradigms such as integrated\nsensing and communications, AI-assisted Internet of Things, semantic\ncommunications, and digital twin networks for railway maintenance, monitoring,\nprediction, and accident warning. Finally, possible future research and\ndevelopment directions are discussed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart railways integrate advanced information technologies into railway\noperating systems to improve efficiency and reliability. Although the\ndevelopment of 5G has enhanced railway services, future smart railways require\nultra-high speeds, ultra-low latency, ultra-high security, full coverage, and\nultra-high positioning accuracy, which 5G cannot fully meet. Therefore, 6G is\nenvisioned to provide green and efficient all-day operations, strong\ninformation security, fully automatic driving, and low-cost intelligent\nmaintenance. To achieve these requirements, we propose an integrated network\narchitecture leveraging communications, computing, edge intelligence, and\ncaching in railway systems. We have conducted in-depth investigations on key\nenabling technologies for reliable transmissions and wireless coverage. For\nhigh-speed mobile scenarios, we propose an AI-enabled cross-domain channel\nmodeling and orthogonal time-frequency space-time spread multiple access\nmechanism to alleviate the conflict between limited spectrum availability and\nmassive user access. The roles of blockchain, edge intelligence, and privacy\ntechnologies in endogenously secure rail communications are also evaluated. We\nfurther explore the application of emerging paradigms such as integrated\nsensing and communications, AI-assisted Internet of Things, semantic\ncommunications, and digital twin networks for railway maintenance, monitoring,\nprediction, and accident warning. Finally, possible future research and\ndevelopment directions are discussed."
                },
                "authors": [
                    {
                        "name": "Bo Ai"
                    },
                    {
                        "name": "Yunlong Lu"
                    },
                    {
                        "name": "Yuguang Fang"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Ruisi He"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Jiayi Zhang"
                    },
                    {
                        "name": "Guoyu Ma"
                    },
                    {
                        "name": "Yong Niu"
                    },
                    {
                        "name": "Zhangdui Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Zhangdui Zhong"
                },
                "author": "Zhangdui Zhong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12942v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12942v1",
                "updated": "2025-05-19T10:29:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    10,
                    29,
                    32,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T10:29:32Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    10,
                    29,
                    32,
                    0,
                    139,
                    0
                ],
                "title": "A3 : an Analytical Low-Rank Approximation Framework for Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A3 : an Analytical Low-Rank Approximation Framework for Attention"
                },
                "summary": "Large language models have demonstrated remarkable performance; however,\ntheir massive parameter counts make deployment highly expensive. Low-rank\napproximation offers a promising compression solution, yet existing approaches\nhave two main limitations: (1) They focus on minimizing the output error of\nindividual linear layers, without considering the architectural characteristics\nof Transformers, and (2) they decompose a large weight matrix into two small\nlow-rank matrices. Consequently, these methods often fall short compared to\nother compression techniques like pruning and quantization, and introduce\nruntime overhead such as the extra GEMM kernel launches for decomposed small\nmatrices. To address these limitations, we propose $\\tt A^\\tt 3$, a\npost-training low-rank approximation framework. $\\tt A^\\tt 3$ splits a\nTransformer layer into three functional components, namely $\\tt QK$, $\\tt OV$,\nand $\\tt MLP$. For each component, $\\tt A^\\tt 3$ provides an analytical\nsolution that reduces the hidden dimension size inside each component while\nminimizing the component's functional loss ($\\it i.e.$, error in attention\nscores, attention outputs, and MLP outputs). This approach directly reduces\nmodel sizes, KV cache sizes, and FLOPs without introducing any runtime\noverheads. In addition, it provides a new narrative in advancing the\noptimization problem from singular linear layer loss optimization toward\nimproved end-to-end performance. Through extensive experiments, we show that\n$\\tt A^\\tt 3$ maintains superior performance compared to SoTAs. For example,\nunder the same reduction budget in computation and memory, our low-rank\napproximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2,\noutperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the\nversatility of $\\tt A^\\tt 3$, including KV cache compression, quantization, and\nmixed-rank assignments for enhanced performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have demonstrated remarkable performance; however,\ntheir massive parameter counts make deployment highly expensive. Low-rank\napproximation offers a promising compression solution, yet existing approaches\nhave two main limitations: (1) They focus on minimizing the output error of\nindividual linear layers, without considering the architectural characteristics\nof Transformers, and (2) they decompose a large weight matrix into two small\nlow-rank matrices. Consequently, these methods often fall short compared to\nother compression techniques like pruning and quantization, and introduce\nruntime overhead such as the extra GEMM kernel launches for decomposed small\nmatrices. To address these limitations, we propose $\\tt A^\\tt 3$, a\npost-training low-rank approximation framework. $\\tt A^\\tt 3$ splits a\nTransformer layer into three functional components, namely $\\tt QK$, $\\tt OV$,\nand $\\tt MLP$. For each component, $\\tt A^\\tt 3$ provides an analytical\nsolution that reduces the hidden dimension size inside each component while\nminimizing the component's functional loss ($\\it i.e.$, error in attention\nscores, attention outputs, and MLP outputs). This approach directly reduces\nmodel sizes, KV cache sizes, and FLOPs without introducing any runtime\noverheads. In addition, it provides a new narrative in advancing the\noptimization problem from singular linear layer loss optimization toward\nimproved end-to-end performance. Through extensive experiments, we show that\n$\\tt A^\\tt 3$ maintains superior performance compared to SoTAs. For example,\nunder the same reduction budget in computation and memory, our low-rank\napproximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2,\noutperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the\nversatility of $\\tt A^\\tt 3$, including KV cache compression, quantization, and\nmixed-rank assignments for enhanced performance."
                },
                "authors": [
                    {
                        "name": "Jeffrey T. H. Wong"
                    },
                    {
                        "name": "Cheng Zhang"
                    },
                    {
                        "name": "Xinye Cao"
                    },
                    {
                        "name": "Pedro Gimenes"
                    },
                    {
                        "name": "George A. Constantinides"
                    },
                    {
                        "name": "Wayne Luk"
                    },
                    {
                        "name": "Yiren Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yiren Zhao"
                },
                "author": "Yiren Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12942v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12942v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09928v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09928v2",
                "updated": "2025-05-19T10:13:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    10,
                    13,
                    31,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-15T03:25:41Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    3,
                    25,
                    41,
                    3,
                    135,
                    0
                ],
                "title": "DeFeed: Secure Decentralized Cross-Contract Data Feed in Web 3.0 for\n  Connected Autonomous Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeFeed: Secure Decentralized Cross-Contract Data Feed in Web 3.0 for\n  Connected Autonomous Vehicles"
                },
                "summary": "Smart contracts have been a topic of interest in blockchain research and are\na key enabling technology for Connected Autonomous Vehicles (CAVs) in the era\nof Web 3.0. These contracts enable trustless interactions without the need for\nintermediaries, as they operate based on predefined rules encoded on the\nblockchain. However, smart contacts face significant challenges in\ncross-contract communication and information sharing, making it difficult to\nestablish seamless connectivity and collaboration among CAVs with Web 3.0. In\nthis paper, we propose DeFeed, a novel secure protocol that incorporates\nvarious gas-saving functions for CAVs, originated from in-depth research into\nthe interaction among smart contracts for decentralized cross-contract data\nfeed in Web 3.0. DeFeed allows smart contracts to obtain information from other\ncontracts efficiently in a single click, without complicated operations. We\njudiciously design and complete various functions with DeFeed, including a pool\nfunction and a cache function for gas optimization, a subscribe function for\nfacilitating data access, and an update function for the future iteration of\nour protocol. Tailored for CAVs with Web 3.0 use cases, DeFeed enables\nefficient data feed between smart contracts underpinning decentralized\napplications and vehicle coordination. Implemented and tested on the Ethereum\nofficial test network, DeFeed demonstrates significant improvements in contract\ninteraction efficiency, reducing computational complexity and gas costs. Our\nsolution represents a critical step towards seamless, decentralized\ncommunication in Web 3.0 ecosystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart contracts have been a topic of interest in blockchain research and are\na key enabling technology for Connected Autonomous Vehicles (CAVs) in the era\nof Web 3.0. These contracts enable trustless interactions without the need for\nintermediaries, as they operate based on predefined rules encoded on the\nblockchain. However, smart contacts face significant challenges in\ncross-contract communication and information sharing, making it difficult to\nestablish seamless connectivity and collaboration among CAVs with Web 3.0. In\nthis paper, we propose DeFeed, a novel secure protocol that incorporates\nvarious gas-saving functions for CAVs, originated from in-depth research into\nthe interaction among smart contracts for decentralized cross-contract data\nfeed in Web 3.0. DeFeed allows smart contracts to obtain information from other\ncontracts efficiently in a single click, without complicated operations. We\njudiciously design and complete various functions with DeFeed, including a pool\nfunction and a cache function for gas optimization, a subscribe function for\nfacilitating data access, and an update function for the future iteration of\nour protocol. Tailored for CAVs with Web 3.0 use cases, DeFeed enables\nefficient data feed between smart contracts underpinning decentralized\napplications and vehicle coordination. Implemented and tested on the Ethereum\nofficial test network, DeFeed demonstrates significant improvements in contract\ninteraction efficiency, reducing computational complexity and gas costs. Our\nsolution represents a critical step towards seamless, decentralized\ncommunication in Web 3.0 ecosystems."
                },
                "authors": [
                    {
                        "name": "Xingchen Sun"
                    },
                    {
                        "name": "Runhua Xu"
                    },
                    {
                        "name": "Wei Ni"
                    },
                    {
                        "name": "Li Duan"
                    },
                    {
                        "name": "Chao Li"
                    }
                ],
                "author_detail": {
                    "name": "Chao Li"
                },
                "author": "Chao Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09928v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09928v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12742v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12742v1",
                "updated": "2025-05-19T05:56:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    5,
                    56,
                    44,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T05:56:44Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    5,
                    56,
                    44,
                    0,
                    139,
                    0
                ],
                "title": "MVAR: Visual Autoregressive Modeling with Scale and Spatial Markovian\n  Conditioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MVAR: Visual Autoregressive Modeling with Scale and Spatial Markovian\n  Conditioning"
                },
                "summary": "Essential to visual generation is efficient modeling of visual data priors.\nConventional next-token prediction methods define the process as learning the\nconditional probability distribution of successive tokens. Recently, next-scale\nprediction methods redefine the process to learn the distribution over\nmulti-scale representations, significantly reducing generation latency.\nHowever, these methods condition each scale on all previous scales and require\neach token to consider all preceding tokens, exhibiting scale and spatial\nredundancy. To better model the distribution by mitigating redundancy, we\npropose Markovian Visual AutoRegressive modeling (MVAR), a novel autoregressive\nframework that introduces scale and spatial Markov assumptions to reduce the\ncomplexity of conditional probability modeling. Specifically, we introduce a\nscale-Markov trajectory that only takes as input the features of adjacent\npreceding scale for next-scale prediction, enabling the adoption of a parallel\ntraining strategy that significantly reduces GPU memory consumption.\nFurthermore, we propose spatial-Markov attention, which restricts the attention\nof each token to a localized neighborhood of size k at corresponding positions\non adjacent scales, rather than attending to every token across these scales,\nfor the pursuit of reduced modeling complexity. Building on these improvements,\nwe reduce the computational complexity of attention calculation from O(N^2) to\nO(Nk), enabling training with just eight NVIDIA RTX 4090 GPUs and eliminating\nthe need for KV cache during inference. Extensive experiments on ImageNet\ndemonstrate that MVAR achieves comparable or superior performance with both\nsmall model trained from scratch and large fine-tuned models, while reducing\nthe average GPU memory footprint by 3.0x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Essential to visual generation is efficient modeling of visual data priors.\nConventional next-token prediction methods define the process as learning the\nconditional probability distribution of successive tokens. Recently, next-scale\nprediction methods redefine the process to learn the distribution over\nmulti-scale representations, significantly reducing generation latency.\nHowever, these methods condition each scale on all previous scales and require\neach token to consider all preceding tokens, exhibiting scale and spatial\nredundancy. To better model the distribution by mitigating redundancy, we\npropose Markovian Visual AutoRegressive modeling (MVAR), a novel autoregressive\nframework that introduces scale and spatial Markov assumptions to reduce the\ncomplexity of conditional probability modeling. Specifically, we introduce a\nscale-Markov trajectory that only takes as input the features of adjacent\npreceding scale for next-scale prediction, enabling the adoption of a parallel\ntraining strategy that significantly reduces GPU memory consumption.\nFurthermore, we propose spatial-Markov attention, which restricts the attention\nof each token to a localized neighborhood of size k at corresponding positions\non adjacent scales, rather than attending to every token across these scales,\nfor the pursuit of reduced modeling complexity. Building on these improvements,\nwe reduce the computational complexity of attention calculation from O(N^2) to\nO(Nk), enabling training with just eight NVIDIA RTX 4090 GPUs and eliminating\nthe need for KV cache during inference. Extensive experiments on ImageNet\ndemonstrate that MVAR achieves comparable or superior performance with both\nsmall model trained from scratch and large fine-tuned models, while reducing\nthe average GPU memory footprint by 3.0x."
                },
                "authors": [
                    {
                        "name": "Jinhua Zhang"
                    },
                    {
                        "name": "Wei Long"
                    },
                    {
                        "name": "Minghao Han"
                    },
                    {
                        "name": "Weiyi You"
                    },
                    {
                        "name": "Shuhang Gu"
                    }
                ],
                "author_detail": {
                    "name": "Shuhang Gu"
                },
                "author": "Shuhang Gu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12742v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12742v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12731v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12731v1",
                "updated": "2025-05-19T05:39:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    5,
                    39,
                    38,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T05:39:38Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    5,
                    39,
                    38,
                    0,
                    139,
                    0
                ],
                "title": "Accelerating Adaptive Retrieval Augmented Generation via\n  Instruction-Driven Representation Reduction of Retrieval Overlaps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Adaptive Retrieval Augmented Generation via\n  Instruction-Driven Representation Reduction of Retrieval Overlaps"
                },
                "summary": "Retrieval-augmented generation (RAG) has emerged as a pivotal method for\nexpanding the knowledge of large language models. To handle complex queries\nmore effectively, researchers developed Adaptive-RAG (A-RAG) to enhance the\ngenerated quality through multiple interactions with external knowledge bases.\nDespite its effectiveness, A-RAG exacerbates the pre-existing efficiency\nchallenges inherent in RAG, which are attributable to its reliance on multiple\niterations of generation. Existing A-RAG approaches process all retrieved\ncontents from scratch. However, they ignore the situation where there is a\nsignificant overlap in the content of the retrieval results across rounds. The\noverlapping content is redundantly represented, which leads to a large\nproportion of repeated computations, thus affecting the overall efficiency. To\naddress this issue, this paper introduces a model-agnostic approach that can be\ngenerally applied to A-RAG methods, which is dedicated to reducing the\nredundant representation process caused by the overlapping of retrieval\nresults. Specifically, we use cache access and parallel generation to speed up\nthe prefilling and decoding stages respectively. Additionally, we also propose\nan instruction-driven module to further guide the model to more effectively\nattend to each part of the content in a more suitable way for LLMs. Experiments\nshow that our approach achieves 2.79 and 2.33 times significant acceleration on\naverage for prefilling and decoding respectively while maintaining equal\ngeneration quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has emerged as a pivotal method for\nexpanding the knowledge of large language models. To handle complex queries\nmore effectively, researchers developed Adaptive-RAG (A-RAG) to enhance the\ngenerated quality through multiple interactions with external knowledge bases.\nDespite its effectiveness, A-RAG exacerbates the pre-existing efficiency\nchallenges inherent in RAG, which are attributable to its reliance on multiple\niterations of generation. Existing A-RAG approaches process all retrieved\ncontents from scratch. However, they ignore the situation where there is a\nsignificant overlap in the content of the retrieval results across rounds. The\noverlapping content is redundantly represented, which leads to a large\nproportion of repeated computations, thus affecting the overall efficiency. To\naddress this issue, this paper introduces a model-agnostic approach that can be\ngenerally applied to A-RAG methods, which is dedicated to reducing the\nredundant representation process caused by the overlapping of retrieval\nresults. Specifically, we use cache access and parallel generation to speed up\nthe prefilling and decoding stages respectively. Additionally, we also propose\nan instruction-driven module to further guide the model to more effectively\nattend to each part of the content in a more suitable way for LLMs. Experiments\nshow that our approach achieves 2.79 and 2.33 times significant acceleration on\naverage for prefilling and decoding respectively while maintaining equal\ngeneration quality."
                },
                "authors": [
                    {
                        "name": "Jie Ou"
                    },
                    {
                        "name": "Jinyu Guo"
                    },
                    {
                        "name": "Shuaihong Jiang"
                    },
                    {
                        "name": "Zhaokun Wang"
                    },
                    {
                        "name": "Libo Qin"
                    },
                    {
                        "name": "Shunyu Yao"
                    },
                    {
                        "name": "Wenhong Tian"
                    }
                ],
                "author_detail": {
                    "name": "Wenhong Tian"
                },
                "author": "Wenhong Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12731v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12731v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00570v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00570v2",
                "updated": "2025-05-19T02:21:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    2,
                    21,
                    16,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-01T14:53:12Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    14,
                    53,
                    12,
                    3,
                    121,
                    0
                ],
                "title": "FreqKV: Frequency Domain Key-Value Compression for Efficient Context\n  Window Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreqKV: Frequency Domain Key-Value Compression for Efficient Context\n  Window Extension"
                },
                "summary": "Frequency-domain compression has proven effective in reducing redundancies\nfor spatial signals. In this work, we propose FreqKV, a novel frequency domain\nkey-value (KV) compression technique that enables efficient context window\nextension for decoder-only large language models (LLMs). Our approach is\nmotivated by a key observation that, in the frequency domain, the energy\ndistribution of the KV cache is predominantly concentrated in low-frequency\ncomponents. By discarding high-frequency components, we achieve efficient\ncompression of the KV cache with minimal information loss. FreqKV iteratively\ncompresses the increasing KV cache to a fixed size in the frequency domain,\nallowing models to process lengthy contexts efficiently. Introducing no\nadditional parameters or architectural modifications, FreqKV is applicable to\nboth fine-tuning and inference. With minimal fine-tuning, LLMs can learn to\nleverage the limited cache that is compressed in the frequency domain and\nextend the context window. Experiments on a range of long context language\nmodeling and understanding tasks demonstrate the efficiency and effectiveness\nof the proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Frequency-domain compression has proven effective in reducing redundancies\nfor spatial signals. In this work, we propose FreqKV, a novel frequency domain\nkey-value (KV) compression technique that enables efficient context window\nextension for decoder-only large language models (LLMs). Our approach is\nmotivated by a key observation that, in the frequency domain, the energy\ndistribution of the KV cache is predominantly concentrated in low-frequency\ncomponents. By discarding high-frequency components, we achieve efficient\ncompression of the KV cache with minimal information loss. FreqKV iteratively\ncompresses the increasing KV cache to a fixed size in the frequency domain,\nallowing models to process lengthy contexts efficiently. Introducing no\nadditional parameters or architectural modifications, FreqKV is applicable to\nboth fine-tuning and inference. With minimal fine-tuning, LLMs can learn to\nleverage the limited cache that is compressed in the frequency domain and\nextend the context window. Experiments on a range of long context language\nmodeling and understanding tasks demonstrate the efficiency and effectiveness\nof the proposed method."
                },
                "authors": [
                    {
                        "name": "Jushi Kai"
                    },
                    {
                        "name": "Boyi Zeng"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Zhouhan Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zhouhan Lin"
                },
                "author": "Zhouhan Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00570v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00570v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12594v1",
                "updated": "2025-05-19T01:14:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    1,
                    14,
                    57,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T01:14:57Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    1,
                    14,
                    57,
                    0,
                    139,
                    0
                ],
                "title": "AD-AGENT: A Multi-agent Framework for End-to-end Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AD-AGENT: A Multi-agent Framework for End-to-end Anomaly Detection"
                },
                "summary": "Anomaly detection (AD) is essential in areas such as fraud detection, network\nmonitoring, and scientific research. However, the diversity of data modalities\nand the increasing number of specialized AD libraries pose challenges for\nnon-expert users who lack in-depth library-specific knowledge and advanced\nprogramming skills. To tackle this, we present AD-AGENT, an LLM-driven\nmulti-agent framework that turns natural-language instructions into fully\nexecutable AD pipelines. AD-AGENT coordinates specialized agents for intent\nparsing, data preparation, library and model selection, documentation mining,\nand iterative code generation and debugging. Using a shared short-term\nworkspace and a long-term cache, the agents integrate popular AD libraries like\nPyOD, PyGOD, and TSLib into a unified workflow. Experiments demonstrate that\nAD-AGENT produces reliable scripts and recommends competitive models across\nlibraries. The system is open-sourced to support further research and practical\napplications in AD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anomaly detection (AD) is essential in areas such as fraud detection, network\nmonitoring, and scientific research. However, the diversity of data modalities\nand the increasing number of specialized AD libraries pose challenges for\nnon-expert users who lack in-depth library-specific knowledge and advanced\nprogramming skills. To tackle this, we present AD-AGENT, an LLM-driven\nmulti-agent framework that turns natural-language instructions into fully\nexecutable AD pipelines. AD-AGENT coordinates specialized agents for intent\nparsing, data preparation, library and model selection, documentation mining,\nand iterative code generation and debugging. Using a shared short-term\nworkspace and a long-term cache, the agents integrate popular AD libraries like\nPyOD, PyGOD, and TSLib into a unified workflow. Experiments demonstrate that\nAD-AGENT produces reliable scripts and recommends competitive models across\nlibraries. The system is open-sourced to support further research and practical\napplications in AD."
                },
                "authors": [
                    {
                        "name": "Tiankai Yang"
                    },
                    {
                        "name": "Junjun Liu"
                    },
                    {
                        "name": "Wingchun Siu"
                    },
                    {
                        "name": "Jiahang Wang"
                    },
                    {
                        "name": "Zhuangzhuang Qian"
                    },
                    {
                        "name": "Chanjuan Song"
                    },
                    {
                        "name": "Cheng Cheng"
                    },
                    {
                        "name": "Xiyang Hu"
                    },
                    {
                        "name": "Yue Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhao"
                },
                "author": "Yue Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12392v1",
                "updated": "2025-05-18T12:37:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    18,
                    12,
                    37,
                    56,
                    6,
                    138,
                    0
                ],
                "published": "2025-05-18T12:37:56Z",
                "published_parsed": [
                    2025,
                    5,
                    18,
                    12,
                    37,
                    56,
                    6,
                    138,
                    0
                ],
                "title": "SLOT: Sample-specific Language Model Optimization at Test-time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLOT: Sample-specific Language Model Optimization at Test-time"
                },
                "summary": "We propose SLOT (Sample-specific Language Model Optimization at Test-time), a\nnovel and parameter-efficient test-time inference approach that enhances a\nlanguage model's ability to more accurately respond to individual prompts.\nExisting Large Language Models (LLMs) often struggle with complex instructions,\nleading to poor performances on those not well represented among general\nsamples. To address this, SLOT conducts few optimization steps at test-time to\nupdate a light-weight sample-specific parameter vector. It is added to the\nfinal hidden layer before the output head, and enables efficient adaptation by\ncaching the last layer features during per-sample optimization. By minimizing\nthe cross-entropy loss on the input prompt only, SLOT helps the model better\naligned with and follow each given instruction. In experiments, we demonstrate\nthat our method outperforms the compared models across multiple benchmarks and\nLLMs. For example, Qwen2.5-7B with SLOT achieves an accuracy gain of 8.6% on\nGSM8K from 57.54% to 66.19%, while DeepSeek-R1-Distill-Llama-70B with SLOT\nachieves a SOTA accuracy of 68.69% on GPQA among 70B-level models. Our code is\navailable at https://github.com/maple-research-lab/SLOT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose SLOT (Sample-specific Language Model Optimization at Test-time), a\nnovel and parameter-efficient test-time inference approach that enhances a\nlanguage model's ability to more accurately respond to individual prompts.\nExisting Large Language Models (LLMs) often struggle with complex instructions,\nleading to poor performances on those not well represented among general\nsamples. To address this, SLOT conducts few optimization steps at test-time to\nupdate a light-weight sample-specific parameter vector. It is added to the\nfinal hidden layer before the output head, and enables efficient adaptation by\ncaching the last layer features during per-sample optimization. By minimizing\nthe cross-entropy loss on the input prompt only, SLOT helps the model better\naligned with and follow each given instruction. In experiments, we demonstrate\nthat our method outperforms the compared models across multiple benchmarks and\nLLMs. For example, Qwen2.5-7B with SLOT achieves an accuracy gain of 8.6% on\nGSM8K from 57.54% to 66.19%, while DeepSeek-R1-Distill-Llama-70B with SLOT\nachieves a SOTA accuracy of 68.69% on GPQA among 70B-level models. Our code is\navailable at https://github.com/maple-research-lab/SLOT."
                },
                "authors": [
                    {
                        "name": "Yang Hu"
                    },
                    {
                        "name": "Xingyu Zhang"
                    },
                    {
                        "name": "Xueji Fang"
                    },
                    {
                        "name": "Zhiyang Chen"
                    },
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Huatian Zhang"
                    },
                    {
                        "name": "Guojun Qi"
                    }
                ],
                "author_detail": {
                    "name": "Guojun Qi"
                },
                "author": "Guojun Qi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18394v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18394v7",
                "updated": "2025-05-18T03:12:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    18,
                    3,
                    12,
                    25,
                    6,
                    138,
                    0
                ],
                "published": "2025-02-25T17:43:43Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    17,
                    43,
                    43,
                    1,
                    56,
                    0
                ],
                "title": "SPECTRE: An FFT-Based Efficient Drop-In Replacement to Self-Attention\n  for Long Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPECTRE: An FFT-Based Efficient Drop-In Replacement to Self-Attention\n  for Long Contexts"
                },
                "summary": "Long-context transformers face significant efficiency challenges due to the\nquadratic cost of self-attention. However, many modern applications-from\nmulti-turn dialogue to high-resolution vision-require contexts spanning tens of\nthousands of tokens. We introduce SPECTRE, a method that replaces each\nattention head with a fast real FFT, a content-adaptive spectral gate, and an\ninverse FFT, reducing per-layer complexity from $\\mathcal{O}(L^{2})$ to\n$O(L\\log L)$ while preserving the surrounding architecture. We extend this\nefficiency to autoregressive generation through our Prefix-FFT cache and\nenhance local feature representation with an optional wavelet module that adds\nnegligible computational overhead. Our experiments demonstrate that SPECTRE\noperates up to 7$\\times$ faster than FlashAttention-2 on 128k-token contexts\nwhile matching or exceeding baseline performance on PG-19 language modeling and\nImageNet-1k classification tasks. SPECTRE achieves these improvements by adding\nfewer than 6\\% parameters to the base model, making hundred-kilotoken context\nprocessing feasible on commodity GPUs without specialized hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context transformers face significant efficiency challenges due to the\nquadratic cost of self-attention. However, many modern applications-from\nmulti-turn dialogue to high-resolution vision-require contexts spanning tens of\nthousands of tokens. We introduce SPECTRE, a method that replaces each\nattention head with a fast real FFT, a content-adaptive spectral gate, and an\ninverse FFT, reducing per-layer complexity from $\\mathcal{O}(L^{2})$ to\n$O(L\\log L)$ while preserving the surrounding architecture. We extend this\nefficiency to autoregressive generation through our Prefix-FFT cache and\nenhance local feature representation with an optional wavelet module that adds\nnegligible computational overhead. Our experiments demonstrate that SPECTRE\noperates up to 7$\\times$ faster than FlashAttention-2 on 128k-token contexts\nwhile matching or exceeding baseline performance on PG-19 language modeling and\nImageNet-1k classification tasks. SPECTRE achieves these improvements by adding\nfewer than 6\\% parameters to the base model, making hundred-kilotoken context\nprocessing feasible on commodity GPUs without specialized hardware."
                },
                "authors": [
                    {
                        "name": "Jacob Fein-Ashley"
                    },
                    {
                        "name": "Neelesh Gupta"
                    },
                    {
                        "name": "Rajgopal Kannan"
                    },
                    {
                        "name": "Viktor Prasanna"
                    }
                ],
                "author_detail": {
                    "name": "Viktor Prasanna"
                },
                "author": "Viktor Prasanna",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18394v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18394v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02930v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02930v3",
                "updated": "2025-05-17T23:26:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    17,
                    23,
                    26,
                    8,
                    5,
                    137,
                    0
                ],
                "published": "2024-07-03T09:02:05Z",
                "published_parsed": [
                    2024,
                    7,
                    3,
                    9,
                    2,
                    5,
                    2,
                    185,
                    0
                ],
                "title": "Timely Requesting for Time-Critical Content Users in Decentralized\n  F-RANs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timely Requesting for Time-Critical Content Users in Decentralized\n  F-RANs"
                },
                "summary": "With the rising demand for high-rate and timely communications, fog radio\naccess networks (F-RANs) offer a promising solution. This work investigates age\nof information (AoI) performance in F-RANs, consisting of multiple content\nusers (CUs), enhanced remote radio heads (eRRHs), and content providers (CPs).\nTime-critical CUs need rapid content updates from CPs but cannot communicate\ndirectly with them; instead, eRRHs act as intermediaries. CUs decide whether to\nrequest content from a CP and which eRRH to send the request to, while eRRHs\ndecide whether to command CPs to update content or use cached content. We study\ntwo general classes of policies: (i) oblivious policies, where decision-making\nis independent of historical information, and (ii) non-oblivious policies,\nwhere decisions are influenced by historical information. First, we obtain\nclosed-form expressions for the average AoI of eRRHs under both policy types.\nDue to the complexity of calculating closed-form expressions for CUs, we then\nderive general upper bounds for their average AoI. Next, we identify optimal\npolicies for both types. Under both optimal policies, each CU requests content\nfrom each CP at an equal rate, consolidating all requests to a single eRRH when\ndemand is low or resources are limited, and distributing requests evenly among\neRRHs when demand is high and resources are ample. eRRHs command content from\neach CP at an equal rate under an optimal oblivious policy, while prioritize\nthe CP with the highest age under an optimal non-oblivious policy. Our\nnumerical results validate these theoretical findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rising demand for high-rate and timely communications, fog radio\naccess networks (F-RANs) offer a promising solution. This work investigates age\nof information (AoI) performance in F-RANs, consisting of multiple content\nusers (CUs), enhanced remote radio heads (eRRHs), and content providers (CPs).\nTime-critical CUs need rapid content updates from CPs but cannot communicate\ndirectly with them; instead, eRRHs act as intermediaries. CUs decide whether to\nrequest content from a CP and which eRRH to send the request to, while eRRHs\ndecide whether to command CPs to update content or use cached content. We study\ntwo general classes of policies: (i) oblivious policies, where decision-making\nis independent of historical information, and (ii) non-oblivious policies,\nwhere decisions are influenced by historical information. First, we obtain\nclosed-form expressions for the average AoI of eRRHs under both policy types.\nDue to the complexity of calculating closed-form expressions for CUs, we then\nderive general upper bounds for their average AoI. Next, we identify optimal\npolicies for both types. Under both optimal policies, each CU requests content\nfrom each CP at an equal rate, consolidating all requests to a single eRRH when\ndemand is low or resources are limited, and distributing requests evenly among\neRRHs when demand is high and resources are ample. eRRHs command content from\neach CP at an equal rate under an optimal oblivious policy, while prioritize\nthe CP with the highest age under an optimal non-oblivious policy. Our\nnumerical results validate these theoretical findings."
                },
                "authors": [
                    {
                        "name": "Xingran Chen"
                    },
                    {
                        "name": "Kai Li"
                    },
                    {
                        "name": "Kun Yang"
                    }
                ],
                "author_detail": {
                    "name": "Kun Yang"
                },
                "author": "Kun Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02930v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02930v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09573v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09573v3",
                "updated": "2025-05-17T21:15:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    17,
                    21,
                    15,
                    2,
                    5,
                    137,
                    0
                ],
                "published": "2025-03-12T17:43:40Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    43,
                    40,
                    2,
                    71,
                    0
                ],
                "title": "Block Diffusion: Interpolating Between Autoregressive and Diffusion\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block Diffusion: Interpolating Between Autoregressive and Diffusion\n  Language Models"
                },
                "summary": "Diffusion language models offer unique benefits over autoregressive models\ndue to their potential for parallelized generation and controllability, yet\nthey lag in likelihood modeling and are limited to fixed-length generation. In\nthis work, we introduce a class of block diffusion language models that\ninterpolate between discrete denoising diffusion and autoregressive models.\nBlock diffusion overcomes key limitations of both approaches by supporting\nflexible-length generation and improving inference efficiency with KV caching\nand parallel token sampling. We propose a recipe for building effective block\ndiffusion models that includes an efficient training algorithm, estimators of\ngradient variance, and data-driven noise schedules to minimize the variance.\nBlock diffusion sets a new state-of-the-art performance among diffusion models\non language modeling benchmarks and enables generation of arbitrary-length\nsequences. We provide the code, along with the model weights and blog post on\nthe project page: https://m-arriola.com/bd3lms",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models offer unique benefits over autoregressive models\ndue to their potential for parallelized generation and controllability, yet\nthey lag in likelihood modeling and are limited to fixed-length generation. In\nthis work, we introduce a class of block diffusion language models that\ninterpolate between discrete denoising diffusion and autoregressive models.\nBlock diffusion overcomes key limitations of both approaches by supporting\nflexible-length generation and improving inference efficiency with KV caching\nand parallel token sampling. We propose a recipe for building effective block\ndiffusion models that includes an efficient training algorithm, estimators of\ngradient variance, and data-driven noise schedules to minimize the variance.\nBlock diffusion sets a new state-of-the-art performance among diffusion models\non language modeling benchmarks and enables generation of arbitrary-length\nsequences. We provide the code, along with the model weights and blog post on\nthe project page: https://m-arriola.com/bd3lms"
                },
                "authors": [
                    {
                        "name": "Marianne Arriola"
                    },
                    {
                        "name": "Aaron Gokaslan"
                    },
                    {
                        "name": "Justin T. Chiu"
                    },
                    {
                        "name": "Zhihan Yang"
                    },
                    {
                        "name": "Zhixuan Qi"
                    },
                    {
                        "name": "Jiaqi Han"
                    },
                    {
                        "name": "Subham Sekhar Sahoo"
                    },
                    {
                        "name": "Volodymyr Kuleshov"
                    }
                ],
                "author_detail": {
                    "name": "Volodymyr Kuleshov"
                },
                "author": "Volodymyr Kuleshov",
                "arxiv_comment": "ICLR 2025 Oral. We provide the code at\n  https://github.com/kuleshov-group/bd3lms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09573v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09573v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15804v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15804v2",
                "updated": "2025-05-17T12:22:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    17,
                    12,
                    22,
                    59,
                    5,
                    137,
                    0
                ],
                "published": "2025-02-19T06:14:27Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    6,
                    14,
                    27,
                    2,
                    50,
                    0
                ],
                "title": "FairKV: Balancing Per-Head KV Cache for Fast Multi-GPU Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FairKV: Balancing Per-Head KV Cache for Fast Multi-GPU Inference"
                },
                "summary": "KV cache techniques in Transformer models aim to reduce redundant\ncomputations at the expense of substantially increased memory usage, making KV\ncache compression an important and popular research topic. Recently,\nstate-of-the-art KV cache compression methods implement imbalanced, per-head\nallocation algorithms that dynamically adjust the KV cache budget for each\nattention head, achieving excellent performance in single-GPU scenarios.\nHowever, we observe that such imbalanced compression leads to significant load\nimbalance when deploying multi-GPU inference, as some GPUs become overburdened\nwhile others remain underutilized. In this paper, we propose FairKV, a method\ndesigned to ensure fair memory usage among attention heads in systems employing\nimbalanced KV cache compression. The core technique of FairKV is Fair-Copying,\nwhich replicates a small subset of memory-intensive attention heads across GPUs\nusing data parallelism to mitigate load imbalance. Our experiments on popular\nmodels, including LLaMA 70b and Mistral 24b model, demonstrate that FairKV\nincreases throughput by 1.66x compared to standard tensor parallelism\ninference. Our code will be released as open source upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache techniques in Transformer models aim to reduce redundant\ncomputations at the expense of substantially increased memory usage, making KV\ncache compression an important and popular research topic. Recently,\nstate-of-the-art KV cache compression methods implement imbalanced, per-head\nallocation algorithms that dynamically adjust the KV cache budget for each\nattention head, achieving excellent performance in single-GPU scenarios.\nHowever, we observe that such imbalanced compression leads to significant load\nimbalance when deploying multi-GPU inference, as some GPUs become overburdened\nwhile others remain underutilized. In this paper, we propose FairKV, a method\ndesigned to ensure fair memory usage among attention heads in systems employing\nimbalanced KV cache compression. The core technique of FairKV is Fair-Copying,\nwhich replicates a small subset of memory-intensive attention heads across GPUs\nusing data parallelism to mitigate load imbalance. Our experiments on popular\nmodels, including LLaMA 70b and Mistral 24b model, demonstrate that FairKV\nincreases throughput by 1.66x compared to standard tensor parallelism\ninference. Our code will be released as open source upon acceptance."
                },
                "authors": [
                    {
                        "name": "Bingzhe Zhao"
                    },
                    {
                        "name": "Ke Cheng"
                    },
                    {
                        "name": "Aomufei Yuan"
                    },
                    {
                        "name": "Yuxuan Tian"
                    },
                    {
                        "name": "Ruiguang Zhong"
                    },
                    {
                        "name": "Chengchen Hu"
                    },
                    {
                        "name": "Tong Yang"
                    },
                    {
                        "name": "Lian Yu"
                    }
                ],
                "author_detail": {
                    "name": "Lian Yu"
                },
                "author": "Lian Yu",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15804v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15804v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11820v1",
                "updated": "2025-05-17T04:06:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    17,
                    4,
                    6,
                    12,
                    5,
                    137,
                    0
                ],
                "published": "2025-05-17T04:06:12Z",
                "published_parsed": [
                    2025,
                    5,
                    17,
                    4,
                    6,
                    12,
                    5,
                    137,
                    0
                ],
                "title": "Chain-of-Model Learning for Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Model Learning for Language Model"
                },
                "summary": "In this paper, we propose a novel learning paradigm, termed Chain-of-Model\n(CoM), which incorporates the causal relationship into the hidden states of\neach layer as a chain style, thereby introducing great scaling efficiency in\nmodel training and inference flexibility in deployment. We introduce the\nconcept of Chain-of-Representation (CoR), which formulates the hidden states at\neach layer as a combination of multiple sub-representations (i.e., chains) at\nthe hidden dimension level. In each layer, each chain from the output\nrepresentations can only view all of its preceding chains in the input\nrepresentations. Consequently, the model built upon CoM framework can\nprogressively scale up the model size by increasing the chains based on the\nprevious models (i.e., chains), and offer multiple sub-models at varying sizes\nfor elastic inference by using different chain numbers. Based on this\nprinciple, we devise Chain-of-Language-Model (CoLM), which incorporates the\nidea of CoM into each layer of Transformer architecture. Based on CoLM, we\nfurther introduce CoLM-Air by introducing a KV sharing mechanism, that computes\nall keys and values within the first chain and then shares across all chains.\nThis design demonstrates additional extensibility, such as enabling seamless LM\nswitching, prefilling acceleration and so on. Experimental results demonstrate\nour CoLM family can achieve comparable performance to the standard Transformer,\nwhile simultaneously enabling greater flexiblity, such as progressive scaling\nto improve training efficiency and offer multiple varying model sizes for\nelastic inference, paving a a new way toward building language models. Our code\nwill be released in the future at: https://github.com/microsoft/CoLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose a novel learning paradigm, termed Chain-of-Model\n(CoM), which incorporates the causal relationship into the hidden states of\neach layer as a chain style, thereby introducing great scaling efficiency in\nmodel training and inference flexibility in deployment. We introduce the\nconcept of Chain-of-Representation (CoR), which formulates the hidden states at\neach layer as a combination of multiple sub-representations (i.e., chains) at\nthe hidden dimension level. In each layer, each chain from the output\nrepresentations can only view all of its preceding chains in the input\nrepresentations. Consequently, the model built upon CoM framework can\nprogressively scale up the model size by increasing the chains based on the\nprevious models (i.e., chains), and offer multiple sub-models at varying sizes\nfor elastic inference by using different chain numbers. Based on this\nprinciple, we devise Chain-of-Language-Model (CoLM), which incorporates the\nidea of CoM into each layer of Transformer architecture. Based on CoLM, we\nfurther introduce CoLM-Air by introducing a KV sharing mechanism, that computes\nall keys and values within the first chain and then shares across all chains.\nThis design demonstrates additional extensibility, such as enabling seamless LM\nswitching, prefilling acceleration and so on. Experimental results demonstrate\nour CoLM family can achieve comparable performance to the standard Transformer,\nwhile simultaneously enabling greater flexiblity, such as progressive scaling\nto improve training efficiency and offer multiple varying model sizes for\nelastic inference, paving a a new way toward building language models. Our code\nwill be released in the future at: https://github.com/microsoft/CoLM."
                },
                "authors": [
                    {
                        "name": "Kaitao Song"
                    },
                    {
                        "name": "Xiaohua Wang"
                    },
                    {
                        "name": "Xu Tan"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Yongliang Shen"
                    },
                    {
                        "name": "Cen LU"
                    },
                    {
                        "name": "Zihao Li"
                    },
                    {
                        "name": "Zifan Song"
                    },
                    {
                        "name": "Caihua Shan"
                    },
                    {
                        "name": "Yansen Wang"
                    },
                    {
                        "name": "Kan Ren"
                    },
                    {
                        "name": "Xiaoqing Zheng"
                    },
                    {
                        "name": "Tao Qin"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11783v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11783v1",
                "updated": "2025-05-17T01:31:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    17,
                    1,
                    31,
                    21,
                    5,
                    137,
                    0
                ],
                "published": "2025-05-17T01:31:21Z",
                "published_parsed": [
                    2025,
                    5,
                    17,
                    1,
                    31,
                    21,
                    5,
                    137,
                    0
                ],
                "title": "Efficient Vector Search on Disaggregated Memory with d-HNSW",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Vector Search on Disaggregated Memory with d-HNSW"
                },
                "summary": "Efficient vector query processing is critical to enable AI applications at\nscale. Recent solutions struggle with growing vector datasets that exceed\nsingle-machine memory capacity, forcing unnecessary data movement and resource\nunderutilization in monolithic architectures. We present d-HNSW, the first\ndisaggregated vector similarity search engine for RDMA-based remote memory\nsystems that achieves high performance while supporting fast data indexing with\nlow network communication overhead. The core of d-HNSW is a novel\ndisaggregation of the graph-based vector indexing data structure HNSW. It\nexploits the characteristics of greedy searching in HNSW to efficiently\ncoordinate data transfers from the memory pool to the compute pool while\nserving data requests. Specifically, it leverages three ideas: (i)\nRepresentative index caching, a lightweight index constructed from a sampled\nsubset of data, is cached in the compute pool to reduce frequent access to\ncritical components of the hierarchical graph-based index, (ii) RDMA-friendly\ndata layout design to reduce the networking round trips incurred by vector\nquery and insertion and (iii) batched query-aware data loading to reduce\nbandwidth usage on data transfer between pools, addressing the limited cache\ncapacity in compute nodes. We evaluate d-HNSW with extensive benchmarking\ndatasets. The experimental results show that d-HNSW outperforms Naive d-HNSW\nimplementation by up to 117x in latency while maintaining recall as 0.87 in\ndataset SIFT1M@1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient vector query processing is critical to enable AI applications at\nscale. Recent solutions struggle with growing vector datasets that exceed\nsingle-machine memory capacity, forcing unnecessary data movement and resource\nunderutilization in monolithic architectures. We present d-HNSW, the first\ndisaggregated vector similarity search engine for RDMA-based remote memory\nsystems that achieves high performance while supporting fast data indexing with\nlow network communication overhead. The core of d-HNSW is a novel\ndisaggregation of the graph-based vector indexing data structure HNSW. It\nexploits the characteristics of greedy searching in HNSW to efficiently\ncoordinate data transfers from the memory pool to the compute pool while\nserving data requests. Specifically, it leverages three ideas: (i)\nRepresentative index caching, a lightweight index constructed from a sampled\nsubset of data, is cached in the compute pool to reduce frequent access to\ncritical components of the hierarchical graph-based index, (ii) RDMA-friendly\ndata layout design to reduce the networking round trips incurred by vector\nquery and insertion and (iii) batched query-aware data loading to reduce\nbandwidth usage on data transfer between pools, addressing the limited cache\ncapacity in compute nodes. We evaluate d-HNSW with extensive benchmarking\ndatasets. The experimental results show that d-HNSW outperforms Naive d-HNSW\nimplementation by up to 117x in latency while maintaining recall as 0.87 in\ndataset SIFT1M@1."
                },
                "authors": [
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Fei Fang"
                    },
                    {
                        "name": "Chen Qian"
                    }
                ],
                "author_detail": {
                    "name": "Chen Qian"
                },
                "author": "Chen Qian",
                "arxiv_comment": "To appear in HotStorage 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11783v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11783v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11695v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11695v1",
                "updated": "2025-05-16T21:04:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    16,
                    21,
                    4,
                    25,
                    4,
                    136,
                    0
                ],
                "published": "2025-05-16T21:04:25Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    21,
                    4,
                    25,
                    4,
                    136,
                    0
                ],
                "title": "Qronos: Correcting the Past by Shaping the Future... in Post-Training\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qronos: Correcting the Past by Shaping the Future... in Post-Training\n  Quantization"
                },
                "summary": "We introduce Qronos -- a new state-of-the-art post-training quantization\nalgorithm that sequentially rounds and updates neural network weights. Qronos\nnot only explicitly corrects errors due to both weight and activation\nquantization, but also errors resulting from quantizing previous layers. Our\niterative algorithm is based on an interpretable and disciplined optimization\nframework that subsumes and surpasses existing data-driven approaches. At each\nstep, Qronos alternates between error correction and diffusion via optimal\nupdate rules. Importantly, we prove that Qronos admits an efficient\nimplementation that uses the Cholesky decomposition for solving least-squares\nproblems. We also demonstrate that Qronos is compatible with existing\ntransformation techniques such as Hadamard-based incoherence processing and\nweight-activation scaling equalization, among others. We evaluate Qronos using\nrecent autoregressive language generation models in the Llama3 family; Qronos\nconsistently outperforms previous state-of-the-art adaptive rounding methods\nwhen quantizing the weights, activations, and/or KV caches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Qronos -- a new state-of-the-art post-training quantization\nalgorithm that sequentially rounds and updates neural network weights. Qronos\nnot only explicitly corrects errors due to both weight and activation\nquantization, but also errors resulting from quantizing previous layers. Our\niterative algorithm is based on an interpretable and disciplined optimization\nframework that subsumes and surpasses existing data-driven approaches. At each\nstep, Qronos alternates between error correction and diffusion via optimal\nupdate rules. Importantly, we prove that Qronos admits an efficient\nimplementation that uses the Cholesky decomposition for solving least-squares\nproblems. We also demonstrate that Qronos is compatible with existing\ntransformation techniques such as Hadamard-based incoherence processing and\nweight-activation scaling equalization, among others. We evaluate Qronos using\nrecent autoregressive language generation models in the Llama3 family; Qronos\nconsistently outperforms previous state-of-the-art adaptive rounding methods\nwhen quantizing the weights, activations, and/or KV caches."
                },
                "authors": [
                    {
                        "name": "Shihao Zhang"
                    },
                    {
                        "name": "Haoyu Zhang"
                    },
                    {
                        "name": "Ian Colbert"
                    },
                    {
                        "name": "Rayan Saab"
                    }
                ],
                "author_detail": {
                    "name": "Rayan Saab"
                },
                "author": "Rayan Saab",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11695v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11695v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11625v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11625v1",
                "updated": "2025-05-16T18:41:33Z",
                "updated_parsed": [
                    2025,
                    5,
                    16,
                    18,
                    41,
                    33,
                    4,
                    136,
                    0
                ],
                "published": "2025-05-16T18:41:33Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    18,
                    41,
                    33,
                    4,
                    136,
                    0
                ],
                "title": "Nearest Neighbor Multivariate Time Series Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nearest Neighbor Multivariate Time Series Forecasting"
                },
                "summary": "Multivariate time series (MTS) forecasting has a wide range of applications\nin both industry and academia. Recently, spatial-temporal graph neural networks\n(STGNNs) have gained popularity as MTS forecasting methods. However, current\nSTGNNs can only use the finite length of MTS input data due to the\ncomputational complexity. Moreover, they lack the ability to identify similar\npatterns throughout the entire dataset and struggle with data that exhibit\nsparsely and discontinuously distributed correlations among variables over an\nextensive historical period, resulting in only marginal improvements. In this\narticle, we introduce a simple yet effective k-nearest neighbor MTS forecasting\n( kNN-MTS) framework, which forecasts with a nearest neighbor retrieval\nmechanism over a large datastore of cached series, using representations from\nthe MTS model for similarity search. This approach requires no additional\ntraining and scales to give the MTS model direct access to the whole dataset at\ntest time, resulting in a highly expressive model that consistently improves\nperformance, and has the ability to extract sparse distributed but similar\npatterns spanning over multivariables from the entire dataset. Furthermore, a\nhybrid spatial-temporal encoder (HSTEncoder) is designed for kNN-MTS which can\ncapture both long-term temporal and short-term spatial-temporal dependencies\nand is shown to provide accurate representation for kNN-MTSfor better\nforecasting. Experimental results on several real-world datasets show a\nsignificant improvement in the forecasting performance of kNN-MTS. The\nquantitative analysis also illustrates the interpretability and efficiency of\nkNN-MTS, showing better application prospects and opening up a new path for\nefficiently using the large dataset in MTS models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multivariate time series (MTS) forecasting has a wide range of applications\nin both industry and academia. Recently, spatial-temporal graph neural networks\n(STGNNs) have gained popularity as MTS forecasting methods. However, current\nSTGNNs can only use the finite length of MTS input data due to the\ncomputational complexity. Moreover, they lack the ability to identify similar\npatterns throughout the entire dataset and struggle with data that exhibit\nsparsely and discontinuously distributed correlations among variables over an\nextensive historical period, resulting in only marginal improvements. In this\narticle, we introduce a simple yet effective k-nearest neighbor MTS forecasting\n( kNN-MTS) framework, which forecasts with a nearest neighbor retrieval\nmechanism over a large datastore of cached series, using representations from\nthe MTS model for similarity search. This approach requires no additional\ntraining and scales to give the MTS model direct access to the whole dataset at\ntest time, resulting in a highly expressive model that consistently improves\nperformance, and has the ability to extract sparse distributed but similar\npatterns spanning over multivariables from the entire dataset. Furthermore, a\nhybrid spatial-temporal encoder (HSTEncoder) is designed for kNN-MTS which can\ncapture both long-term temporal and short-term spatial-temporal dependencies\nand is shown to provide accurate representation for kNN-MTSfor better\nforecasting. Experimental results on several real-world datasets show a\nsignificant improvement in the forecasting performance of kNN-MTS. The\nquantitative analysis also illustrates the interpretability and efficiency of\nkNN-MTS, showing better application prospects and opening up a new path for\nefficiently using the large dataset in MTS models."
                },
                "authors": [
                    {
                        "name": "Huiliang Zhang"
                    },
                    {
                        "name": "Ping Nie"
                    },
                    {
                        "name": "Lijun Sun"
                    },
                    {
                        "name": "Benoit Boulet"
                    }
                ],
                "author_detail": {
                    "name": "Benoit Boulet"
                },
                "author": "Benoit Boulet",
                "arxiv_doi": "10.1109/TNNLS.2024.3490603",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TNNLS.2024.3490603",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.11625v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11625v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Trans. Neural Netw. Learn. Syst., early access, 14 Nov. 2024",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11302v1",
                "updated": "2025-05-16T14:30:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    16,
                    14,
                    30,
                    46,
                    4,
                    136,
                    0
                ],
                "published": "2025-05-16T14:30:46Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    14,
                    30,
                    46,
                    4,
                    136,
                    0
                ],
                "title": "Depth first representations of $k^2$-trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Depth first representations of $k^2$-trees"
                },
                "summary": "The $k^2$-tree is a compact data structure designed to efficiently store\nsparse binary matrices by leveraging both sparsity and clustering of nonzero\nelements. This representation supports efficiently navigational operations and\ncomplex binary operations, such as matrix-matrix multiplication, while\nmaintaining space efficiency. The standard $k^2$-tree follows a level-by-level\nrepresentation, which, while effective, prevents further compression of\nidentical subtrees and it si not cache friendly when accessing individual\nsubtrees. In this work, we introduce some novel depth-first representations of\nthe $k^2$-tree and propose an efficient linear-time algorithm to identify and\ncompress identical subtrees within these structures. Our experimental results\nshow that the use of a depth-first representations is a strategy worth\npursuing: for the adjacency matrix of web graphs exploiting the presence of\nidentical subtrees does improve the compression ratio, and for some matrices\ndepth-first representations turns out to be faster than the standard $k^2$-tree\nin computing the matrix-matrix multiplication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The $k^2$-tree is a compact data structure designed to efficiently store\nsparse binary matrices by leveraging both sparsity and clustering of nonzero\nelements. This representation supports efficiently navigational operations and\ncomplex binary operations, such as matrix-matrix multiplication, while\nmaintaining space efficiency. The standard $k^2$-tree follows a level-by-level\nrepresentation, which, while effective, prevents further compression of\nidentical subtrees and it si not cache friendly when accessing individual\nsubtrees. In this work, we introduce some novel depth-first representations of\nthe $k^2$-tree and propose an efficient linear-time algorithm to identify and\ncompress identical subtrees within these structures. Our experimental results\nshow that the use of a depth-first representations is a strategy worth\npursuing: for the adjacency matrix of web graphs exploiting the presence of\nidentical subtrees does improve the compression ratio, and for some matrices\ndepth-first representations turns out to be faster than the standard $k^2$-tree\nin computing the matrix-matrix multiplication."
                },
                "authors": [
                    {
                        "name": "Gabriel Carmona"
                    },
                    {
                        "name": "Giovanni Manzini"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Manzini"
                },
                "author": "Giovanni Manzini",
                "arxiv_comment": "extended submission for SPIRE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11271v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11271v1",
                "updated": "2025-05-16T14:04:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    16,
                    14,
                    4,
                    31,
                    4,
                    136,
                    0
                ],
                "published": "2025-05-16T14:04:31Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    14,
                    4,
                    31,
                    4,
                    136,
                    0
                ],
                "title": "Semantic Caching of Contextual Summaries for Efficient\n  Question-Answering with Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Caching of Contextual Summaries for Efficient\n  Question-Answering with Language Models"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed across edge and cloud\nplatforms for real-time question-answering and retrieval-augmented generation.\nHowever, processing lengthy contexts in distributed systems incurs high\ncomputational overhead, memory usage, and network bandwidth. This paper\nintroduces a novel semantic caching approach for storing and reusing\nintermediate contextual summaries, enabling efficient information reuse across\nsimilar queries in LLM-based QA workflows. Our method reduces redundant\ncomputations by up to 50-60% while maintaining answer accuracy comparable to\nfull document processing, as demonstrated on NaturalQuestions, TriviaQA, and a\nsynthetic ArXiv dataset. This approach balances computational cost and response\nquality, critical for real-time AI assistants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed across edge and cloud\nplatforms for real-time question-answering and retrieval-augmented generation.\nHowever, processing lengthy contexts in distributed systems incurs high\ncomputational overhead, memory usage, and network bandwidth. This paper\nintroduces a novel semantic caching approach for storing and reusing\nintermediate contextual summaries, enabling efficient information reuse across\nsimilar queries in LLM-based QA workflows. Our method reduces redundant\ncomputations by up to 50-60% while maintaining answer accuracy comparable to\nfull document processing, as demonstrated on NaturalQuestions, TriviaQA, and a\nsynthetic ArXiv dataset. This approach balances computational cost and response\nquality, critical for real-time AI assistants."
                },
                "authors": [
                    {
                        "name": "Camille Couturier"
                    },
                    {
                        "name": "Spyros Mastorakis"
                    },
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    },
                    {
                        "name": "Victor Rühle"
                    }
                ],
                "author_detail": {
                    "name": "Victor Rühle"
                },
                "author": "Victor Rühle",
                "arxiv_comment": "Preprint. Paper accepted at ICCCN 2025, the final version will appear\n  in the proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11271v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11271v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2209.10272v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2209.10272v2",
                "updated": "2025-05-16T13:56:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    16,
                    13,
                    56,
                    7,
                    4,
                    136,
                    0
                ],
                "published": "2022-09-21T11:24:10Z",
                "published_parsed": [
                    2022,
                    9,
                    21,
                    11,
                    24,
                    10,
                    2,
                    264,
                    0
                ],
                "title": "Evaluating Continuous Basic Graph Patterns over Dynamic Link Data Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Continuous Basic Graph Patterns over Dynamic Link Data Graphs"
                },
                "summary": "In this paper, we investigate the problem of evaluating Basic Graph Patterns\n(BGP, for short, a subclass of SPARQL queries) over dynamic Linked Data graphs;\ni.e., Linked Data graphs that are continuously updated. We consider a setting\nwhere the updates are continuously received through a stream of messages and\nsupport both insertions and deletions of triples (updates are straightforwardly\nhandled as a combination of deletions and insertions). In this context, we\npropose a set of in-memory algorithms minimizing the cached data to efficiently\nand continuously answer BGP queries. The queries are typically submitted into a\nsystem and continuously result in the delta answers while the update messages\nare processed.\n  To efficiently and continuously evaluate the submitted query over the\nstreaming data, as well as to minimize the amount of cached data, we propose an\napproach where the submitted query is decomposed into simpler subqueries and\nthe query evaluation is achieved by combining the intermediate answers of the\nsubqueries. Using this approach, the proposed algorithms compute the delta\nanswers of a BGP query in polynomial time and space. Note that for certain\nsubclasses of BGP queries, we show that the evaluation can be achieved in\nconstant or linear time and space. Consolidating all the historical delta\nanswers, the algorithms ensure that the answer to each query is constructed at\nany given time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate the problem of evaluating Basic Graph Patterns\n(BGP, for short, a subclass of SPARQL queries) over dynamic Linked Data graphs;\ni.e., Linked Data graphs that are continuously updated. We consider a setting\nwhere the updates are continuously received through a stream of messages and\nsupport both insertions and deletions of triples (updates are straightforwardly\nhandled as a combination of deletions and insertions). In this context, we\npropose a set of in-memory algorithms minimizing the cached data to efficiently\nand continuously answer BGP queries. The queries are typically submitted into a\nsystem and continuously result in the delta answers while the update messages\nare processed.\n  To efficiently and continuously evaluate the submitted query over the\nstreaming data, as well as to minimize the amount of cached data, we propose an\napproach where the submitted query is decomposed into simpler subqueries and\nthe query evaluation is achieved by combining the intermediate answers of the\nsubqueries. Using this approach, the proposed algorithms compute the delta\nanswers of a BGP query in polynomial time and space. Note that for certain\nsubclasses of BGP queries, we show that the evaluation can be achieved in\nconstant or linear time and space. Consolidating all the historical delta\nanswers, the algorithms ensure that the answer to each query is constructed at\nany given time."
                },
                "authors": [
                    {
                        "name": "Manolis Gergatsoulis"
                    },
                    {
                        "name": "Matthew Damigos"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Damigos"
                },
                "author": "Matthew Damigos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2209.10272v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2209.10272v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16525v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16525v2",
                "updated": "2025-05-16T12:42:48Z",
                "updated_parsed": [
                    2025,
                    5,
                    16,
                    12,
                    42,
                    48,
                    4,
                    136,
                    0
                ],
                "published": "2025-03-17T16:43:35Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    16,
                    43,
                    35,
                    0,
                    76,
                    0
                ],
                "title": "KVShare: An LLM Service System with Efficient and Effective Multi-Tenant\n  KV Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVShare: An LLM Service System with Efficient and Effective Multi-Tenant\n  KV Cache Reuse"
                },
                "summary": "Recent advances in long-text understanding have pushed the context length of\nlarge language models (LLMs) up to one million tokens. It boosts LLMs's\naccuracy and reasoning capacity but causes exorbitant computational costs and\nunsatisfactory Time to First Token (TTFT). KV cache reuse, which reuses the\nexact same KV cache of prefixes and templates or shares similar ones but with\nextra selective recomputation, offers a promising way to tackle this issue.\nHowever, prior studies overlook the cross-request KV reuse and the attention\ndeviations introduced by new tokens during the decoding stage. In this paper,\nwe present a KV cache management module that shares the KV cache across\nrequests under multi-tenant scenarios without sacrificing model accuracy. Our\nsystem, KVShare, enables accurate and efficient LLM serving by 1) a Dual-Stage\nHigh Deviation algorithm (DHD) that conditionally selects a small portion of KV\ncache to be recomputed during both prefill and decode phases, and 2) a\ncache-aware scheduler that prioritizes requests based on their KV cache hit\nrates and orchestrates continuous batching to achieve enhanced system\nefficiency and faster TTFT. Multi-task experiments conducted on models such as\nQwen2.5-7B,Llama3.1-8B and Yi1.5-9B demonstrate that KVShare reduces TTFT by up\nto 9.39x and increases 1.2x of the throughput compared to the full KV\nrecompute. Moreover, KVShare achieves 20.38% boost in terms of accuracy\ncompared to SOTA methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in long-text understanding have pushed the context length of\nlarge language models (LLMs) up to one million tokens. It boosts LLMs's\naccuracy and reasoning capacity but causes exorbitant computational costs and\nunsatisfactory Time to First Token (TTFT). KV cache reuse, which reuses the\nexact same KV cache of prefixes and templates or shares similar ones but with\nextra selective recomputation, offers a promising way to tackle this issue.\nHowever, prior studies overlook the cross-request KV reuse and the attention\ndeviations introduced by new tokens during the decoding stage. In this paper,\nwe present a KV cache management module that shares the KV cache across\nrequests under multi-tenant scenarios without sacrificing model accuracy. Our\nsystem, KVShare, enables accurate and efficient LLM serving by 1) a Dual-Stage\nHigh Deviation algorithm (DHD) that conditionally selects a small portion of KV\ncache to be recomputed during both prefill and decode phases, and 2) a\ncache-aware scheduler that prioritizes requests based on their KV cache hit\nrates and orchestrates continuous batching to achieve enhanced system\nefficiency and faster TTFT. Multi-task experiments conducted on models such as\nQwen2.5-7B,Llama3.1-8B and Yi1.5-9B demonstrate that KVShare reduces TTFT by up\nto 9.39x and increases 1.2x of the throughput compared to the full KV\nrecompute. Moreover, KVShare achieves 20.38% boost in terms of accuracy\ncompared to SOTA methods."
                },
                "authors": [
                    {
                        "name": "Huan Yang"
                    },
                    {
                        "name": "Renji Zhang"
                    },
                    {
                        "name": "Mingzhe Huang"
                    },
                    {
                        "name": "Weijun Wang"
                    },
                    {
                        "name": "Yin Tang"
                    },
                    {
                        "name": "Yuanchun Li"
                    },
                    {
                        "name": "Yunxin Liu"
                    },
                    {
                        "name": "Deyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Deyu Zhang"
                },
                "author": "Deyu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16525v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16525v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04987v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04987v3",
                "updated": "2025-05-16T12:32:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    16,
                    12,
                    32,
                    36,
                    4,
                    136,
                    0
                ],
                "published": "2025-01-09T06:00:27Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    0,
                    27,
                    3,
                    9,
                    0
                ],
                "title": "TreeKV: Smooth Key-Value Cache Compression with Tree Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TreeKV: Smooth Key-Value Cache Compression with Tree Structures"
                },
                "summary": "Efficient key-value (KV) cache compression is critical for scaling\ntransformer-based Large Language Models (LLMs) in long sequences and\nresource-limited settings. Existing methods evict tokens based on their\npositions or importance scores, but position-based strategies can miss crucial\ninformation outside predefined regions, while those relying on global\nimportance scores resulting in strong regional biases, limiting the KV cache's\noverall context retention and potentially impairing the performance of LLMs on\ncomplex tasks. Our wavelet analysis reveals that as tokens approach the end of\nsequence, their contributions to generation gradually increase and tends to\ndiverge more from neighboring tokens, indicating a smooth transition with\nincreasing complexity and variability from distant to nearby context. Motivated\nby this observation, we propose TreeKV, an intuitive, training-free method that\nemploys a tree structure for smooth cache compression. TreeKV maintains a fixed\ncache size, allowing LLMs to deliver high-quality output even in long text\nscenarios. Unlike most compression methods, TreeKV is applicable to both the\ngeneration and prefilling stages. TreeKV consistently surpasses all baseline\nmodels in language modeling tasks on PG19 and OpenWebText2, allowing LLMs\ntrained with short context window to generalize to longer window with a 16x\ncache reduction. On the Longbench benchmark, TreeKV achieves the best\nperformance with only 6\\% of the budget at optimal efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient key-value (KV) cache compression is critical for scaling\ntransformer-based Large Language Models (LLMs) in long sequences and\nresource-limited settings. Existing methods evict tokens based on their\npositions or importance scores, but position-based strategies can miss crucial\ninformation outside predefined regions, while those relying on global\nimportance scores resulting in strong regional biases, limiting the KV cache's\noverall context retention and potentially impairing the performance of LLMs on\ncomplex tasks. Our wavelet analysis reveals that as tokens approach the end of\nsequence, their contributions to generation gradually increase and tends to\ndiverge more from neighboring tokens, indicating a smooth transition with\nincreasing complexity and variability from distant to nearby context. Motivated\nby this observation, we propose TreeKV, an intuitive, training-free method that\nemploys a tree structure for smooth cache compression. TreeKV maintains a fixed\ncache size, allowing LLMs to deliver high-quality output even in long text\nscenarios. Unlike most compression methods, TreeKV is applicable to both the\ngeneration and prefilling stages. TreeKV consistently surpasses all baseline\nmodels in language modeling tasks on PG19 and OpenWebText2, allowing LLMs\ntrained with short context window to generalize to longer window with a 16x\ncache reduction. On the Longbench benchmark, TreeKV achieves the best\nperformance with only 6\\% of the budget at optimal efficiency."
                },
                "authors": [
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Jian Yuan"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Bo Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Jiang"
                },
                "author": "Bo Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04987v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04987v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14731v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14731v2",
                "updated": "2025-05-16T09:40:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    16,
                    9,
                    40,
                    1,
                    4,
                    136,
                    0
                ],
                "published": "2024-10-16T08:34:51Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    8,
                    34,
                    51,
                    2,
                    290,
                    0
                ],
                "title": "MatryoshkaKV: Adaptive KV Compression via Trainable Orthogonal\n  Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MatryoshkaKV: Adaptive KV Compression via Trainable Orthogonal\n  Projection"
                },
                "summary": "KV cache has become a de facto technique for the inference of large language\nmodels (LLMs), where tensors of shape (layer number, head number, sequence\nlength, feature dimension) are introduced to cache historical information for\nself-attention. As the size of the model and data grows, the KV cache can\nquickly become a bottleneck within the system in both storage and memory\ntransfer. To address this, prior studies usually focus on the first three axes\nof the cache tensors for compression. This paper supplements them, focusing on\nthe feature dimension axis, by utilizing low-rank projection matrices to\ntransform the cache features into spaces with reduced dimensions. We begin by\ninvestigating the canonical orthogonal projection method for data compression\nthrough principal component analysis (PCA). We observe the issue with PCA\nprojection where significant performance degradation is observed at low\ncompression rates. To bridge the gap, we propose to directly tune the\northogonal projection matrices with a distillation objective using an elaborate\nMatryoshka training strategy. After training, we adaptively search for the\noptimal compression rates for various layers and heads given varying\ncompression budgets. Compared to previous works, our method can easily embrace\npre-trained LLMs and hold a smooth tradeoff between performance and compression\nrate. We empirically witness the high data efficiency of our training procedure\nand find that our method can sustain over 90% performance with an average KV\ncache compression rate of 60% (and up to 75% in certain extreme scenarios) for\npopular LLMs like LLaMA2-7B-base and Mistral-7B-v0.3-base.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache has become a de facto technique for the inference of large language\nmodels (LLMs), where tensors of shape (layer number, head number, sequence\nlength, feature dimension) are introduced to cache historical information for\nself-attention. As the size of the model and data grows, the KV cache can\nquickly become a bottleneck within the system in both storage and memory\ntransfer. To address this, prior studies usually focus on the first three axes\nof the cache tensors for compression. This paper supplements them, focusing on\nthe feature dimension axis, by utilizing low-rank projection matrices to\ntransform the cache features into spaces with reduced dimensions. We begin by\ninvestigating the canonical orthogonal projection method for data compression\nthrough principal component analysis (PCA). We observe the issue with PCA\nprojection where significant performance degradation is observed at low\ncompression rates. To bridge the gap, we propose to directly tune the\northogonal projection matrices with a distillation objective using an elaborate\nMatryoshka training strategy. After training, we adaptively search for the\noptimal compression rates for various layers and heads given varying\ncompression budgets. Compared to previous works, our method can easily embrace\npre-trained LLMs and hold a smooth tradeoff between performance and compression\nrate. We empirically witness the high data efficiency of our training procedure\nand find that our method can sustain over 90% performance with an average KV\ncache compression rate of 60% (and up to 75% in certain extreme scenarios) for\npopular LLMs like LLaMA2-7B-base and Mistral-7B-v0.3-base."
                },
                "authors": [
                    {
                        "name": "Bokai Lin"
                    },
                    {
                        "name": "Zihao Zeng"
                    },
                    {
                        "name": "Zipeng Xiao"
                    },
                    {
                        "name": "Siqi Kou"
                    },
                    {
                        "name": "Tianqi Hou"
                    },
                    {
                        "name": "Xiaofeng Gao"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Zhijie Deng"
                    }
                ],
                "author_detail": {
                    "name": "Zhijie Deng"
                },
                "author": "Zhijie Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14731v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14731v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10938v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10938v1",
                "updated": "2025-05-16T07:23:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    16,
                    7,
                    23,
                    12,
                    4,
                    136,
                    0
                ],
                "published": "2025-05-16T07:23:12Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    7,
                    23,
                    12,
                    4,
                    136,
                    0
                ],
                "title": "Accurate KV Cache Quantization with Outlier Tokens Tracing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate KV Cache Quantization with Outlier Tokens Tracing"
                },
                "summary": "The impressive capabilities of Large Language Models (LLMs) come at the cost\nof substantial computational resources during deployment. While KV Cache can\nsignificantly reduce recomputation during inference, it also introduces\nadditional memory overhead. KV Cache quantization presents a promising\nsolution, striking a good balance between memory usage and accuracy. Previous\nresearch has shown that the Keys are distributed by channel, while the Values\nare distributed by token. Consequently, the common practice is to apply\nchannel-wise quantization to the Keys and token-wise quantization to the\nValues. However, our further investigation reveals that a small subset of\nunusual tokens exhibit unique characteristics that deviate from this pattern,\nwhich can substantially impact quantization accuracy. To address this, we\ndevelop a simple yet effective method to identify these tokens accurately\nduring the decoding process and exclude them from quantization as outlier\ntokens, significantly improving overall accuracy. Extensive experiments show\nthat our method achieves significant accuracy improvements under 2-bit\nquantization and can deliver a 6.4 times reduction in memory usage and a 2.3\ntimes increase in throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The impressive capabilities of Large Language Models (LLMs) come at the cost\nof substantial computational resources during deployment. While KV Cache can\nsignificantly reduce recomputation during inference, it also introduces\nadditional memory overhead. KV Cache quantization presents a promising\nsolution, striking a good balance between memory usage and accuracy. Previous\nresearch has shown that the Keys are distributed by channel, while the Values\nare distributed by token. Consequently, the common practice is to apply\nchannel-wise quantization to the Keys and token-wise quantization to the\nValues. However, our further investigation reveals that a small subset of\nunusual tokens exhibit unique characteristics that deviate from this pattern,\nwhich can substantially impact quantization accuracy. To address this, we\ndevelop a simple yet effective method to identify these tokens accurately\nduring the decoding process and exclude them from quantization as outlier\ntokens, significantly improving overall accuracy. Extensive experiments show\nthat our method achieves significant accuracy improvements under 2-bit\nquantization and can deliver a 6.4 times reduction in memory usage and a 2.3\ntimes increase in throughput."
                },
                "authors": [
                    {
                        "name": "Yi Su"
                    },
                    {
                        "name": "Yuechi Zhou"
                    },
                    {
                        "name": "Quantong Qiu"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Qingrong Xia"
                    },
                    {
                        "name": "Ping Li"
                    },
                    {
                        "name": "Xinyu Duan"
                    },
                    {
                        "name": "Zhefeng Wang"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "ACL2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10938v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10938v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02882v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02882v3",
                "updated": "2025-05-16T03:34:33Z",
                "updated_parsed": [
                    2025,
                    5,
                    16,
                    3,
                    34,
                    33,
                    4,
                    136,
                    0
                ],
                "published": "2024-04-03T17:33:21Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    17,
                    33,
                    21,
                    2,
                    94,
                    0
                ],
                "title": "Linear Attention Sequence Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear Attention Sequence Parallelism"
                },
                "summary": "Sequence parallelism (SP) serves as a prevalent strategy to handle long\nsequences that exceed the memory limit of a single device. However, for linear\nsequence modeling methods like linear attention, existing SP approaches do not\ntake advantage of their right-product-first feature, resulting in sub-optimal\ncommunication efficiency and usability. In this paper, we introduce Linear\nAttention Sequence Parallelism (LASP), an efficient SP approach designed for\nlinear attention-based transformer models. Specifically, we design an efficient\npoint-to-point ring-style communication mechanism to leverage the right-product\nkernel trick of linear attention, which sharply decreases the communication\noverhead, comparing with existing SP methods. We enhance the computation\nefficiency of LASP by performing kernel fusion and intermediate state caching,\nmaking the implementation of LASP hardware-friendly on GPUs. Furthermore, we\nmeticulously ensure the compatibility of sequence-level LASP with all types of\nbatch-level data parallel methods, which is vital for distributed training on\nlarge clusters with very-long sequences. We also discuss the generalization of\nLASP on other linear sequence modeling methods. Extensive experiments on linear\nattention-based models are conducted with varying sequence lengths from 2K to\n4096K. LASP scales sequence length up to 4096K on 128 GPUs, which is 8$\\times$\nlonger than existing SP methods. Code is available at:\nhttps://github.com/OpenNLPLab/LASP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequence parallelism (SP) serves as a prevalent strategy to handle long\nsequences that exceed the memory limit of a single device. However, for linear\nsequence modeling methods like linear attention, existing SP approaches do not\ntake advantage of their right-product-first feature, resulting in sub-optimal\ncommunication efficiency and usability. In this paper, we introduce Linear\nAttention Sequence Parallelism (LASP), an efficient SP approach designed for\nlinear attention-based transformer models. Specifically, we design an efficient\npoint-to-point ring-style communication mechanism to leverage the right-product\nkernel trick of linear attention, which sharply decreases the communication\noverhead, comparing with existing SP methods. We enhance the computation\nefficiency of LASP by performing kernel fusion and intermediate state caching,\nmaking the implementation of LASP hardware-friendly on GPUs. Furthermore, we\nmeticulously ensure the compatibility of sequence-level LASP with all types of\nbatch-level data parallel methods, which is vital for distributed training on\nlarge clusters with very-long sequences. We also discuss the generalization of\nLASP on other linear sequence modeling methods. Extensive experiments on linear\nattention-based models are conducted with varying sequence lengths from 2K to\n4096K. LASP scales sequence length up to 4096K on 128 GPUs, which is 8$\\times$\nlonger than existing SP methods. Code is available at:\nhttps://github.com/OpenNLPLab/LASP."
                },
                "authors": [
                    {
                        "name": "Weigao Sun"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Xuyang Shen"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Yiran Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Yiran Zhong"
                },
                "author": "Yiran Zhong",
                "arxiv_comment": "Accepted by TMLR, 23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02882v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02882v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10806v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10806v1",
                "updated": "2025-05-16T03:01:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    16,
                    3,
                    1,
                    47,
                    4,
                    136,
                    0
                ],
                "published": "2025-05-16T03:01:47Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    3,
                    1,
                    47,
                    4,
                    136,
                    0
                ],
                "title": "RapidGNN: Communication Efficient Large-Scale Distributed Training of\n  Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RapidGNN: Communication Efficient Large-Scale Distributed Training of\n  Graph Neural Networks"
                },
                "summary": "Graph Neural Networks (GNNs) have achieved state-of-the-art (SOTA)\nperformance in diverse domains. However, training GNNs on large-scale graphs\nposes significant challenges due to high memory demands and significant\ncommunication overhead in distributed settings. Traditional sampling-based\napproaches mitigate computation load to some extent but often fail to address\ncommunication inefficiencies inherent in distributed environments. This paper\npresents RapidGNN that introduces a deterministic sampling strategy to\nprecompute mini-batches. By leveraging the sampling strategy, RapidGNN\naccurately anticipates feature access patterns, enabling optimal cache\nconstruction and timely prefetching of remote features. This reduces the\nfrequency and latency of remote data transfers without compromising the\nstochastic nature of training. Evaluations on Reddit and OGBN-Products datasets\ndemonstrate that RapidGNN achieves significant reductions in training time and\nremote feature fetches, outperforming existing models in both communication\nefficiency and throughput. Our findings highlight RapidGNN's potential for\nscalable, high-performance GNN training across large, real-world graph datasets\nalong with improving energy efficiency. Our model improves end-to-end training\nthroughput by 2.10x on average over SOTA model GraphSAGE-METIS (up to 2.45x in\nsome settings), while cutting remote feature fetches by over 4x. It also\nreduces energy consumption up to 23%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) have achieved state-of-the-art (SOTA)\nperformance in diverse domains. However, training GNNs on large-scale graphs\nposes significant challenges due to high memory demands and significant\ncommunication overhead in distributed settings. Traditional sampling-based\napproaches mitigate computation load to some extent but often fail to address\ncommunication inefficiencies inherent in distributed environments. This paper\npresents RapidGNN that introduces a deterministic sampling strategy to\nprecompute mini-batches. By leveraging the sampling strategy, RapidGNN\naccurately anticipates feature access patterns, enabling optimal cache\nconstruction and timely prefetching of remote features. This reduces the\nfrequency and latency of remote data transfers without compromising the\nstochastic nature of training. Evaluations on Reddit and OGBN-Products datasets\ndemonstrate that RapidGNN achieves significant reductions in training time and\nremote feature fetches, outperforming existing models in both communication\nefficiency and throughput. Our findings highlight RapidGNN's potential for\nscalable, high-performance GNN training across large, real-world graph datasets\nalong with improving energy efficiency. Our model improves end-to-end training\nthroughput by 2.10x on average over SOTA model GraphSAGE-METIS (up to 2.45x in\nsome settings), while cutting remote feature fetches by over 4x. It also\nreduces energy consumption up to 23%."
                },
                "authors": [
                    {
                        "name": "Arefin Niam"
                    },
                    {
                        "name": "M S Q Zulkar Nine"
                    }
                ],
                "author_detail": {
                    "name": "M S Q Zulkar Nine"
                },
                "author": "M S Q Zulkar Nine",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10806v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10806v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17720v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17720v2",
                "updated": "2025-05-16T00:56:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    16,
                    0,
                    56,
                    30,
                    4,
                    136,
                    0
                ],
                "published": "2024-11-20T19:44:26Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    19,
                    44,
                    26,
                    2,
                    325,
                    0
                ],
                "title": "MAS-Attention: Memory-Aware Stream Processing for Attention Acceleration\n  on Resource-Constrained Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAS-Attention: Memory-Aware Stream Processing for Attention Acceleration\n  on Resource-Constrained Edge Devices"
                },
                "summary": "The advent of foundation models have revolutionized various fields, enabling\nunprecedented task accuracy and flexibility in computational linguistics,\ncomputer vision and other domains. Attention mechanism has become an essential\ncomponent of foundation models, due to their superb capability of capturing\ncorrelations in a sequence. However, attention results in quadratic complexity\nin memory and compute as the context length grows. Although many fusion-based\nexact attention acceleration algorithms have been developed for\ndatacenter-grade GPUs and accelerators leveraging multi-core parallelism and\ndata locality, yet it remains a significant challenge to accelerate attention\non resource-constrained edge neural accelerators with limited compute units and\nstringent on-chip caches. In this paper, we propose a scheme for exact\nattention inference acceleration on memory-constrained edge accelerators, by\nparallelizing the utilization of heterogeneous compute units, i.e., vector\nprocessing units and matrix processing units. Our method involves scheduling\nworkloads onto these different compute units in a multi-tiered tiling scheme to\nprocess tiled vector workloads and matrix workloads in attention as two\nstreams, respecting the workload dependencies. We search for tiling factors to\nmaximize the parallelization of both compute units while considering I/O\noverhead, and propose a proactive cache overwrite strategy to avoid undesirable\ncache spills in reality. Extensive results based on open-sourced simulation\nframeworks show up to 2.75x speedup and 54% reduction in energy consumption as\ncompared to the state-of-the-art attention fusion method (FLAT) in the edge\ncomputing scenario. Further experiments on a real-world edge neural processing\nunit demonstrate speedup of up to 1.76x for attention as compared to FLAT,\nwithout affecting model output accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of foundation models have revolutionized various fields, enabling\nunprecedented task accuracy and flexibility in computational linguistics,\ncomputer vision and other domains. Attention mechanism has become an essential\ncomponent of foundation models, due to their superb capability of capturing\ncorrelations in a sequence. However, attention results in quadratic complexity\nin memory and compute as the context length grows. Although many fusion-based\nexact attention acceleration algorithms have been developed for\ndatacenter-grade GPUs and accelerators leveraging multi-core parallelism and\ndata locality, yet it remains a significant challenge to accelerate attention\non resource-constrained edge neural accelerators with limited compute units and\nstringent on-chip caches. In this paper, we propose a scheme for exact\nattention inference acceleration on memory-constrained edge accelerators, by\nparallelizing the utilization of heterogeneous compute units, i.e., vector\nprocessing units and matrix processing units. Our method involves scheduling\nworkloads onto these different compute units in a multi-tiered tiling scheme to\nprocess tiled vector workloads and matrix workloads in attention as two\nstreams, respecting the workload dependencies. We search for tiling factors to\nmaximize the parallelization of both compute units while considering I/O\noverhead, and propose a proactive cache overwrite strategy to avoid undesirable\ncache spills in reality. Extensive results based on open-sourced simulation\nframeworks show up to 2.75x speedup and 54% reduction in energy consumption as\ncompared to the state-of-the-art attention fusion method (FLAT) in the edge\ncomputing scenario. Further experiments on a real-world edge neural processing\nunit demonstrate speedup of up to 1.76x for attention as compared to FLAT,\nwithout affecting model output accuracy."
                },
                "authors": [
                    {
                        "name": "Mohammadali Shakerdargah"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Chao Gao"
                    },
                    {
                        "name": "Di Niu"
                    }
                ],
                "author_detail": {
                    "name": "Di Niu"
                },
                "author": "Di Niu",
                "arxiv_comment": "Accepted to MLSys 2025,",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17720v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17720v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1.4; I.2.7; I.5.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10560v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10560v1",
                "updated": "2025-05-15T17:59:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    17,
                    59,
                    24,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T17:59:24Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    17,
                    59,
                    24,
                    3,
                    135,
                    0
                ],
                "title": "Approximation-First Timeseries Monitoring Query At Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximation-First Timeseries Monitoring Query At Scale"
                },
                "summary": "Timeseries monitoring systems such as Prometheus play a crucial role in\ngaining observability of the underlying system components. These systems\ncollect timeseries metrics from various system components and perform\nmonitoring queries over periodic window-based aggregations (i.e., rule\nqueries). However, despite wide adoption, the operational costs and query\nlatency of rule queries remain high. In this paper, we identify major\nbottlenecks associated with repeated data scans and query computations\nconcerning window overlaps in rule queries, and present PromSketch, an\napproximation-first query framework as intermediate caches for monitoring\nsystems. It enables low operational costs and query latency, by combining\napproximate window-based query frameworks and sketch-based precomputation.\nPromSketch is implemented as a standalone module that can be integrated into\nPrometheus and VictoriaMetrics, covering 70% of Prometheus' aggregation over\ntime queries. Our evaluation shows that PromSketch achieves up to a two orders\nof magnitude reduction in query latency over Prometheus and VictoriaMetrics,\nwhile lowering operational dollar costs of query processing by two orders of\nmagnitude compared to Prometheus and by at least 4x compared to VictoriaMetrics\nwith at most 5% average errors across statistics. The source code has been made\navailable at https://github.com/Froot-NetSys/promsketch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timeseries monitoring systems such as Prometheus play a crucial role in\ngaining observability of the underlying system components. These systems\ncollect timeseries metrics from various system components and perform\nmonitoring queries over periodic window-based aggregations (i.e., rule\nqueries). However, despite wide adoption, the operational costs and query\nlatency of rule queries remain high. In this paper, we identify major\nbottlenecks associated with repeated data scans and query computations\nconcerning window overlaps in rule queries, and present PromSketch, an\napproximation-first query framework as intermediate caches for monitoring\nsystems. It enables low operational costs and query latency, by combining\napproximate window-based query frameworks and sketch-based precomputation.\nPromSketch is implemented as a standalone module that can be integrated into\nPrometheus and VictoriaMetrics, covering 70% of Prometheus' aggregation over\ntime queries. Our evaluation shows that PromSketch achieves up to a two orders\nof magnitude reduction in query latency over Prometheus and VictoriaMetrics,\nwhile lowering operational dollar costs of query processing by two orders of\nmagnitude compared to Prometheus and by at least 4x compared to VictoriaMetrics\nwith at most 5% average errors across statistics. The source code has been made\navailable at https://github.com/Froot-NetSys/promsketch."
                },
                "authors": [
                    {
                        "name": "Zeying Zhu"
                    },
                    {
                        "name": "Jonathan Chamberlain"
                    },
                    {
                        "name": "Kenny Wu"
                    },
                    {
                        "name": "David Starobinski"
                    },
                    {
                        "name": "Zaoxing Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zaoxing Liu"
                },
                "author": "Zaoxing Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10560v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10560v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02069v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02069v4",
                "updated": "2025-05-15T17:18:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    17,
                    18,
                    12,
                    3,
                    135,
                    0
                ],
                "published": "2024-06-04T07:51:30Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    7,
                    51,
                    30,
                    1,
                    156,
                    0
                ],
                "title": "PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information\n  Funneling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information\n  Funneling"
                },
                "summary": "In this study, we investigate whether attention-based information flow inside\nlarge language models (LLMs) is aggregated through noticeable patterns for long\ncontext processing. Our observations reveal that LLMs aggregate information\nthrough Pyramidal Information Funneling where attention is scattering widely in\nlower layers, progressively consolidating within specific contexts, and\nultimately focusing on critical tokens (a.k.a massive activation or attention\nsink) in higher layers. Motivated by these insights, we developed PyramidKV, a\nnovel and effective KV cache compression method. This approach dynamically\nadjusts the KV cache size across different layers, allocating more cache in\nlower layers and less in higher ones, diverging from traditional methods that\nmaintain a uniform KV cache size. Our experimental evaluations, utilizing the\nLongBench benchmark, show that PyramidKV matches the performance of models with\na full KV cache while retaining only 12% of the KV cache, thus significantly\nreducing memory usage. In scenarios emphasizing memory efficiency, where only\n0.7% of the KV cache is maintained, PyramidKV surpasses other KV cache\ncompression techniques, achieving up to a 20.5 absolute accuracy improvement on\nTREC dataset. In the Needle-in-a-Haystack experiment, PyramidKV outperforms\ncompeting methods in maintaining long-context comprehension in LLMs; notably,\nretaining just 128 KV cache entries enables the LLAMA-3-70B model to achieve\n100.0 Acc. performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we investigate whether attention-based information flow inside\nlarge language models (LLMs) is aggregated through noticeable patterns for long\ncontext processing. Our observations reveal that LLMs aggregate information\nthrough Pyramidal Information Funneling where attention is scattering widely in\nlower layers, progressively consolidating within specific contexts, and\nultimately focusing on critical tokens (a.k.a massive activation or attention\nsink) in higher layers. Motivated by these insights, we developed PyramidKV, a\nnovel and effective KV cache compression method. This approach dynamically\nadjusts the KV cache size across different layers, allocating more cache in\nlower layers and less in higher ones, diverging from traditional methods that\nmaintain a uniform KV cache size. Our experimental evaluations, utilizing the\nLongBench benchmark, show that PyramidKV matches the performance of models with\na full KV cache while retaining only 12% of the KV cache, thus significantly\nreducing memory usage. In scenarios emphasizing memory efficiency, where only\n0.7% of the KV cache is maintained, PyramidKV surpasses other KV cache\ncompression techniques, achieving up to a 20.5 absolute accuracy improvement on\nTREC dataset. In the Needle-in-a-Haystack experiment, PyramidKV outperforms\ncompeting methods in maintaining long-context comprehension in LLMs; notably,\nretaining just 128 KV cache entries enables the LLAMA-3-70B model to achieve\n100.0 Acc. performance."
                },
                "authors": [
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Yichi Zhang"
                    },
                    {
                        "name": "Bofei Gao"
                    },
                    {
                        "name": "Yuliang Liu"
                    },
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Keming Lu"
                    },
                    {
                        "name": "Wayne Xiong"
                    },
                    {
                        "name": "Yue Dong"
                    },
                    {
                        "name": "Junjie Hu"
                    },
                    {
                        "name": "Wen Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Xiao"
                },
                "author": "Wen Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02069v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02069v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11554v1",
                "updated": "2025-05-15T16:40:14Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    16,
                    40,
                    14,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T16:40:14Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    16,
                    40,
                    14,
                    3,
                    135,
                    0
                ],
                "title": "Multi-Objective Memory Bandwidth Regulation and Cache Partitioning for\n  Multicore Real-Time Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Objective Memory Bandwidth Regulation and Cache Partitioning for\n  Multicore Real-Time Systems"
                },
                "summary": "Memory bandwidth regulation and cache partitioning are widely used techniques\nfor achieving predictable timing in real-time computing systems. Combined with\npartitioned scheduling, these methods require careful co-allocation of tasks\nand resources to cores, as task execution times strongly depend on available\nallocated resources. To address this challenge, this paper presents a 0-1\nlinear program for task-resource co-allocation, along with a multi-objective\nheuristic designed to minimize resource usage while guaranteeing schedulability\nunder a preemptive EDF scheduling policy. Our heuristic employs a multi-layer\nframework, where an outer layer explores resource allocations using\nPareto-pruned search, and an inner layer optimizes task allocation by solving a\nknapsack problem using dynamic programming. To evaluate the performance of the\nproposed optimization algorithm, we profile real-world benchmarks on an\nembedded AMD UltraScale+ ZCU102 platform, with fine-grained resource\npartitioning enabled by the Jailhouse hypervisor, leveraging cache set\npartitioning and MemGuard for memory bandwidth regulation. Experiments based on\nthe benchmarking results show that the proposed 0-1 linear program outperforms\nexisting mixed-integer programs by finding more optimal solutions within the\nsame time limit. Moreover, the proposed multi-objective multi-layer heuristic\nperforms consistently better than the state-of-the-art multi-resource-task\nco-allocation algorithm in terms of schedulability, resource usage, number of\nnon-dominated solutions, and computational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory bandwidth regulation and cache partitioning are widely used techniques\nfor achieving predictable timing in real-time computing systems. Combined with\npartitioned scheduling, these methods require careful co-allocation of tasks\nand resources to cores, as task execution times strongly depend on available\nallocated resources. To address this challenge, this paper presents a 0-1\nlinear program for task-resource co-allocation, along with a multi-objective\nheuristic designed to minimize resource usage while guaranteeing schedulability\nunder a preemptive EDF scheduling policy. Our heuristic employs a multi-layer\nframework, where an outer layer explores resource allocations using\nPareto-pruned search, and an inner layer optimizes task allocation by solving a\nknapsack problem using dynamic programming. To evaluate the performance of the\nproposed optimization algorithm, we profile real-world benchmarks on an\nembedded AMD UltraScale+ ZCU102 platform, with fine-grained resource\npartitioning enabled by the Jailhouse hypervisor, leveraging cache set\npartitioning and MemGuard for memory bandwidth regulation. Experiments based on\nthe benchmarking results show that the proposed 0-1 linear program outperforms\nexisting mixed-integer programs by finding more optimal solutions within the\nsame time limit. Moreover, the proposed multi-objective multi-layer heuristic\nperforms consistently better than the state-of-the-art multi-resource-task\nco-allocation algorithm in terms of schedulability, resource usage, number of\nnon-dominated solutions, and computational efficiency."
                },
                "authors": [
                    {
                        "name": "Binqi Sun"
                    },
                    {
                        "name": "Zhihang Wei"
                    },
                    {
                        "name": "Andrea Bastoni"
                    },
                    {
                        "name": "Debayan Roy"
                    },
                    {
                        "name": "Mirco Theile"
                    },
                    {
                        "name": "Tomasz Kloda"
                    },
                    {
                        "name": "Rodolfo Pellizzoni"
                    },
                    {
                        "name": "Marco Caccamo"
                    }
                ],
                "author_detail": {
                    "name": "Marco Caccamo"
                },
                "author": "Marco Caccamo",
                "arxiv_doi": "10.4230/LIPIcs.ECRTS.2025.7",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.4230/LIPIcs.ECRTS.2025.7",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.11554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted in the 37th Euromicro Conference on Real-Time Systems (ECRTS\n  2025)",
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02380v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02380v7",
                "updated": "2025-05-15T13:48:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    13,
                    48,
                    40,
                    3,
                    135,
                    0
                ],
                "published": "2025-01-04T20:59:34Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    20,
                    59,
                    34,
                    5,
                    4,
                    0
                ],
                "title": "Reciprocating Locks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reciprocating Locks"
                },
                "summary": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks."
                },
                "authors": [
                    {
                        "name": "Dave Dice"
                    },
                    {
                        "name": "Alex Kogan"
                    }
                ],
                "author_detail": {
                    "name": "Alex Kogan"
                },
                "author": "Alex Kogan",
                "arxiv_comment": "Added additional variations in appendix, at the request of\n  collaborators who want to prove various properties",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02380v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02380v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13779v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13779v3",
                "updated": "2025-05-15T03:29:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    3,
                    29,
                    15,
                    3,
                    135,
                    0
                ],
                "published": "2024-12-18T12:16:41Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    16,
                    41,
                    2,
                    353,
                    0
                ],
                "title": "Rehearsal-Free Continual Federated Learning with Synergistic Synaptic\n  Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rehearsal-Free Continual Federated Learning with Synergistic Synaptic\n  Intelligence"
                },
                "summary": "Continual Federated Learning (CFL) allows distributed devices to\ncollaboratively learn novel concepts from continuously shifting training data\nwhile avoiding knowledge forgetting of previously seen tasks. To tackle this\nchallenge, most current CFL approaches rely on extensive rehearsal of previous\ndata. Despite effectiveness, rehearsal comes at a cost to memory, and it may\nalso violate data privacy. Considering these, we seek to apply regularization\ntechniques to CFL by considering their cost-efficient properties that do not\nrequire sample caching or rehearsal. Specifically, we first apply traditional\nregularization techniques to CFL and observe that existing regularization\ntechniques, especially synaptic intelligence, can achieve promising results\nunder homogeneous data distribution but fail when the data is heterogeneous.\nBased on this observation, we propose a simple yet effective regularization\nalgorithm for CFL named FedSSI, which tailors the synaptic intelligence for the\nCFL with heterogeneous data settings. FedSSI can not only reduce computational\noverhead without rehearsal but also address the data heterogeneity issue.\nExtensive experiments show that FedSSI achieves superior performance compared\nto state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Federated Learning (CFL) allows distributed devices to\ncollaboratively learn novel concepts from continuously shifting training data\nwhile avoiding knowledge forgetting of previously seen tasks. To tackle this\nchallenge, most current CFL approaches rely on extensive rehearsal of previous\ndata. Despite effectiveness, rehearsal comes at a cost to memory, and it may\nalso violate data privacy. Considering these, we seek to apply regularization\ntechniques to CFL by considering their cost-efficient properties that do not\nrequire sample caching or rehearsal. Specifically, we first apply traditional\nregularization techniques to CFL and observe that existing regularization\ntechniques, especially synaptic intelligence, can achieve promising results\nunder homogeneous data distribution but fail when the data is heterogeneous.\nBased on this observation, we propose a simple yet effective regularization\nalgorithm for CFL named FedSSI, which tailors the synaptic intelligence for the\nCFL with heterogeneous data settings. FedSSI can not only reduce computational\noverhead without rehearsal but also address the data heterogeneity issue.\nExtensive experiments show that FedSSI achieves superior performance compared\nto state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Yichen Li"
                    },
                    {
                        "name": "Yuying Wang"
                    },
                    {
                        "name": "Haozhao Wang"
                    },
                    {
                        "name": "Yining Qi"
                    },
                    {
                        "name": "Tianzhe Xiao"
                    },
                    {
                        "name": "Ruixuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruixuan Li"
                },
                "author": "Ruixuan Li",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2403.05890",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13779v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13779v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16112v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16112v2",
                "updated": "2025-05-15T03:27:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    3,
                    27,
                    28,
                    3,
                    135,
                    0
                ],
                "published": "2025-03-20T13:00:36Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    0,
                    36,
                    3,
                    79,
                    0
                ],
                "title": "PromptMobile: Efficient Promptus for Low Bandwidth Mobile Video\n  Streaming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromptMobile: Efficient Promptus for Low Bandwidth Mobile Video\n  Streaming"
                },
                "summary": "Traditional video compression algorithms exhibit significant quality\ndegradation at extremely low bitrates. Promptus emerges as a new paradigm for\nvideo streaming, substantially cutting down the bandwidth essential for video\nstreaming. However, Promptus is computationally intensive and can not run in\nreal-time on mobile devices. This paper presents PromptMobile, an efficient\nacceleration framework tailored for on-device Promptus. Specifically, we\npropose (1) a two-stage efficient generation framework to reduce computational\ncost by 8.1x, (2) a fine-grained inter-frame caching to reduce redundant\ncomputations by 16.6%, (3) system-level optimizations to further enhance\nefficiency. The evaluations demonstrate that compared with the original\nPromptus, PromptMobile achieves a 13.6x increase in image generation speed.\nCompared with other streaming methods, PromptMobile achives an average LPIPS\nimprovement of 0.016 (compared with H.265), reducing 60% of severely distorted\nframes (compared to VQGAN).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional video compression algorithms exhibit significant quality\ndegradation at extremely low bitrates. Promptus emerges as a new paradigm for\nvideo streaming, substantially cutting down the bandwidth essential for video\nstreaming. However, Promptus is computationally intensive and can not run in\nreal-time on mobile devices. This paper presents PromptMobile, an efficient\nacceleration framework tailored for on-device Promptus. Specifically, we\npropose (1) a two-stage efficient generation framework to reduce computational\ncost by 8.1x, (2) a fine-grained inter-frame caching to reduce redundant\ncomputations by 16.6%, (3) system-level optimizations to further enhance\nefficiency. The evaluations demonstrate that compared with the original\nPromptus, PromptMobile achieves a 13.6x increase in image generation speed.\nCompared with other streaming methods, PromptMobile achives an average LPIPS\nimprovement of 0.016 (compared with H.265), reducing 60% of severely distorted\nframes (compared to VQGAN)."
                },
                "authors": [
                    {
                        "name": "Liming Liu"
                    },
                    {
                        "name": "Jiangkai Wu"
                    },
                    {
                        "name": "Haoyang Wang"
                    },
                    {
                        "name": "Peiheng Wang"
                    },
                    {
                        "name": "Zongming Guo"
                    },
                    {
                        "name": "Xinggong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xinggong Zhang"
                },
                "author": "Xinggong Zhang",
                "arxiv_doi": "10.1145/3735358.3735383",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3735358.3735383",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.16112v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16112v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "6 pages (excluding references), 10 figures, to appear in APNET 2025",
                "arxiv_journal_ref": "Proc. 9th Asia-Pacific Workshop on Networking (APNET), Aug 2025,\n  Paper No. 24",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09561v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09561v2",
                "updated": "2025-05-19T20:37:41Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    20,
                    37,
                    41,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-14T17:00:47Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    17,
                    0,
                    47,
                    2,
                    134,
                    0
                ],
                "title": "Learning Long-Context Diffusion Policies via Past-Token Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Long-Context Diffusion Policies via Past-Token Prediction"
                },
                "summary": "Reasoning over long sequences of observations and actions is essential for\nmany robotic tasks. Yet, learning effective long-context policies from\ndemonstrations remains challenging. As context length increases, training\nbecomes increasingly expensive due to rising memory demands, and policy\nperformance often degrades as a result of spurious correlations. Recent methods\ntypically sidestep these issues by truncating context length, discarding\nhistorical information that may be critical for subsequent decisions. In this\npaper, we propose an alternative approach that explicitly regularizes the\nretention of past information. We first revisit the copycat problem in\nimitation learning and identify an opposite challenge in recent diffusion\npolicies: rather than over-relying on prior actions, they often fail to capture\nessential dependencies between past and future actions. To address this, we\nintroduce Past-Token Prediction (PTP), an auxiliary task in which the policy\nlearns to predict past action tokens alongside future ones. This regularization\nsignificantly improves temporal modeling in the policy head, with minimal\nreliance on visual representations. Building on this observation, we further\nintroduce a multistage training strategy: pre-train the visual encoder with\nshort contexts, and fine-tune the policy head using cached long-context\nembeddings. This strategy preserves the benefits of PTP while greatly reducing\nmemory and computational overhead. Finally, we extend PTP into a\nself-verification mechanism at test time, enabling the policy to score and\nselect candidates consistent with past actions during inference. Experiments\nacross four real-world and six simulated tasks demonstrate that our proposed\nmethod improves the performance of long-context diffusion policies by 3x and\naccelerates policy training by more than 10x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning over long sequences of observations and actions is essential for\nmany robotic tasks. Yet, learning effective long-context policies from\ndemonstrations remains challenging. As context length increases, training\nbecomes increasingly expensive due to rising memory demands, and policy\nperformance often degrades as a result of spurious correlations. Recent methods\ntypically sidestep these issues by truncating context length, discarding\nhistorical information that may be critical for subsequent decisions. In this\npaper, we propose an alternative approach that explicitly regularizes the\nretention of past information. We first revisit the copycat problem in\nimitation learning and identify an opposite challenge in recent diffusion\npolicies: rather than over-relying on prior actions, they often fail to capture\nessential dependencies between past and future actions. To address this, we\nintroduce Past-Token Prediction (PTP), an auxiliary task in which the policy\nlearns to predict past action tokens alongside future ones. This regularization\nsignificantly improves temporal modeling in the policy head, with minimal\nreliance on visual representations. Building on this observation, we further\nintroduce a multistage training strategy: pre-train the visual encoder with\nshort contexts, and fine-tune the policy head using cached long-context\nembeddings. This strategy preserves the benefits of PTP while greatly reducing\nmemory and computational overhead. Finally, we extend PTP into a\nself-verification mechanism at test time, enabling the policy to score and\nselect candidates consistent with past actions during inference. Experiments\nacross four real-world and six simulated tasks demonstrate that our proposed\nmethod improves the performance of long-context diffusion policies by 3x and\naccelerates policy training by more than 10x."
                },
                "authors": [
                    {
                        "name": "Marcel Torne"
                    },
                    {
                        "name": "Andy Tang"
                    },
                    {
                        "name": "Yuejiang Liu"
                    },
                    {
                        "name": "Chelsea Finn"
                    }
                ],
                "author_detail": {
                    "name": "Chelsea Finn"
                },
                "author": "Chelsea Finn",
                "arxiv_comment": "Videos are available at https://long-context-dp.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09561v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09561v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06738v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06738v2",
                "updated": "2025-05-14T16:04:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    16,
                    4,
                    57,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-10T19:06:37Z",
                "published_parsed": [
                    2025,
                    5,
                    10,
                    19,
                    6,
                    37,
                    5,
                    130,
                    0
                ],
                "title": "I Know What You Said: Unveiling Hardware Cache Side-Channels in Local\n  Large Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I Know What You Said: Unveiling Hardware Cache Side-Channels in Local\n  Large Language Model Inference"
                },
                "summary": "Large Language Models (LLMs) that can be deployed locally have recently\ngained popularity for privacy-sensitive tasks, with companies such as Meta,\nGoogle, and Intel playing significant roles in their development. However, the\nsecurity of local LLMs through the lens of hardware cache side-channels remains\nunexplored. In this paper, we unveil novel side-channel vulnerabilities in\nlocal LLM inference: token value and token position leakage, which can expose\nboth the victim's input and output text, thereby compromising user privacy.\nSpecifically, we found that adversaries can infer the token values from the\ncache access patterns of the token embedding operation, and deduce the token\npositions from the timing of autoregressive decoding phases. To demonstrate the\npotential of these leaks, we design a novel eavesdropping attack framework\ntargeting both open-source and proprietary LLM inference systems. The attack\nframework does not directly interact with the victim's LLM and can be executed\nwithout privilege.\n  We evaluate the attack on a range of practical local LLM deployments (e.g.,\nLlama, Falcon, and Gemma), and the results show that our attack achieves\npromising accuracy. The restored output and input text have an average edit\ndistance of 5.2% and 17.3% to the ground truth, respectively. Furthermore, the\nreconstructed texts achieve average cosine similarity scores of 98.7% (input)\nand 98.0% (output).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) that can be deployed locally have recently\ngained popularity for privacy-sensitive tasks, with companies such as Meta,\nGoogle, and Intel playing significant roles in their development. However, the\nsecurity of local LLMs through the lens of hardware cache side-channels remains\nunexplored. In this paper, we unveil novel side-channel vulnerabilities in\nlocal LLM inference: token value and token position leakage, which can expose\nboth the victim's input and output text, thereby compromising user privacy.\nSpecifically, we found that adversaries can infer the token values from the\ncache access patterns of the token embedding operation, and deduce the token\npositions from the timing of autoregressive decoding phases. To demonstrate the\npotential of these leaks, we design a novel eavesdropping attack framework\ntargeting both open-source and proprietary LLM inference systems. The attack\nframework does not directly interact with the victim's LLM and can be executed\nwithout privilege.\n  We evaluate the attack on a range of practical local LLM deployments (e.g.,\nLlama, Falcon, and Gemma), and the results show that our attack achieves\npromising accuracy. The restored output and input text have an average edit\ndistance of 5.2% and 17.3% to the ground truth, respectively. Furthermore, the\nreconstructed texts achieve average cosine similarity scores of 98.7% (input)\nand 98.0% (output)."
                },
                "authors": [
                    {
                        "name": "Zibo Gao"
                    },
                    {
                        "name": "Junjie Hu"
                    },
                    {
                        "name": "Feng Guo"
                    },
                    {
                        "name": "Yixin Zhang"
                    },
                    {
                        "name": "Yinglong Han"
                    },
                    {
                        "name": "Siyuan Liu"
                    },
                    {
                        "name": "Haiyang Li"
                    },
                    {
                        "name": "Zhiqiang Lv"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Lv"
                },
                "author": "Zhiqiang Lv",
                "arxiv_comment": "Submitted for review in January 22, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06738v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06738v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10584v1",
                "updated": "2025-05-14T13:39:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    13,
                    39,
                    53,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T13:39:53Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    13,
                    39,
                    53,
                    2,
                    134,
                    0
                ],
                "title": "Aquarius: A Family of Industry-Level Video Generation Models for\n  Marketing Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aquarius: A Family of Industry-Level Video Generation Models for\n  Marketing Scenarios"
                },
                "summary": "This report introduces Aquarius, a family of industry-level video generation\nmodels for marketing scenarios designed for thousands-xPU clusters and models\nwith hundreds of billions of parameters. Leveraging efficient engineering\narchitecture and algorithmic innovation, Aquarius demonstrates exceptional\nperformance in high-fidelity, multi-aspect-ratio, and long-duration video\nsynthesis. By disclosing the framework's design details, we aim to demystify\nindustrial-scale video generation systems and catalyze advancements in the\ngenerative video community. The Aquarius framework consists of five components:\nDistributed Graph and Video Data Processing Pipeline: Manages tens of thousands\nof CPUs and thousands of xPUs via automated task distribution, enabling\nefficient video data processing. Additionally, we are about to open-source the\nentire data processing framework named \"Aquarius-Datapipe\". Model Architectures\nfor Different Scales: Include a Single-DiT architecture for 2B models and a\nMultimodal-DiT architecture for 13.4B models, supporting multi-aspect ratios,\nmulti-resolution, and multi-duration video generation. High-Performance\ninfrastructure designed for video generation model training: Incorporating\nhybrid parallelism and fine-grained memory optimization strategies, this\ninfrastructure achieves 36% MFU at large scale. Multi-xPU Parallel Inference\nAcceleration: Utilizes diffusion cache and attention optimization to achieve a\n2.35x inference speedup. Multiple marketing-scenarios applications: Including\nimage-to-video, text-to-video (avatar), video inpainting and video\npersonalization, among others. More downstream applications and\nmulti-dimensional evaluation metrics will be added in the upcoming version\nupdates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This report introduces Aquarius, a family of industry-level video generation\nmodels for marketing scenarios designed for thousands-xPU clusters and models\nwith hundreds of billions of parameters. Leveraging efficient engineering\narchitecture and algorithmic innovation, Aquarius demonstrates exceptional\nperformance in high-fidelity, multi-aspect-ratio, and long-duration video\nsynthesis. By disclosing the framework's design details, we aim to demystify\nindustrial-scale video generation systems and catalyze advancements in the\ngenerative video community. The Aquarius framework consists of five components:\nDistributed Graph and Video Data Processing Pipeline: Manages tens of thousands\nof CPUs and thousands of xPUs via automated task distribution, enabling\nefficient video data processing. Additionally, we are about to open-source the\nentire data processing framework named \"Aquarius-Datapipe\". Model Architectures\nfor Different Scales: Include a Single-DiT architecture for 2B models and a\nMultimodal-DiT architecture for 13.4B models, supporting multi-aspect ratios,\nmulti-resolution, and multi-duration video generation. High-Performance\ninfrastructure designed for video generation model training: Incorporating\nhybrid parallelism and fine-grained memory optimization strategies, this\ninfrastructure achieves 36% MFU at large scale. Multi-xPU Parallel Inference\nAcceleration: Utilizes diffusion cache and attention optimization to achieve a\n2.35x inference speedup. Multiple marketing-scenarios applications: Including\nimage-to-video, text-to-video (avatar), video inpainting and video\npersonalization, among others. More downstream applications and\nmulti-dimensional evaluation metrics will be added in the upcoming version\nupdates."
                },
                "authors": [
                    {
                        "name": "Huafeng Shi"
                    },
                    {
                        "name": "Jianzhong Liang"
                    },
                    {
                        "name": "Rongchang Xie"
                    },
                    {
                        "name": "Xian Wu"
                    },
                    {
                        "name": "Cheng Chen"
                    },
                    {
                        "name": "Chang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Chang Liu"
                },
                "author": "Chang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v7",
                "updated": "2025-05-14T04:38:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    4,
                    38,
                    42,
                    2,
                    134,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty, and only those\nelements are processed to draw the next frame, dramatically reducing latency.\nHowever, the standard incremental layout algorithm must search the page for\ndirty elements, accessing auxiliary elements in the process. These auxiliary\nelements add cache misses and stalled cycles, and are responsible for a sizable\nfraction of all layout latency. We introduce a new, faster incremental layout\nalgorithm called Spineless Traversal. Spineless Traversal uses a\ncache-friendlier priority queue algorithm that avoids accessing auxiliary nodes\nand thus reduces cache traffic and stalls. This leads to dramatic speedups on\nthe most latency-critical interactions such as hovering, typing, and animation.\nMoreover, thanks to numerous low-level optimizations, Spineless Traversal is\ncompetitive across the whole spectrum of incremental layout workloads.\nSpineless Traversal is faster than the standard approach on 83.0% of 2216\nbenchmarks, with a mean speedup of 1.80x concentrated in the most\nlatency-critical interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty, and only those\nelements are processed to draw the next frame, dramatically reducing latency.\nHowever, the standard incremental layout algorithm must search the page for\ndirty elements, accessing auxiliary elements in the process. These auxiliary\nelements add cache misses and stalled cycles, and are responsible for a sizable\nfraction of all layout latency. We introduce a new, faster incremental layout\nalgorithm called Spineless Traversal. Spineless Traversal uses a\ncache-friendlier priority queue algorithm that avoids accessing auxiliary nodes\nand thus reduces cache traffic and stalls. This leads to dramatic speedups on\nthe most latency-critical interactions such as hovering, typing, and animation.\nMoreover, thanks to numerous low-level optimizations, Spineless Traversal is\ncompetitive across the whole spectrum of incremental layout workloads.\nSpineless Traversal is faster than the standard approach on 83.0% of 2216\nbenchmarks, with a mean speedup of 1.80x concentrated in the most\nlatency-critical interactions."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18599v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18599v2",
                "updated": "2025-05-14T04:22:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    4,
                    22,
                    24,
                    2,
                    134,
                    0
                ],
                "published": "2025-03-24T11:56:50Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    56,
                    50,
                    0,
                    83,
                    0
                ],
                "title": "Oaken: Fast and Efficient LLM Serving with Online-Offline Hybrid KV\n  Cache Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oaken: Fast and Efficient LLM Serving with Online-Offline Hybrid KV\n  Cache Quantization"
                },
                "summary": "Modern Large Language Model serving system batches multiple requests to\nachieve high throughput, while batching attention operations is challenging,\nrendering memory bandwidth a critical bottleneck. The community relies on\nhigh-end GPUs with multiple high-bandwidth memory channels. Unfortunately,\nHBM's high bandwidth often comes at the expense of limited memory capacity,\nwhich reduces core utilization and increases costs. Recent advancements\nenabling longer contexts for LLMs have substantially increased the key-value\ncache size, further intensifying the pressures on memory capacity. The\nliterature has explored KV cache quantization techniques, which commonly use\nlow bitwidth for most values, selectively using higher bitwidth for outlier\nvalues. While this approach helps achieve high accuracy and low bitwidth\nsimultaneously, it comes with the limitation that cost for online outlier\ndetection is excessively high, negating the advantages. We propose Oaken, an\nacceleration solution that achieves high accuracy and high performance\nsimultaneously through co-designing algorithm and hardware. To effectively find\na sweet spot in the accuracy-performance trade-off space of KV cache\nquantization, Oaken employs an online-offline hybrid approach, setting outlier\nthresholds offline, which are then used to determine the quantization scale\nonline. To translate the proposed algorithmic technique into tangible\nperformance gains, Oaken also comes with custom quantization engines and memory\nmanagement units that can be integrated with any LLM accelerators. We built an\nOaken accelerator on top of an LLM accelerator, LPU, and conducted a\ncomprehensive evaluation. Our experiments show that for a batch size of 256,\nOaken achieves up to 1.58x throughput improvement over NVIDIA A100 GPU,\nincurring a minimal accuracy loss of only 0.54\\% on average, compared to\nstate-of-the-art KV cache quantization techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Large Language Model serving system batches multiple requests to\nachieve high throughput, while batching attention operations is challenging,\nrendering memory bandwidth a critical bottleneck. The community relies on\nhigh-end GPUs with multiple high-bandwidth memory channels. Unfortunately,\nHBM's high bandwidth often comes at the expense of limited memory capacity,\nwhich reduces core utilization and increases costs. Recent advancements\nenabling longer contexts for LLMs have substantially increased the key-value\ncache size, further intensifying the pressures on memory capacity. The\nliterature has explored KV cache quantization techniques, which commonly use\nlow bitwidth for most values, selectively using higher bitwidth for outlier\nvalues. While this approach helps achieve high accuracy and low bitwidth\nsimultaneously, it comes with the limitation that cost for online outlier\ndetection is excessively high, negating the advantages. We propose Oaken, an\nacceleration solution that achieves high accuracy and high performance\nsimultaneously through co-designing algorithm and hardware. To effectively find\na sweet spot in the accuracy-performance trade-off space of KV cache\nquantization, Oaken employs an online-offline hybrid approach, setting outlier\nthresholds offline, which are then used to determine the quantization scale\nonline. To translate the proposed algorithmic technique into tangible\nperformance gains, Oaken also comes with custom quantization engines and memory\nmanagement units that can be integrated with any LLM accelerators. We built an\nOaken accelerator on top of an LLM accelerator, LPU, and conducted a\ncomprehensive evaluation. Our experiments show that for a batch size of 256,\nOaken achieves up to 1.58x throughput improvement over NVIDIA A100 GPU,\nincurring a minimal accuracy loss of only 0.54\\% on average, compared to\nstate-of-the-art KV cache quantization techniques."
                },
                "authors": [
                    {
                        "name": "Minsu Kim"
                    },
                    {
                        "name": "Seongmin Hong"
                    },
                    {
                        "name": "RyeoWook Ko"
                    },
                    {
                        "name": "Soongyu Choi"
                    },
                    {
                        "name": "Hunjong Lee"
                    },
                    {
                        "name": "Junsoo Kim"
                    },
                    {
                        "name": "Joo-Young Kim"
                    },
                    {
                        "name": "Jongse Park"
                    }
                ],
                "author_detail": {
                    "name": "Jongse Park"
                },
                "author": "Jongse Park",
                "arxiv_doi": "10.1145/3695053.3731019",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3695053.3731019",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.18599v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18599v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "16 pages, 14 figures, and 4 tables",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09081v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09081v1",
                "updated": "2025-05-14T02:29:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    2,
                    29,
                    46,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T02:29:46Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    2,
                    29,
                    46,
                    2,
                    134,
                    0
                ],
                "title": "SALM: A Multi-Agent Framework for Language Model-Driven Social Network\n  Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SALM: A Multi-Agent Framework for Language Model-Driven Social Network\n  Simulation"
                },
                "summary": "Contemporary approaches to agent-based modeling (ABM) of social systems have\ntraditionally emphasized rule-based behaviors, limiting their ability to\ncapture nuanced dynamics by moving beyond predefined rules and leveraging\ncontextual understanding from LMs of human social interaction. This paper\npresents SALM (Social Agent LM Framework), a novel approach for integrating\nlanguage models (LMs) into social network simulation that achieves\nunprecedented temporal stability in multi-agent scenarios. Our primary\ncontributions include: (1) a hierarchical prompting architecture enabling\nstable simulation beyond 4,000 timesteps while reducing token usage by 73%, (2)\nan attention-based memory system achieving 80% cache hit rates (95% CI [78%,\n82%]) with sub-linear memory growth of 9.5%, and (3) formal bounds on\npersonality stability. Through extensive validation against SNAP ego networks,\nwe demonstrate the first LLM-based framework capable of modeling long-term\nsocial phenomena while maintaining empirically validated behavioral fidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary approaches to agent-based modeling (ABM) of social systems have\ntraditionally emphasized rule-based behaviors, limiting their ability to\ncapture nuanced dynamics by moving beyond predefined rules and leveraging\ncontextual understanding from LMs of human social interaction. This paper\npresents SALM (Social Agent LM Framework), a novel approach for integrating\nlanguage models (LMs) into social network simulation that achieves\nunprecedented temporal stability in multi-agent scenarios. Our primary\ncontributions include: (1) a hierarchical prompting architecture enabling\nstable simulation beyond 4,000 timesteps while reducing token usage by 73%, (2)\nan attention-based memory system achieving 80% cache hit rates (95% CI [78%,\n82%]) with sub-linear memory growth of 9.5%, and (3) formal bounds on\npersonality stability. Through extensive validation against SNAP ego networks,\nwe demonstrate the first LLM-based framework capable of modeling long-term\nsocial phenomena while maintaining empirically validated behavioral fidelity."
                },
                "authors": [
                    {
                        "name": "Gaurav Koley"
                    }
                ],
                "author_detail": {
                    "name": "Gaurav Koley"
                },
                "author": "Gaurav Koley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09081v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09081v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09040v1",
                "updated": "2025-05-14T00:41:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    0,
                    41,
                    44,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T00:41:44Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    0,
                    41,
                    44,
                    2,
                    134,
                    0
                ],
                "title": "RT-cache: Efficient Robot Trajectory Retrieval System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RT-cache: Efficient Robot Trajectory Retrieval System"
                },
                "summary": "This paper introduces RT-cache, a novel trajectorymemory pipeline that\naccelerates real-world robot inference by leveraging big-data retrieval and\nlearning from experience. While modern Vision-Language-Action (VLA) models can\nhandle diverse robotic tasks, they often incur high per-step inference costs,\nresulting in significant latency, sometimes minutes per task. In contrast,\nRT-cache stores a large-scale Memory of previously successful robot\ntrajectories and retrieves relevant multistep motion snippets, drastically\nreducing inference overhead. By integrating a Memory Builder with a Trajectory\nRetrieval, we develop an efficient retrieval process that remains tractable\neven for extremely large datasets. RT-cache flexibly accumulates real-world\nexperiences and replays them whenever the current scene matches past states,\nadapting quickly to new or unseen environments with only a few additional\nsamples. Experiments on the Open-X Embodiment Dataset and other real-world data\ndemonstrate that RT-cache completes tasks both faster and more successfully\nthan a baseline lacking retrieval, suggesting a practical, data-driven solution\nfor real-time manipulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces RT-cache, a novel trajectorymemory pipeline that\naccelerates real-world robot inference by leveraging big-data retrieval and\nlearning from experience. While modern Vision-Language-Action (VLA) models can\nhandle diverse robotic tasks, they often incur high per-step inference costs,\nresulting in significant latency, sometimes minutes per task. In contrast,\nRT-cache stores a large-scale Memory of previously successful robot\ntrajectories and retrieves relevant multistep motion snippets, drastically\nreducing inference overhead. By integrating a Memory Builder with a Trajectory\nRetrieval, we develop an efficient retrieval process that remains tractable\neven for extremely large datasets. RT-cache flexibly accumulates real-world\nexperiences and replays them whenever the current scene matches past states,\nadapting quickly to new or unseen environments with only a few additional\nsamples. Experiments on the Open-X Embodiment Dataset and other real-world data\ndemonstrate that RT-cache completes tasks both faster and more successfully\nthan a baseline lacking retrieval, suggesting a practical, data-driven solution\nfor real-time manipulation."
                },
                "authors": [
                    {
                        "name": "Owen Kwon"
                    },
                    {
                        "name": "Abraham George"
                    },
                    {
                        "name": "Alison Bartsch"
                    },
                    {
                        "name": "Amir Barati Farimani"
                    }
                ],
                "author_detail": {
                    "name": "Amir Barati Farimani"
                },
                "author": "Amir Barati Farimani",
                "arxiv_comment": "9 pages, 5 figures. Submitted to an IEEE robotics conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08958v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08958v1",
                "updated": "2025-05-13T20:51:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    20,
                    51,
                    59,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T20:51:59Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    20,
                    51,
                    59,
                    1,
                    133,
                    0
                ],
                "title": "Adaptive Entanglement Generation for Quantum Routing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Entanglement Generation for Quantum Routing"
                },
                "summary": "Entanglement generation in long-distance quantum networks is a difficult\nprocess due to resource limitations and the probabilistic nature of\nentanglement swapping. To maximize success probability, existing quantum\nrouting algorithms employ computationally expensive solutions (e.g., linear\nprogramming) to determine which links to entangle and use for end-to-end\nentanglement generation. Such optimization methods, however, cannot meet the\ndelay requirements of real-world quantum networks, necessitating swift yet\nefficient real-time optimization models. In this paper, we propose\nreinforcement learning (RL)-based models to determine which links to entangle\nand proactively swap to meet connection requests. We show that the proposed\nRL-based approach is 20x faster compared to linear programming. Moreover, we\nshow that one can take advantage of the longevity of entanglements to (i) cache\nentangled links for future use and (ii) proactively swap entanglement on\nhigh-demand path segments, thereby increasing the likelihood of request\nsuccess. Through comprehensive simulations, we demonstrate that caching unused\nentanglements leads to a 10-15% improvement in the performance of\nstate-of-the-art quantum routing algorithms. Complementing caching with\nproactive entanglement swapping further enhances the request success rate by up\nto 52.55%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entanglement generation in long-distance quantum networks is a difficult\nprocess due to resource limitations and the probabilistic nature of\nentanglement swapping. To maximize success probability, existing quantum\nrouting algorithms employ computationally expensive solutions (e.g., linear\nprogramming) to determine which links to entangle and use for end-to-end\nentanglement generation. Such optimization methods, however, cannot meet the\ndelay requirements of real-world quantum networks, necessitating swift yet\nefficient real-time optimization models. In this paper, we propose\nreinforcement learning (RL)-based models to determine which links to entangle\nand proactively swap to meet connection requests. We show that the proposed\nRL-based approach is 20x faster compared to linear programming. Moreover, we\nshow that one can take advantage of the longevity of entanglements to (i) cache\nentangled links for future use and (ii) proactively swap entanglement on\nhigh-demand path segments, thereby increasing the likelihood of request\nsuccess. Through comprehensive simulations, we demonstrate that caching unused\nentanglements leads to a 10-15% improvement in the performance of\nstate-of-the-art quantum routing algorithms. Complementing caching with\nproactive entanglement swapping further enhances the request success rate by up\nto 52.55%."
                },
                "authors": [
                    {
                        "name": "Tasdiqul Islam"
                    },
                    {
                        "name": "Md Arifuzzaman"
                    },
                    {
                        "name": "Engin Arslan"
                    }
                ],
                "author_detail": {
                    "name": "Engin Arslan"
                },
                "author": "Engin Arslan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08958v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08958v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01723v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01723v5",
                "updated": "2025-05-13T17:43:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    17,
                    43,
                    47,
                    1,
                    133,
                    0
                ],
                "published": "2024-10-02T16:34:29Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    16,
                    34,
                    29,
                    2,
                    276,
                    0
                ],
                "title": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration"
                },
                "summary": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method. Our code is available at\nhttps://github.com/ModelTC/HarmoniCa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method. Our code is available at\nhttps://github.com/ModelTC/HarmoniCa."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Zining Wang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Xinjie Zhang"
                    },
                    {
                        "name": "Jinyang Guo"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhang"
                },
                "author": "Jun Zhang",
                "arxiv_comment": "Accepted by ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01723v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01723v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08587v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08587v1",
                "updated": "2025-05-13T13:58:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    13,
                    58,
                    22,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T13:58:22Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    13,
                    58,
                    22,
                    1,
                    133,
                    0
                ],
                "title": "Two-Level Sketching Alternating Anderson acceleration for Complex\n  Physics Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-Level Sketching Alternating Anderson acceleration for Complex\n  Physics Applications"
                },
                "summary": "We present a novel two-level sketching extension of the Alternating\nAnderson-Picard (AAP) method for accelerating fixed-point iterations in\nchallenging single- and multi-physics simulations governed by discretized\npartial differential equations. Our approach combines a static, physics-based\nprojection that reduces the least-squares problem to the most informative field\n(e.g., via Schur-complement insight) with a dynamic, algebraic sketching stage\ndriven by a backward stability analysis under Lipschitz continuity. We\nintroduce inexpensive estimators for stability thresholds and cache-aware\nrandomized selection strategies to balance computational cost against\nmemory-access overhead. The resulting algorithm solves reduced least-squares\nsystems in place, minimizes memory footprints, and seamlessly alternates\nbetween low-cost Picard updates and Anderson mixing. Implemented in Julia, our\ntwo-level sketching AAP achieves up to 50% time-to-solution reductions compared\nto standard Anderson acceleration-without degrading convergence rates-on\nbenchmark problems including Stokes, p-Laplacian, Bidomain, and Navier-Stokes\nformulations at varying problem sizes. These results demonstrate the method's\nrobustness, scalability, and potential for integration into high-performance\nscientific computing frameworks. Our implementation is available open-source in\nthe AAP.jl library.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel two-level sketching extension of the Alternating\nAnderson-Picard (AAP) method for accelerating fixed-point iterations in\nchallenging single- and multi-physics simulations governed by discretized\npartial differential equations. Our approach combines a static, physics-based\nprojection that reduces the least-squares problem to the most informative field\n(e.g., via Schur-complement insight) with a dynamic, algebraic sketching stage\ndriven by a backward stability analysis under Lipschitz continuity. We\nintroduce inexpensive estimators for stability thresholds and cache-aware\nrandomized selection strategies to balance computational cost against\nmemory-access overhead. The resulting algorithm solves reduced least-squares\nsystems in place, minimizes memory footprints, and seamlessly alternates\nbetween low-cost Picard updates and Anderson mixing. Implemented in Julia, our\ntwo-level sketching AAP achieves up to 50% time-to-solution reductions compared\nto standard Anderson acceleration-without degrading convergence rates-on\nbenchmark problems including Stokes, p-Laplacian, Bidomain, and Navier-Stokes\nformulations at varying problem sizes. These results demonstrate the method's\nrobustness, scalability, and potential for integration into high-performance\nscientific computing frameworks. Our implementation is available open-source in\nthe AAP.jl library."
                },
                "authors": [
                    {
                        "name": "Nicolás A. Barnafi"
                    },
                    {
                        "name": "Massimiliano Lupo Pasini"
                    }
                ],
                "author_detail": {
                    "name": "Massimiliano Lupo Pasini"
                },
                "author": "Massimiliano Lupo Pasini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08587v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08587v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65N12, 65N22, 65K10, 65F10, 65F99, 65B99",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13989v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13989v2",
                "updated": "2025-05-13T09:36:03Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    9,
                    36,
                    3,
                    1,
                    133,
                    0
                ],
                "published": "2025-04-18T13:46:58Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    13,
                    46,
                    58,
                    4,
                    108,
                    0
                ],
                "title": "Gradual Binary Search and Dimension Expansion : A general method for\n  activation quantization in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gradual Binary Search and Dimension Expansion : A general method for\n  activation quantization in LLMs"
                },
                "summary": "Large language models (LLMs) have become pivotal in artificial intelligence,\ndemonstrating strong capabilities in reasoning, understanding, and generating\ndata. However, their deployment on edge devices is hindered by their\nsubstantial size, often reaching several billion parameters. Quantization is a\nwidely used method to reduce memory usage and inference time, however LLMs\npresent unique challenges due to the prevalence of outliers in their\nactivations. In this work, we leverage the theoretical advantages of Hadamard\nmatrices over random rotation matrices to push the boundaries of quantization\nin LLMs. We demonstrate that Hadamard matrices are more effective in reducing\noutliers, which are a significant obstacle in achieving low-bit quantization.\nOur method based on a gradual binary search enables 3-bit quantization for\nweights, activations, and key-value (KV) caches, resulting in a 40% increase in\naccuracy on common benchmarks compared to SoTA methods. We extend the use of\nrotation matrices to support non-power-of-2 embedding dimensions, similar to\nthe Qwen architecture, by employing the Paley algorithm. We theoretically\ndemonstrates the superiority of Hadamard matrices in reducing outliers.We\nachieved 3-bit quantization for weights, activations, and KV cache,\nsignificantly enhancing model performance. Our experimental results on multiple\nmodels family like Mistral, LLaMA, and Qwen demonstrate the effectiveness of\nour approach, outperforming existing methods and enabling practical 3-bit\nquantization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have become pivotal in artificial intelligence,\ndemonstrating strong capabilities in reasoning, understanding, and generating\ndata. However, their deployment on edge devices is hindered by their\nsubstantial size, often reaching several billion parameters. Quantization is a\nwidely used method to reduce memory usage and inference time, however LLMs\npresent unique challenges due to the prevalence of outliers in their\nactivations. In this work, we leverage the theoretical advantages of Hadamard\nmatrices over random rotation matrices to push the boundaries of quantization\nin LLMs. We demonstrate that Hadamard matrices are more effective in reducing\noutliers, which are a significant obstacle in achieving low-bit quantization.\nOur method based on a gradual binary search enables 3-bit quantization for\nweights, activations, and key-value (KV) caches, resulting in a 40% increase in\naccuracy on common benchmarks compared to SoTA methods. We extend the use of\nrotation matrices to support non-power-of-2 embedding dimensions, similar to\nthe Qwen architecture, by employing the Paley algorithm. We theoretically\ndemonstrates the superiority of Hadamard matrices in reducing outliers.We\nachieved 3-bit quantization for weights, activations, and KV cache,\nsignificantly enhancing model performance. Our experimental results on multiple\nmodels family like Mistral, LLaMA, and Qwen demonstrate the effectiveness of\nour approach, outperforming existing methods and enabling practical 3-bit\nquantization."
                },
                "authors": [
                    {
                        "name": "Lucas Maisonnave"
                    },
                    {
                        "name": "Cyril Moineau"
                    },
                    {
                        "name": "Olivier Bichler"
                    },
                    {
                        "name": "Fabrice Rastello"
                    }
                ],
                "author_detail": {
                    "name": "Fabrice Rastello"
                },
                "author": "Fabrice Rastello",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13989v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13989v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08261v1",
                "updated": "2025-05-13T06:24:48Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    6,
                    24,
                    48,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T06:24:48Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    6,
                    24,
                    48,
                    1,
                    133,
                    0
                ],
                "title": "Enhancing Cache-Augmented Generation (CAG) with Adaptive Contextual\n  Compression for Scalable Knowledge Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Cache-Augmented Generation (CAG) with Adaptive Contextual\n  Compression for Scalable Knowledge Integration"
                },
                "summary": "The rapid progress in large language models (LLMs) has paved the way for\nnovel approaches in knowledge-intensive tasks. Among these, Cache-Augmented\nGeneration (CAG) has emerged as a promising alternative to Retrieval-Augmented\nGeneration (RAG). CAG minimizes retrieval latency and simplifies system design\nby preloading knowledge into the model's context. However, challenges persist\nin scaling CAG to accommodate large and dynamic knowledge bases effectively.\nThis paper introduces Adaptive Contextual Compression (ACC), an innovative\ntechnique designed to dynamically compress and manage context inputs, enabling\nefficient utilization of the extended memory capabilities of modern LLMs. To\nfurther address the limitations of standalone CAG, we propose a Hybrid CAG-RAG\nFramework, which integrates selective retrieval to augment preloaded contexts\nin scenarios requiring additional information. Comprehensive evaluations on\ndiverse datasets highlight the proposed methods' ability to enhance\nscalability, optimize efficiency, and improve multi-hop reasoning performance,\noffering practical solutions for real-world knowledge integration challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid progress in large language models (LLMs) has paved the way for\nnovel approaches in knowledge-intensive tasks. Among these, Cache-Augmented\nGeneration (CAG) has emerged as a promising alternative to Retrieval-Augmented\nGeneration (RAG). CAG minimizes retrieval latency and simplifies system design\nby preloading knowledge into the model's context. However, challenges persist\nin scaling CAG to accommodate large and dynamic knowledge bases effectively.\nThis paper introduces Adaptive Contextual Compression (ACC), an innovative\ntechnique designed to dynamically compress and manage context inputs, enabling\nefficient utilization of the extended memory capabilities of modern LLMs. To\nfurther address the limitations of standalone CAG, we propose a Hybrid CAG-RAG\nFramework, which integrates selective retrieval to augment preloaded contexts\nin scenarios requiring additional information. Comprehensive evaluations on\ndiverse datasets highlight the proposed methods' ability to enhance\nscalability, optimize efficiency, and improve multi-hop reasoning performance,\noffering practical solutions for real-world knowledge integration challenges."
                },
                "authors": [
                    {
                        "name": "Rishabh Agrawal"
                    },
                    {
                        "name": "Himanshu Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Himanshu Kumar"
                },
                "author": "Himanshu Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07692v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07692v1",
                "updated": "2025-05-12T15:58:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    12,
                    15,
                    58,
                    39,
                    0,
                    132,
                    0
                ],
                "published": "2025-05-12T15:58:39Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    15,
                    58,
                    39,
                    0,
                    132,
                    0
                ],
                "title": "ABase: the Multi-Tenant NoSQL Serverless Database for Diverse and\n  Dynamic Workloads in Large-scale Cloud Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ABase: the Multi-Tenant NoSQL Serverless Database for Diverse and\n  Dynamic Workloads in Large-scale Cloud Environments"
                },
                "summary": "Multi-tenant architectures enhance the elasticity and resource utilization of\nNoSQL databases by allowing multiple tenants to co-locate and share resources.\nHowever, in large-scale cloud environments, the diverse and dynamic nature of\nworkloads poses significant challenges for multi-tenant NoSQL databases. Based\non our practical observations, we have identified three crucial challenges: (1)\nthe impact of caching on performance isolation, as cache hits alter request\nexecution and resource consumption, leading to inaccurate traffic control; (2)\nthe dynamic changes in traffic, with changes in tenant traffic trends causing\nthrottling or resource wastage, and changes in access distribution causing hot\nkey pressure or cache hit ratio drops; and (3) the imbalanced layout of data\nnodes due to tenants' diverse resource requirements, leading to low resource\nutilization. To address these challenges, we introduce ABase, a multi-tenant\nNoSQL serverless database developed at ByteDance. ABase introduces a two-layer\ncaching mechanism with a cache-aware isolation mechanism to ensure accurate\nresource consumption estimates. Furthermore, ABase employs a predictive\nautoscaling policy to dynamically adjust resources in response to tenant\ntraffic changes and a multi-resource rescheduling algorithm to balance resource\nutilization across data nodes. With these innovations, ABase has successfully\nserved ByteDance's large-scale cloud environment, supporting a total workload\nthat has achieved a peak QPS of over 13 billion and total storage exceeding 1\nEB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-tenant architectures enhance the elasticity and resource utilization of\nNoSQL databases by allowing multiple tenants to co-locate and share resources.\nHowever, in large-scale cloud environments, the diverse and dynamic nature of\nworkloads poses significant challenges for multi-tenant NoSQL databases. Based\non our practical observations, we have identified three crucial challenges: (1)\nthe impact of caching on performance isolation, as cache hits alter request\nexecution and resource consumption, leading to inaccurate traffic control; (2)\nthe dynamic changes in traffic, with changes in tenant traffic trends causing\nthrottling or resource wastage, and changes in access distribution causing hot\nkey pressure or cache hit ratio drops; and (3) the imbalanced layout of data\nnodes due to tenants' diverse resource requirements, leading to low resource\nutilization. To address these challenges, we introduce ABase, a multi-tenant\nNoSQL serverless database developed at ByteDance. ABase introduces a two-layer\ncaching mechanism with a cache-aware isolation mechanism to ensure accurate\nresource consumption estimates. Furthermore, ABase employs a predictive\nautoscaling policy to dynamically adjust resources in response to tenant\ntraffic changes and a multi-resource rescheduling algorithm to balance resource\nutilization across data nodes. With these innovations, ABase has successfully\nserved ByteDance's large-scale cloud environment, supporting a total workload\nthat has achieved a peak QPS of over 13 billion and total storage exceeding 1\nEB."
                },
                "authors": [
                    {
                        "name": "Rong Kang"
                    },
                    {
                        "name": "Yanbin Chen"
                    },
                    {
                        "name": "Ye Liu"
                    },
                    {
                        "name": "Fuxin Jiang"
                    },
                    {
                        "name": "Qingshuo Li"
                    },
                    {
                        "name": "Miao Ma"
                    },
                    {
                        "name": "Jian Liu"
                    },
                    {
                        "name": "Guangliang Zhao"
                    },
                    {
                        "name": "Tieying Zhang"
                    },
                    {
                        "name": "Jianjun Chen"
                    },
                    {
                        "name": "Lei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lei Zhang"
                },
                "author": "Lei Zhang",
                "arxiv_comment": "SIGMOD 2025 accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07692v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07692v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07680v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07680v1",
                "updated": "2025-05-12T15:46:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    12,
                    15,
                    46,
                    28,
                    0,
                    132,
                    0
                ],
                "published": "2025-05-12T15:46:28Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    15,
                    46,
                    28,
                    0,
                    132,
                    0
                ],
                "title": "SpecRouter: Adaptive Routing for Multi-Level Speculative Decoding in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecRouter: Adaptive Routing for Multi-Level Speculative Decoding in\n  Large Language Models"
                },
                "summary": "Large Language Models (LLMs) present a critical trade-off between inference\nquality and computational cost: larger models offer superior capabilities but\nincur significant latency, while smaller models are faster but less powerful.\nExisting serving strategies often employ fixed model scales or static two-stage\nspeculative decoding, failing to dynamically adapt to the varying complexities\nof user requests or fluctuations in system performance. This paper introduces\n\\systemname{}, a novel framework that reimagines LLM inference as an adaptive\nrouting problem solved through multi-level speculative decoding. \\systemname{}\ndynamically constructs and optimizes inference \"paths\" (chains of models) based\non real-time feedback, addressing the limitations of static approaches. Our\ncontributions are threefold: (1) An \\textbf{adaptive model chain scheduling}\nmechanism that leverages performance profiling (execution times) and predictive\nsimilarity metrics (derived from token distribution divergence) to continuously\nselect the optimal sequence of draft and verifier models, minimizing predicted\nlatency per generated token. (2) A \\textbf{multi-level collaborative\nverification} framework where intermediate models within the selected chain can\nvalidate speculative tokens, reducing the verification burden on the final,\nmost powerful target model. (3) A \\textbf{synchronized state management} system\nproviding efficient, consistent KV cache handling across heterogeneous models\nin the chain, including precise, low-overhead rollbacks tailored for\nasynchronous batch processing inherent in multi-level speculation. Preliminary\nexperiments demonstrate the validity of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) present a critical trade-off between inference\nquality and computational cost: larger models offer superior capabilities but\nincur significant latency, while smaller models are faster but less powerful.\nExisting serving strategies often employ fixed model scales or static two-stage\nspeculative decoding, failing to dynamically adapt to the varying complexities\nof user requests or fluctuations in system performance. This paper introduces\n\\systemname{}, a novel framework that reimagines LLM inference as an adaptive\nrouting problem solved through multi-level speculative decoding. \\systemname{}\ndynamically constructs and optimizes inference \"paths\" (chains of models) based\non real-time feedback, addressing the limitations of static approaches. Our\ncontributions are threefold: (1) An \\textbf{adaptive model chain scheduling}\nmechanism that leverages performance profiling (execution times) and predictive\nsimilarity metrics (derived from token distribution divergence) to continuously\nselect the optimal sequence of draft and verifier models, minimizing predicted\nlatency per generated token. (2) A \\textbf{multi-level collaborative\nverification} framework where intermediate models within the selected chain can\nvalidate speculative tokens, reducing the verification burden on the final,\nmost powerful target model. (3) A \\textbf{synchronized state management} system\nproviding efficient, consistent KV cache handling across heterogeneous models\nin the chain, including precise, low-overhead rollbacks tailored for\nasynchronous batch processing inherent in multi-level speculation. Preliminary\nexperiments demonstrate the validity of our method."
                },
                "authors": [
                    {
                        "name": "Hang Wu"
                    },
                    {
                        "name": "Jianian Zhu"
                    },
                    {
                        "name": "Yinghui Li"
                    },
                    {
                        "name": "Haojie Wang"
                    },
                    {
                        "name": "Biao Hou"
                    },
                    {
                        "name": "Jidong Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Jidong Zhai"
                },
                "author": "Jidong Zhai",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07680v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07680v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07350v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07350v1",
                "updated": "2025-05-12T08:44:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    12,
                    8,
                    44,
                    10,
                    0,
                    132,
                    0
                ],
                "published": "2025-05-12T08:44:10Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    8,
                    44,
                    10,
                    0,
                    132,
                    0
                ],
                "title": "All-optical electric field sensing with nanodiamond-doped polymer thin\n  films",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "All-optical electric field sensing with nanodiamond-doped polymer thin\n  films"
                },
                "summary": "The nitrogen-vacancy (NV) center is a photoluminescent defect in diamond that\nexists in different charge states, NV$^-$ and NV$^0$, that are sensitive to the\nNV's nanoscale environment. Here, we show that photoluminescence (PL) from NV\ncenters in fluorescent nanodiamonds (FNDs) can be employed for all-optical\nvoltage sensing based on electric field-induced NV charge state modulation.\nMore than 95% of FNDs integrated into a capacitor device show a transient\nincrease in NV$^-$ PL intensity of up to 31% within 0.1 ms after application of\nan external voltage, accompanied by a simultaneous decrease in NV$^0$ PL. The\nchange in NV$^-$ PL increases with increasing applied voltage from 0 to 100 V,\ncorresponding to an electric field of 0 to 625 kV cm$^ {-1}$ in our devices.\nThe electric field sensitivity of a single FND is 19 V cm$^{-1}$ Hz$^ {-1/2}$.\nWe investigate the NV charge state photodynamics on the millisecond timescale\nand find that the change in NV PL strongly depends on the rate of\nphotoexcitation. We propose a model that qualitatively explains the observed\nchanges in NV PL based on an electric field-induced redistribution of\nphotoexcited electrons from substitutional nitrogen defects to NV centers,\nleading to a transient conversion of NV$^0$ to NV$^-$ centers upon application\nof an external voltage. Our results contribute to the development of FNDs as\nreliable, all-optical, nanoscale electric field sensors in solid-state systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The nitrogen-vacancy (NV) center is a photoluminescent defect in diamond that\nexists in different charge states, NV$^-$ and NV$^0$, that are sensitive to the\nNV's nanoscale environment. Here, we show that photoluminescence (PL) from NV\ncenters in fluorescent nanodiamonds (FNDs) can be employed for all-optical\nvoltage sensing based on electric field-induced NV charge state modulation.\nMore than 95% of FNDs integrated into a capacitor device show a transient\nincrease in NV$^-$ PL intensity of up to 31% within 0.1 ms after application of\nan external voltage, accompanied by a simultaneous decrease in NV$^0$ PL. The\nchange in NV$^-$ PL increases with increasing applied voltage from 0 to 100 V,\ncorresponding to an electric field of 0 to 625 kV cm$^ {-1}$ in our devices.\nThe electric field sensitivity of a single FND is 19 V cm$^{-1}$ Hz$^ {-1/2}$.\nWe investigate the NV charge state photodynamics on the millisecond timescale\nand find that the change in NV PL strongly depends on the rate of\nphotoexcitation. We propose a model that qualitatively explains the observed\nchanges in NV PL based on an electric field-induced redistribution of\nphotoexcited electrons from substitutional nitrogen defects to NV centers,\nleading to a transient conversion of NV$^0$ to NV$^-$ centers upon application\nof an external voltage. Our results contribute to the development of FNDs as\nreliable, all-optical, nanoscale electric field sensors in solid-state systems."
                },
                "authors": [
                    {
                        "name": "Roy Styles"
                    },
                    {
                        "name": "Mengke Han"
                    },
                    {
                        "name": "Toon Goris"
                    },
                    {
                        "name": "James Partridge"
                    },
                    {
                        "name": "Brett C. Johnson"
                    },
                    {
                        "name": "Blanca del Rosal"
                    },
                    {
                        "name": "Amanda N. Abraham"
                    },
                    {
                        "name": "Heike Ebendorff-Heidepriem"
                    },
                    {
                        "name": "Brant C. Gibson"
                    },
                    {
                        "name": "Nikolai Dontschuk"
                    },
                    {
                        "name": "Jean-Philippe Tetienne"
                    },
                    {
                        "name": "Philipp Reineck"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Reineck"
                },
                "author": "Philipp Reineck",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07350v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07350v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07274v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07274v1",
                "updated": "2025-05-12T06:53:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    12,
                    6,
                    53,
                    24,
                    0,
                    132,
                    0
                ],
                "published": "2025-05-12T06:53:24Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    6,
                    53,
                    24,
                    0,
                    132,
                    0
                ],
                "title": "Cache-Efficient Posterior Sampling for Reinforcement Learning with\n  LLM-Derived Priors Across Discrete and Continuous Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Efficient Posterior Sampling for Reinforcement Learning with\n  LLM-Derived Priors Across Discrete and Continuous Domains"
                },
                "summary": "Integrating large language models (LLMs) as priors in reinforcement learning\n(RL) offers significant advantages but comes with substantial computational\ncosts. We present a principled cache-efficient framework for posterior sampling\nwith LLM-derived priors that dramatically reduces these costs while maintaining\nhigh performance. At the core of our approach is an adaptive caching mechanism,\nwhere cache parameters are meta-optimized using surrogate gradients derived\nfrom policy performance. This design enables efficient inference across both\ndiscrete text environments (e.g., TextWorld, ALFWorld) and continuous control\ndomains (e.g., MuJoCo), achieving a 3.8--4.7$\\times$ reduction in LLM queries\nand 4.0--12.0$\\times$ lower median latencies (85--93\\,ms on a consumer GPU)\nwhile retaining 96--98\\% of uncached performance. Our theoretical analysis\nprovides KL divergence bounds on approximation quality, validated empirically.\nThe framework extends to offline RL, where our CQL-Prior variant improves\nperformance by 14--29\\% and reduces training time by 38--40\\%. Extensive\nevaluations across a diverse suite of eight tasks demonstrate the\ngeneralizability and practical viability of LLM-guided RL in\nresource-constrained settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating large language models (LLMs) as priors in reinforcement learning\n(RL) offers significant advantages but comes with substantial computational\ncosts. We present a principled cache-efficient framework for posterior sampling\nwith LLM-derived priors that dramatically reduces these costs while maintaining\nhigh performance. At the core of our approach is an adaptive caching mechanism,\nwhere cache parameters are meta-optimized using surrogate gradients derived\nfrom policy performance. This design enables efficient inference across both\ndiscrete text environments (e.g., TextWorld, ALFWorld) and continuous control\ndomains (e.g., MuJoCo), achieving a 3.8--4.7$\\times$ reduction in LLM queries\nand 4.0--12.0$\\times$ lower median latencies (85--93\\,ms on a consumer GPU)\nwhile retaining 96--98\\% of uncached performance. Our theoretical analysis\nprovides KL divergence bounds on approximation quality, validated empirically.\nThe framework extends to offline RL, where our CQL-Prior variant improves\nperformance by 14--29\\% and reduces training time by 38--40\\%. Extensive\nevaluations across a diverse suite of eight tasks demonstrate the\ngeneralizability and practical viability of LLM-guided RL in\nresource-constrained settings."
                },
                "authors": [
                    {
                        "name": "Ibne Farabi Shihab"
                    },
                    {
                        "name": "Sanjeda Akter"
                    },
                    {
                        "name": "Anuj Sharma"
                    }
                ],
                "author_detail": {
                    "name": "Anuj Sharma"
                },
                "author": "Anuj Sharma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07274v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07274v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07239v1",
                "updated": "2025-05-12T05:29:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    12,
                    5,
                    29,
                    30,
                    0,
                    132,
                    0
                ],
                "published": "2025-05-12T05:29:30Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    5,
                    29,
                    30,
                    0,
                    132,
                    0
                ],
                "title": "Comet: Accelerating Private Inference for Large Language Model by\n  Predicting Activation Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comet: Accelerating Private Inference for Large Language Model by\n  Predicting Activation Sparsity"
                },
                "summary": "With the growing use of large language models (LLMs) hosted on cloud\nplatforms to offer inference services, privacy concerns about the potential\nleakage of sensitive information are escalating. Secure multi-party computation\n(MPC) is a promising solution to protect the privacy in LLM inference. However,\nMPC requires frequent inter-server communication, causing high performance\noverhead.\n  Inspired by the prevalent activation sparsity of LLMs, where most neuron are\nnot activated after non-linear activation functions, we propose an efficient\nprivate inference system, Comet. This system employs an accurate and fast\npredictor to predict the sparsity distribution of activation function output.\nAdditionally, we introduce a new private inference protocol. It efficiently and\nsecurely avoids computations involving zero values by exploiting the spatial\nlocality of the predicted sparse distribution. While this computation-avoidance\napproach impacts the spatiotemporal continuity of KV cache entries, we address\nthis challenge with a low-communication overhead cache refilling strategy that\nmerges miss requests and incorporates a prefetching mechanism. Finally, we\nevaluate Comet on four common LLMs and compare it with six state-of-the-art\nprivate inference systems. Comet achieves a 1.87x-2.63x speedup and a\n1.94x-2.64x communication reduction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing use of large language models (LLMs) hosted on cloud\nplatforms to offer inference services, privacy concerns about the potential\nleakage of sensitive information are escalating. Secure multi-party computation\n(MPC) is a promising solution to protect the privacy in LLM inference. However,\nMPC requires frequent inter-server communication, causing high performance\noverhead.\n  Inspired by the prevalent activation sparsity of LLMs, where most neuron are\nnot activated after non-linear activation functions, we propose an efficient\nprivate inference system, Comet. This system employs an accurate and fast\npredictor to predict the sparsity distribution of activation function output.\nAdditionally, we introduce a new private inference protocol. It efficiently and\nsecurely avoids computations involving zero values by exploiting the spatial\nlocality of the predicted sparse distribution. While this computation-avoidance\napproach impacts the spatiotemporal continuity of KV cache entries, we address\nthis challenge with a low-communication overhead cache refilling strategy that\nmerges miss requests and incorporates a prefetching mechanism. Finally, we\nevaluate Comet on four common LLMs and compare it with six state-of-the-art\nprivate inference systems. Comet achieves a 1.87x-2.63x speedup and a\n1.94x-2.64x communication reduction."
                },
                "authors": [
                    {
                        "name": "Guang Yan"
                    },
                    {
                        "name": "Yuhui Zhang"
                    },
                    {
                        "name": "Zimu Guo"
                    },
                    {
                        "name": "Lutan Zhao"
                    },
                    {
                        "name": "Xiaojun Chen"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "arxiv_doi": "10.1109/SP61157.2025.00182",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/SP61157.2025.00182",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.07239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to SP 2025",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07203v1",
                "updated": "2025-05-12T03:22:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    12,
                    3,
                    22,
                    29,
                    0,
                    132,
                    0
                ],
                "published": "2025-05-12T03:22:29Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    3,
                    22,
                    29,
                    0,
                    132,
                    0
                ],
                "title": "PrefillOnly: An Inference Engine for Prefill-only Workloads in Large\n  Language Model Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefillOnly: An Inference Engine for Prefill-only Workloads in Large\n  Language Model Applications"
                },
                "summary": "Besides typical generative applications, like ChatGPT, GitHub Copilot, and\nCursor, we observe an emerging trend that LLMs are increasingly used in\ntraditional discriminative tasks, such as recommendation, credit verification,\nand data labeling. The key characteristic of these emerging use cases is that\nthe LLM generates only a single output token, rather than an arbitrarily long\nsequence of tokens. We call this prefill-only workload. However, since existing\nLLM engines assume arbitrary output lengths, they fail to leverage the unique\nproperties of prefill-only workloads. In this paper, we present PrefillOnly,\nthe first LLM inference engine that improves the inference throughput and\nlatency by fully embracing the properties of prefill-only workloads. First,\nsince it generates only one token, PrefillOnly only needs to store the KV cache\nof only the last computed layer, rather than of all layers. This drastically\nreduces the GPU memory footprint of LLM inference and allows handling long\ninputs without using solutions that reduces throughput, such as cross-GPU KV\ncache parallelization. Second, because the output length is fixed, rather than\narbitrary, PrefillOnly can precisely determine the job completion time (JCT) of\neach prefill-only request before it starts. This enables efficient JCT-aware\nscheduling policies such as shortest remaining job first. PrefillOnly can\nprocess upto 4x larger queries per second without inflating average and P99\nlatency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Besides typical generative applications, like ChatGPT, GitHub Copilot, and\nCursor, we observe an emerging trend that LLMs are increasingly used in\ntraditional discriminative tasks, such as recommendation, credit verification,\nand data labeling. The key characteristic of these emerging use cases is that\nthe LLM generates only a single output token, rather than an arbitrarily long\nsequence of tokens. We call this prefill-only workload. However, since existing\nLLM engines assume arbitrary output lengths, they fail to leverage the unique\nproperties of prefill-only workloads. In this paper, we present PrefillOnly,\nthe first LLM inference engine that improves the inference throughput and\nlatency by fully embracing the properties of prefill-only workloads. First,\nsince it generates only one token, PrefillOnly only needs to store the KV cache\nof only the last computed layer, rather than of all layers. This drastically\nreduces the GPU memory footprint of LLM inference and allows handling long\ninputs without using solutions that reduces throughput, such as cross-GPU KV\ncache parallelization. Second, because the output length is fixed, rather than\narbitrary, PrefillOnly can precisely determine the job completion time (JCT) of\neach prefill-only request before it starts. This enables efficient JCT-aware\nscheduling policies such as shortest remaining job first. PrefillOnly can\nprocess upto 4x larger queries per second without inflating average and P99\nlatency."
                },
                "authors": [
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Bowen Wang"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Yiming Cheng"
                    },
                    {
                        "name": "Qing Lan"
                    },
                    {
                        "name": "Hejian Sang"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Xiaoxuan Liu"
                    },
                    {
                        "name": "Yifan Qiao"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06901v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06901v1",
                "updated": "2025-05-11T08:44:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    11,
                    8,
                    44,
                    31,
                    6,
                    131,
                    0
                ],
                "published": "2025-05-11T08:44:31Z",
                "published_parsed": [
                    2025,
                    5,
                    11,
                    8,
                    44,
                    31,
                    6,
                    131,
                    0
                ],
                "title": "Ecco: Improving Memory Bandwidth and Capacity for LLMs via Entropy-aware\n  Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ecco: Improving Memory Bandwidth and Capacity for LLMs via Entropy-aware\n  Cache Compression"
                },
                "summary": "Large language models (LLMs) have demonstrated transformative capabilities\nacross diverse artificial intelligence applications, yet their deployment is\nhindered by substantial memory and computational demands, especially in\nresource-constrained environments. Quantization techniques have emerged as a\ncritical solution, reducing data precision to enhance memory and computational\nefficiency. However, existing methods often suffer from high runtime overheads\nand potential accuracy degradation. To address these challenges, we propose\nEcco, an entropy-based cache compression technique tailored for LLMs. Ecco\ncombines group-wise and non-uniform quantization with pre-defined shared\nk-means patterns and Huffman coding to exploit the inherent entropy\ncharacteristics of LLM cache data. Recognizing the inefficiencies of\ntraditional Huffman coding in terms of parallelism and latency, we introduce a\nnovel parallel Huffman-based decoding process with a multi-stage pipeline\ndesign, reducing latency by two orders of magnitude and achieving throughput\ncomparable to GPU L2 caches. Comprehensive evaluations demonstrate that Ecco\nachieves an up to 2.9$\\times$ and 1.9$\\times$ speedup over the state-of-the-art\nAWQ and SmoothQuant framework, 2.4$\\times$ over the Olive accelerator, all\nwhile increasing memory capacity by nearly 4$\\times$ and maintaining\nstate-of-the-art LLM accuracy. These results underscore the effectiveness of\nour entropy-based cache compression in enhancing LLM performance and\nefficiency, paving the way for more deployable large-scale AI models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated transformative capabilities\nacross diverse artificial intelligence applications, yet their deployment is\nhindered by substantial memory and computational demands, especially in\nresource-constrained environments. Quantization techniques have emerged as a\ncritical solution, reducing data precision to enhance memory and computational\nefficiency. However, existing methods often suffer from high runtime overheads\nand potential accuracy degradation. To address these challenges, we propose\nEcco, an entropy-based cache compression technique tailored for LLMs. Ecco\ncombines group-wise and non-uniform quantization with pre-defined shared\nk-means patterns and Huffman coding to exploit the inherent entropy\ncharacteristics of LLM cache data. Recognizing the inefficiencies of\ntraditional Huffman coding in terms of parallelism and latency, we introduce a\nnovel parallel Huffman-based decoding process with a multi-stage pipeline\ndesign, reducing latency by two orders of magnitude and achieving throughput\ncomparable to GPU L2 caches. Comprehensive evaluations demonstrate that Ecco\nachieves an up to 2.9$\\times$ and 1.9$\\times$ speedup over the state-of-the-art\nAWQ and SmoothQuant framework, 2.4$\\times$ over the Olive accelerator, all\nwhile increasing memory capacity by nearly 4$\\times$ and maintaining\nstate-of-the-art LLM accuracy. These results underscore the effectiveness of\nour entropy-based cache compression in enhancing LLM performance and\nefficiency, paving the way for more deployable large-scale AI models."
                },
                "authors": [
                    {
                        "name": "Feng Cheng"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Chiyue Wei"
                    },
                    {
                        "name": "Junyao Zhang"
                    },
                    {
                        "name": "Changchun Zhou"
                    },
                    {
                        "name": "Edward Hanson"
                    },
                    {
                        "name": "Jiaqi Zhang"
                    },
                    {
                        "name": "Xiaoxiao Liu"
                    },
                    {
                        "name": "Hai \"Helen\" Li"
                    },
                    {
                        "name": "Yiran Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yiran Chen"
                },
                "author": "Yiran Chen",
                "arxiv_comment": "ISCA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06901v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06901v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06625v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06625v1",
                "updated": "2025-05-10T12:16:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    10,
                    12,
                    16,
                    50,
                    5,
                    130,
                    0
                ],
                "published": "2025-05-10T12:16:50Z",
                "published_parsed": [
                    2025,
                    5,
                    10,
                    12,
                    16,
                    50,
                    5,
                    130,
                    0
                ],
                "title": "CaMDN: Enhancing Cache Efficiency for Multi-tenant DNNs on Integrated\n  NPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CaMDN: Enhancing Cache Efficiency for Multi-tenant DNNs on Integrated\n  NPUs"
                },
                "summary": "With the rapid development of DNN applications, multi-tenant execution, where\nmultiple DNNs are co-located on a single SoC, is becoming a prevailing trend.\nAlthough many methods are proposed in prior works to improve multi-tenant\nperformance, the impact of shared cache is not well studied. This paper\nproposes CaMDN, an architecture-scheduling co-design to enhance cache\nefficiency for multi-tenant DNNs on integrated NPUs. Specifically, a\nlightweight architecture is proposed to support model-exclusive, NPU-controlled\nregions inside shared cache to eliminate unexpected cache contention. Moreover,\na cache scheduling method is proposed to improve shared cache utilization. In\nparticular, it includes a cache-aware mapping method for adaptability to the\nvarying available cache capacity and a dynamic allocation algorithm to adjust\nthe usage among co-located DNNs at runtime. Compared to prior works, CaMDN\nreduces the memory access by 33.4% on average and achieves a model speedup of\nup to 2.56$\\times$ (1.88$\\times$ on average).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development of DNN applications, multi-tenant execution, where\nmultiple DNNs are co-located on a single SoC, is becoming a prevailing trend.\nAlthough many methods are proposed in prior works to improve multi-tenant\nperformance, the impact of shared cache is not well studied. This paper\nproposes CaMDN, an architecture-scheduling co-design to enhance cache\nefficiency for multi-tenant DNNs on integrated NPUs. Specifically, a\nlightweight architecture is proposed to support model-exclusive, NPU-controlled\nregions inside shared cache to eliminate unexpected cache contention. Moreover,\na cache scheduling method is proposed to improve shared cache utilization. In\nparticular, it includes a cache-aware mapping method for adaptability to the\nvarying available cache capacity and a dynamic allocation algorithm to adjust\nthe usage among co-located DNNs at runtime. Compared to prior works, CaMDN\nreduces the memory access by 33.4% on average and achieves a model speedup of\nup to 2.56$\\times$ (1.88$\\times$ on average)."
                },
                "authors": [
                    {
                        "name": "Tianhao Cai"
                    },
                    {
                        "name": "Liang Wang"
                    },
                    {
                        "name": "Limin Xiao"
                    },
                    {
                        "name": "Meng Han"
                    },
                    {
                        "name": "Zeyu Wang"
                    },
                    {
                        "name": "Lin Sun"
                    },
                    {
                        "name": "Xiaojian Liao"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojian Liao"
                },
                "author": "Xiaojian Liao",
                "arxiv_comment": "7 pages, 9 figures. This paper has been accepted to the 2025 Design\n  Automation Conference (DAC)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06625v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06625v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06556v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06556v1",
                "updated": "2025-05-10T07:57:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    10,
                    7,
                    57,
                    2,
                    5,
                    130,
                    0
                ],
                "published": "2025-05-10T07:57:02Z",
                "published_parsed": [
                    2025,
                    5,
                    10,
                    7,
                    57,
                    2,
                    5,
                    130,
                    0
                ],
                "title": "TierBase: A Workload-Driven Cost-Optimized Key-Value Store",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TierBase: A Workload-Driven Cost-Optimized Key-Value Store"
                },
                "summary": "In the current era of data-intensive applications, the demand for\nhigh-performance, cost-effective storage solutions is paramount. This paper\nintroduces a Space-Performance Cost Model for key-value store, designed to\nguide cost-effective storage configuration decisions. The model quantifies the\ntrade-offs between performance and storage costs, providing a framework for\noptimizing resource allocation in large-scale data serving environments. Guided\nby this cost model, we present TierBase, a distributed key-value store\ndeveloped by Ant Group that optimizes total cost by strategically synchronizing\ndata between cache and storage tiers, maximizing resource utilization and\neffectively handling skewed workloads. To enhance cost-efficiency, TierBase\nincorporates several optimization techniques, including pre-trained data\ncompression, elastic threading mechanisms, and the utilization of persistent\nmemory. We detail TierBase's architecture, key components, and the\nimplementation of cost optimization strategies. Extensive evaluations using\nboth synthetic benchmarks and real-world workloads demonstrate TierBase's\nsuperior cost-effectiveness compared to existing solutions. Furthermore, case\nstudies from Ant Group's production environments showcase TierBase's ability to\nachieve up to 62% cost reduction in primary scenarios, highlighting its\npractical impact in large-scale online data serving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the current era of data-intensive applications, the demand for\nhigh-performance, cost-effective storage solutions is paramount. This paper\nintroduces a Space-Performance Cost Model for key-value store, designed to\nguide cost-effective storage configuration decisions. The model quantifies the\ntrade-offs between performance and storage costs, providing a framework for\noptimizing resource allocation in large-scale data serving environments. Guided\nby this cost model, we present TierBase, a distributed key-value store\ndeveloped by Ant Group that optimizes total cost by strategically synchronizing\ndata between cache and storage tiers, maximizing resource utilization and\neffectively handling skewed workloads. To enhance cost-efficiency, TierBase\nincorporates several optimization techniques, including pre-trained data\ncompression, elastic threading mechanisms, and the utilization of persistent\nmemory. We detail TierBase's architecture, key components, and the\nimplementation of cost optimization strategies. Extensive evaluations using\nboth synthetic benchmarks and real-world workloads demonstrate TierBase's\nsuperior cost-effectiveness compared to existing solutions. Furthermore, case\nstudies from Ant Group's production environments showcase TierBase's ability to\nachieve up to 62% cost reduction in primary scenarios, highlighting its\npractical impact in large-scale online data serving."
                },
                "authors": [
                    {
                        "name": "Zhitao Shen"
                    },
                    {
                        "name": "Shiyu Yang"
                    },
                    {
                        "name": "Weibo Chen"
                    },
                    {
                        "name": "Kunming Wang"
                    },
                    {
                        "name": "Yue Li"
                    },
                    {
                        "name": "Jiabao Jin"
                    },
                    {
                        "name": "Wei Jia"
                    },
                    {
                        "name": "Junwei Chen"
                    },
                    {
                        "name": "Yuan Su"
                    },
                    {
                        "name": "Xiaoxia Duan"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Jie Song"
                    },
                    {
                        "name": "Ruoyi Ruan"
                    },
                    {
                        "name": "Xuemin Lin"
                    }
                ],
                "author_detail": {
                    "name": "Xuemin Lin"
                },
                "author": "Xuemin Lin",
                "arxiv_comment": "Accepted by ICDE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06556v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06556v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07872v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07872v1",
                "updated": "2025-05-09T21:05:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    21,
                    5,
                    20,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T21:05:20Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    21,
                    5,
                    20,
                    4,
                    129,
                    0
                ],
                "title": "Revenue Optimization in Video Caching Networks with Privacy-Preserving\n  Demand Predictions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revenue Optimization in Video Caching Networks with Privacy-Preserving\n  Demand Predictions"
                },
                "summary": "Performance of video streaming, which accounts for most of the traffic in\nwireless communication, can be significantly improved by caching popular videos\nat the wireless edge. Determining the cache content that optimizes performance\n(defined via a revenue function) is thus an important task, and prediction of\nthe future demands based on past history can make this process much more\nefficient. However, since practical video caching networks involve various\nparties (e.g., users, isp, and csp) that do not wish to reveal information such\nas past history to each other, privacy-preserving solutions are required.\nMotivated by this, we propose a proactive caching method based on users'\nprivacy-preserving multi-slot future demand predictions -- obtained from a\ntrained Transformer -- to optimize revenue. Specifically, we first use a\nprivacy-preserving fl algorithm to train a Transformer to predict multi-slot\nfuture demands of the users. However, prediction accuracy is not perfect and\ndecreases the farther into the future the prediction is done. We model the\nimpact of prediction errors invoking the file popularities, based on which we\nformulate a long-term system revenue optimization to make the cache placement\ndecisions. As the formulated problem is NP-hard, we use a greedy algorithm to\nefficiently obtain an approximate solution. Simulation results validate that\n(i) the fl solution achieves results close to the centralized\n(non-privacy-preserving) solution and (ii) optimization of revenue may provide\ndifferent solutions than the classical chr criterion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance of video streaming, which accounts for most of the traffic in\nwireless communication, can be significantly improved by caching popular videos\nat the wireless edge. Determining the cache content that optimizes performance\n(defined via a revenue function) is thus an important task, and prediction of\nthe future demands based on past history can make this process much more\nefficient. However, since practical video caching networks involve various\nparties (e.g., users, isp, and csp) that do not wish to reveal information such\nas past history to each other, privacy-preserving solutions are required.\nMotivated by this, we propose a proactive caching method based on users'\nprivacy-preserving multi-slot future demand predictions -- obtained from a\ntrained Transformer -- to optimize revenue. Specifically, we first use a\nprivacy-preserving fl algorithm to train a Transformer to predict multi-slot\nfuture demands of the users. However, prediction accuracy is not perfect and\ndecreases the farther into the future the prediction is done. We model the\nimpact of prediction errors invoking the file popularities, based on which we\nformulate a long-term system revenue optimization to make the cache placement\ndecisions. As the formulated problem is NP-hard, we use a greedy algorithm to\nefficiently obtain an approximate solution. Simulation results validate that\n(i) the fl solution achieves results close to the centralized\n(non-privacy-preserving) solution and (ii) optimization of revenue may provide\ndifferent solutions than the classical chr criterion."
                },
                "authors": [
                    {
                        "name": "Yijing Zhang"
                    },
                    {
                        "name": "Ferdous Pervej"
                    },
                    {
                        "name": "Andreas F. Molisch"
                    }
                ],
                "author_detail": {
                    "name": "Andreas F. Molisch"
                },
                "author": "Andreas F. Molisch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07872v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07872v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06095v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06095v3",
                "updated": "2025-05-09T07:26:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    7,
                    26,
                    29,
                    4,
                    129,
                    0
                ],
                "published": "2024-06-10T08:26:27Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    8,
                    26,
                    27,
                    0,
                    162,
                    0
                ],
                "title": "An extension of C++ with memory-centric specifications for HPC to reduce\n  memory footprints and streamline MPI development",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An extension of C++ with memory-centric specifications for HPC to reduce\n  memory footprints and streamline MPI development"
                },
                "summary": "The C++ programming language and its cousins lean towards a\nmemory-inefficient storage of structs: The compiler inserts helper bits such\nthat individual instance variables fit to byte or cache boundaries, while it is\nnot able to exploit knowledge about the range of integers, enums or bitsets.\nFurthermore, the language provides neither support for data exchange via MPI\nnor for arbitrary floating-point precisions. We propose C++ attributes through\nwhich developers can guide the compiler what memory arrangements would be\nbeneficial: Can multiple booleans or integers with limited range be squeezed\ninto one bit field, do floating point numbers hold fewer significant bits than\nin the IEEE standard, or does the code benefit from a MPI datatype for subsets\nof attributes? The extension offers the opportunity to fall back to normal\nalignment via plain C++ assignments, no dependencies upon external libraries\nare introduced, and the resulting code remains standard C++ subject to some\nweakened guarantees on addresses and pointer arithmetics. Our work implements\nthe language annotations within LLVM and demonstrates their potential impact,\nboth upon the runtime and the memory footprint, through smoothed particle\nhydrodynamics (SPH) benchmarks. They uncover the potential gains in terms of\nperformance and development productivity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The C++ programming language and its cousins lean towards a\nmemory-inefficient storage of structs: The compiler inserts helper bits such\nthat individual instance variables fit to byte or cache boundaries, while it is\nnot able to exploit knowledge about the range of integers, enums or bitsets.\nFurthermore, the language provides neither support for data exchange via MPI\nnor for arbitrary floating-point precisions. We propose C++ attributes through\nwhich developers can guide the compiler what memory arrangements would be\nbeneficial: Can multiple booleans or integers with limited range be squeezed\ninto one bit field, do floating point numbers hold fewer significant bits than\nin the IEEE standard, or does the code benefit from a MPI datatype for subsets\nof attributes? The extension offers the opportunity to fall back to normal\nalignment via plain C++ assignments, no dependencies upon external libraries\nare introduced, and the resulting code remains standard C++ subject to some\nweakened guarantees on addresses and pointer arithmetics. Our work implements\nthe language annotations within LLVM and demonstrates their potential impact,\nboth upon the runtime and the memory footprint, through smoothed particle\nhydrodynamics (SPH) benchmarks. They uncover the potential gains in terms of\nperformance and development productivity."
                },
                "authors": [
                    {
                        "name": "Pawel K. Radtke"
                    },
                    {
                        "name": "Cristian G. Barrera-Hinojosa"
                    },
                    {
                        "name": "Mladen Ivkovic"
                    },
                    {
                        "name": "Tobias Weinzierl"
                    }
                ],
                "author_detail": {
                    "name": "Tobias Weinzierl"
                },
                "author": "Tobias Weinzierl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06095v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06095v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05829v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05829v1",
                "updated": "2025-05-09T06:56:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    6,
                    56,
                    17,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T06:56:17Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    6,
                    56,
                    17,
                    4,
                    129,
                    0
                ],
                "title": "Accelerating Diffusion Transformer via Increment-Calibrated Caching with\n  Channel-Aware Singular Value Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer via Increment-Calibrated Caching with\n  Channel-Aware Singular Value Decomposition"
                },
                "summary": "Diffusion transformer (DiT) models have achieved remarkable success in image\ngeneration, thanks for their exceptional generative capabilities and\nscalability. Nonetheless, the iterative nature of diffusion models (DMs)\nresults in high computation complexity, posing challenges for deployment.\nAlthough existing cache-based acceleration methods try to utilize the inherent\ntemporal similarity to skip redundant computations of DiT, the lack of\ncorrection may induce potential quality degradation. In this paper, we propose\nincrement-calibrated caching, a training-free method for DiT acceleration,\nwhere the calibration parameters are generated from the pre-trained model\nitself with low-rank approximation. To deal with the possible correction\nfailure arising from outlier activations, we introduce channel-aware Singular\nValue Decomposition (SVD), which further strengthens the calibration effect.\nExperimental results show that our method always achieve better performance\nthan existing naive caching methods with a similar computation resource budget.\nWhen compared with 35-step DDIM, our method eliminates more than 45%\ncomputation and improves IS by 12 at the cost of less than 0.06 FID increase.\nCode is available at https://github.com/ccccczzy/icc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformer (DiT) models have achieved remarkable success in image\ngeneration, thanks for their exceptional generative capabilities and\nscalability. Nonetheless, the iterative nature of diffusion models (DMs)\nresults in high computation complexity, posing challenges for deployment.\nAlthough existing cache-based acceleration methods try to utilize the inherent\ntemporal similarity to skip redundant computations of DiT, the lack of\ncorrection may induce potential quality degradation. In this paper, we propose\nincrement-calibrated caching, a training-free method for DiT acceleration,\nwhere the calibration parameters are generated from the pre-trained model\nitself with low-rank approximation. To deal with the possible correction\nfailure arising from outlier activations, we introduce channel-aware Singular\nValue Decomposition (SVD), which further strengthens the calibration effect.\nExperimental results show that our method always achieve better performance\nthan existing naive caching methods with a similar computation resource budget.\nWhen compared with 35-step DDIM, our method eliminates more than 45%\ncomputation and improves IS by 12 at the cost of less than 0.06 FID increase.\nCode is available at https://github.com/ccccczzy/icc."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Chen"
                    },
                    {
                        "name": "Keyi Li"
                    },
                    {
                        "name": "Yifan Jia"
                    },
                    {
                        "name": "Le Ye"
                    },
                    {
                        "name": "Yufei Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yufei Ma"
                },
                "author": "Yufei Ma",
                "arxiv_comment": "accepted by CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05829v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05829v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05772v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05772v1",
                "updated": "2025-05-09T04:17:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    4,
                    17,
                    5,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T04:17:05Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    4,
                    17,
                    5,
                    4,
                    129,
                    0
                ],
                "title": "Sparse Attention Remapping with Clustering for Efficient LLM Decoding on\n  PIM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Attention Remapping with Clustering for Efficient LLM Decoding on\n  PIM"
                },
                "summary": "Transformer-based models are the foundation of modern machine learning, but\ntheir execution, particularly during autoregressive decoding in large language\nmodels (LLMs), places significant pressure on memory systems due to frequent\nmemory accesses and growing key-value (KV) caches. This creates a bottleneck in\nmemory bandwidth, especially as context lengths increase. Processing-in-memory\n(PIM) architectures are a promising solution, offering high internal bandwidth\nand compute parallelism near memory. However, current PIM designs are primarily\noptimized for dense attention and struggle with the dynamic, irregular access\npatterns introduced by modern KV cache sparsity techniques. Consequently, they\nsuffer from workload imbalance, reducing throughput and resource utilization.\nIn this work, we propose STARC, a novel sparsity-optimized data mapping scheme\ntailored specifically for efficient LLM decoding on PIM architectures. STARC\nclusters KV pairs by semantic similarity and maps them to contiguous memory\nregions aligned with PIM bank structures. During decoding, queries retrieve\nrelevant tokens at cluster granularity by matching against precomputed\ncentroids, enabling selective attention and parallel processing without\nfrequent reclustering or data movement overhead. Experiments on the HBM-PIM\nsystem show that, compared to common token-wise sparsity methods, STARC reduces\nattention-layer latency by 19%--31% and energy consumption by 19%--27%. Under a\nKV cache budget of 1024, it achieves up to 54%--74% latency reduction and\n45%--67% energy reduction compared to full KV cache retrieval. Meanwhile, STARC\nmaintains model accuracy comparable to state-of-the-art sparse attention\nmethods, demonstrating its effectiveness in enabling efficient and\nhardware-friendly long-context LLM inference on PIM architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based models are the foundation of modern machine learning, but\ntheir execution, particularly during autoregressive decoding in large language\nmodels (LLMs), places significant pressure on memory systems due to frequent\nmemory accesses and growing key-value (KV) caches. This creates a bottleneck in\nmemory bandwidth, especially as context lengths increase. Processing-in-memory\n(PIM) architectures are a promising solution, offering high internal bandwidth\nand compute parallelism near memory. However, current PIM designs are primarily\noptimized for dense attention and struggle with the dynamic, irregular access\npatterns introduced by modern KV cache sparsity techniques. Consequently, they\nsuffer from workload imbalance, reducing throughput and resource utilization.\nIn this work, we propose STARC, a novel sparsity-optimized data mapping scheme\ntailored specifically for efficient LLM decoding on PIM architectures. STARC\nclusters KV pairs by semantic similarity and maps them to contiguous memory\nregions aligned with PIM bank structures. During decoding, queries retrieve\nrelevant tokens at cluster granularity by matching against precomputed\ncentroids, enabling selective attention and parallel processing without\nfrequent reclustering or data movement overhead. Experiments on the HBM-PIM\nsystem show that, compared to common token-wise sparsity methods, STARC reduces\nattention-layer latency by 19%--31% and energy consumption by 19%--27%. Under a\nKV cache budget of 1024, it achieves up to 54%--74% latency reduction and\n45%--67% energy reduction compared to full KV cache retrieval. Meanwhile, STARC\nmaintains model accuracy comparable to state-of-the-art sparse attention\nmethods, demonstrating its effectiveness in enabling efficient and\nhardware-friendly long-context LLM inference on PIM architectures."
                },
                "authors": [
                    {
                        "name": "Zehao Fan"
                    },
                    {
                        "name": "Garrett Gagnon"
                    },
                    {
                        "name": "Zhenyu Liu"
                    },
                    {
                        "name": "Liu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Liu Liu"
                },
                "author": "Liu Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05772v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05772v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17264v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17264v3",
                "updated": "2025-05-09T00:31:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    0,
                    31,
                    24,
                    4,
                    129,
                    0
                ],
                "published": "2024-09-25T18:21:05Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    18,
                    21,
                    5,
                    2,
                    269,
                    0
                ],
                "title": "Medha: Efficiently Serving Multi-Million Context Length LLM Inference\n  Requests Without Approximations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medha: Efficiently Serving Multi-Million Context Length LLM Inference\n  Requests Without Approximations"
                },
                "summary": "As large language models (LLMs) handle increasingly longer contexts, serving\nlong inference requests of millions of tokens presents unique challenges. We\nshow that existing work for long context inference is largely based on\ntechniques from long context training, and does not handle the high variability\nin input lengths during inference. This leads to inefficient resource\nutilization, server fragmentation, and head-of-line (HOL) blocking.\n  We present Medha, an end-to-end system for efficient long-context LLM\ninference that addresses these challenges through fine-grained time sharing.\nMedha introduces three key innovations: (1) the mechanism of adaptive prefill\nchunking to help mitigate HOL blocking with preemption; (2) two new parallelism\nstrategies: Sequence Pipeline Parallelism (SPP) to reduce time-to-first-token\nby pipelining prefill chunks, and KV-Cache Parallelism (KVP) to lower\ntime-peroutput-token by distributing decoding across servers; and (3) a novel\ninput-length aware least remaining slack scheduling to meet Service Level\nObjectives (SLOs).\n  Medha enables exact inference scaling beyond 10 million tokens, maintaining\nhigh throughput and low latency across mixed-length workloads. Compared to\nstate-of-the-art systems, Medha reduces server fragmentation, cuts median\nlatency by up to 30x, and improves throughput by over 5x, delivering\nproduction-scale long-context inference without compromising performance on\nshorter requests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) handle increasingly longer contexts, serving\nlong inference requests of millions of tokens presents unique challenges. We\nshow that existing work for long context inference is largely based on\ntechniques from long context training, and does not handle the high variability\nin input lengths during inference. This leads to inefficient resource\nutilization, server fragmentation, and head-of-line (HOL) blocking.\n  We present Medha, an end-to-end system for efficient long-context LLM\ninference that addresses these challenges through fine-grained time sharing.\nMedha introduces three key innovations: (1) the mechanism of adaptive prefill\nchunking to help mitigate HOL blocking with preemption; (2) two new parallelism\nstrategies: Sequence Pipeline Parallelism (SPP) to reduce time-to-first-token\nby pipelining prefill chunks, and KV-Cache Parallelism (KVP) to lower\ntime-peroutput-token by distributing decoding across servers; and (3) a novel\ninput-length aware least remaining slack scheduling to meet Service Level\nObjectives (SLOs).\n  Medha enables exact inference scaling beyond 10 million tokens, maintaining\nhigh throughput and low latency across mixed-length workloads. Compared to\nstate-of-the-art systems, Medha reduces server fragmentation, cuts median\nlatency by up to 30x, and improves throughput by over 5x, delivering\nproduction-scale long-context inference without compromising performance on\nshorter requests."
                },
                "authors": [
                    {
                        "name": "Amey Agrawal"
                    },
                    {
                        "name": "Haoran Qiu"
                    },
                    {
                        "name": "Junda Chen"
                    },
                    {
                        "name": "Íñigo Goiri"
                    },
                    {
                        "name": "Chaojie Zhang"
                    },
                    {
                        "name": "Rayyan Shahid"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    },
                    {
                        "name": "Alexey Tumanov"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17264v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17264v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05251v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05251v1",
                "updated": "2025-05-08T13:56:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    13,
                    56,
                    20,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T13:56:20Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    13,
                    56,
                    20,
                    3,
                    128,
                    0
                ],
                "title": "High Altitude Platform-Based Caching and Multicasting for Rural\n  Connectivity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High Altitude Platform-Based Caching and Multicasting for Rural\n  Connectivity"
                },
                "summary": "Providing efficient and reliable content delivery in rural areas remains a\nsignificant challenge due to the lack of communication infrastructure. To\nbridge the digital divide, this paper investigates the potential of leveraging\nmultiple high-altitude platforms (HAPs) for energy-efficient content delivery\nin wide rural regions. Each caching-enabled HAP is equipped with both\nFree-Space Optical (FSO) transceivers for backhaul links and Radio Frequency\n(RF) antenna arrays for access links. To further enhance network efficiency, we\nconsider a network coding-based multicasting scheme, where different types of\ncontent are treated as distinct multicast sessions. With the objective of\nminimizing long-term power cost, we propose a hierarchical framework that\nintegrates deep reinforcement learn-ing (DRL) and convex optimization to\njointly optimize dynamic caching strategies and resource allocation across the\nnetwork. Simulation results demonstrate that our approach significantly reduces\npower cost compared to several baseline approaches, providing a practical\nsolution for improving rural connectivity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Providing efficient and reliable content delivery in rural areas remains a\nsignificant challenge due to the lack of communication infrastructure. To\nbridge the digital divide, this paper investigates the potential of leveraging\nmultiple high-altitude platforms (HAPs) for energy-efficient content delivery\nin wide rural regions. Each caching-enabled HAP is equipped with both\nFree-Space Optical (FSO) transceivers for backhaul links and Radio Frequency\n(RF) antenna arrays for access links. To further enhance network efficiency, we\nconsider a network coding-based multicasting scheme, where different types of\ncontent are treated as distinct multicast sessions. With the objective of\nminimizing long-term power cost, we propose a hierarchical framework that\nintegrates deep reinforcement learn-ing (DRL) and convex optimization to\njointly optimize dynamic caching strategies and resource allocation across the\nnetwork. Simulation results demonstrate that our approach significantly reduces\npower cost compared to several baseline approaches, providing a practical\nsolution for improving rural connectivity."
                },
                "authors": [
                    {
                        "name": "Yongqiang Zhang"
                    },
                    {
                        "name": "Mustafa A. Kishk"
                    },
                    {
                        "name": "Mohamed-Slim Alouini"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed-Slim Alouini"
                },
                "author": "Mohamed-Slim Alouini",
                "arxiv_comment": "13 pages, 8 figures, submitted to IEEE journals for possible\n  publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05251v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05251v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "49",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.4.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05130v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05130v1",
                "updated": "2025-05-08T11:07:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    11,
                    7,
                    35,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T11:07:35Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    11,
                    7,
                    35,
                    3,
                    128,
                    0
                ],
                "title": "CacheFL: Efficient Federated Cache Model Fine-Tuning for Vision-Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheFL: Efficient Federated Cache Model Fine-Tuning for Vision-Language\n  Models"
                },
                "summary": "Large pre-trained Vision-Language Models (VLMs), such as Contrastive\nLanguage-Image Pre-training (CLIP), have exhibited remarkable zero-shot\nperformance across various image classification tasks. Fine-tuning these models\non domain-specific datasets further enhances their effectiveness for downstream\napplications. However, fine-tuning in cloud environments raises significant\nconcerns regarding data security and privacy. Federated Learning (FL) offers a\ndecentralized solution by enabling model training across local clients without\ncentralizing sensitive data, but the high communication and computation costs\nof transmitting full pre-trained models during training limit its scalability.\nAdditionally, non-Independent and Identically Distributed (non-IID) data across\nlocal clients can negatively impact model convergence and performance. To\naddress these challenges, we propose CacheFL, a novel federated learning method\nthat replaces traditional full model fine-tuning with lightweight cache model\nfine-tuning. The cache model is initialized using a class-balanced dataset\ngenerated by a generative pre-trained model, effectively mitigating the impact\nof non-IID data. This cache model is then distributed to local clients for\nfine-tuning, and the updated parameters from each client are aggregated on the\nserver and redistributed. With the updated cache model, the classification\nperformance of CLIP is improved after just a few epochs. By limiting the\ntraining and communication to the cache model, CacheFL significantly reduces\nresource demands while ensuring data privacy and security. Extensive\nexperiments conducted on ImageNet and 10 additional datasets demonstrate that\nCacheFL outperforms traditional approaches in terms of classification accuracy,\nresource efficiency, and privacy preservation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large pre-trained Vision-Language Models (VLMs), such as Contrastive\nLanguage-Image Pre-training (CLIP), have exhibited remarkable zero-shot\nperformance across various image classification tasks. Fine-tuning these models\non domain-specific datasets further enhances their effectiveness for downstream\napplications. However, fine-tuning in cloud environments raises significant\nconcerns regarding data security and privacy. Federated Learning (FL) offers a\ndecentralized solution by enabling model training across local clients without\ncentralizing sensitive data, but the high communication and computation costs\nof transmitting full pre-trained models during training limit its scalability.\nAdditionally, non-Independent and Identically Distributed (non-IID) data across\nlocal clients can negatively impact model convergence and performance. To\naddress these challenges, we propose CacheFL, a novel federated learning method\nthat replaces traditional full model fine-tuning with lightweight cache model\nfine-tuning. The cache model is initialized using a class-balanced dataset\ngenerated by a generative pre-trained model, effectively mitigating the impact\nof non-IID data. This cache model is then distributed to local clients for\nfine-tuning, and the updated parameters from each client are aggregated on the\nserver and redistributed. With the updated cache model, the classification\nperformance of CLIP is improved after just a few epochs. By limiting the\ntraining and communication to the cache model, CacheFL significantly reduces\nresource demands while ensuring data privacy and security. Extensive\nexperiments conducted on ImageNet and 10 additional datasets demonstrate that\nCacheFL outperforms traditional approaches in terms of classification accuracy,\nresource efficiency, and privacy preservation."
                },
                "authors": [
                    {
                        "name": "Mengjun Yi"
                    },
                    {
                        "name": "Hanwen Zhang"
                    },
                    {
                        "name": "Hui Dou"
                    },
                    {
                        "name": "Jian Zhao"
                    },
                    {
                        "name": "Furao Shen"
                    }
                ],
                "author_detail": {
                    "name": "Furao Shen"
                },
                "author": "Furao Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05130v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05130v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03762v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03762v2",
                "updated": "2025-05-08T09:05:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    9,
                    5,
                    51,
                    3,
                    128,
                    0
                ],
                "published": "2025-04-20T17:48:54Z",
                "published_parsed": [
                    2025,
                    4,
                    20,
                    17,
                    48,
                    54,
                    6,
                    110,
                    0
                ],
                "title": "CVA6S+: A Superscalar RISC-V Core with High-Throughput Memory\n  Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CVA6S+: A Superscalar RISC-V Core with High-Throughput Memory\n  Architecture"
                },
                "summary": "Open-source RISC-V cores are increasingly adopted in high-end embedded\ndomains such as automotive, where maximizing instructions per cycle (IPC) is\nbecoming critical. Building on the industry-supported open-source CVA6 core and\nits superscalar variant, CVA6S, we introduce CVA6S+, an enhanced version\nincorporating improved branch prediction, register renaming and enhanced\noperand forwarding. These optimizations enable CVA6S+ to achieve a 43.5%\nperformance improvement over the scalar configuration and 10.9% over CVA6S,\nwith an area overhead of just 9.30% over the scalar core (CVA6). Furthermore,\nwe integrate CVA6S+ with the OpenHW Core-V High-Performance L1 Dcache\n(HPDCache) and report a 74.1% bandwidth improvement over the legacy CVA6 cache\nsubsystem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-source RISC-V cores are increasingly adopted in high-end embedded\ndomains such as automotive, where maximizing instructions per cycle (IPC) is\nbecoming critical. Building on the industry-supported open-source CVA6 core and\nits superscalar variant, CVA6S, we introduce CVA6S+, an enhanced version\nincorporating improved branch prediction, register renaming and enhanced\noperand forwarding. These optimizations enable CVA6S+ to achieve a 43.5%\nperformance improvement over the scalar configuration and 10.9% over CVA6S,\nwith an area overhead of just 9.30% over the scalar core (CVA6). Furthermore,\nwe integrate CVA6S+ with the OpenHW Core-V High-Performance L1 Dcache\n(HPDCache) and report a 74.1% bandwidth improvement over the legacy CVA6 cache\nsubsystem."
                },
                "authors": [
                    {
                        "name": "Riccardo Tedeschi"
                    },
                    {
                        "name": "Gianmarco Ottavi"
                    },
                    {
                        "name": "Côme Allart"
                    },
                    {
                        "name": "Nils Wistoff"
                    },
                    {
                        "name": "Zexin Fu"
                    },
                    {
                        "name": "Filippo Grillotti"
                    },
                    {
                        "name": "Fabio De Ambroggi"
                    },
                    {
                        "name": "Elio Guidetti"
                    },
                    {
                        "name": "Jean-Baptiste Rigaud"
                    },
                    {
                        "name": "Olivier Potin"
                    },
                    {
                        "name": "Jean Roch Coulon"
                    },
                    {
                        "name": "César Fuguet"
                    },
                    {
                        "name": "Luca Benini"
                    },
                    {
                        "name": "Davide Rossi"
                    }
                ],
                "author_detail": {
                    "name": "Davide Rossi"
                },
                "author": "Davide Rossi",
                "arxiv_comment": "3 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03762v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03762v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12110v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12110v2",
                "updated": "2025-05-08T07:55:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    7,
                    55,
                    38,
                    3,
                    128,
                    0
                ],
                "published": "2024-06-17T21:43:39Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    21,
                    43,
                    39,
                    0,
                    169,
                    0
                ],
                "title": "CacheSquash: Making caches speculation-aware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheSquash: Making caches speculation-aware"
                },
                "summary": "Speculation is key to achieving high CPU performance, yet it enables risks\nlike Spectre attacks which remain a significant challenge to mitigate without\nincurring substantial performance overheads. These attacks typically unfold in\nthree stages: access, transmit, and receive. Typically, they exploit a cache\ntiming side channel during the transmit and receive phases: speculatively\naccessing sensitive data (access), altering cache state (transmit), and then\nutilizing a cache timing attack (e.g., Flush+Reload) to extract the secret\n(receive). Our key observation is that Spectre attacks only require the\ntransmit instruction to execute and dispatch a request to the cache hierarchy.\nIt need not complete before a misprediction is detected (and mis-speculated\ninstructions squashed) because responses from memory that arrive at the cache\nafter squashing still alter cache state. We propose a novel mitigation,\nCacheSquash, that cancels mis-speculated memory accesses. Immediately upon\nsquashing, a cancellation is sent to the cache hierarchy, propagating\ndownstream and preventing any changes to caches that have not yet received a\nresponse. This minimizes cache state changes, thereby reducing the likelihood\nof Spectre attacks succeeding. We implement CacheSquash on gem5 and show that\nit thwarts practical Spectre attacks, with near-zero performance overheads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculation is key to achieving high CPU performance, yet it enables risks\nlike Spectre attacks which remain a significant challenge to mitigate without\nincurring substantial performance overheads. These attacks typically unfold in\nthree stages: access, transmit, and receive. Typically, they exploit a cache\ntiming side channel during the transmit and receive phases: speculatively\naccessing sensitive data (access), altering cache state (transmit), and then\nutilizing a cache timing attack (e.g., Flush+Reload) to extract the secret\n(receive). Our key observation is that Spectre attacks only require the\ntransmit instruction to execute and dispatch a request to the cache hierarchy.\nIt need not complete before a misprediction is detected (and mis-speculated\ninstructions squashed) because responses from memory that arrive at the cache\nafter squashing still alter cache state. We propose a novel mitigation,\nCacheSquash, that cancels mis-speculated memory accesses. Immediately upon\nsquashing, a cancellation is sent to the cache hierarchy, propagating\ndownstream and preventing any changes to caches that have not yet received a\nresponse. This minimizes cache state changes, thereby reducing the likelihood\nof Spectre attacks succeeding. We implement CacheSquash on gem5 and show that\nit thwarts practical Spectre attacks, with near-zero performance overheads."
                },
                "authors": [
                    {
                        "name": "Hossam ElAtali"
                    },
                    {
                        "name": "N. Asokan"
                    }
                ],
                "author_detail": {
                    "name": "N. Asokan"
                },
                "author": "N. Asokan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12110v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12110v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01658v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01658v2",
                "updated": "2025-05-08T07:08:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    7,
                    8,
                    40,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-03T02:47:43Z",
                "published_parsed": [
                    2025,
                    5,
                    3,
                    2,
                    47,
                    43,
                    5,
                    123,
                    0
                ],
                "title": "A Survey on Inference Engines for Large Language Models: Perspectives on\n  Optimization and Efficiency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Inference Engines for Large Language Models: Perspectives on\n  Optimization and Efficiency"
                },
                "summary": "Large language models (LLMs) are widely applied in chatbots, code generators,\nand search engines. Workloads such as chain-of-thought, complex reasoning, and\nagent services significantly increase the inference cost by invoking the model\nrepeatedly. Optimization methods such as parallelism, compression, and caching\nhave been adopted to reduce costs, but the diverse service requirements make it\nhard to select the right method. Recently, specialized LLM inference engines\nhave emerged as a key component for integrating the optimization methods into\nservice-oriented infrastructures. However, a systematic study on inference\nengines is still lacking. This paper provides a comprehensive evaluation of 25\nopen-source and commercial inference engines. We examine each inference engine\nin terms of ease-of-use, ease-of-deployment, general-purpose support,\nscalability, and suitability for throughput- and latency-aware computation.\nFurthermore, we explore the design goals of each inference engine by\ninvestigating the optimization techniques it supports. In addition, we assess\nthe ecosystem maturity of open source inference engines and handle the\nperformance and cost policy of commercial solutions. We outline future research\ndirections that include support for complex LLM-based services, support of\nvarious hardware, and enhanced security, offering practical guidance to\nresearchers and developers in selecting and designing optimized LLM inference\nengines. We also provide a public repository to continually track developments\nin this fast-evolving field:\nhttps://github.com/sihyeong/Awesome-LLM-Inference-Engine",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are widely applied in chatbots, code generators,\nand search engines. Workloads such as chain-of-thought, complex reasoning, and\nagent services significantly increase the inference cost by invoking the model\nrepeatedly. Optimization methods such as parallelism, compression, and caching\nhave been adopted to reduce costs, but the diverse service requirements make it\nhard to select the right method. Recently, specialized LLM inference engines\nhave emerged as a key component for integrating the optimization methods into\nservice-oriented infrastructures. However, a systematic study on inference\nengines is still lacking. This paper provides a comprehensive evaluation of 25\nopen-source and commercial inference engines. We examine each inference engine\nin terms of ease-of-use, ease-of-deployment, general-purpose support,\nscalability, and suitability for throughput- and latency-aware computation.\nFurthermore, we explore the design goals of each inference engine by\ninvestigating the optimization techniques it supports. In addition, we assess\nthe ecosystem maturity of open source inference engines and handle the\nperformance and cost policy of commercial solutions. We outline future research\ndirections that include support for complex LLM-based services, support of\nvarious hardware, and enhanced security, offering practical guidance to\nresearchers and developers in selecting and designing optimized LLM inference\nengines. We also provide a public repository to continually track developments\nin this fast-evolving field:\nhttps://github.com/sihyeong/Awesome-LLM-Inference-Engine"
                },
                "authors": [
                    {
                        "name": "Sihyeong Park"
                    },
                    {
                        "name": "Sungryeol Jeon"
                    },
                    {
                        "name": "Chaelyn Lee"
                    },
                    {
                        "name": "Seokhun Jeon"
                    },
                    {
                        "name": "Byung-Soo Kim"
                    },
                    {
                        "name": "Jemin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jemin Lee"
                },
                "author": "Jemin Lee",
                "arxiv_comment": "Under review; 65 pages; 27 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01658v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01658v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04896v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04896v1",
                "updated": "2025-05-08T02:16:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    2,
                    16,
                    8,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T02:16:08Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    2,
                    16,
                    8,
                    3,
                    128,
                    0
                ],
                "title": "Memory Under Siege: A Comprehensive Survey of Side-Channel Attacks on\n  Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory Under Siege: A Comprehensive Survey of Side-Channel Attacks on\n  Memory"
                },
                "summary": "Side-channel attacks on memory (SCAM) exploit unintended data leaks from\nmemory subsystems to infer sensitive information, posing significant threats to\nsystem security. These attacks exploit vulnerabilities in memory access\npatterns, cache behaviors, and other microarchitectural features to bypass\ntraditional security measures. The purpose of this research is to examine SCAM,\nclassify various attack techniques, and evaluate existing defense mechanisms.\nIt guides researchers and industry professionals in improving memory security\nand mitigating emerging threats. We begin by identifying the major\nvulnerabilities in the memory system that are frequently exploited in SCAM,\nsuch as cache timing, speculative execution, \\textit{Rowhammer}, and other\nsophisticated approaches. Next, we outline a comprehensive taxonomy that\nsystematically classifies these attacks based on their types, target systems,\nattack vectors, and adversarial capabilities required to execute them. In\naddition, we review the current landscape of mitigation strategies, emphasizing\ntheir strengths and limitations. This work aims to provide a comprehensive\noverview of memory-based side-channel attacks with the goal of providing\nsignificant insights for researchers and practitioners to better understand,\ndetect, and mitigate SCAM risks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Side-channel attacks on memory (SCAM) exploit unintended data leaks from\nmemory subsystems to infer sensitive information, posing significant threats to\nsystem security. These attacks exploit vulnerabilities in memory access\npatterns, cache behaviors, and other microarchitectural features to bypass\ntraditional security measures. The purpose of this research is to examine SCAM,\nclassify various attack techniques, and evaluate existing defense mechanisms.\nIt guides researchers and industry professionals in improving memory security\nand mitigating emerging threats. We begin by identifying the major\nvulnerabilities in the memory system that are frequently exploited in SCAM,\nsuch as cache timing, speculative execution, \\textit{Rowhammer}, and other\nsophisticated approaches. Next, we outline a comprehensive taxonomy that\nsystematically classifies these attacks based on their types, target systems,\nattack vectors, and adversarial capabilities required to execute them. In\naddition, we review the current landscape of mitigation strategies, emphasizing\ntheir strengths and limitations. This work aims to provide a comprehensive\noverview of memory-based side-channel attacks with the goal of providing\nsignificant insights for researchers and practitioners to better understand,\ndetect, and mitigate SCAM risks."
                },
                "authors": [
                    {
                        "name": "MD Mahady Hassan"
                    },
                    {
                        "name": "Shanto Roy"
                    },
                    {
                        "name": "Reza Rahaeimehr"
                    }
                ],
                "author_detail": {
                    "name": "Reza Rahaeimehr"
                },
                "author": "Reza Rahaeimehr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04896v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04896v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04556v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04556v1",
                "updated": "2025-05-07T16:44:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    16,
                    44,
                    21,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T16:44:21Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    16,
                    44,
                    21,
                    2,
                    127,
                    0
                ],
                "title": "Comparing CPU and GPU compute of PERMANOVA on MI300A",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing CPU and GPU compute of PERMANOVA on MI300A"
                },
                "summary": "Comparing the tradeoffs of CPU and GPU compute for memory-heavy algorithms is\noften challenging, due to the drastically different memory subsystems on host\nCPUs and discrete GPUs. The AMD MI300A is an exception, since it sports both\nCPU and GPU cores in a single package, all backed by the same type of HBM\nmemory. In this paper we analyze the performance of Permutational Multivariate\nAnalysis of Variance (PERMANOVA), a non-parametric method that tests whether\ntwo or more groups of objects are significantly different based on a\ncategorical factor. This method is memory-bound and has been recently optimized\nfor CPU cache locality. Our tests show that GPU cores on the MI300A prefer the\nbrute force approach instead, significantly outperforming the CPU-based\nimplementation. The significant benefit of Simultaneous Multithreading (SMT)\nwas also a pleasant surprise.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing the tradeoffs of CPU and GPU compute for memory-heavy algorithms is\noften challenging, due to the drastically different memory subsystems on host\nCPUs and discrete GPUs. The AMD MI300A is an exception, since it sports both\nCPU and GPU cores in a single package, all backed by the same type of HBM\nmemory. In this paper we analyze the performance of Permutational Multivariate\nAnalysis of Variance (PERMANOVA), a non-parametric method that tests whether\ntwo or more groups of objects are significantly different based on a\ncategorical factor. This method is memory-bound and has been recently optimized\nfor CPU cache locality. Our tests show that GPU cores on the MI300A prefer the\nbrute force approach instead, significantly outperforming the CPU-based\nimplementation. The significant benefit of Simultaneous Multithreading (SMT)\nwas also a pleasant surprise."
                },
                "authors": [
                    {
                        "name": "Igor Sfiligoi"
                    }
                ],
                "author_detail": {
                    "name": "Igor Sfiligoi"
                },
                "author": "Igor Sfiligoi",
                "arxiv_comment": "7 pages, 1 figure, Accepted at PEARC25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04556v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04556v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04466v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04466v1",
                "updated": "2025-05-07T14:37:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    14,
                    37,
                    13,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T14:37:13Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    14,
                    37,
                    13,
                    2,
                    127,
                    0
                ],
                "title": "Securing Immersive 360 Video Streams through Attribute-Based Selective\n  Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Securing Immersive 360 Video Streams through Attribute-Based Selective\n  Encryption"
                },
                "summary": "Delivering high-quality, secure 360{\\deg} video content introduces unique\nchallenges, primarily due to the high bitrates and interactive demands of\nimmersive media. Traditional HTTPS-based methods, although widely used, face\nlimitations in computational efficiency and scalability when securing these\nhigh-resolution streams. To address these issues, this paper proposes a novel\nframework integrating Attribute-Based Encryption (ABE) with selective\nencryption techniques tailored specifically for tiled 360{\\deg} video\nstreaming. Our approach employs selective encryption of frames at varying\nlevels to reduce computational overhead while ensuring robust protection\nagainst unauthorized access.\n  Moreover, we explore viewport-adaptive encryption, dynamically encrypting\nmore frames within tiles occupying larger portions of the viewer's field of\nview. This targeted method significantly enhances security in critical viewing\nareas without unnecessary overhead in peripheral regions. We deploy and\nevaluate our proposed approach using the CloudLab testbed, comparing its\nperformance against traditional HTTPS streaming. Experimental results\ndemonstrate that our ABE-based model achieves reduced computational load on\nintermediate caches, improves cache hit rates, and maintains comparable visual\nquality to HTTPS, as assessed by Video Multimethod Assessment Fusion (VMAF).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Delivering high-quality, secure 360{\\deg} video content introduces unique\nchallenges, primarily due to the high bitrates and interactive demands of\nimmersive media. Traditional HTTPS-based methods, although widely used, face\nlimitations in computational efficiency and scalability when securing these\nhigh-resolution streams. To address these issues, this paper proposes a novel\nframework integrating Attribute-Based Encryption (ABE) with selective\nencryption techniques tailored specifically for tiled 360{\\deg} video\nstreaming. Our approach employs selective encryption of frames at varying\nlevels to reduce computational overhead while ensuring robust protection\nagainst unauthorized access.\n  Moreover, we explore viewport-adaptive encryption, dynamically encrypting\nmore frames within tiles occupying larger portions of the viewer's field of\nview. This targeted method significantly enhances security in critical viewing\nareas without unnecessary overhead in peripheral regions. We deploy and\nevaluate our proposed approach using the CloudLab testbed, comparing its\nperformance against traditional HTTPS streaming. Experimental results\ndemonstrate that our ABE-based model achieves reduced computational load on\nintermediate caches, improves cache hit rates, and maintains comparable visual\nquality to HTTPS, as assessed by Video Multimethod Assessment Fusion (VMAF)."
                },
                "authors": [
                    {
                        "name": "Mohammad Waquas Usmani"
                    },
                    {
                        "name": "Susmit Shannigrahi"
                    },
                    {
                        "name": "Michael Zink"
                    }
                ],
                "author_detail": {
                    "name": "Michael Zink"
                },
                "author": "Michael Zink",
                "arxiv_comment": "8 pages plus references, 10 figures, some with subfigures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04466v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04466v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04421v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04421v1",
                "updated": "2025-05-07T13:54:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    13,
                    54,
                    26,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T13:54:26Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    13,
                    54,
                    26,
                    2,
                    127,
                    0
                ],
                "title": "LONGER: Scaling Up Long Sequence Modeling in Industrial Recommenders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LONGER: Scaling Up Long Sequence Modeling in Industrial Recommenders"
                },
                "summary": "Modeling ultra-long user behavior sequences is critical for capturing both\nlong- and short-term preferences in industrial recommender systems. Existing\nsolutions typically rely on two-stage retrieval or indirect modeling paradigms,\nincuring upstream-downstream inconsistency and computational inefficiency. In\nthis paper, we present LONGER, a Long-sequence Optimized traNsformer for\nGPU-Efficient Recommenders. LONGER incorporates (i) a global token mechanism\nfor stabilizing attention over long contexts, (ii) a token merge module with\nlightweight InnerTransformers and hybrid attention strategy to reduce quadratic\ncomplexity, and (iii) a series of engineering optimizations, including training\nwith mixed-precision and activation recomputation, KV cache serving, and the\nfully synchronous model training and serving framework for unified GPU-based\ndense and sparse parameter updates. LONGER consistently outperforms strong\nbaselines in both offline metrics and online A/B testing in both advertising\nand e-commerce services at ByteDance, validating its consistent effectiveness\nand industrial-level scaling laws. Currently, LONGER has been fully deployed at\nmore than 10 influential scenarios at ByteDance, serving billion users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling ultra-long user behavior sequences is critical for capturing both\nlong- and short-term preferences in industrial recommender systems. Existing\nsolutions typically rely on two-stage retrieval or indirect modeling paradigms,\nincuring upstream-downstream inconsistency and computational inefficiency. In\nthis paper, we present LONGER, a Long-sequence Optimized traNsformer for\nGPU-Efficient Recommenders. LONGER incorporates (i) a global token mechanism\nfor stabilizing attention over long contexts, (ii) a token merge module with\nlightweight InnerTransformers and hybrid attention strategy to reduce quadratic\ncomplexity, and (iii) a series of engineering optimizations, including training\nwith mixed-precision and activation recomputation, KV cache serving, and the\nfully synchronous model training and serving framework for unified GPU-based\ndense and sparse parameter updates. LONGER consistently outperforms strong\nbaselines in both offline metrics and online A/B testing in both advertising\nand e-commerce services at ByteDance, validating its consistent effectiveness\nand industrial-level scaling laws. Currently, LONGER has been fully deployed at\nmore than 10 influential scenarios at ByteDance, serving billion users."
                },
                "authors": [
                    {
                        "name": "Zheng Chai"
                    },
                    {
                        "name": "Qin Ren"
                    },
                    {
                        "name": "Xijun Xiao"
                    },
                    {
                        "name": "Huizhi Yang"
                    },
                    {
                        "name": "Bo Han"
                    },
                    {
                        "name": "Sijun Zhang"
                    },
                    {
                        "name": "Di Chen"
                    },
                    {
                        "name": "Hui Lu"
                    },
                    {
                        "name": "Wenlin Zhao"
                    },
                    {
                        "name": "Lele Yu"
                    },
                    {
                        "name": "Xionghang Xie"
                    },
                    {
                        "name": "Shiru Ren"
                    },
                    {
                        "name": "Xiang Sun"
                    },
                    {
                        "name": "Yaocheng Tan"
                    },
                    {
                        "name": "Peng Xu"
                    },
                    {
                        "name": "Yuchao Zheng"
                    },
                    {
                        "name": "Di Wu"
                    }
                ],
                "author_detail": {
                    "name": "Di Wu"
                },
                "author": "Di Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04421v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04421v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04326v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04326v1",
                "updated": "2025-05-07T11:21:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    11,
                    21,
                    12,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T11:21:12Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    11,
                    21,
                    12,
                    2,
                    127,
                    0
                ],
                "title": "Design and Evaluation of an NDN-Based Network for Distributed Digital\n  Twins",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design and Evaluation of an NDN-Based Network for Distributed Digital\n  Twins"
                },
                "summary": "Digital twins (DT) have received significant attention due to their numerous\nbenefits, such as real-time data analytics and cost reduction in production. DT\nserves as a fundamental component of many applications, encompassing smart\nmanufacturing, intelligent vehicles, and smart cities. By using Machine\nLearning (ML) and Artificial Intelligence (AI) techniques, DTs can efficiently\nfacilitate decision-making and productivity by simulating the status and\nchanges of a physical entity. To handle the massive amount of data brought by\nDTs, it is challenging to achieve low response latency for data fetching over\nexisting IP-based networks. IP-based networks use host addresses for end-to-end\ncommunication, making data distribution between DTs inefficient. Thus, we\npropose to use DTs in a distributed manner over Named Data Networking (NDN)\nnetworks. NDN is data-centric where data is routed based on content names,\ndynamically adjusting paths to optimize latency. Popular data is cached in\nnetwork nodes, reducing data transmission and network congestion. Since data is\nfetched by content names, users and mobile devices can move freely without IP\naddress reassignment. By using in-network caching and adaptive routing, we\nreckon NDN is an ideal fit for Future G Networks in the context of Digital\nTwins. We compared DTs in edge scenarios with cloud scenarios over NDN and\nIP-based networks to validate our insights. Extensive simulation results show\nthat using DT in the edge reduces response latency by 10.2x. This position\npaper represents an initial investigation into the gap in distributed DTs over\nNDN, serving as an early-stage study.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital twins (DT) have received significant attention due to their numerous\nbenefits, such as real-time data analytics and cost reduction in production. DT\nserves as a fundamental component of many applications, encompassing smart\nmanufacturing, intelligent vehicles, and smart cities. By using Machine\nLearning (ML) and Artificial Intelligence (AI) techniques, DTs can efficiently\nfacilitate decision-making and productivity by simulating the status and\nchanges of a physical entity. To handle the massive amount of data brought by\nDTs, it is challenging to achieve low response latency for data fetching over\nexisting IP-based networks. IP-based networks use host addresses for end-to-end\ncommunication, making data distribution between DTs inefficient. Thus, we\npropose to use DTs in a distributed manner over Named Data Networking (NDN)\nnetworks. NDN is data-centric where data is routed based on content names,\ndynamically adjusting paths to optimize latency. Popular data is cached in\nnetwork nodes, reducing data transmission and network congestion. Since data is\nfetched by content names, users and mobile devices can move freely without IP\naddress reassignment. By using in-network caching and adaptive routing, we\nreckon NDN is an ideal fit for Future G Networks in the context of Digital\nTwins. We compared DTs in edge scenarios with cloud scenarios over NDN and\nIP-based networks to validate our insights. Extensive simulation results show\nthat using DT in the edge reduces response latency by 10.2x. This position\npaper represents an initial investigation into the gap in distributed DTs over\nNDN, serving as an early-stage study."
                },
                "authors": [
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Zihan Jia"
                    },
                    {
                        "name": "Ze Wang"
                    },
                    {
                        "name": "Lin Cui"
                    },
                    {
                        "name": "Fung Po Tso"
                    }
                ],
                "author_detail": {
                    "name": "Fung Po Tso"
                },
                "author": "Fung Po Tso",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04326v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04326v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04216v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04216v1",
                "updated": "2025-05-07T08:10:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    8,
                    10,
                    39,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T08:10:39Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    8,
                    10,
                    39,
                    2,
                    127,
                    0
                ],
                "title": "Computational Model for Photoionization in Pure SF6 Streamer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational Model for Photoionization in Pure SF6 Streamer"
                },
                "summary": "Photoionization plays a crucial role in achieving spatial numerical\nconvergence and accurate quantitative predictions in SF6 streamer simulations,\nbut accurate models for SF6 photoionization remains limited, motivating this\npaper. First, we develop a computational model for SF6 photoionization and\nprovide the detailed modeling process. Then, we perform comparative studies\nagainst simplified approaches. The results demonstrate that the proposed model\neffectively captures the non-local effects of SF6 photoionization, enhancing\nboth the spatial numerical convergence and the accuracy of the streamer\nstructure. Finally, we perform comparative studies by artificially increasing\nthe photoionization intensity through multiplying the photoionization source\nterm Sph by a factor of 10 (10*Sph) relative to the baseline intensity.\nRegarding breakdown voltage prediction, 10*Sph leads to a significant\nunderestimation of the breakdown voltage for positive streamers, introducing\nerrors greater than 0.5 kV, while exerting a relatively small impact on\nnegative streamers. Regarding streamer propagation dynamics, 10*Sph reduces the\ncontraction at the positive streamer head and significantly lowers the local\nfield by more than 700 Td, thereby slowing down its speed. In contrast, 10*Sph\nhas little impact on the morphology of the negative streamers and slightly\nenhances the local field by less than 200 Td, thereby consistently accelerating\nits propagation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Photoionization plays a crucial role in achieving spatial numerical\nconvergence and accurate quantitative predictions in SF6 streamer simulations,\nbut accurate models for SF6 photoionization remains limited, motivating this\npaper. First, we develop a computational model for SF6 photoionization and\nprovide the detailed modeling process. Then, we perform comparative studies\nagainst simplified approaches. The results demonstrate that the proposed model\neffectively captures the non-local effects of SF6 photoionization, enhancing\nboth the spatial numerical convergence and the accuracy of the streamer\nstructure. Finally, we perform comparative studies by artificially increasing\nthe photoionization intensity through multiplying the photoionization source\nterm Sph by a factor of 10 (10*Sph) relative to the baseline intensity.\nRegarding breakdown voltage prediction, 10*Sph leads to a significant\nunderestimation of the breakdown voltage for positive streamers, introducing\nerrors greater than 0.5 kV, while exerting a relatively small impact on\nnegative streamers. Regarding streamer propagation dynamics, 10*Sph reduces the\ncontraction at the positive streamer head and significantly lowers the local\nfield by more than 700 Td, thereby slowing down its speed. In contrast, 10*Sph\nhas little impact on the morphology of the negative streamers and slightly\nenhances the local field by less than 200 Td, thereby consistently accelerating\nits propagation."
                },
                "authors": [
                    {
                        "name": "Zihao Feng"
                    }
                ],
                "author_detail": {
                    "name": "Zihao Feng"
                },
                "author": "Zihao Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04216v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04216v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12224v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12224v2",
                "updated": "2025-05-07T07:57:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    7,
                    57,
                    21,
                    2,
                    127,
                    0
                ],
                "published": "2025-02-17T14:54:14Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    14,
                    54,
                    14,
                    0,
                    48,
                    0
                ],
                "title": "Fate: Fast Edge Inference of Mixture-of-Experts Models via Cross-Layer\n  Gate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fate: Fast Edge Inference of Mixture-of-Experts Models via Cross-Layer\n  Gate"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive performance across\nvarious tasks, and their application in edge scenarios has attracted\nsignificant attention. However, sparse-activated Mixture-of-Experts (MoE)\nmodels, which are well suited for edge scenarios, have received relatively\nlittle attention due to their high memory demands. Offload-based methods have\nbeen proposed to address this challenge, but they face difficulties with expert\nprediction. Inaccurate expert predictions can result in prolonged inference\ndelays. To promote the application of MoE models in edge scenarios, we propose\nFate, an offloading system designed for MoE models to enable efficient\ninference in resource-constrained environments. The key insight behind Fate is\nthat gate inputs from adjacent layers can be effectively used for expert\nprefetching, achieving high prediction accuracy without additional GPU\noverhead. Furthermore, Fate employs a shallow-favoring expert caching strategy\nthat increases the expert hit rate to 99\\%. Additionally, Fate integrates\ntailored quantization strategies for cache optimization and IO efficiency.\nExperimental results show that, compared to Load on Demand and Expert\nActivation Path-based method, Fate achieves up to 4.5x and 1.9x speedups in\nprefill speed and up to 4.1x and 2.2x speedups in decoding speed, respectively,\nwhile maintaining inference quality. Moreover, Fate's performance improvements\nare scalable across different memory budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive performance across\nvarious tasks, and their application in edge scenarios has attracted\nsignificant attention. However, sparse-activated Mixture-of-Experts (MoE)\nmodels, which are well suited for edge scenarios, have received relatively\nlittle attention due to their high memory demands. Offload-based methods have\nbeen proposed to address this challenge, but they face difficulties with expert\nprediction. Inaccurate expert predictions can result in prolonged inference\ndelays. To promote the application of MoE models in edge scenarios, we propose\nFate, an offloading system designed for MoE models to enable efficient\ninference in resource-constrained environments. The key insight behind Fate is\nthat gate inputs from adjacent layers can be effectively used for expert\nprefetching, achieving high prediction accuracy without additional GPU\noverhead. Furthermore, Fate employs a shallow-favoring expert caching strategy\nthat increases the expert hit rate to 99\\%. Additionally, Fate integrates\ntailored quantization strategies for cache optimization and IO efficiency.\nExperimental results show that, compared to Load on Demand and Expert\nActivation Path-based method, Fate achieves up to 4.5x and 1.9x speedups in\nprefill speed and up to 4.1x and 2.2x speedups in decoding speed, respectively,\nwhile maintaining inference quality. Moreover, Fate's performance improvements\nare scalable across different memory budgets."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Fang"
                    },
                    {
                        "name": "Zicong Hong"
                    },
                    {
                        "name": "Yuegui Huang"
                    },
                    {
                        "name": "Yufeng Lyu"
                    },
                    {
                        "name": "Wuhui Chen"
                    },
                    {
                        "name": "Yue Yu"
                    },
                    {
                        "name": "Fan Yu"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12224v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12224v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04129v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04129v1",
                "updated": "2025-05-07T05:00:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    5,
                    0,
                    10,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T05:00:10Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    5,
                    0,
                    10,
                    2,
                    127,
                    0
                ],
                "title": "Maxing Out the SVM: Performance Impact of Memory and Program Cache Sizes\n  in the Agave Validator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Maxing Out the SVM: Performance Impact of Memory and Program Cache Sizes\n  in the Agave Validator"
                },
                "summary": "In this paper we analyze some of the bottlenecks in the execution pipeline of\nSolana's Agave validator client, focusing on RAM and program cache usage under\nmainnet conditions. Through a series of controlled experiments, we measure the\nvalidator's throughput and resource efficiency as RAM availability ranges\nbetween 128 GB to 1,536 GB (1.5 TB). We discover that the validator performance\ndegrades significantly below 256 GB, with transaction processing falling behind\nreal-time block production. Additionally, we study the program cache behavior,\nidentifying inefficiencies in program eviction and load latency. Our results\nprovide practical guidance for hardware provisioning and suggest improvements\nto the Solana execution and caching strategy, reducing latency due to the\nprogram cache by 90%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper we analyze some of the bottlenecks in the execution pipeline of\nSolana's Agave validator client, focusing on RAM and program cache usage under\nmainnet conditions. Through a series of controlled experiments, we measure the\nvalidator's throughput and resource efficiency as RAM availability ranges\nbetween 128 GB to 1,536 GB (1.5 TB). We discover that the validator performance\ndegrades significantly below 256 GB, with transaction processing falling behind\nreal-time block production. Additionally, we study the program cache behavior,\nidentifying inefficiencies in program eviction and load latency. Our results\nprovide practical guidance for hardware provisioning and suggest improvements\nto the Solana execution and caching strategy, reducing latency due to the\nprogram cache by 90%."
                },
                "authors": [
                    {
                        "name": "Turan Vural"
                    },
                    {
                        "name": "Yuki Yuminaga"
                    },
                    {
                        "name": "Alex Petrosyan"
                    },
                    {
                        "name": "Ben Livshits"
                    }
                ],
                "author_detail": {
                    "name": "Ben Livshits"
                },
                "author": "Ben Livshits",
                "arxiv_comment": "15 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04129v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04129v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12240v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12240v3",
                "updated": "2025-05-06T15:23:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    15,
                    23,
                    12,
                    1,
                    126,
                    0
                ],
                "published": "2025-04-16T16:45:19Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    16,
                    45,
                    19,
                    2,
                    106,
                    0
                ],
                "title": "Cobra: Efficient Line Art COlorization with BRoAder References",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cobra: Efficient Line Art COlorization with BRoAder References"
                },
                "summary": "The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/."
                },
                "authors": [
                    {
                        "name": "Junhao Zhuang"
                    },
                    {
                        "name": "Lingen Li"
                    },
                    {
                        "name": "Xuan Ju"
                    },
                    {
                        "name": "Zhaoyang Zhang"
                    },
                    {
                        "name": "Chun Yuan"
                    },
                    {
                        "name": "Ying Shan"
                    }
                ],
                "author_detail": {
                    "name": "Ying Shan"
                },
                "author": "Ying Shan",
                "arxiv_comment": "Project page with code: https://zhuang2002.github.io/Cobra/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12240v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12240v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02922v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02922v1",
                "updated": "2025-05-05T18:01:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    18,
                    1,
                    17,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T18:01:17Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    18,
                    1,
                    17,
                    0,
                    125,
                    0
                ],
                "title": "RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM\n  Inference"
                },
                "summary": "The growing context lengths of large language models (LLMs) pose significant\nchallenges for efficient inference, primarily due to GPU memory and bandwidth\nconstraints. We present RetroInfer, a novel system that reconceptualizes the\nkey-value (KV) cache as a vector storage system which exploits the inherent\nattention sparsity to accelerate long-context LLM inference. At its core is the\nwave index, an Attention-aWare VEctor index that enables efficient and accurate\nretrieval of critical tokens through techniques such as tripartite attention\napproximation, accuracy-bounded attention estimation, and segmented clustering.\nComplementing this is the wave buffer, which coordinates KV cache placement and\noverlaps computation and data transfer across GPU and CPU to sustain high\nthroughput. Unlike prior sparsity-based methods that struggle with token\nselection and hardware coordination, RetroInfer delivers robust performance\nwithout compromising model accuracy. Experiments on long-context benchmarks\nshow up to 4.5X speedup over full attention within GPU memory limits and up to\n10.5X over sparse attention baselines when KV cache is extended to CPU memory,\nall while preserving full-attention-level accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing context lengths of large language models (LLMs) pose significant\nchallenges for efficient inference, primarily due to GPU memory and bandwidth\nconstraints. We present RetroInfer, a novel system that reconceptualizes the\nkey-value (KV) cache as a vector storage system which exploits the inherent\nattention sparsity to accelerate long-context LLM inference. At its core is the\nwave index, an Attention-aWare VEctor index that enables efficient and accurate\nretrieval of critical tokens through techniques such as tripartite attention\napproximation, accuracy-bounded attention estimation, and segmented clustering.\nComplementing this is the wave buffer, which coordinates KV cache placement and\noverlaps computation and data transfer across GPU and CPU to sustain high\nthroughput. Unlike prior sparsity-based methods that struggle with token\nselection and hardware coordination, RetroInfer delivers robust performance\nwithout compromising model accuracy. Experiments on long-context benchmarks\nshow up to 4.5X speedup over full attention within GPU memory limits and up to\n10.5X over sparse attention baselines when KV cache is extended to CPU memory,\nall while preserving full-attention-level accuracy."
                },
                "authors": [
                    {
                        "name": "Yaoqi Chen"
                    },
                    {
                        "name": "Jinkai Zhang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Jingjia Luo"
                    },
                    {
                        "name": "Di Liu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Bailu Ding"
                    },
                    {
                        "name": "Xiao Yan"
                    },
                    {
                        "name": "Jiawei Jiang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Mingxing Zhang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02922v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02922v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02533v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02533v1",
                "updated": "2025-05-05T10:16:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    10,
                    16,
                    16,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T10:16:16Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    10,
                    16,
                    16,
                    0,
                    125,
                    0
                ],
                "title": "Large Language Model Partitioning for Low-Latency Inference at the Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Partitioning for Low-Latency Inference at the Edge"
                },
                "summary": "Large Language Models (LLMs) based on autoregressive, decoder-only\nTransformers generate text one token at a time, where a token represents a\ndiscrete unit of text. As each newly produced token is appended to the partial\noutput sequence, the length grows and so does the memory and compute load, due\nto the expanding key-value caches, which store intermediate representations of\nall previously generated tokens in the multi-head attention (MHA) layer. As\nthis iterative process steadily increases memory and compute demands,\nlayer-based partitioning in resource-constrained edge environments often\nresults in memory overload or high inference latency. To address this and\nreduce inference latency, we propose a resource-aware Transformer architecture\npartitioning algorithm, where the partitioning decision is updated at regular\nintervals during token generation. The approach is myopic in that it is based\non instantaneous information about device resource availability and network\nlink bandwidths. When first executed, the algorithm places blocks on devices,\nand in later executions, it migrates these blocks among devices so that the sum\nof migration delay and inference delay remains low. Our approach partitions the\ndecoder at the attention head level, co-locating each attention head with its\nkey-value cache and allowing dynamic migrations whenever resources become\ntight. By allocating different attention heads to different devices, we exploit\nparallel execution of attention heads and thus achieve substantial reductions\nin inference delays. Our experiments show that in small-scale settings (3-5\ndevices), the proposed method achieves within 15 to 20 percent of an exact\noptimal solver's latency, while in larger-scale tests it achieves notable\nimprovements in inference speed and memory usage compared to state-of-the-art\nlayer-based partitioning approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) based on autoregressive, decoder-only\nTransformers generate text one token at a time, where a token represents a\ndiscrete unit of text. As each newly produced token is appended to the partial\noutput sequence, the length grows and so does the memory and compute load, due\nto the expanding key-value caches, which store intermediate representations of\nall previously generated tokens in the multi-head attention (MHA) layer. As\nthis iterative process steadily increases memory and compute demands,\nlayer-based partitioning in resource-constrained edge environments often\nresults in memory overload or high inference latency. To address this and\nreduce inference latency, we propose a resource-aware Transformer architecture\npartitioning algorithm, where the partitioning decision is updated at regular\nintervals during token generation. The approach is myopic in that it is based\non instantaneous information about device resource availability and network\nlink bandwidths. When first executed, the algorithm places blocks on devices,\nand in later executions, it migrates these blocks among devices so that the sum\nof migration delay and inference delay remains low. Our approach partitions the\ndecoder at the attention head level, co-locating each attention head with its\nkey-value cache and allowing dynamic migrations whenever resources become\ntight. By allocating different attention heads to different devices, we exploit\nparallel execution of attention heads and thus achieve substantial reductions\nin inference delays. Our experiments show that in small-scale settings (3-5\ndevices), the proposed method achieves within 15 to 20 percent of an exact\noptimal solver's latency, while in larger-scale tests it achieves notable\nimprovements in inference speed and memory usage compared to state-of-the-art\nlayer-based partitioning approaches."
                },
                "authors": [
                    {
                        "name": "Dimitrios Kafetzis"
                    },
                    {
                        "name": "Ramin Khalili"
                    },
                    {
                        "name": "Iordanis Koutsopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Iordanis Koutsopoulos"
                },
                "author": "Iordanis Koutsopoulos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02533v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02533v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02346v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02346v1",
                "updated": "2025-05-05T04:01:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    4,
                    1,
                    56,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T04:01:56Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    4,
                    1,
                    56,
                    0,
                    125,
                    0
                ],
                "title": "An Empirical Study on the Performance and Energy Usage of Compiled\n  Python Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study on the Performance and Energy Usage of Compiled\n  Python Code"
                },
                "summary": "Python is a popular programming language known for its ease of learning and\nextensive libraries. However, concerns about performance and energy consumption\nhave led to the development of compilers to enhance Python code efficiency.\nDespite the proven benefits of existing compilers on the efficiency of Python\ncode, there is limited analysis comparing their performance and energy\nefficiency, particularly considering code characteristics and factors like CPU\nfrequency and core count. Our study investigates how compilation impacts the\nperformance and energy consumption of Python code, using seven benchmarks\ncompiled with eight different tools: PyPy, Numba, Nuitka, Mypyc, Codon, Cython,\nPyston-lite, and the experimental Python 3.13 version, compared to CPython. The\nbenchmarks are single-threaded and executed on an NUC and a server, measuring\nenergy usage, execution time, memory usage, and Last-Level Cache (LLC) miss\nrates at a fixed frequency and on a single core. The results show that\ncompilation can significantly enhance execution time, energy and memory usage,\nwith Codon, PyPy, and Numba achieving over 90\\% speed and energy improvements.\nNuitka optimizes memory usage consistently on both testbeds. The impact of\ncompilation on LLC miss rate is not clear since it varies considerably across\nbenchmarks for each compiler. Our study is important for researchers and\npractitioners focused on improving Python code performance and energy\nefficiency. We outline future research directions, such as exploring caching\neffects on energy usage. Our findings help practitioners choose the best\ncompiler based on their efficiency benefits and accessibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Python is a popular programming language known for its ease of learning and\nextensive libraries. However, concerns about performance and energy consumption\nhave led to the development of compilers to enhance Python code efficiency.\nDespite the proven benefits of existing compilers on the efficiency of Python\ncode, there is limited analysis comparing their performance and energy\nefficiency, particularly considering code characteristics and factors like CPU\nfrequency and core count. Our study investigates how compilation impacts the\nperformance and energy consumption of Python code, using seven benchmarks\ncompiled with eight different tools: PyPy, Numba, Nuitka, Mypyc, Codon, Cython,\nPyston-lite, and the experimental Python 3.13 version, compared to CPython. The\nbenchmarks are single-threaded and executed on an NUC and a server, measuring\nenergy usage, execution time, memory usage, and Last-Level Cache (LLC) miss\nrates at a fixed frequency and on a single core. The results show that\ncompilation can significantly enhance execution time, energy and memory usage,\nwith Codon, PyPy, and Numba achieving over 90\\% speed and energy improvements.\nNuitka optimizes memory usage consistently on both testbeds. The impact of\ncompilation on LLC miss rate is not clear since it varies considerably across\nbenchmarks for each compiler. Our study is important for researchers and\npractitioners focused on improving Python code performance and energy\nefficiency. We outline future research directions, such as exploring caching\neffects on energy usage. Our findings help practitioners choose the best\ncompiler based on their efficiency benefits and accessibility."
                },
                "authors": [
                    {
                        "name": "Vincenzo Stoico"
                    },
                    {
                        "name": "Andrei Calin Dragomir"
                    },
                    {
                        "name": "Patricia Lago"
                    }
                ],
                "author_detail": {
                    "name": "Patricia Lago"
                },
                "author": "Patricia Lago",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02346v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02346v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10375v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10375v2",
                "updated": "2025-05-04T09:49:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    4,
                    9,
                    49,
                    42,
                    6,
                    124,
                    0
                ],
                "published": "2024-12-16T07:59:21Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    7,
                    59,
                    21,
                    0,
                    351,
                    0
                ],
                "title": "DAOP: Data-Aware Offloading and Predictive Pre-Calculation for Efficient\n  MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DAOP: Data-Aware Offloading and Predictive Pre-Calculation for Efficient\n  MoE Inference"
                },
                "summary": "Mixture-of-Experts (MoE) models, though highly effective for various machine\nlearning tasks, face significant deployment challenges on memory-constrained\ndevices. While GPUs offer fast inference, their limited memory compared to CPUs\nmeans not all experts can be stored on the GPU simultaneously, necessitating\nfrequent, costly data transfers from CPU memory, often negating GPU speed\nadvantages. To address this, we present DAOP, an on-device MoE inference engine\nto optimize parallel GPU-CPU execution. DAOP dynamically allocates experts\nbetween CPU and GPU based on per-sequence activation patterns, and selectively\npre-calculates predicted experts on CPUs to minimize transfer latency. This\napproach enables efficient resource utilization across various expert cache\nratios while maintaining model accuracy through a novel graceful degradation\nmechanism. Comprehensive evaluations across various datasets show that DAOP\noutperforms traditional expert caching and prefetching methods by up to 8.20x\nand offloading techniques by 1.35x while maintaining accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) models, though highly effective for various machine\nlearning tasks, face significant deployment challenges on memory-constrained\ndevices. While GPUs offer fast inference, their limited memory compared to CPUs\nmeans not all experts can be stored on the GPU simultaneously, necessitating\nfrequent, costly data transfers from CPU memory, often negating GPU speed\nadvantages. To address this, we present DAOP, an on-device MoE inference engine\nto optimize parallel GPU-CPU execution. DAOP dynamically allocates experts\nbetween CPU and GPU based on per-sequence activation patterns, and selectively\npre-calculates predicted experts on CPUs to minimize transfer latency. This\napproach enables efficient resource utilization across various expert cache\nratios while maintaining model accuracy through a novel graceful degradation\nmechanism. Comprehensive evaluations across various datasets show that DAOP\noutperforms traditional expert caching and prefetching methods by up to 8.20x\nand offloading techniques by 1.35x while maintaining accuracy."
                },
                "authors": [
                    {
                        "name": "Yujie Zhang"
                    },
                    {
                        "name": "Shivam Aggarwal"
                    },
                    {
                        "name": "Tulika Mitra"
                    }
                ],
                "author_detail": {
                    "name": "Tulika Mitra"
                },
                "author": "Tulika Mitra",
                "arxiv_comment": "7 pages, 10 figures, Accepted by DATE Conference 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10375v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10375v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02027v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02027v1",
                "updated": "2025-05-04T08:30:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    4,
                    8,
                    30,
                    0,
                    6,
                    124,
                    0
                ],
                "published": "2025-05-04T08:30:00Z",
                "published_parsed": [
                    2025,
                    5,
                    4,
                    8,
                    30,
                    0,
                    6,
                    124,
                    0
                ],
                "title": "GraphPrompter: Multi-stage Adaptive Prompt Optimization for Graph\n  In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphPrompter: Multi-stage Adaptive Prompt Optimization for Graph\n  In-Context Learning"
                },
                "summary": "Graph In-Context Learning, with the ability to adapt pre-trained graph models\nto novel and diverse downstream graphs without updating any parameters, has\ngained much attention in the community. The key to graph in-context learning is\nto perform downstream graphs conditioned on chosen prompt examples. Existing\nmethods randomly select subgraphs or edges as prompts, leading to noisy graph\nprompts and inferior model performance. Additionally, due to the gap between\npre-training and testing graphs, when the number of classes in the testing\ngraphs is much greater than that in the training, the in-context learning\nability will also significantly deteriorate. To tackle the aforementioned\nchallenges, we develop a multi-stage adaptive prompt optimization method\nGraphPrompter, which optimizes the entire process of generating, selecting, and\nusing graph prompts for better in-context learning capabilities. Firstly,\nPrompt Generator introduces a reconstruction layer to highlight the most\ninformative edges and reduce irrelevant noise for graph prompt construction.\nFurthermore, in the selection stage, Prompt Selector employs the $k$-nearest\nneighbors algorithm and pre-trained selection layers to dynamically choose\nappropriate samples and minimize the influence of irrelevant prompts. Finally,\nwe leverage a Prompt Augmenter with a cache replacement strategy to enhance the\ngeneralization capability of the pre-trained model on new datasets. Extensive\nexperiments show that GraphPrompter effectively enhances the in-context\nlearning ability of graph models. On average across all the settings, our\napproach surpasses the state-of-the-art baselines by over 8%. Our code is\nreleased at https://github.com/karin0018/GraphPrompter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph In-Context Learning, with the ability to adapt pre-trained graph models\nto novel and diverse downstream graphs without updating any parameters, has\ngained much attention in the community. The key to graph in-context learning is\nto perform downstream graphs conditioned on chosen prompt examples. Existing\nmethods randomly select subgraphs or edges as prompts, leading to noisy graph\nprompts and inferior model performance. Additionally, due to the gap between\npre-training and testing graphs, when the number of classes in the testing\ngraphs is much greater than that in the training, the in-context learning\nability will also significantly deteriorate. To tackle the aforementioned\nchallenges, we develop a multi-stage adaptive prompt optimization method\nGraphPrompter, which optimizes the entire process of generating, selecting, and\nusing graph prompts for better in-context learning capabilities. Firstly,\nPrompt Generator introduces a reconstruction layer to highlight the most\ninformative edges and reduce irrelevant noise for graph prompt construction.\nFurthermore, in the selection stage, Prompt Selector employs the $k$-nearest\nneighbors algorithm and pre-trained selection layers to dynamically choose\nappropriate samples and minimize the influence of irrelevant prompts. Finally,\nwe leverage a Prompt Augmenter with a cache replacement strategy to enhance the\ngeneralization capability of the pre-trained model on new datasets. Extensive\nexperiments show that GraphPrompter effectively enhances the in-context\nlearning ability of graph models. On average across all the settings, our\napproach surpasses the state-of-the-art baselines by over 8%. Our code is\nreleased at https://github.com/karin0018/GraphPrompter."
                },
                "authors": [
                    {
                        "name": "Rui Lv"
                    },
                    {
                        "name": "Zaixi Zhang"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Weibo Gao"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Jiaxia Yan"
                    },
                    {
                        "name": "Linan Yue"
                    },
                    {
                        "name": "Fangzhou Yao"
                    }
                ],
                "author_detail": {
                    "name": "Fangzhou Yao"
                },
                "author": "Fangzhou Yao",
                "arxiv_comment": "14 pages. IEEE International Conference on Data Engineering\n  (ICDE'2025), accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02027v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02027v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07578v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07578v3",
                "updated": "2025-05-03T04:07:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    3,
                    4,
                    7,
                    7,
                    5,
                    123,
                    0
                ],
                "published": "2025-02-11T14:25:20Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    25,
                    20,
                    1,
                    42,
                    0
                ],
                "title": "PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language\n  Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language\n  Model Inference"
                },
                "summary": "Large Language Model (LLM) inference uses an autoregressive manner to\ngenerate one token at a time, which exhibits notably lower operational\nintensity compared to earlier Machine Learning (ML) models such as encoder-only\ntransformers and Convolutional Neural Networks. At the same time, LLMs possess\nlarge parameter sizes and use key-value caches to store context information.\nModern LLMs support context windows with up to 1 million tokens to generate\nversatile text, audio, and video content. A large key-value cache unique to\neach prompt requires a large memory capacity, limiting the inference batch\nsize. Both low operational intensity and limited batch size necessitate a high\nmemory bandwidth. However, contemporary hardware systems for ML model\ndeployment, such as GPUs and TPUs, are primarily optimized for compute\nthroughput. This mismatch challenges the efficient deployment of advanced LLMs\nand makes users pay for expensive compute resources that are poorly utilized\nfor the memory-bound LLM inference tasks.\n  We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which\nharnesses CXL memory expansion capabilities to accommodate substantial LLM\nsizes, and utilizes near-bank processing units to deliver high memory\nbandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable\nCXL network to support peer-to-peer and collective communication primitives\nacross CXL devices. We implement various parallelism strategies to distribute\nLLMs across these devices. Compared to GPU baselines with maximum supported\nbatch sizes and similar average power, CENT achieves 2.3$\\times$ higher\nthroughput and consumes 2.9$\\times$ less energy. CENT enhances the Total Cost\nof Ownership (TCO), generating 5.2$\\times$ more tokens per dollar than GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference uses an autoregressive manner to\ngenerate one token at a time, which exhibits notably lower operational\nintensity compared to earlier Machine Learning (ML) models such as encoder-only\ntransformers and Convolutional Neural Networks. At the same time, LLMs possess\nlarge parameter sizes and use key-value caches to store context information.\nModern LLMs support context windows with up to 1 million tokens to generate\nversatile text, audio, and video content. A large key-value cache unique to\neach prompt requires a large memory capacity, limiting the inference batch\nsize. Both low operational intensity and limited batch size necessitate a high\nmemory bandwidth. However, contemporary hardware systems for ML model\ndeployment, such as GPUs and TPUs, are primarily optimized for compute\nthroughput. This mismatch challenges the efficient deployment of advanced LLMs\nand makes users pay for expensive compute resources that are poorly utilized\nfor the memory-bound LLM inference tasks.\n  We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which\nharnesses CXL memory expansion capabilities to accommodate substantial LLM\nsizes, and utilizes near-bank processing units to deliver high memory\nbandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable\nCXL network to support peer-to-peer and collective communication primitives\nacross CXL devices. We implement various parallelism strategies to distribute\nLLMs across these devices. Compared to GPU baselines with maximum supported\nbatch sizes and similar average power, CENT achieves 2.3$\\times$ higher\nthroughput and consumes 2.9$\\times$ less energy. CENT enhances the Total Cost\nof Ownership (TCO), generating 5.2$\\times$ more tokens per dollar than GPUs."
                },
                "authors": [
                    {
                        "name": "Yufeng Gu"
                    },
                    {
                        "name": "Alireza Khadem"
                    },
                    {
                        "name": "Sumanth Umesh"
                    },
                    {
                        "name": "Ning Liang"
                    },
                    {
                        "name": "Xavier Servot"
                    },
                    {
                        "name": "Onur Mutlu"
                    },
                    {
                        "name": "Ravi Iyer"
                    },
                    {
                        "name": "Reetuparna Das"
                    }
                ],
                "author_detail": {
                    "name": "Reetuparna Das"
                },
                "author": "Reetuparna Das",
                "arxiv_doi": "10.1145/3676641.3716267",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676641.3716267",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.07578v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07578v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings of the 30th ACM International Conference on\n  Architectural Support for Programming Languages and Operating Systems, Volume\n  2 (ASPLOS'25)",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20335v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20335v2",
                "updated": "2025-05-03T01:10:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    3,
                    1,
                    10,
                    30,
                    5,
                    123,
                    0
                ],
                "published": "2025-04-29T00:58:59Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    0,
                    58,
                    59,
                    1,
                    119,
                    0
                ],
                "title": "VA-CDH: A Variance-Aware Method to Optimize Latency for Caching with\n  Delayed Hits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VA-CDH: A Variance-Aware Method to Optimize Latency for Caching with\n  Delayed Hits"
                },
                "summary": "Caches are fundamental to latency-sensitive systems like Content Delivery\nNetworks (CDNs) and Mobile Edge Computing (MEC). However, the delayed hit\nphenomenon where multiple requests for an object occur during its fetch from\nthe remote server after a miss significantly inflates user-perceived latency.\nWhile recent algorithms acknowledge delayed hits by estimating the resulting\naggregate delay, they predominantly focus on its mean value. We identify and\ndemonstrate that such approaches are insufficient, as the real aggregate delay\nfrequently exhibits substantial variance in the true production system, leading\nto suboptimal latency performance when ignored. Thus, we propose VA-CDH, a\nvariance-aware method to optimize latency for caching with delayed hits. It\nemploys a novel ranking function that explicitly incorporates both the\nempirically estimated mean and standard deviation of aggregate delay, allowing\ncaching decisions to account for its variation. We derive the analytical\ndistribution of aggregate delay under Poisson arrivals as a theoretical\ncontribution, offering more statistical insight beyond the mean value. Through\nthe simulations conducted on synthetic and real-world datasets, we show that\nVA-CDH reduces the total latency by 1%-6% approximately compared to\nstate-of-the-art algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caches are fundamental to latency-sensitive systems like Content Delivery\nNetworks (CDNs) and Mobile Edge Computing (MEC). However, the delayed hit\nphenomenon where multiple requests for an object occur during its fetch from\nthe remote server after a miss significantly inflates user-perceived latency.\nWhile recent algorithms acknowledge delayed hits by estimating the resulting\naggregate delay, they predominantly focus on its mean value. We identify and\ndemonstrate that such approaches are insufficient, as the real aggregate delay\nfrequently exhibits substantial variance in the true production system, leading\nto suboptimal latency performance when ignored. Thus, we propose VA-CDH, a\nvariance-aware method to optimize latency for caching with delayed hits. It\nemploys a novel ranking function that explicitly incorporates both the\nempirically estimated mean and standard deviation of aggregate delay, allowing\ncaching decisions to account for its variation. We derive the analytical\ndistribution of aggregate delay under Poisson arrivals as a theoretical\ncontribution, offering more statistical insight beyond the mean value. Through\nthe simulations conducted on synthetic and real-world datasets, we show that\nVA-CDH reduces the total latency by 1%-6% approximately compared to\nstate-of-the-art algorithms."
                },
                "authors": [
                    {
                        "name": "Bowen Jiang"
                    },
                    {
                        "name": "Chaofan Ma"
                    },
                    {
                        "name": "Duo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Duo Wang"
                },
                "author": "Duo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20335v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20335v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13298v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13298v3",
                "updated": "2025-05-02T13:55:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    13,
                    55,
                    21,
                    4,
                    122,
                    0
                ],
                "published": "2025-01-23T00:57:01Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    0,
                    57,
                    1,
                    3,
                    23,
                    0
                ],
                "title": "Collaborative Coded Caching for Partially Connected Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative Coded Caching for Partially Connected Networks"
                },
                "summary": "Coded caching leverages the differences in user cache memories to achieve\ngains that scale with the total cache size, alleviating network congestion due\nto high-quality content requests. Additionally, distributing transmitters over\na wide area can mitigate the adverse effects of path loss. In this work, we\nconsider a partially connected network where the channel between distributed\ntransmitters (helpers) and users is modeled as a distributed\nmultiple-input-multiple-output (MIMO) Gaussian broadcast channel. We propose a\nnovel delivery scheme consisting of two phases: partitioning and transmission.\nIn the partitioning phase, users with identical cache profiles are partitioned\ninto the minimum number of sets, such that users within each set can\nsuccessfully decode their desired message from a joint transmission enabled by\nMIMO precoding. To optimally partition the users, we employ the branch and\nbound method. In the transmission phase, each partition is treated as a single\nentity, and codewords are multicast to partitions with distinct cache profiles.\nThe proposed delivery scheme is applicable to any partially connected network,\nand while the partitioning is optimal, the overall delivery scheme, including\ntransmission, is heuristic. Interestingly, simulation results show that its\nperformance closely approximates that of the fully connected optimal solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching leverages the differences in user cache memories to achieve\ngains that scale with the total cache size, alleviating network congestion due\nto high-quality content requests. Additionally, distributing transmitters over\na wide area can mitigate the adverse effects of path loss. In this work, we\nconsider a partially connected network where the channel between distributed\ntransmitters (helpers) and users is modeled as a distributed\nmultiple-input-multiple-output (MIMO) Gaussian broadcast channel. We propose a\nnovel delivery scheme consisting of two phases: partitioning and transmission.\nIn the partitioning phase, users with identical cache profiles are partitioned\ninto the minimum number of sets, such that users within each set can\nsuccessfully decode their desired message from a joint transmission enabled by\nMIMO precoding. To optimally partition the users, we employ the branch and\nbound method. In the transmission phase, each partition is treated as a single\nentity, and codewords are multicast to partitions with distinct cache profiles.\nThe proposed delivery scheme is applicable to any partially connected network,\nand while the partitioning is optimal, the overall delivery scheme, including\ntransmission, is heuristic. Interestingly, simulation results show that its\nperformance closely approximates that of the fully connected optimal solution."
                },
                "authors": [
                    {
                        "name": "Kagan Akcay"
                    },
                    {
                        "name": "Eleftherios Lampiris"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13298v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13298v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01164v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01164v1",
                "updated": "2025-05-02T10:13:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    10,
                    13,
                    12,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T10:13:12Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    10,
                    13,
                    12,
                    4,
                    122,
                    0
                ],
                "title": "CaGR-RAG: Context-aware Query Grouping for Disk-based Vector Search in\n  RAG Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CaGR-RAG: Context-aware Query Grouping for Disk-based Vector Search in\n  RAG Systems"
                },
                "summary": "Modern embedding models capture both semantic and syntactic structures of\nqueries, often mapping different queries to similar regions in vector space.\nThis results in non-uniform cluster access patterns in disk-based vector search\nsystems, particularly in Retrieval Augmented Generation (RAG) framework. While\nexisting approaches optimize individual queries, they overlook the impact of\ncluster access patterns, failing to account for the locality effects of queries\nthat access similar clusters. This oversight reduces cache efficiency and\nincreases search latency due to excessive disk I/O. To address this, we\nintroduce CaGR-RAG, a context-aware query grouping mechanism that organizes\nqueries based on shared cluster access patterns. Additionally, it incorporates\nopportunistic cluster prefetching to minimize cache misses during transitions\nbetween query groups, further optimizing retrieval performance. Experimental\nresults show that CaGR-RAG reduces 99th percentile tail latency by up to 51.55%\nwhile consistently maintaining a higher cache hit ratio than the baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern embedding models capture both semantic and syntactic structures of\nqueries, often mapping different queries to similar regions in vector space.\nThis results in non-uniform cluster access patterns in disk-based vector search\nsystems, particularly in Retrieval Augmented Generation (RAG) framework. While\nexisting approaches optimize individual queries, they overlook the impact of\ncluster access patterns, failing to account for the locality effects of queries\nthat access similar clusters. This oversight reduces cache efficiency and\nincreases search latency due to excessive disk I/O. To address this, we\nintroduce CaGR-RAG, a context-aware query grouping mechanism that organizes\nqueries based on shared cluster access patterns. Additionally, it incorporates\nopportunistic cluster prefetching to minimize cache misses during transitions\nbetween query groups, further optimizing retrieval performance. Experimental\nresults show that CaGR-RAG reduces 99th percentile tail latency by up to 51.55%\nwhile consistently maintaining a higher cache hit ratio than the baseline."
                },
                "authors": [
                    {
                        "name": "Yeonwoo Jeong"
                    },
                    {
                        "name": "Kyuli Park"
                    },
                    {
                        "name": "Hyunji Cho"
                    },
                    {
                        "name": "Sungyong Park"
                    }
                ],
                "author_detail": {
                    "name": "Sungyong Park"
                },
                "author": "Sungyong Park",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01164v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01164v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01002v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01002v1",
                "updated": "2025-05-02T04:57:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    4,
                    57,
                    6,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T04:57:06Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    4,
                    57,
                    6,
                    4,
                    122,
                    0
                ],
                "title": "High Voltage Delivery and Distribution for the NEXT-100 Time Projection\n  Chamber",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High Voltage Delivery and Distribution for the NEXT-100 Time Projection\n  Chamber"
                },
                "summary": "A critical element in the realization of large liquid and gas time projection\nchambers (TPCs) is the delivery and distribution of high voltages into and\naround the detector. Such experiments require of order tens of kilovolts to\nenable electron drift over meter-scale distances. This paper describes the\ndesign and operation of the cathode feedthrough and high voltage distribution\nthrough the field cage of the NEXT-100 experiment, an underground TPC that will\nsearch for neutrinoless double beta decay $0\\nu\\beta\\beta$. The feedthrough has\nbeen demonstrated to hold pressures up to 20~bar and sustain voltages as high\nas -65~kV, and the TPC is operating stably at its design high voltages. The\nsystem has been realized within the constraints of a stringent radiopurity\nbudget and is now being used to execute a suite of sensitive double beta decay\nanalyses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A critical element in the realization of large liquid and gas time projection\nchambers (TPCs) is the delivery and distribution of high voltages into and\naround the detector. Such experiments require of order tens of kilovolts to\nenable electron drift over meter-scale distances. This paper describes the\ndesign and operation of the cathode feedthrough and high voltage distribution\nthrough the field cage of the NEXT-100 experiment, an underground TPC that will\nsearch for neutrinoless double beta decay $0\\nu\\beta\\beta$. The feedthrough has\nbeen demonstrated to hold pressures up to 20~bar and sustain voltages as high\nas -65~kV, and the TPC is operating stably at its design high voltages. The\nsystem has been realized within the constraints of a stringent radiopurity\nbudget and is now being used to execute a suite of sensitive double beta decay\nanalyses."
                },
                "authors": [
                    {
                        "name": "NEXT Collaboration"
                    },
                    {
                        "name": "C. Adams"
                    },
                    {
                        "name": "H. Almazán"
                    },
                    {
                        "name": "V. Álvarez"
                    },
                    {
                        "name": "K. Bailey"
                    },
                    {
                        "name": "R. Guenette"
                    },
                    {
                        "name": "B. J. P. Jones"
                    },
                    {
                        "name": "S. Johnston"
                    },
                    {
                        "name": "K. Mistry"
                    },
                    {
                        "name": "F. Monrabal"
                    },
                    {
                        "name": "D. R. Nygren"
                    },
                    {
                        "name": "B. Palmeiro"
                    },
                    {
                        "name": "L. Rogers"
                    },
                    {
                        "name": "J. Waldschmidt"
                    },
                    {
                        "name": "B. Aparicio"
                    },
                    {
                        "name": "A. I. Aranburu"
                    },
                    {
                        "name": "L. Arazi"
                    },
                    {
                        "name": "I. J. Arnquist"
                    },
                    {
                        "name": "F. Auria-Luna"
                    },
                    {
                        "name": "S. Ayet"
                    },
                    {
                        "name": "C. D. R. Azevedo"
                    },
                    {
                        "name": "F. Ballester"
                    },
                    {
                        "name": "M. del Barrio-Torregrosa"
                    },
                    {
                        "name": "A. Bayo"
                    },
                    {
                        "name": "J. M. Benlloch-Rodríguez"
                    },
                    {
                        "name": "F. I. G. M. Borges"
                    },
                    {
                        "name": "A. Brodolin"
                    },
                    {
                        "name": "S. Cárcel"
                    },
                    {
                        "name": "A. Castillo"
                    },
                    {
                        "name": "L. Cid"
                    },
                    {
                        "name": "C. A. N. Conde"
                    },
                    {
                        "name": "T. Contreras"
                    },
                    {
                        "name": "F. P. Cossío"
                    },
                    {
                        "name": "R. Coupe"
                    },
                    {
                        "name": "E. Dey"
                    },
                    {
                        "name": "G. Díaz"
                    },
                    {
                        "name": "C. Echevarria"
                    },
                    {
                        "name": "M. Elorza"
                    },
                    {
                        "name": "J. Escada"
                    },
                    {
                        "name": "R. Esteve"
                    },
                    {
                        "name": "R. Felkai"
                    },
                    {
                        "name": "L. M. P. Fernandes"
                    },
                    {
                        "name": "P. Ferrario"
                    },
                    {
                        "name": "A. L. Ferreira"
                    },
                    {
                        "name": "F. W. Foss"
                    },
                    {
                        "name": "Z. Freixa"
                    },
                    {
                        "name": "J. García-Barrena"
                    },
                    {
                        "name": "J. J. Gómez-Cadenas"
                    },
                    {
                        "name": "J. W. R. Grocott"
                    },
                    {
                        "name": "R. Guenette"
                    },
                    {
                        "name": "J. Hauptman"
                    },
                    {
                        "name": "C. A. O. Henriques"
                    },
                    {
                        "name": "J. A. Hernando Morata"
                    },
                    {
                        "name": "P. Herrero-Gómez"
                    },
                    {
                        "name": "V. Herrero"
                    },
                    {
                        "name": "C. Hervés Carrete"
                    },
                    {
                        "name": "Y. Ifergan"
                    },
                    {
                        "name": "F. Kellerer"
                    },
                    {
                        "name": "L. Larizgoitia"
                    },
                    {
                        "name": "A. Larumbe"
                    },
                    {
                        "name": "P. Lebrun"
                    },
                    {
                        "name": "F. Lopez"
                    },
                    {
                        "name": "N. López-March"
                    },
                    {
                        "name": "R. Madigan"
                    },
                    {
                        "name": "R. D. P. Mano"
                    },
                    {
                        "name": "A. P. Marques"
                    },
                    {
                        "name": "J. Martín-Albo"
                    },
                    {
                        "name": "G. Martínez-Lema"
                    },
                    {
                        "name": "M. Martínez-Vara"
                    },
                    {
                        "name": "R. L. Miller"
                    },
                    {
                        "name": "J. Molina-Canteras"
                    },
                    {
                        "name": "F. Monrabal"
                    },
                    {
                        "name": "C. M. B. Monteiro"
                    },
                    {
                        "name": "F. J. Mora"
                    },
                    {
                        "name": "P. Novella"
                    },
                    {
                        "name": "A. Nuñez"
                    },
                    {
                        "name": "E. Oblak"
                    },
                    {
                        "name": "J. Palacio"
                    },
                    {
                        "name": "B. Palmeiro"
                    },
                    {
                        "name": "A. Para"
                    },
                    {
                        "name": "A. Pazos"
                    },
                    {
                        "name": "J. Pelegrin"
                    },
                    {
                        "name": "M. Pérez Maneiro"
                    },
                    {
                        "name": "M. Querol"
                    },
                    {
                        "name": "J. Renner"
                    },
                    {
                        "name": "I. Rivilla"
                    },
                    {
                        "name": "C. Rogero"
                    },
                    {
                        "name": "B. Romeo"
                    },
                    {
                        "name": "C. Romo-Luque"
                    },
                    {
                        "name": "V. San Nacienciano"
                    },
                    {
                        "name": "F. P. Santos"
                    },
                    {
                        "name": "J. M. F. dos Santos"
                    },
                    {
                        "name": "M. Seemann"
                    },
                    {
                        "name": "I. Shomroni"
                    },
                    {
                        "name": "P. A. O. C. Silva"
                    },
                    {
                        "name": "A. Simón"
                    },
                    {
                        "name": "S. R. Soleti"
                    },
                    {
                        "name": "M. Sorel"
                    },
                    {
                        "name": "J. Soto-Oton"
                    },
                    {
                        "name": "J. M. R. Teixeira"
                    },
                    {
                        "name": "S. Teruel-Pardo"
                    },
                    {
                        "name": "J. F. Toledo"
                    },
                    {
                        "name": "C. Tonnelé"
                    },
                    {
                        "name": "S. Torelli"
                    },
                    {
                        "name": "J. Torrent"
                    },
                    {
                        "name": "A. Trettin"
                    },
                    {
                        "name": "A. Usón"
                    },
                    {
                        "name": "P. R. G. Valle"
                    },
                    {
                        "name": "J. F. C. A. Veloso"
                    },
                    {
                        "name": "J. Waiton"
                    },
                    {
                        "name": "A. Yubero-Navarro"
                    }
                ],
                "author_detail": {
                    "name": "A. Yubero-Navarro"
                },
                "author": "A. Yubero-Navarro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01002v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00962v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00962v1",
                "updated": "2025-05-02T02:36:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    2,
                    36,
                    23,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T02:36:23Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    2,
                    36,
                    23,
                    4,
                    122,
                    0
                ],
                "title": "The Open-Source BlackParrot-BedRock Cache Coherence System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Open-Source BlackParrot-BedRock Cache Coherence System"
                },
                "summary": "This dissertation revisits the topic of programmable cache coherence engines\nin the context of modern shared-memory multicore processors. First, the\nopen-source BedRock cache coherence protocol is described. BedRock employs the\ncanonical MOESIF coherence states and reduces implementation burden by\neliminating transient coherence states from the protocol. The protocol's design\ncomplexity, concurrency, and verification effort are analyzed and compared to a\ncanonical directory-based invalidate coherence protocol. Second, the\narchitecture and microarchitecture of three separate cache coherence\ndirectories implementing the BedRock protocol within the BlackParrot 64-bit\nRISC-V multicore processor, collectively called BlackParrot-BedRock\n(BP-BedRock), are described. A fixed-function coherence directory engine\nimplementation provides a baseline design for performance and area comparisons.\nA microcode-programmable coherence directory implementation demonstrates the\nfeasibility of implementing a programmable coherence engine capable of\nmaintaining sufficient protocol processing performance. A hybrid fixed-function\nand programmable coherence directory blends the protocol processing performance\nof the fixed-function design with the programmable flexibility of the\nmicrocode-programmable design. Collectively, the BedRock coherence protocol and\nits three BP-BedRock implementations demonstrate the feasibility and challenges\nof including programmable logic within the coherence system of modern\nshared-memory multicore processors, paving the way for future research into the\napplication- and system-level benefits of programmable coherence engines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This dissertation revisits the topic of programmable cache coherence engines\nin the context of modern shared-memory multicore processors. First, the\nopen-source BedRock cache coherence protocol is described. BedRock employs the\ncanonical MOESIF coherence states and reduces implementation burden by\neliminating transient coherence states from the protocol. The protocol's design\ncomplexity, concurrency, and verification effort are analyzed and compared to a\ncanonical directory-based invalidate coherence protocol. Second, the\narchitecture and microarchitecture of three separate cache coherence\ndirectories implementing the BedRock protocol within the BlackParrot 64-bit\nRISC-V multicore processor, collectively called BlackParrot-BedRock\n(BP-BedRock), are described. A fixed-function coherence directory engine\nimplementation provides a baseline design for performance and area comparisons.\nA microcode-programmable coherence directory implementation demonstrates the\nfeasibility of implementing a programmable coherence engine capable of\nmaintaining sufficient protocol processing performance. A hybrid fixed-function\nand programmable coherence directory blends the protocol processing performance\nof the fixed-function design with the programmable flexibility of the\nmicrocode-programmable design. Collectively, the BedRock coherence protocol and\nits three BP-BedRock implementations demonstrate the feasibility and challenges\nof including programmable logic within the coherence system of modern\nshared-memory multicore processors, paving the way for future research into the\napplication- and system-level benefits of programmable coherence engines."
                },
                "authors": [
                    {
                        "name": "Mark Unruh Wyse"
                    }
                ],
                "author_detail": {
                    "name": "Mark Unruh Wyse"
                },
                "author": "Mark Unruh Wyse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00962v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00962v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00901v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00901v1",
                "updated": "2025-05-01T22:32:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    22,
                    32,
                    29,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T22:32:29Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    22,
                    32,
                    29,
                    3,
                    121,
                    0
                ],
                "title": "Heterogeneous Memory Benchmarking Toolkit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous Memory Benchmarking Toolkit"
                },
                "summary": "This paper presents an open-source kernel-level heterogeneous memory\ncharacterization framework (MemScope) for embedded systems that enables users\nto understand and precisely characterize the temporal behavior of all available\nmemory modules under configurable contention stress scenarios. Since\nkernel-level provides a high degree of control over allocation, cache\nmaintenance, $CPUs$, interrupts, and I/O device activity, seeking the most\naccurate way to benchmark heterogeneous memory subsystems, would be achieved by\nimplementing it in the kernel. This gives us the privilege to directly map\npieces of contiguous physical memory and instantiate allocators, allowing us to\nfinely control cores to create and eliminate interference. Additionally, we can\nminimize noise and interruptions, guaranteeing more consistent and precise\nresults compared to equivalent user-space solutions. Running our Framework on a\nXilinx Zynq UltraScale+ ZCU102 CPU_FPGA platform, demonstrates its capability\nto precisely benchmark bandwidth and latency across various memory types,\nincluding PL-side DRAM and BRAM, in a multi-core system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an open-source kernel-level heterogeneous memory\ncharacterization framework (MemScope) for embedded systems that enables users\nto understand and precisely characterize the temporal behavior of all available\nmemory modules under configurable contention stress scenarios. Since\nkernel-level provides a high degree of control over allocation, cache\nmaintenance, $CPUs$, interrupts, and I/O device activity, seeking the most\naccurate way to benchmark heterogeneous memory subsystems, would be achieved by\nimplementing it in the kernel. This gives us the privilege to directly map\npieces of contiguous physical memory and instantiate allocators, allowing us to\nfinely control cores to create and eliminate interference. Additionally, we can\nminimize noise and interruptions, guaranteeing more consistent and precise\nresults compared to equivalent user-space solutions. Running our Framework on a\nXilinx Zynq UltraScale+ ZCU102 CPU_FPGA platform, demonstrates its capability\nto precisely benchmark bandwidth and latency across various memory types,\nincluding PL-side DRAM and BRAM, in a multi-core system."
                },
                "authors": [
                    {
                        "name": "Golsana Ghaemi"
                    },
                    {
                        "name": "Kazem Taram"
                    },
                    {
                        "name": "Renato Mancuso"
                    }
                ],
                "author_detail": {
                    "name": "Renato Mancuso"
                },
                "author": "Renato Mancuso",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00901v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00901v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00817v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00817v1",
                "updated": "2025-05-01T19:18:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    19,
                    18,
                    56,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T19:18:56Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    19,
                    18,
                    56,
                    3,
                    121,
                    0
                ],
                "title": "Spill The Beans: Exploiting CPU Cache Side-Channels to Leak Tokens from\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spill The Beans: Exploiting CPU Cache Side-Channels to Leak Tokens from\n  Large Language Models"
                },
                "summary": "Side-channel attacks on shared hardware resources increasingly threaten\nconfidentiality, especially with the rise of Large Language Models (LLMs). In\nthis work, we introduce Spill The Beans, a novel application of cache\nside-channels to leak tokens generated by an LLM. By co-locating an attack\nprocess on the same hardware as the victim model, we flush and reload embedding\nvectors from the embedding layer, where each token corresponds to a unique\nembedding vector. When accessed during token generation, it results in a cache\nhit detectable by our attack on shared lower-level caches.\n  A significant challenge is the massive size of LLMs, which, by nature of\ntheir compute intensive operation, quickly evicts embedding vectors from the\ncache. We address this by balancing the number of tokens monitored against the\namount of information leaked. Monitoring more tokens increases potential\nvocabulary leakage but raises the chance of missing cache hits due to eviction;\nmonitoring fewer tokens improves detection reliability but limits vocabulary\ncoverage.\n  Through extensive experimentation, we demonstrate the feasibility of leaking\ntokens from LLMs via cache side-channels. Our findings reveal a new\nvulnerability in LLM deployments, highlighting that even sophisticated models\nare susceptible to traditional side-channel attacks. We discuss the\nimplications for privacy and security in LLM-serving infrastructures and\nsuggest considerations for mitigating such threats. For proof of concept we\nconsider two concrete attack scenarios: Our experiments show that an attacker\ncan recover as much as 80%-90% of a high entropy API key with single shot\nmonitoring. As for English text we can reach a 40% recovery rate with a single\nshot. We should note that the rate highly depends on the monitored token set\nand these rates can be improved by targeting more specialized output domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Side-channel attacks on shared hardware resources increasingly threaten\nconfidentiality, especially with the rise of Large Language Models (LLMs). In\nthis work, we introduce Spill The Beans, a novel application of cache\nside-channels to leak tokens generated by an LLM. By co-locating an attack\nprocess on the same hardware as the victim model, we flush and reload embedding\nvectors from the embedding layer, where each token corresponds to a unique\nembedding vector. When accessed during token generation, it results in a cache\nhit detectable by our attack on shared lower-level caches.\n  A significant challenge is the massive size of LLMs, which, by nature of\ntheir compute intensive operation, quickly evicts embedding vectors from the\ncache. We address this by balancing the number of tokens monitored against the\namount of information leaked. Monitoring more tokens increases potential\nvocabulary leakage but raises the chance of missing cache hits due to eviction;\nmonitoring fewer tokens improves detection reliability but limits vocabulary\ncoverage.\n  Through extensive experimentation, we demonstrate the feasibility of leaking\ntokens from LLMs via cache side-channels. Our findings reveal a new\nvulnerability in LLM deployments, highlighting that even sophisticated models\nare susceptible to traditional side-channel attacks. We discuss the\nimplications for privacy and security in LLM-serving infrastructures and\nsuggest considerations for mitigating such threats. For proof of concept we\nconsider two concrete attack scenarios: Our experiments show that an attacker\ncan recover as much as 80%-90% of a high entropy API key with single shot\nmonitoring. As for English text we can reach a 40% recovery rate with a single\nshot. We should note that the rate highly depends on the monitored token set\nand these rates can be improved by targeting more specialized output domains."
                },
                "authors": [
                    {
                        "name": "Andrew Adiletta"
                    },
                    {
                        "name": "Berk Sunar"
                    }
                ],
                "author_detail": {
                    "name": "Berk Sunar"
                },
                "author": "Berk Sunar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00817v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00817v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00768v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00768v1",
                "updated": "2025-05-01T18:00:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    18,
                    0,
                    40,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T18:00:40Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    18,
                    0,
                    40,
                    3,
                    121,
                    0
                ],
                "title": "Optomechanical resource for fault-tolerant quantum computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optomechanical resource for fault-tolerant quantum computing"
                },
                "summary": "Fusion-based quantum computing with dual-rail qubits is a leading candidate\nfor scalable quantum computing using linear optics. This paradigm requires\nsingle photons which are entangled into small resource states before being fed\ninto a fusion network. The most common sources for single optical photons and\nfor small entangled states are probabilistic and heralded. The realization of a\nsingle reliable deterministic source requires many redundant probabilistic\nsources and a complex optical network for rerouting and retiming probabilistic\noutputs. In this work, we show how optomechanics enables reliable production of\nresources for photonic quantum computing without the redundancy of the\nall-optical approach. This is achieved by using acoustic modes as caches of\nquantum resources, ranging from single-particle states to small entangled\nstates, with on-demand read-out. The advantages of acoustic modes as optical\nquantum memories, compared to other technologies, include their intrinsically\nlong lifetimes and that they are solid state, highly tailorable, and\ninsensitive to electromagnetic noise. We show how the resource states can be\nprepared directly in the acoustic modes using optical controls. This is still\nprobabilistic and heralded, as in the all-optical approach, but the acoustic\nmodes act as a quantum memory which is integrated into the production of the\nstates. The quantum states may be deterministically transferred from acoustic\nmodes to optical modes, on demand, with another optical drive.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fusion-based quantum computing with dual-rail qubits is a leading candidate\nfor scalable quantum computing using linear optics. This paradigm requires\nsingle photons which are entangled into small resource states before being fed\ninto a fusion network. The most common sources for single optical photons and\nfor small entangled states are probabilistic and heralded. The realization of a\nsingle reliable deterministic source requires many redundant probabilistic\nsources and a complex optical network for rerouting and retiming probabilistic\noutputs. In this work, we show how optomechanics enables reliable production of\nresources for photonic quantum computing without the redundancy of the\nall-optical approach. This is achieved by using acoustic modes as caches of\nquantum resources, ranging from single-particle states to small entangled\nstates, with on-demand read-out. The advantages of acoustic modes as optical\nquantum memories, compared to other technologies, include their intrinsically\nlong lifetimes and that they are solid state, highly tailorable, and\ninsensitive to electromagnetic noise. We show how the resource states can be\nprepared directly in the acoustic modes using optical controls. This is still\nprobabilistic and heralded, as in the all-optical approach, but the acoustic\nmodes act as a quantum memory which is integrated into the production of the\nstates. The quantum states may be deterministically transferred from acoustic\nmodes to optical modes, on demand, with another optical drive."
                },
                "authors": [
                    {
                        "name": "Margaret Pavlovich"
                    },
                    {
                        "name": "Peter Rakich"
                    },
                    {
                        "name": "Shruti Puri"
                    }
                ],
                "author_detail": {
                    "name": "Shruti Puri"
                },
                "author": "Shruti Puri",
                "arxiv_comment": "19 pages, 9 figures. Supplement 29 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00768v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00768v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00315v1",
                "updated": "2025-05-01T05:22:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    5,
                    22,
                    11,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T05:22:11Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    5,
                    22,
                    11,
                    3,
                    121,
                    0
                ],
                "title": "Mixture of Sparse Attention: Content-Based Learnable Sparse Attention\n  via Expert-Choice Routing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Sparse Attention: Content-Based Learnable Sparse Attention\n  via Expert-Choice Routing"
                },
                "summary": "Recent advances in large language models highlighted the excessive quadratic\ncost of self-attention. Despite the significant research efforts, subquadratic\nattention methods still suffer from inferior performance in practice. We\nhypothesize that dynamic, learned content-based sparsity can lead to more\nefficient attention mechanisms. We present Mixture of Sparse Attention (MoSA),\na novel approach inspired by Mixture of Experts (MoE) with expert choice\nrouting. MoSA dynamically selects tokens for each attention head, allowing\narbitrary sparse attention patterns. By selecting $k$ tokens from a sequence of\nlength $T$, MoSA reduces the computational complexity of each attention head\nfrom $O(T^2)$ to $O(k^2 + T)$. This enables using more heads within the same\ncomputational budget, allowing higher specialization. We show that among the\ntested sparse attention variants, MoSA is the only one that can outperform the\ndense baseline, sometimes with up to 27% better perplexity for an identical\ncompute budget. MoSA can also reduce the resource usage compared to dense\nself-attention. Despite using torch implementation without an optimized kernel,\nperplexity-matched MoSA models are simultaneously faster in wall-clock time,\nrequire less memory for training, and drastically reduce the size of the\nKV-cache compared to the dense transformer baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models highlighted the excessive quadratic\ncost of self-attention. Despite the significant research efforts, subquadratic\nattention methods still suffer from inferior performance in practice. We\nhypothesize that dynamic, learned content-based sparsity can lead to more\nefficient attention mechanisms. We present Mixture of Sparse Attention (MoSA),\na novel approach inspired by Mixture of Experts (MoE) with expert choice\nrouting. MoSA dynamically selects tokens for each attention head, allowing\narbitrary sparse attention patterns. By selecting $k$ tokens from a sequence of\nlength $T$, MoSA reduces the computational complexity of each attention head\nfrom $O(T^2)$ to $O(k^2 + T)$. This enables using more heads within the same\ncomputational budget, allowing higher specialization. We show that among the\ntested sparse attention variants, MoSA is the only one that can outperform the\ndense baseline, sometimes with up to 27% better perplexity for an identical\ncompute budget. MoSA can also reduce the resource usage compared to dense\nself-attention. Despite using torch implementation without an optimized kernel,\nperplexity-matched MoSA models are simultaneously faster in wall-clock time,\nrequire less memory for training, and drastically reduce the size of the\nKV-cache compared to the dense transformer baselines."
                },
                "authors": [
                    {
                        "name": "Piotr Piękos"
                    },
                    {
                        "name": "Róbert Csordás"
                    },
                    {
                        "name": "Jürgen Schmidhuber"
                    }
                ],
                "author_detail": {
                    "name": "Jürgen Schmidhuber"
                },
                "author": "Jürgen Schmidhuber",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04532v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04532v3",
                "updated": "2025-05-01T02:14:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    2,
                    14,
                    5,
                    3,
                    121,
                    0
                ],
                "published": "2024-05-07T17:59:30Z",
                "published_parsed": [
                    2024,
                    5,
                    7,
                    17,
                    59,
                    30,
                    1,
                    128,
                    0
                ],
                "title": "QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM\n  Serving"
                },
                "summary": "Quantization can accelerate large language model (LLM) inference. Going\nbeyond INT8 quantization, the research community is actively exploring even\nlower precision, such as INT4. Nonetheless, state-of-the-art INT4 quantization\ntechniques only accelerate low-batch, edge LLM inference, failing to deliver\nperformance gains in large-batch, cloud-based LLM serving. We uncover a\ncritical issue: existing INT4 quantization methods suffer from significant\nruntime overhead (20-90%) when dequantizing either weights or partial sums on\nGPUs. To address this challenge, we introduce QoQ, a W4A8KV4 quantization\nalgorithm with 4-bit weight, 8-bit activation, and 4-bit KV cache. QoQ stands\nfor quattuor-octo-quattuor, which represents 4-8-4 in Latin. QoQ is implemented\nby the QServe inference library that achieves measured speedup. The key insight\ndriving QServe is that the efficiency of LLM serving on GPUs is critically\ninfluenced by operations on low-throughput CUDA cores. Building upon this\ninsight, in QoQ algorithm, we introduce progressive quantization that can allow\nlow dequantization overhead in W4A8 GEMM. Additionally, we develop\nSmoothAttention to effectively mitigate the accuracy degradation incurred by\n4-bit KV quantization. In the QServe system, we perform compute-aware weight\nreordering and take advantage of register-level parallelism to reduce\ndequantization latency. We also make fused attention memory-bound, harnessing\nthe performance gain brought by KV4 quantization. As a result, QServe improves\nthe maximum achievable serving throughput of Llama-3-8B by 1.2x on A100, 1.4x\non L40S; and Qwen1.5-72B by 2.4x on A100, 3.5x on L40S, compared to\nTensorRT-LLM. Remarkably, QServe on L40S GPU can achieve even higher throughput\nthan TensorRT-LLM on A100. Thus, QServe effectively reduces the dollar cost of\nLLM serving by 3x. Code is available at\nhttps://github.com/mit-han-lab/omniserve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization can accelerate large language model (LLM) inference. Going\nbeyond INT8 quantization, the research community is actively exploring even\nlower precision, such as INT4. Nonetheless, state-of-the-art INT4 quantization\ntechniques only accelerate low-batch, edge LLM inference, failing to deliver\nperformance gains in large-batch, cloud-based LLM serving. We uncover a\ncritical issue: existing INT4 quantization methods suffer from significant\nruntime overhead (20-90%) when dequantizing either weights or partial sums on\nGPUs. To address this challenge, we introduce QoQ, a W4A8KV4 quantization\nalgorithm with 4-bit weight, 8-bit activation, and 4-bit KV cache. QoQ stands\nfor quattuor-octo-quattuor, which represents 4-8-4 in Latin. QoQ is implemented\nby the QServe inference library that achieves measured speedup. The key insight\ndriving QServe is that the efficiency of LLM serving on GPUs is critically\ninfluenced by operations on low-throughput CUDA cores. Building upon this\ninsight, in QoQ algorithm, we introduce progressive quantization that can allow\nlow dequantization overhead in W4A8 GEMM. Additionally, we develop\nSmoothAttention to effectively mitigate the accuracy degradation incurred by\n4-bit KV quantization. In the QServe system, we perform compute-aware weight\nreordering and take advantage of register-level parallelism to reduce\ndequantization latency. We also make fused attention memory-bound, harnessing\nthe performance gain brought by KV4 quantization. As a result, QServe improves\nthe maximum achievable serving throughput of Llama-3-8B by 1.2x on A100, 1.4x\non L40S; and Qwen1.5-72B by 2.4x on A100, 3.5x on L40S, compared to\nTensorRT-LLM. Remarkably, QServe on L40S GPU can achieve even higher throughput\nthan TensorRT-LLM on A100. Thus, QServe effectively reduces the dollar cost of\nLLM serving by 3x. Code is available at\nhttps://github.com/mit-han-lab/omniserve."
                },
                "authors": [
                    {
                        "name": "Yujun Lin"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Zhekai Zhang"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Chuang Gan"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "The first three authors contribute equally to this project and are\n  listed in the alphabetical order. Yujun Lin leads the quantization algorithm,\n  Haotian Tang and Shang Yang lead the GPU kernels and the serving system. Code\n  is available at https://github.com/mit-han-lab/omniserve",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.04532v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04532v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19602v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19602v2",
                "updated": "2025-05-01T00:13:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    0,
                    13,
                    6,
                    3,
                    121,
                    0
                ],
                "published": "2025-04-28T09:04:30Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    4,
                    30,
                    0,
                    118,
                    0
                ],
                "title": "Soft-Label Caching and Sharpening for Communication-Efficient Federated\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soft-Label Caching and Sharpening for Communication-Efficient Federated\n  Distillation"
                },
                "summary": "Federated Learning (FL) enables collaborative model training across\ndecentralized clients, enhancing privacy by keeping data local. Yet\nconventional FL, relying on frequent parameter-sharing, suffers from high\ncommunication overhead and limited model heterogeneity. Distillation-based FL\napproaches address these issues by sharing predictions (soft-labels) instead,\nbut they often involve redundant transmissions across communication rounds,\nreducing efficiency. We propose SCARLET, a novel framework integrating\nsynchronized soft-label caching and an enhanced Entropy Reduction Aggregation\n(Enhanced ERA) mechanism. SCARLET minimizes redundant communication by reusing\ncached soft-labels, achieving up to 50% reduction in communication costs\ncompared to existing methods while maintaining accuracy. Enhanced ERA can be\ntuned to adapt to non-IID data variations, ensuring robust aggregation and\nperformance in diverse client scenarios. Experimental evaluations demonstrate\nthat SCARLET consistently outperforms state-of-the-art distillation-based FL\nmethods in terms of accuracy and communication efficiency. The implementation\nof SCARLET is publicly available at https://github.com/kitsuyaazuma/SCARLET.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) enables collaborative model training across\ndecentralized clients, enhancing privacy by keeping data local. Yet\nconventional FL, relying on frequent parameter-sharing, suffers from high\ncommunication overhead and limited model heterogeneity. Distillation-based FL\napproaches address these issues by sharing predictions (soft-labels) instead,\nbut they often involve redundant transmissions across communication rounds,\nreducing efficiency. We propose SCARLET, a novel framework integrating\nsynchronized soft-label caching and an enhanced Entropy Reduction Aggregation\n(Enhanced ERA) mechanism. SCARLET minimizes redundant communication by reusing\ncached soft-labels, achieving up to 50% reduction in communication costs\ncompared to existing methods while maintaining accuracy. Enhanced ERA can be\ntuned to adapt to non-IID data variations, ensuring robust aggregation and\nperformance in diverse client scenarios. Experimental evaluations demonstrate\nthat SCARLET consistently outperforms state-of-the-art distillation-based FL\nmethods in terms of accuracy and communication efficiency. The implementation\nof SCARLET is publicly available at https://github.com/kitsuyaazuma/SCARLET."
                },
                "authors": [
                    {
                        "name": "Kitsuya Azuma"
                    },
                    {
                        "name": "Takayuki Nishio"
                    },
                    {
                        "name": "Yuichi Kitagawa"
                    },
                    {
                        "name": "Wakako Nakano"
                    },
                    {
                        "name": "Takahito Tanimura"
                    }
                ],
                "author_detail": {
                    "name": "Takahito Tanimura"
                },
                "author": "Takahito Tanimura",
                "arxiv_comment": "15 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19602v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19602v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19243v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19243v2",
                "updated": "2025-04-30T19:48:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    19,
                    48,
                    41,
                    2,
                    120,
                    0
                ],
                "published": "2025-01-31T15:58:15Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    58,
                    15,
                    4,
                    31,
                    0
                ],
                "title": "Accelerating Diffusion Transformer via Error-Optimized Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer via Error-Optimized Cache"
                },
                "summary": "Diffusion Transformer (DiT) is a crucial method for content generation.\nHowever, it needs a lot of time to sample. Many studies have attempted to use\ncaching to reduce the time consumption of sampling. Existing caching methods\naccelerate generation by reusing DiT features from the previous time step and\nskipping calculations in the next, but they tend to locate and cache low-error\nmodules without focusing on reducing caching-induced errors, resulting in a\nsharp decline in generated content quality when increasing caching intensity.\nTo solve this problem, we propose the Error-Optimized Cache (EOC). This method\nintroduces three key improvements: (1) Prior knowledge extraction: Extract and\nprocess the caching differences; (2) A judgment method for cache optimization:\nDetermine whether certain caching steps need to be optimized; (3) Cache\noptimization: reduce caching errors. Experiments show that this algorithm\nsignificantly reduces the error accumulation caused by caching, especially\nexcessive caching. On the ImageNet dataset, without substantially increasing\nthe computational load, this method improves the FID of the generated images\nwhen the rule-based model FORA has a caching level of 75%, 50%, and 25%, and\nthe training-based model Learning-to-cache has a caching level of 22%.\nSpecifically, the FID values change from 30.454 to 21.690 (28.8%), from 6.857\nto 5.821 (15.1%), from 3.870 to 3.692 (4.6%), and from 3.539 to 3.451 (2.5%)\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT) is a crucial method for content generation.\nHowever, it needs a lot of time to sample. Many studies have attempted to use\ncaching to reduce the time consumption of sampling. Existing caching methods\naccelerate generation by reusing DiT features from the previous time step and\nskipping calculations in the next, but they tend to locate and cache low-error\nmodules without focusing on reducing caching-induced errors, resulting in a\nsharp decline in generated content quality when increasing caching intensity.\nTo solve this problem, we propose the Error-Optimized Cache (EOC). This method\nintroduces three key improvements: (1) Prior knowledge extraction: Extract and\nprocess the caching differences; (2) A judgment method for cache optimization:\nDetermine whether certain caching steps need to be optimized; (3) Cache\noptimization: reduce caching errors. Experiments show that this algorithm\nsignificantly reduces the error accumulation caused by caching, especially\nexcessive caching. On the ImageNet dataset, without substantially increasing\nthe computational load, this method improves the FID of the generated images\nwhen the rule-based model FORA has a caching level of 75%, 50%, and 25%, and\nthe training-based model Learning-to-cache has a caching level of 22%.\nSpecifically, the FID values change from 30.454 to 21.690 (28.8%), from 6.857\nto 5.821 (15.1%), from 3.870 to 3.692 (4.6%), and from 3.539 to 3.451 (2.5%)\nrespectively."
                },
                "authors": [
                    {
                        "name": "Junxiang Qiu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Jinda Lu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Houcheng Jiang"
                    },
                    {
                        "name": "Xingyu Zhu"
                    },
                    {
                        "name": "Yanbin Hao"
                    }
                ],
                "author_detail": {
                    "name": "Yanbin Hao"
                },
                "author": "Yanbin Hao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19243v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19243v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00074v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00074v1",
                "updated": "2025-04-30T18:00:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    18,
                    0,
                    2,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T18:00:02Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    18,
                    0,
                    2,
                    2,
                    120,
                    0
                ],
                "title": "SDW driven \"magnetic breakdown\" in a d-wave altermagnet KV$_2$Se$_2$O",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SDW driven \"magnetic breakdown\" in a d-wave altermagnet KV$_2$Se$_2$O"
                },
                "summary": "Altermagnets, combining zero net magnetization with intrinsic spin splitting,\ndemonstrate unique quantum phenomena crucial for spintronic applications.\nKV$_2$Se$_2$O is proven to be a d-wave altermagnet with phase transition from a\ncheckerboard-type (C-type) antiferromagnetic (AFM) state to a spin density wave\n(SDW) state as the temperature decreases. After phase transition, the apparent\nparadox emerges where angle-resolved photoemission spectroscopy (ARPES) reveals\nnegligible Fermi surface modifications, while physical property measurement\nsystem (PPMS) measurements uncover substantial changes in transport properties.\nOur study explores the microscopic mechanisms governing phase-dependent\ntransport properties of KV$_2$Se$_2$O base on first-principles calculations.\nThe spin canting driven by periodic spin modulation in the SDW phase reduces\nthe magnetic symmetry of KV$_2$Se$_2$O. The resultant band degeneracy lifting\nand Fermi surface reconstruction induce the ``magnetic breakdown\" phenomenon,\nwhich alters carrier trajectories, modifies carrier concentration, strengthens\nelectron-hole compensation, and ultimately accounts for the contrasting\nmagnetic-field-dependent Hall resistivity relative to the C-type AFM state. Our\nwork proposes an innovative method for identifying the electronic structure\nevolution across phase transitions from transport signatures, providing a novel\nparadigm for altermagnets research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Altermagnets, combining zero net magnetization with intrinsic spin splitting,\ndemonstrate unique quantum phenomena crucial for spintronic applications.\nKV$_2$Se$_2$O is proven to be a d-wave altermagnet with phase transition from a\ncheckerboard-type (C-type) antiferromagnetic (AFM) state to a spin density wave\n(SDW) state as the temperature decreases. After phase transition, the apparent\nparadox emerges where angle-resolved photoemission spectroscopy (ARPES) reveals\nnegligible Fermi surface modifications, while physical property measurement\nsystem (PPMS) measurements uncover substantial changes in transport properties.\nOur study explores the microscopic mechanisms governing phase-dependent\ntransport properties of KV$_2$Se$_2$O base on first-principles calculations.\nThe spin canting driven by periodic spin modulation in the SDW phase reduces\nthe magnetic symmetry of KV$_2$Se$_2$O. The resultant band degeneracy lifting\nand Fermi surface reconstruction induce the ``magnetic breakdown\" phenomenon,\nwhich alters carrier trajectories, modifies carrier concentration, strengthens\nelectron-hole compensation, and ultimately accounts for the contrasting\nmagnetic-field-dependent Hall resistivity relative to the C-type AFM state. Our\nwork proposes an innovative method for identifying the electronic structure\nevolution across phase transitions from transport signatures, providing a novel\nparadigm for altermagnets research."
                },
                "authors": [
                    {
                        "name": "Xu Yan"
                    },
                    {
                        "name": "Ziyin Song"
                    },
                    {
                        "name": "Juntao Song"
                    },
                    {
                        "name": "Zhong Fang"
                    },
                    {
                        "name": "Hongming Weng"
                    },
                    {
                        "name": "Quansheng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Quansheng Wu"
                },
                "author": "Quansheng Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00074v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00074v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21594v1",
                "updated": "2025-04-30T12:51:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    12,
                    51,
                    59,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T12:51:59Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    12,
                    51,
                    59,
                    2,
                    120,
                    0
                ],
                "title": "Switching Transients in Constrained Transformer-Line/Cable\n  Configurations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Switching Transients in Constrained Transformer-Line/Cable\n  Configurations"
                },
                "summary": "This paper investigates the transient phenomena that occur in two special\ncases in the Netherlands: (A) during the energization of a power transformer\nvia a cable feeder and (B) the energization of a power transformer together\nwith an overhead line (OHL). In Case A a 7 km long 150 kV cable and a 150/50 kV\ntransformer are connected and energized at the same time. In Case B a 150/50 kV\ntransformer and a short 50 kV OHL are connected and energized simultaneously.\nThe reason behind this kind of situations is related to space restrictions and\ncost efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the transient phenomena that occur in two special\ncases in the Netherlands: (A) during the energization of a power transformer\nvia a cable feeder and (B) the energization of a power transformer together\nwith an overhead line (OHL). In Case A a 7 km long 150 kV cable and a 150/50 kV\ntransformer are connected and energized at the same time. In Case B a 150/50 kV\ntransformer and a short 50 kV OHL are connected and energized simultaneously.\nThe reason behind this kind of situations is related to space restrictions and\ncost efficiency."
                },
                "authors": [
                    {
                        "name": "Y. Xiang"
                    },
                    {
                        "name": "L. Wu"
                    },
                    {
                        "name": "K. Velitsikakis"
                    },
                    {
                        "name": "A. L. J. Janssen"
                    }
                ],
                "author_detail": {
                    "name": "A. L. J. Janssen"
                },
                "author": "A. L. J. Janssen",
                "arxiv_comment": "11 pages, 17 figures, CIGRE conference 2016",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00745v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00745v1",
                "updated": "2025-04-30T08:08:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    8,
                    8,
                    15,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T08:08:15Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    8,
                    8,
                    15,
                    2,
                    120,
                    0
                ],
                "title": "Responsive DNN Adaptation for Video Analytics against Environment Shift\n  via Hierarchical Mobile-Cloud Collaborations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Responsive DNN Adaptation for Video Analytics against Environment Shift\n  via Hierarchical Mobile-Cloud Collaborations"
                },
                "summary": "Mobile video analysis systems often encounter various deploying environments,\nwhere environment shifts present greater demands for responsiveness in\nadaptations of deployed \"expert DNN models\". Existing model adaptation\nframeworks primarily operate in a cloud-centric way, exhibiting degraded\nperformance during adaptation and delayed reactions to environment shifts.\nInstead, this paper proposes MOCHA, a novel framework optimizing the\nresponsiveness of continuous model adaptation through hierarchical\ncollaborations between mobile and cloud resources. Specifically, MOCHA (1)\nreduces adaptation response delays by performing on-device model reuse and fast\nfine-tuning before requesting cloud model retrieval and end-to-end retraining;\n(2) accelerates history expert model retrieval by organizing them into a\nstructured taxonomy utilizing domain semantics analyzed by a cloud foundation\nmodel as indices; (3) enables efficient local model reuse by maintaining\nonboard expert model caches for frequent scenes, which proactively prefetch\nmodel weights from the cloud model database. Extensive evaluations with\nreal-world videos on three DNN tasks show MOCHA improves the model accuracy\nduring adaptation by up to 6.8% while saving the response delay and retraining\ntime by up to 35.5x and 3.0x respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile video analysis systems often encounter various deploying environments,\nwhere environment shifts present greater demands for responsiveness in\nadaptations of deployed \"expert DNN models\". Existing model adaptation\nframeworks primarily operate in a cloud-centric way, exhibiting degraded\nperformance during adaptation and delayed reactions to environment shifts.\nInstead, this paper proposes MOCHA, a novel framework optimizing the\nresponsiveness of continuous model adaptation through hierarchical\ncollaborations between mobile and cloud resources. Specifically, MOCHA (1)\nreduces adaptation response delays by performing on-device model reuse and fast\nfine-tuning before requesting cloud model retrieval and end-to-end retraining;\n(2) accelerates history expert model retrieval by organizing them into a\nstructured taxonomy utilizing domain semantics analyzed by a cloud foundation\nmodel as indices; (3) enables efficient local model reuse by maintaining\nonboard expert model caches for frequent scenes, which proactively prefetch\nmodel weights from the cloud model database. Extensive evaluations with\nreal-world videos on three DNN tasks show MOCHA improves the model accuracy\nduring adaptation by up to 6.8% while saving the response delay and retraining\ntime by up to 35.5x and 3.0x respectively."
                },
                "authors": [
                    {
                        "name": "Maozhe Zhao"
                    },
                    {
                        "name": "Shengzhong Liu"
                    },
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "arxiv_comment": "Sensys 2025 final version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00745v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21230v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21230v1",
                "updated": "2025-04-29T23:43:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    23,
                    43,
                    59,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T23:43:59Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    23,
                    43,
                    59,
                    1,
                    119,
                    0
                ],
                "title": "Kimina Lean Server: Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kimina Lean Server: Technical Report"
                },
                "summary": "We introduce the Kimina Lean Server, an open-source project that enables fast\nand scalable interaction with Lean 4 via a unified REST API, designed as a\nsimple verifier for reinforcement learning pipelines. Built on top of the Lean\nFRO's LeanREPL, it combines server-side parallelization by managing multiple\nLean REPL processes in parallel, with an LRU caching strategy that reuses Lean\nimports across multiple requests. These features help reduce initialization\noverhead and allow large-scale batch processing of Lean code. The client-side\ninterface allows users to submit batches of proofs and receive Lean feedback,\nincluding extracted tactics and tactic states via infotree processing. These\nfeatures enable a high-performance, scalable workflow for both interaction and\nextraction of proofs, tactics, and tactic states. We open source our\nimplementation on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the Kimina Lean Server, an open-source project that enables fast\nand scalable interaction with Lean 4 via a unified REST API, designed as a\nsimple verifier for reinforcement learning pipelines. Built on top of the Lean\nFRO's LeanREPL, it combines server-side parallelization by managing multiple\nLean REPL processes in parallel, with an LRU caching strategy that reuses Lean\nimports across multiple requests. These features help reduce initialization\noverhead and allow large-scale batch processing of Lean code. The client-side\ninterface allows users to submit batches of proofs and receive Lean feedback,\nincluding extracted tactics and tactic states via infotree processing. These\nfeatures enable a high-performance, scalable workflow for both interaction and\nextraction of proofs, tactics, and tactic states. We open source our\nimplementation on GitHub."
                },
                "authors": [
                    {
                        "name": "Marco Dos Santos"
                    },
                    {
                        "name": "Haiming Wang"
                    },
                    {
                        "name": "Hugues de Saxcé"
                    },
                    {
                        "name": "Ran Wang"
                    },
                    {
                        "name": "Mantas Baksys"
                    },
                    {
                        "name": "Mert Unsal"
                    },
                    {
                        "name": "Junqi Liu"
                    },
                    {
                        "name": "Zhengying Liu"
                    },
                    {
                        "name": "Jia Li"
                    }
                ],
                "author_detail": {
                    "name": "Jia Li"
                },
                "author": "Jia Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21230v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21230v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21228v1",
                "updated": "2025-04-29T23:42:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    23,
                    42,
                    21,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T23:42:21Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    23,
                    42,
                    21,
                    1,
                    119,
                    0
                ],
                "title": "CachePrune: Neural-Based Attribution Defense Against Indirect Prompt\n  Injection Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CachePrune: Neural-Based Attribution Defense Against Indirect Prompt\n  Injection Attacks"
                },
                "summary": "Large Language Models (LLMs) are identified as being susceptible to indirect\nprompt injection attack, where the model undesirably deviates from\nuser-provided instructions by executing tasks injected in the prompt context.\nThis vulnerability stems from LLMs' inability to distinguish between data and\ninstructions within a prompt. In this paper, we propose CachePrune that defends\nagainst this attack by identifying and pruning task-triggering neurons from the\nKV cache of the input prompt context. By pruning such neurons, we encourage the\nLLM to treat the text spans of input prompt context as only pure data, instead\nof any indicator of instruction following. These neurons are identified via\nfeature attribution with a loss function induced from an upperbound of the\nDirect Preference Optimization (DPO) objective. We show that such a loss\nfunction enables effective feature attribution with only a few samples. We\nfurther improve on the quality of feature attribution, by exploiting an\nobserved triggering effect in instruction following. Our approach does not\nimpose any formatting on the original prompt or introduce extra test-time LLM\ncalls. Experiments show that CachePrune significantly reduces attack success\nrates without compromising the response quality. Note: This paper aims to\ndefend against indirect prompt injection attacks, with the goal of developing\nmore secure and robust AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are identified as being susceptible to indirect\nprompt injection attack, where the model undesirably deviates from\nuser-provided instructions by executing tasks injected in the prompt context.\nThis vulnerability stems from LLMs' inability to distinguish between data and\ninstructions within a prompt. In this paper, we propose CachePrune that defends\nagainst this attack by identifying and pruning task-triggering neurons from the\nKV cache of the input prompt context. By pruning such neurons, we encourage the\nLLM to treat the text spans of input prompt context as only pure data, instead\nof any indicator of instruction following. These neurons are identified via\nfeature attribution with a loss function induced from an upperbound of the\nDirect Preference Optimization (DPO) objective. We show that such a loss\nfunction enables effective feature attribution with only a few samples. We\nfurther improve on the quality of feature attribution, by exploiting an\nobserved triggering effect in instruction following. Our approach does not\nimpose any formatting on the original prompt or introduce extra test-time LLM\ncalls. Experiments show that CachePrune significantly reduces attack success\nrates without compromising the response quality. Note: This paper aims to\ndefend against indirect prompt injection attacks, with the goal of developing\nmore secure and robust AI systems."
                },
                "authors": [
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Junda Wu"
                    },
                    {
                        "name": "Yu Xia"
                    },
                    {
                        "name": "Tong Yu"
                    },
                    {
                        "name": "Ruiyi Zhang"
                    },
                    {
                        "name": "Ryan Rossi"
                    },
                    {
                        "name": "Lina Yao"
                    },
                    {
                        "name": "Julian McAuley"
                    }
                ],
                "author_detail": {
                    "name": "Julian McAuley"
                },
                "author": "Julian McAuley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2505.13446v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13446v1",
                "updated": "2025-05-19T17:59:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    59,
                    35,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T17:59:35Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    59,
                    35,
                    0,
                    139,
                    0
                ],
                "title": "Unlocking Non-Invasive Brain-to-Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking Non-Invasive Brain-to-Text"
                },
                "summary": "Despite major advances in surgical brain-to-text (B2T), i.e. transcribing\nspeech from invasive brain recordings, non-invasive alternatives have yet to\nsurpass even chance on standard metrics. This remains a barrier to building a\nnon-invasive brain-computer interface (BCI) capable of restoring communication\nin paralysed individuals without surgery. Here, we present the first\nnon-invasive B2T result that significantly exceeds these critical baselines,\nraising BLEU by $1.4\\mathrm{-}2.6\\times$ over prior work. This result is driven\nby three contributions: (1) we extend recent word-classification models with\nLLM-based rescoring, transforming single-word predictors into closed-vocabulary\nB2T systems; (2) we introduce a predictive in-filling approach to handle\nout-of-vocabulary (OOV) words, substantially expanding the effective\nvocabulary; and (3) we demonstrate, for the first time, how to scale\nnon-invasive B2T models across datasets, unlocking deep learning at scale and\nimproving accuracy by $2.1\\mathrm{-}2.3\\times$. Through these contributions, we\noffer new insights into the roles of data quality and vocabulary size.\nTogether, our results remove a major obstacle to realising practical\nnon-invasive B2T systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite major advances in surgical brain-to-text (B2T), i.e. transcribing\nspeech from invasive brain recordings, non-invasive alternatives have yet to\nsurpass even chance on standard metrics. This remains a barrier to building a\nnon-invasive brain-computer interface (BCI) capable of restoring communication\nin paralysed individuals without surgery. Here, we present the first\nnon-invasive B2T result that significantly exceeds these critical baselines,\nraising BLEU by $1.4\\mathrm{-}2.6\\times$ over prior work. This result is driven\nby three contributions: (1) we extend recent word-classification models with\nLLM-based rescoring, transforming single-word predictors into closed-vocabulary\nB2T systems; (2) we introduce a predictive in-filling approach to handle\nout-of-vocabulary (OOV) words, substantially expanding the effective\nvocabulary; and (3) we demonstrate, for the first time, how to scale\nnon-invasive B2T models across datasets, unlocking deep learning at scale and\nimproving accuracy by $2.1\\mathrm{-}2.3\\times$. Through these contributions, we\noffer new insights into the roles of data quality and vocabulary size.\nTogether, our results remove a major obstacle to realising practical\nnon-invasive B2T systems."
                },
                "authors": [
                    {
                        "name": "Dulhan Jayalath"
                    },
                    {
                        "name": "Gilad Landau"
                    },
                    {
                        "name": "Oiwi Parker Jones"
                    }
                ],
                "author_detail": {
                    "name": "Oiwi Parker Jones"
                },
                "author": "Oiwi Parker Jones",
                "arxiv_comment": "27 pages, 10 figures, 10 tables. Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13446v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13446v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13445v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13445v1",
                "updated": "2025-05-19T17:59:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    59,
                    31,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T17:59:31Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    59,
                    31,
                    0,
                    139,
                    0
                ],
                "title": "Trust, But Verify: A Self-Verification Approach to Reinforcement\n  Learning with Verifiable Rewards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trust, But Verify: A Self-Verification Approach to Reinforcement\n  Learning with Verifiable Rewards"
                },
                "summary": "Large Language Models (LLMs) show great promise in complex reasoning, with\nReinforcement Learning with Verifiable Rewards (RLVR) being a key enhancement\nstrategy. However, a prevalent issue is ``superficial self-reflection'', where\nmodels fail to robustly verify their own outputs. We introduce RISE\n(Reinforcing Reasoning with Self-Verification), a novel online RL framework\ndesigned to tackle this. RISE explicitly and simultaneously trains an LLM to\nimprove both its problem-solving and self-verification abilities within a\nsingle, integrated RL process. The core mechanism involves leveraging\nverifiable rewards from an outcome verifier to provide on-the-fly feedback for\nboth solution generation and self-verification tasks. In each iteration, the\nmodel generates solutions, then critiques its own on-policy generated\nsolutions, with both trajectories contributing to the policy update. Extensive\nexperiments on diverse mathematical reasoning benchmarks show that RISE\nconsistently improves model's problem-solving accuracy while concurrently\nfostering strong self-verification skills. Our analyses highlight the\nadvantages of online verification and the benefits of increased verification\ncompute. Additionally, RISE models exhibit more frequent and accurate\nself-verification behaviors during reasoning. These advantages reinforce RISE\nas a flexible and effective path towards developing more robust and self-aware\nreasoners.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) show great promise in complex reasoning, with\nReinforcement Learning with Verifiable Rewards (RLVR) being a key enhancement\nstrategy. However, a prevalent issue is ``superficial self-reflection'', where\nmodels fail to robustly verify their own outputs. We introduce RISE\n(Reinforcing Reasoning with Self-Verification), a novel online RL framework\ndesigned to tackle this. RISE explicitly and simultaneously trains an LLM to\nimprove both its problem-solving and self-verification abilities within a\nsingle, integrated RL process. The core mechanism involves leveraging\nverifiable rewards from an outcome verifier to provide on-the-fly feedback for\nboth solution generation and self-verification tasks. In each iteration, the\nmodel generates solutions, then critiques its own on-policy generated\nsolutions, with both trajectories contributing to the policy update. Extensive\nexperiments on diverse mathematical reasoning benchmarks show that RISE\nconsistently improves model's problem-solving accuracy while concurrently\nfostering strong self-verification skills. Our analyses highlight the\nadvantages of online verification and the benefits of increased verification\ncompute. Additionally, RISE models exhibit more frequent and accurate\nself-verification behaviors during reasoning. These advantages reinforce RISE\nas a flexible and effective path towards developing more robust and self-aware\nreasoners."
                },
                "authors": [
                    {
                        "name": "Xiaoyuan Liu"
                    },
                    {
                        "name": "Tian Liang"
                    },
                    {
                        "name": "Zhiwei He"
                    },
                    {
                        "name": "Jiahao Xu"
                    },
                    {
                        "name": "Wenxuan Wang"
                    },
                    {
                        "name": "Pinjia He"
                    },
                    {
                        "name": "Zhaopeng Tu"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Dong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yu"
                },
                "author": "Dong Yu",
                "arxiv_comment": "code available at https://github.com/xyliu-cs/RISE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13445v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13445v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15175v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15175v3",
                "updated": "2025-05-19T17:58:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    58,
                    53,
                    0,
                    139,
                    0
                ],
                "published": "2025-01-25T11:06:37Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    11,
                    6,
                    37,
                    5,
                    25,
                    0
                ],
                "title": "Option-ID Based Elimination For Multiple Choice Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Option-ID Based Elimination For Multiple Choice Questions"
                },
                "summary": "Multiple choice questions (MCQs) are a popular and important task for\nevaluating large language models (LLMs). Based on common strategies people use\nwhen answering MCQs, the process of elimination (PoE) has been proposed as an\neffective problem-solving method. Existing PoE methods typically either have\nLLMs directly identify incorrect options or score options and replace\nlower-scoring ones with [MASK]. However, both methods suffer from\ninapplicability or suboptimal performance. To address these issues, this paper\nproposes a novel option-ID based PoE ($\\text{PoE}_{\\text{ID}}$).\n$\\text{PoE}_{\\text{ID}}$ critically incorporates a debiasing technique to\ncounteract LLMs token bias, enhancing robustness over naive ID-based\nelimination. It features two strategies: $\\text{PoE}_{\\text{ID}}^{\\text{log}}$,\nwhich eliminates options whose IDs have log probabilities below the average\nthreshold, and $\\text{PoE}_{\\text{ID}}^{\\text{seq}}$, which iteratively removes\nthe option with the lowest ID probability. We conduct extensive experiments\nwith 6 different LLMs on 4 diverse datasets. The results demonstrate that\n$\\text{PoE}_{\\text{ID}}$, especially $\\text{PoE}_{\\text{ID}}^{\\text{log}}$,\nsignificantly improves zero-shot and few-shot MCQs performance, particularly in\ndatasets with more options. Our analyses demonstrate that\n$\\text{PoE}_{\\text{ID}}^{\\text{log}}$ enhances the LLMs' confidence in\nselecting the correct option, and the option elimination strategy outperforms\nmethods relying on [MASK] replacement. We further investigate the limitations\nof LLMs in directly identifying incorrect options, which stem from their\ninherent deficiencies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiple choice questions (MCQs) are a popular and important task for\nevaluating large language models (LLMs). Based on common strategies people use\nwhen answering MCQs, the process of elimination (PoE) has been proposed as an\neffective problem-solving method. Existing PoE methods typically either have\nLLMs directly identify incorrect options or score options and replace\nlower-scoring ones with [MASK]. However, both methods suffer from\ninapplicability or suboptimal performance. To address these issues, this paper\nproposes a novel option-ID based PoE ($\\text{PoE}_{\\text{ID}}$).\n$\\text{PoE}_{\\text{ID}}$ critically incorporates a debiasing technique to\ncounteract LLMs token bias, enhancing robustness over naive ID-based\nelimination. It features two strategies: $\\text{PoE}_{\\text{ID}}^{\\text{log}}$,\nwhich eliminates options whose IDs have log probabilities below the average\nthreshold, and $\\text{PoE}_{\\text{ID}}^{\\text{seq}}$, which iteratively removes\nthe option with the lowest ID probability. We conduct extensive experiments\nwith 6 different LLMs on 4 diverse datasets. The results demonstrate that\n$\\text{PoE}_{\\text{ID}}$, especially $\\text{PoE}_{\\text{ID}}^{\\text{log}}$,\nsignificantly improves zero-shot and few-shot MCQs performance, particularly in\ndatasets with more options. Our analyses demonstrate that\n$\\text{PoE}_{\\text{ID}}^{\\text{log}}$ enhances the LLMs' confidence in\nselecting the correct option, and the option elimination strategy outperforms\nmethods relying on [MASK] replacement. We further investigate the limitations\nof LLMs in directly identifying incorrect options, which stem from their\ninherent deficiencies."
                },
                "authors": [
                    {
                        "name": "Zhenhao Zhu"
                    },
                    {
                        "name": "Bulou Liu"
                    },
                    {
                        "name": "Qingyao Ai"
                    },
                    {
                        "name": "Yiqun Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yiqun Liu"
                },
                "author": "Yiqun Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15175v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15175v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13438v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13438v1",
                "updated": "2025-05-19T17:58:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    58,
                    44,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T17:58:44Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    58,
                    44,
                    0,
                    139,
                    0
                ],
                "title": "Optimizing Anytime Reasoning via Budget Relative Policy Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Anytime Reasoning via Budget Relative Policy Optimization"
                },
                "summary": "Scaling test-time compute is crucial for enhancing the reasoning capabilities\nof large language models (LLMs). Existing approaches typically employ\nreinforcement learning (RL) to maximize a verifiable reward obtained at the end\nof reasoning traces. However, such methods optimize only the final performance\nunder a large and fixed token budget, which hinders efficiency in both training\nand deployment. In this work, we present a novel framework, AnytimeReasoner, to\noptimize anytime reasoning performance, which aims to improve token efficiency\nand the flexibility of reasoning under varying token budget constraints. To\nachieve this, we truncate the complete thinking process to fit within sampled\ntoken budgets from a prior distribution, compelling the model to summarize the\noptimal answer for each truncated thinking for verification. This introduces\nverifiable dense rewards into the reasoning process, facilitating more\neffective credit assignment in RL optimization. We then optimize the thinking\nand summary policies in a decoupled manner to maximize the cumulative reward.\nAdditionally, we introduce a novel variance reduction technique, Budget\nRelative Policy Optimization (BRPO), to enhance the robustness and efficiency\nof the learning process when reinforcing the thinking policy. Empirical results\nin mathematical reasoning tasks demonstrate that our method consistently\noutperforms GRPO across all thinking budgets under various prior distributions,\nenhancing both training and token efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling test-time compute is crucial for enhancing the reasoning capabilities\nof large language models (LLMs). Existing approaches typically employ\nreinforcement learning (RL) to maximize a verifiable reward obtained at the end\nof reasoning traces. However, such methods optimize only the final performance\nunder a large and fixed token budget, which hinders efficiency in both training\nand deployment. In this work, we present a novel framework, AnytimeReasoner, to\noptimize anytime reasoning performance, which aims to improve token efficiency\nand the flexibility of reasoning under varying token budget constraints. To\nachieve this, we truncate the complete thinking process to fit within sampled\ntoken budgets from a prior distribution, compelling the model to summarize the\noptimal answer for each truncated thinking for verification. This introduces\nverifiable dense rewards into the reasoning process, facilitating more\neffective credit assignment in RL optimization. We then optimize the thinking\nand summary policies in a decoupled manner to maximize the cumulative reward.\nAdditionally, we introduce a novel variance reduction technique, Budget\nRelative Policy Optimization (BRPO), to enhance the robustness and efficiency\nof the learning process when reinforcing the thinking policy. Empirical results\nin mathematical reasoning tasks demonstrate that our method consistently\noutperforms GRPO across all thinking budgets under various prior distributions,\nenhancing both training and token efficiency."
                },
                "authors": [
                    {
                        "name": "Penghui Qi"
                    },
                    {
                        "name": "Zichen Liu"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Wee Sun Lee"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13438v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13438v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13436v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13436v1",
                "updated": "2025-05-19T17:58:03Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    58,
                    3,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T17:58:03Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    58,
                    3,
                    0,
                    139,
                    0
                ],
                "title": "KinTwin: Imitation Learning with Torque and Muscle Driven Biomechanical\n  Models Enables Precise Replication of Able-Bodied and Impaired Movement from\n  Markerless Motion Capture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KinTwin: Imitation Learning with Torque and Muscle Driven Biomechanical\n  Models Enables Precise Replication of Able-Bodied and Impaired Movement from\n  Markerless Motion Capture"
                },
                "summary": "Broader access to high-quality movement analysis could greatly benefit\nmovement science and rehabilitation, such as allowing more detailed\ncharacterization of movement impairments and responses to interventions, or\neven enabling early detection of new neurological conditions or fall risk.\nWhile emerging technologies are making it easier to capture kinematics with\nbiomechanical models, or how joint angles change over time, inferring the\nunderlying physics that give rise to these movements, including ground reaction\nforces, joint torques, or even muscle activations, is still challenging. Here\nwe explore whether imitation learning applied to a biomechanical model from a\nlarge dataset of movements from able-bodied and impaired individuals can learn\nto compute these inverse dynamics. Although imitation learning in human pose\nestimation has seen great interest in recent years, our work differences in\nseveral ways: we focus on using an accurate biomechanical model instead of\nmodels adopted for computer vision, we test it on a dataset that contains\nparticipants with impaired movements, we reported detailed tracking metrics\nrelevant for the clinical measurement of movement including joint angles and\nground contact events, and finally we apply imitation learning to a\nmuscle-driven neuromusculoskeletal model. We show that our imitation learning\npolicy, KinTwin, can accurately replicate the kinematics of a wide range of\nmovements, including those with assistive devices or therapist assistance, and\nthat it can infer clinically meaningful differences in joint torques and muscle\nactivations. Our work demonstrates the potential for using imitation learning\nto enable high-quality movement analysis in clinical practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Broader access to high-quality movement analysis could greatly benefit\nmovement science and rehabilitation, such as allowing more detailed\ncharacterization of movement impairments and responses to interventions, or\neven enabling early detection of new neurological conditions or fall risk.\nWhile emerging technologies are making it easier to capture kinematics with\nbiomechanical models, or how joint angles change over time, inferring the\nunderlying physics that give rise to these movements, including ground reaction\nforces, joint torques, or even muscle activations, is still challenging. Here\nwe explore whether imitation learning applied to a biomechanical model from a\nlarge dataset of movements from able-bodied and impaired individuals can learn\nto compute these inverse dynamics. Although imitation learning in human pose\nestimation has seen great interest in recent years, our work differences in\nseveral ways: we focus on using an accurate biomechanical model instead of\nmodels adopted for computer vision, we test it on a dataset that contains\nparticipants with impaired movements, we reported detailed tracking metrics\nrelevant for the clinical measurement of movement including joint angles and\nground contact events, and finally we apply imitation learning to a\nmuscle-driven neuromusculoskeletal model. We show that our imitation learning\npolicy, KinTwin, can accurately replicate the kinematics of a wide range of\nmovements, including those with assistive devices or therapist assistance, and\nthat it can infer clinically meaningful differences in joint torques and muscle\nactivations. Our work demonstrates the potential for using imitation learning\nto enable high-quality movement analysis in clinical practice."
                },
                "authors": [
                    {
                        "name": "R. James Cotton"
                    }
                ],
                "author_detail": {
                    "name": "R. James Cotton"
                },
                "author": "R. James Cotton",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13436v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13436v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14234v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14234v3",
                "updated": "2025-05-19T17:56:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    56,
                    42,
                    0,
                    139,
                    0
                ],
                "published": "2025-03-18T13:11:43Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    13,
                    11,
                    43,
                    1,
                    77,
                    0
                ],
                "title": "Beyond Single Pass, Looping Through Time: KG-IRAG with Iterative\n  Knowledge Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Single Pass, Looping Through Time: KG-IRAG with Iterative\n  Knowledge Retrieval"
                },
                "summary": "Graph Retrieval-Augmented Generation (GraphRAG) has proven highly effective\nin enhancing the performance of Large Language Models (LLMs) on tasks that\nrequire external knowledge. By leveraging Knowledge Graphs (KGs), GraphRAG\nimproves information retrieval for complex reasoning tasks, providing more\nprecise and comprehensive retrieval and generating more accurate responses to\nQAs. However, most RAG methods fall short in addressing multi-step reasoning,\nparticularly when both information extraction and inference are necessary. To\naddress this limitation, this paper presents Knowledge Graph-Based Iterative\nRetrieval-Augmented Generation (KG-IRAG), a novel framework that integrates KGs\nwith iterative reasoning to improve LLMs' ability to handle queries involving\ntemporal and logical dependencies. Through iterative retrieval steps, KG-IRAG\nincrementally gathers relevant data from external KGs, enabling step-by-step\nreasoning. The proposed approach is particularly suited for scenarios where\nreasoning is required alongside dynamic temporal data extraction, such as\ndetermining optimal travel times based on weather conditions or traffic\npatterns. Experimental results show that KG-IRAG improves accuracy in complex\nreasoning tasks by effectively integrating external knowledge with iterative,\nlogic-based retrieval. Additionally, three new datasets: weatherQA-Irish,\nweatherQA-Sydney, and trafficQA-TFNSW, are formed to evaluate KG-IRAG's\nperformance, demonstrating its potential beyond traditional RAG applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Retrieval-Augmented Generation (GraphRAG) has proven highly effective\nin enhancing the performance of Large Language Models (LLMs) on tasks that\nrequire external knowledge. By leveraging Knowledge Graphs (KGs), GraphRAG\nimproves information retrieval for complex reasoning tasks, providing more\nprecise and comprehensive retrieval and generating more accurate responses to\nQAs. However, most RAG methods fall short in addressing multi-step reasoning,\nparticularly when both information extraction and inference are necessary. To\naddress this limitation, this paper presents Knowledge Graph-Based Iterative\nRetrieval-Augmented Generation (KG-IRAG), a novel framework that integrates KGs\nwith iterative reasoning to improve LLMs' ability to handle queries involving\ntemporal and logical dependencies. Through iterative retrieval steps, KG-IRAG\nincrementally gathers relevant data from external KGs, enabling step-by-step\nreasoning. The proposed approach is particularly suited for scenarios where\nreasoning is required alongside dynamic temporal data extraction, such as\ndetermining optimal travel times based on weather conditions or traffic\npatterns. Experimental results show that KG-IRAG improves accuracy in complex\nreasoning tasks by effectively integrating external knowledge with iterative,\nlogic-based retrieval. Additionally, three new datasets: weatherQA-Irish,\nweatherQA-Sydney, and trafficQA-TFNSW, are formed to evaluate KG-IRAG's\nperformance, demonstrating its potential beyond traditional RAG applications."
                },
                "authors": [
                    {
                        "name": "Ruiyi Yang"
                    },
                    {
                        "name": "Hao Xue"
                    },
                    {
                        "name": "Imran Razzak"
                    },
                    {
                        "name": "Hakim Hacid"
                    },
                    {
                        "name": "Flora D. Salim"
                    }
                ],
                "author_detail": {
                    "name": "Flora D. Salim"
                },
                "author": "Flora D. Salim",
                "arxiv_comment": "15 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14234v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14234v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13432v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13432v1",
                "updated": "2025-05-19T17:55:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    55,
                    56,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T17:55:56Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    55,
                    56,
                    0,
                    139,
                    0
                ],
                "title": "Synthetic-Powered Predictive Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic-Powered Predictive Inference"
                },
                "summary": "Conformal prediction is a framework for predictive inference with a\ndistribution-free, finite-sample guarantee. However, it tends to provide\nuninformative prediction sets when calibration data are scarce. This paper\nintroduces Synthetic-powered predictive inference (SPPI), a novel framework\nthat incorporates synthetic data -- e.g., from a generative model -- to improve\nsample efficiency. At the core of our method is a score transporter: an\nempirical quantile mapping that aligns nonconformity scores from trusted, real\ndata with those from synthetic data. By carefully integrating the score\ntransporter into the calibration process, SPPI provably achieves finite-sample\ncoverage guarantees without making any assumptions about the real and synthetic\ndata distributions. When the score distributions are well aligned, SPPI yields\nsubstantially tighter and more informative prediction sets than standard\nconformal prediction. Experiments on image classification and tabular\nregression demonstrate notable improvements in predictive efficiency in\ndata-scarce settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conformal prediction is a framework for predictive inference with a\ndistribution-free, finite-sample guarantee. However, it tends to provide\nuninformative prediction sets when calibration data are scarce. This paper\nintroduces Synthetic-powered predictive inference (SPPI), a novel framework\nthat incorporates synthetic data -- e.g., from a generative model -- to improve\nsample efficiency. At the core of our method is a score transporter: an\nempirical quantile mapping that aligns nonconformity scores from trusted, real\ndata with those from synthetic data. By carefully integrating the score\ntransporter into the calibration process, SPPI provably achieves finite-sample\ncoverage guarantees without making any assumptions about the real and synthetic\ndata distributions. When the score distributions are well aligned, SPPI yields\nsubstantially tighter and more informative prediction sets than standard\nconformal prediction. Experiments on image classification and tabular\nregression demonstrate notable improvements in predictive efficiency in\ndata-scarce settings."
                },
                "authors": [
                    {
                        "name": "Meshi Bashari"
                    },
                    {
                        "name": "Roy Maor Lotan"
                    },
                    {
                        "name": "Yonghoon Lee"
                    },
                    {
                        "name": "Edgar Dobriban"
                    },
                    {
                        "name": "Yaniv Romano"
                    }
                ],
                "author_detail": {
                    "name": "Yaniv Romano"
                },
                "author": "Yaniv Romano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13432v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13432v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13430v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13430v1",
                "updated": "2025-05-19T17:55:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    55,
                    15,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T17:55:15Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    55,
                    15,
                    0,
                    139,
                    0
                ],
                "title": "Fine-tuning Quantized Neural Networks with Zeroth-order Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning Quantized Neural Networks with Zeroth-order Optimization"
                },
                "summary": "As the size of large language models grows exponentially, GPU memory has\nbecome a bottleneck for adapting these models to downstream tasks. In this\npaper, we aim to push the limits of memory-efficient training by minimizing\nmemory usage on model weights, gradients, and optimizer states, within a\nunified framework. Our idea is to eliminate both gradients and optimizer states\nusing zeroth-order optimization, which approximates gradients by perturbing\nweights during forward passes to identify gradient directions. To minimize\nmemory usage on weights, we employ model quantization, e.g., converting from\nbfloat16 to int4. However, directly applying zeroth-order optimization to\nquantized weights is infeasible due to the precision gap between discrete\nweights and continuous gradients, which would otherwise require de-quantization\nand re-quantization. To overcome this challenge, we propose Quantized\nZeroth-order Optimization (QZO), a novel approach that perturbs the continuous\nquantization scale for gradient estimation and uses a directional derivative\nclipping method to stabilize training. QZO is orthogonal to both scalar-based\nand codebook-based post-training quantization methods. Compared to\nfull-parameter fine-tuning in bfloat16, QZO can reduce the total memory cost by\nmore than 18$\\times$ for 4-bit LLMs, and enables fine-tuning Llama-2-13B and\nStable Diffusion 3.5 Large within a single 24GB GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the size of large language models grows exponentially, GPU memory has\nbecome a bottleneck for adapting these models to downstream tasks. In this\npaper, we aim to push the limits of memory-efficient training by minimizing\nmemory usage on model weights, gradients, and optimizer states, within a\nunified framework. Our idea is to eliminate both gradients and optimizer states\nusing zeroth-order optimization, which approximates gradients by perturbing\nweights during forward passes to identify gradient directions. To minimize\nmemory usage on weights, we employ model quantization, e.g., converting from\nbfloat16 to int4. However, directly applying zeroth-order optimization to\nquantized weights is infeasible due to the precision gap between discrete\nweights and continuous gradients, which would otherwise require de-quantization\nand re-quantization. To overcome this challenge, we propose Quantized\nZeroth-order Optimization (QZO), a novel approach that perturbs the continuous\nquantization scale for gradient estimation and uses a directional derivative\nclipping method to stabilize training. QZO is orthogonal to both scalar-based\nand codebook-based post-training quantization methods. Compared to\nfull-parameter fine-tuning in bfloat16, QZO can reduce the total memory cost by\nmore than 18$\\times$ for 4-bit LLMs, and enables fine-tuning Llama-2-13B and\nStable Diffusion 3.5 Large within a single 24GB GPU."
                },
                "authors": [
                    {
                        "name": "Sifeng Shang"
                    },
                    {
                        "name": "Jiayi Zhou"
                    },
                    {
                        "name": "Chenyu Lin"
                    },
                    {
                        "name": "Minxian Li"
                    },
                    {
                        "name": "Kaiyang Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Kaiyang Zhou"
                },
                "author": "Kaiyang Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13430v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13430v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13427v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13427v1",
                "updated": "2025-05-19T17:55:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    55,
                    8,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T17:55:08Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    55,
                    8,
                    0,
                    139,
                    0
                ],
                "title": "MM-PRM: Enhancing Multimodal Mathematical Reasoning with Scalable\n  Step-Level Supervision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MM-PRM: Enhancing Multimodal Mathematical Reasoning with Scalable\n  Step-Level Supervision"
                },
                "summary": "While Multimodal Large Language Models (MLLMs) have achieved impressive\nprogress in vision-language understanding, they still struggle with complex\nmulti-step reasoning, often producing logically inconsistent or partially\ncorrect solutions. A key limitation lies in the lack of fine-grained\nsupervision over intermediate reasoning steps. To address this, we propose\nMM-PRM, a process reward model trained within a fully automated, scalable\nframework. We first build MM-Policy, a strong multimodal model trained on\ndiverse mathematical reasoning data. Then, we construct MM-K12, a curated\ndataset of 10,000 multimodal math problems with verifiable answers, which\nserves as seed data. Leveraging a Monte Carlo Tree Search (MCTS)-based\npipeline, we generate over 700k step-level annotations without human labeling.\nThe resulting PRM is used to score candidate reasoning paths in the Best-of-N\ninference setup and achieves significant improvements across both in-domain\n(MM-K12 test set) and out-of-domain (OlympiadBench, MathVista, etc.)\nbenchmarks. Further analysis confirms the effectiveness of soft labels, smaller\nlearning rates, and path diversity in optimizing PRM performance. MM-PRM\ndemonstrates that process supervision is a powerful tool for enhancing the\nlogical robustness of multimodal reasoning systems. We release all our codes\nand data at https://github.com/ModalMinds/MM-PRM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Multimodal Large Language Models (MLLMs) have achieved impressive\nprogress in vision-language understanding, they still struggle with complex\nmulti-step reasoning, often producing logically inconsistent or partially\ncorrect solutions. A key limitation lies in the lack of fine-grained\nsupervision over intermediate reasoning steps. To address this, we propose\nMM-PRM, a process reward model trained within a fully automated, scalable\nframework. We first build MM-Policy, a strong multimodal model trained on\ndiverse mathematical reasoning data. Then, we construct MM-K12, a curated\ndataset of 10,000 multimodal math problems with verifiable answers, which\nserves as seed data. Leveraging a Monte Carlo Tree Search (MCTS)-based\npipeline, we generate over 700k step-level annotations without human labeling.\nThe resulting PRM is used to score candidate reasoning paths in the Best-of-N\ninference setup and achieves significant improvements across both in-domain\n(MM-K12 test set) and out-of-domain (OlympiadBench, MathVista, etc.)\nbenchmarks. Further analysis confirms the effectiveness of soft labels, smaller\nlearning rates, and path diversity in optimizing PRM performance. MM-PRM\ndemonstrates that process supervision is a powerful tool for enhancing the\nlogical robustness of multimodal reasoning systems. We release all our codes\nand data at https://github.com/ModalMinds/MM-PRM."
                },
                "authors": [
                    {
                        "name": "Lingxiao Du"
                    },
                    {
                        "name": "Fanqing Meng"
                    },
                    {
                        "name": "Zongkai Liu"
                    },
                    {
                        "name": "Zhixiang Zhou"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Qiaosheng Zhang"
                    },
                    {
                        "name": "Wenqi Shao"
                    }
                ],
                "author_detail": {
                    "name": "Wenqi Shao"
                },
                "author": "Wenqi Shao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13427v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13427v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13425v1",
                "updated": "2025-05-19T17:54:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    54,
                    35,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T17:54:35Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    54,
                    35,
                    0,
                    139,
                    0
                ],
                "title": "Learnware of Language Models: Specialized Small Language Models Can Do\n  Big",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learnware of Language Models: Specialized Small Language Models Can Do\n  Big"
                },
                "summary": "The learnware paradigm offers a novel approach to machine learning by\nenabling users to reuse a set of well-trained models for tasks beyond the\nmodels' original purposes. It eliminates the need to build models from scratch,\ninstead relying on specifications (representations of a model's capabilities)\nto identify and leverage the most suitable models for new tasks. While\nlearnware has proven effective in many scenarios, its application to language\nmodels has remained largely unexplored. At the same time, large language models\n(LLMs) have demonstrated remarkable universal question-answering abilities, yet\nthey face challenges in specialized scenarios due to data scarcity, privacy\nconcerns, and high computational costs, thus more and more specialized small\nlanguage models (SLMs) are being trained for specific domains. To address these\nlimitations systematically, the learnware paradigm provides a promising\nsolution by enabling maximum utilization of specialized SLMs, and allowing\nusers to identify and reuse them in a collaborative and privacy-preserving\nmanner.\n  This paper presents a preliminary attempt to apply the learnware paradigm to\nlanguage models. We simulated a learnware system comprising approximately 100\nlearnwares of specialized SLMs with 8B parameters, fine-tuned across finance,\nhealthcare, and mathematics domains. Each learnware contains an SLM and a\nspecification, which enables users to identify the most relevant models without\nexposing their own data. Experimental results demonstrate promising\nperformance: by selecting one suitable learnware for each task-specific\ninference, the system outperforms the base SLMs on all benchmarks. Compared to\nLLMs, the system outperforms Qwen1.5-110B, Qwen2.5-72B, and\nLlama3.1-70B-Instruct by at least 14% in finance domain tasks, and surpasses\nFlan-PaLM-540B (ranked 7th on the Open Medical LLM Leaderboard) in medical\ndomain tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The learnware paradigm offers a novel approach to machine learning by\nenabling users to reuse a set of well-trained models for tasks beyond the\nmodels' original purposes. It eliminates the need to build models from scratch,\ninstead relying on specifications (representations of a model's capabilities)\nto identify and leverage the most suitable models for new tasks. While\nlearnware has proven effective in many scenarios, its application to language\nmodels has remained largely unexplored. At the same time, large language models\n(LLMs) have demonstrated remarkable universal question-answering abilities, yet\nthey face challenges in specialized scenarios due to data scarcity, privacy\nconcerns, and high computational costs, thus more and more specialized small\nlanguage models (SLMs) are being trained for specific domains. To address these\nlimitations systematically, the learnware paradigm provides a promising\nsolution by enabling maximum utilization of specialized SLMs, and allowing\nusers to identify and reuse them in a collaborative and privacy-preserving\nmanner.\n  This paper presents a preliminary attempt to apply the learnware paradigm to\nlanguage models. We simulated a learnware system comprising approximately 100\nlearnwares of specialized SLMs with 8B parameters, fine-tuned across finance,\nhealthcare, and mathematics domains. Each learnware contains an SLM and a\nspecification, which enables users to identify the most relevant models without\nexposing their own data. Experimental results demonstrate promising\nperformance: by selecting one suitable learnware for each task-specific\ninference, the system outperforms the base SLMs on all benchmarks. Compared to\nLLMs, the system outperforms Qwen1.5-110B, Qwen2.5-72B, and\nLlama3.1-70B-Instruct by at least 14% in finance domain tasks, and surpasses\nFlan-PaLM-540B (ranked 7th on the Open Medical LLM Leaderboard) in medical\ndomain tasks."
                },
                "authors": [
                    {
                        "name": "Zhi-Hao Tan"
                    },
                    {
                        "name": "Zi-Chen Zhao"
                    },
                    {
                        "name": "Hao-Yu Shi"
                    },
                    {
                        "name": "Xin-Yu Zhang"
                    },
                    {
                        "name": "Peng Tan"
                    },
                    {
                        "name": "Yang Yu"
                    },
                    {
                        "name": "Zhi-Hua Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zhi-Hua Zhou"
                },
                "author": "Zhi-Hua Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13422v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13422v1",
                "updated": "2025-05-19T17:53:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    53,
                    15,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T17:53:15Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    53,
                    15,
                    0,
                    139,
                    0
                ],
                "title": "Machine learning the first stage in 2SLS: Practical guidance from bias\n  decomposition and simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning the first stage in 2SLS: Practical guidance from bias\n  decomposition and simulation"
                },
                "summary": "Machine learning (ML) primarily evolved to solve \"prediction problems.\" The\nfirst stage of two-stage least squares (2SLS) is a prediction problem,\nsuggesting potential gains from ML first-stage assistance. However, little\nguidance exists on when ML helps 2SLS$\\unicode{x2014}$or when it hurts. We\ninvestigate the implications of inserting ML into 2SLS, decomposing the bias\ninto three informative components. Mechanically, ML-in-2SLS procedures face\nissues common to prediction and causal-inference settings$\\unicode{x2014}$and\ntheir interaction. Through simulation, we show linear ML methods (e.g.,\npost-Lasso) work well, while nonlinear methods (e.g., random forests, neural\nnets) generate substantial bias in second-stage\nestimates$\\unicode{x2014}$potentially exceeding the bias of endogenous OLS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning (ML) primarily evolved to solve \"prediction problems.\" The\nfirst stage of two-stage least squares (2SLS) is a prediction problem,\nsuggesting potential gains from ML first-stage assistance. However, little\nguidance exists on when ML helps 2SLS$\\unicode{x2014}$or when it hurts. We\ninvestigate the implications of inserting ML into 2SLS, decomposing the bias\ninto three informative components. Mechanically, ML-in-2SLS procedures face\nissues common to prediction and causal-inference settings$\\unicode{x2014}$and\ntheir interaction. Through simulation, we show linear ML methods (e.g.,\npost-Lasso) work well, while nonlinear methods (e.g., random forests, neural\nnets) generate substantial bias in second-stage\nestimates$\\unicode{x2014}$potentially exceeding the bias of endogenous OLS."
                },
                "authors": [
                    {
                        "name": "Connor Lennon"
                    },
                    {
                        "name": "Edward Rubin"
                    },
                    {
                        "name": "Glen Waddell"
                    }
                ],
                "author_detail": {
                    "name": "Glen Waddell"
                },
                "author": "Glen Waddell",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13422v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13422v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13421v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13421v1",
                "updated": "2025-05-19T17:52:58Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    52,
                    58,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T17:52:58Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    52,
                    58,
                    0,
                    139,
                    0
                ],
                "title": "Make Still Further Progress: Chain of Thoughts for Tabular Data\n  Leaderboard",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Make Still Further Progress: Chain of Thoughts for Tabular Data\n  Leaderboard"
                },
                "summary": "Tabular data, a fundamental data format in machine learning, is predominantly\nutilized in competitions and real-world applications. The performance of\ntabular models--such as gradient boosted decision trees and neural\nnetworks--can vary significantly across datasets due to differences in feature\ndistributions and task characteristics. Achieving top performance on each\ndataset often requires specialized expert knowledge. To address this\nvariability, practitioners often aggregate the predictions of multiple models.\nHowever, conventional aggregation strategies typically rely on static\ncombination rules and lack instance-level adaptability. In this work, we\npropose an in-context ensemble framework for tabular prediction that leverages\nlarge language models (LLMs) to perform dynamic, instance-specific integration\nof external model predictions. Without access to raw tabular features or\nsemantic information, our method constructs a context around each test instance\nusing its nearest neighbors and the predictions from a pool of external models.\nWithin this enriched context, we introduce Chain of Tabular Thoughts (CoT$^2$),\na prompting strategy that guides LLMs through multi-step, interpretable\nreasoning, making still further progress toward expert-level decision-making.\nExperimental results show that our method outperforms well-tuned baselines and\nstandard ensemble techniques across a wide range of tabular datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tabular data, a fundamental data format in machine learning, is predominantly\nutilized in competitions and real-world applications. The performance of\ntabular models--such as gradient boosted decision trees and neural\nnetworks--can vary significantly across datasets due to differences in feature\ndistributions and task characteristics. Achieving top performance on each\ndataset often requires specialized expert knowledge. To address this\nvariability, practitioners often aggregate the predictions of multiple models.\nHowever, conventional aggregation strategies typically rely on static\ncombination rules and lack instance-level adaptability. In this work, we\npropose an in-context ensemble framework for tabular prediction that leverages\nlarge language models (LLMs) to perform dynamic, instance-specific integration\nof external model predictions. Without access to raw tabular features or\nsemantic information, our method constructs a context around each test instance\nusing its nearest neighbors and the predictions from a pool of external models.\nWithin this enriched context, we introduce Chain of Tabular Thoughts (CoT$^2$),\na prompting strategy that guides LLMs through multi-step, interpretable\nreasoning, making still further progress toward expert-level decision-making.\nExperimental results show that our method outperforms well-tuned baselines and\nstandard ensemble techniques across a wide range of tabular datasets."
                },
                "authors": [
                    {
                        "name": "Si-Yang Liu"
                    },
                    {
                        "name": "Qile Zhou"
                    },
                    {
                        "name": "Han-Jia Ye"
                    }
                ],
                "author_detail": {
                    "name": "Han-Jia Ye"
                },
                "author": "Han-Jia Ye",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13421v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13421v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13419v1",
                "updated": "2025-05-19T17:52:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    52,
                    15,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T17:52:15Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    52,
                    15,
                    0,
                    139,
                    0
                ],
                "title": "FEALLM: Advancing Facial Emotion Analysis in Multimodal Large Language\n  Models with Emotional Synergy and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FEALLM: Advancing Facial Emotion Analysis in Multimodal Large Language\n  Models with Emotional Synergy and Reasoning"
                },
                "summary": "Facial Emotion Analysis (FEA) plays a crucial role in visual affective\ncomputing, aiming to infer a person's emotional state based on facial data.\nScientifically, facial expressions (FEs) result from the coordinated movement\nof facial muscles, which can be decomposed into specific action units (AUs)\nthat provide detailed emotional insights. However, traditional methods often\nstruggle with limited interpretability, constrained generalization and\nreasoning abilities. Recently, Multimodal Large Language Models (MLLMs) have\nshown exceptional performance in various visual tasks, while they still face\nsignificant challenges in FEA due to the lack of specialized datasets and their\ninability to capture the intricate relationships between FEs and AUs. To\naddress these issues, we introduce a novel FEA Instruction Dataset that\nprovides accurate and aligned FE and AU descriptions and establishes causal\nreasoning relationships between them, followed by constructing a new benchmark,\nFEABench. Moreover, we propose FEALLM, a novel MLLM architecture designed to\ncapture more detailed facial information, enhancing its capability in FEA\ntasks. Our model demonstrates strong performance on FEABench and impressive\ngeneralization capability through zero-shot evaluation on various datasets,\nincluding RAF-DB, AffectNet, BP4D, and DISFA, showcasing its robustness and\neffectiveness in FEA tasks. The dataset and code will be available at\nhttps://github.com/953206211/FEALLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Facial Emotion Analysis (FEA) plays a crucial role in visual affective\ncomputing, aiming to infer a person's emotional state based on facial data.\nScientifically, facial expressions (FEs) result from the coordinated movement\nof facial muscles, which can be decomposed into specific action units (AUs)\nthat provide detailed emotional insights. However, traditional methods often\nstruggle with limited interpretability, constrained generalization and\nreasoning abilities. Recently, Multimodal Large Language Models (MLLMs) have\nshown exceptional performance in various visual tasks, while they still face\nsignificant challenges in FEA due to the lack of specialized datasets and their\ninability to capture the intricate relationships between FEs and AUs. To\naddress these issues, we introduce a novel FEA Instruction Dataset that\nprovides accurate and aligned FE and AU descriptions and establishes causal\nreasoning relationships between them, followed by constructing a new benchmark,\nFEABench. Moreover, we propose FEALLM, a novel MLLM architecture designed to\ncapture more detailed facial information, enhancing its capability in FEA\ntasks. Our model demonstrates strong performance on FEABench and impressive\ngeneralization capability through zero-shot evaluation on various datasets,\nincluding RAF-DB, AffectNet, BP4D, and DISFA, showcasing its robustness and\neffectiveness in FEA tasks. The dataset and code will be available at\nhttps://github.com/953206211/FEALLM."
                },
                "authors": [
                    {
                        "name": "Zhuozhao Hu"
                    },
                    {
                        "name": "Kaishen Yuan"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Zitong Yu"
                    },
                    {
                        "name": "Yuan Zong"
                    },
                    {
                        "name": "Jingang Shi"
                    },
                    {
                        "name": "Huanjing Yue"
                    },
                    {
                        "name": "Jingyu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jingyu Yang"
                },
                "author": "Jingyu Yang",
                "arxiv_comment": "10 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13418v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13418v1",
                "updated": "2025-05-19T17:51:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    51,
                    35,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T17:51:35Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    51,
                    35,
                    0,
                    139,
                    0
                ],
                "title": "Dementia Through Different Eyes: Explainable Modeling of Human and LLM\n  Perceptions for Early Awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dementia Through Different Eyes: Explainable Modeling of Human and LLM\n  Perceptions for Early Awareness"
                },
                "summary": "Cognitive decline often surfaces in language years before diagnosis. It is\nfrequently non-experts, such as those closest to the patient, who first sense a\nchange and raise concern. As LLMs become integrated into daily communication\nand used over prolonged periods, it may even be an LLM that notices something\nis off. But what exactly do they notice--and should be noticing--when making\nthat judgment? This paper investigates how dementia is perceived through\nlanguage by non-experts. We presented transcribed picture descriptions to\nnon-expert humans and LLMs, asking them to intuitively judge whether each text\nwas produced by someone healthy or with dementia. We introduce an explainable\nmethod that uses LLMs to extract high-level, expert-guided features\nrepresenting these picture descriptions, and use logistic regression to model\nhuman and LLM perceptions and compare with clinical diagnoses. Our analysis\nreveals that human perception of dementia is inconsistent and relies on a\nnarrow, and sometimes misleading, set of cues. LLMs, by contrast, draw on a\nricher, more nuanced feature set that aligns more closely with clinical\npatterns. Still, both groups show a tendency toward false negatives, frequently\noverlooking dementia cases. Through our interpretable framework and the\ninsights it provides, we hope to help non-experts better recognize the\nlinguistic signs that matter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive decline often surfaces in language years before diagnosis. It is\nfrequently non-experts, such as those closest to the patient, who first sense a\nchange and raise concern. As LLMs become integrated into daily communication\nand used over prolonged periods, it may even be an LLM that notices something\nis off. But what exactly do they notice--and should be noticing--when making\nthat judgment? This paper investigates how dementia is perceived through\nlanguage by non-experts. We presented transcribed picture descriptions to\nnon-expert humans and LLMs, asking them to intuitively judge whether each text\nwas produced by someone healthy or with dementia. We introduce an explainable\nmethod that uses LLMs to extract high-level, expert-guided features\nrepresenting these picture descriptions, and use logistic regression to model\nhuman and LLM perceptions and compare with clinical diagnoses. Our analysis\nreveals that human perception of dementia is inconsistent and relies on a\nnarrow, and sometimes misleading, set of cues. LLMs, by contrast, draw on a\nricher, more nuanced feature set that aligns more closely with clinical\npatterns. Still, both groups show a tendency toward false negatives, frequently\noverlooking dementia cases. Through our interpretable framework and the\ninsights it provides, we hope to help non-experts better recognize the\nlinguistic signs that matter."
                },
                "authors": [
                    {
                        "name": "Lotem Peled-Cohen"
                    },
                    {
                        "name": "Maya Zadok"
                    },
                    {
                        "name": "Nitay Calderon"
                    },
                    {
                        "name": "Hila Gonen"
                    },
                    {
                        "name": "Roi Reichart"
                    }
                ],
                "author_detail": {
                    "name": "Roi Reichart"
                },
                "author": "Roi Reichart",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13418v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13418v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10951v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10951v2",
                "updated": "2025-05-19T17:51:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    51,
                    26,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-16T07:39:41Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    7,
                    39,
                    41,
                    4,
                    136,
                    0
                ],
                "title": "SubGCache: Accelerating Graph-based RAG with Subgraph-level KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SubGCache: Accelerating Graph-based RAG with Subgraph-level KV Cache"
                },
                "summary": "Graph-based retrieval-augmented generation (RAG) enables large language\nmodels (LLMs) to incorporate structured knowledge via graph retrieval as\ncontextual input, enhancing more accurate and context-aware reasoning. We\nobserve that for different queries, it could retrieve similar subgraphs as\nprompts, and thus we propose SubGCache, which aims to reduce inference latency\nby reusing computation across queries with similar structural prompts (i.e.,\nsubgraphs). Specifically, SubGCache clusters queries based on subgraph\nembeddings, constructs a representative subgraph for each cluster, and\npre-computes the key-value (KV) cache of the representative subgraph. For each\nquery with its retrieved subgraph within a cluster, it reuses the pre-computed\nKV cache of the representative subgraph of the cluster without computing the KV\ntensors again for saving computation. Experiments on two new datasets across\nmultiple LLM backbones and graph-based RAG frameworks demonstrate that\nSubGCache consistently reduces inference latency with comparable and even\nimproved generation quality, achieving up to 6.68$\\times$ reduction in\ntime-to-first-token (TTFT).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-based retrieval-augmented generation (RAG) enables large language\nmodels (LLMs) to incorporate structured knowledge via graph retrieval as\ncontextual input, enhancing more accurate and context-aware reasoning. We\nobserve that for different queries, it could retrieve similar subgraphs as\nprompts, and thus we propose SubGCache, which aims to reduce inference latency\nby reusing computation across queries with similar structural prompts (i.e.,\nsubgraphs). Specifically, SubGCache clusters queries based on subgraph\nembeddings, constructs a representative subgraph for each cluster, and\npre-computes the key-value (KV) cache of the representative subgraph. For each\nquery with its retrieved subgraph within a cluster, it reuses the pre-computed\nKV cache of the representative subgraph of the cluster without computing the KV\ntensors again for saving computation. Experiments on two new datasets across\nmultiple LLM backbones and graph-based RAG frameworks demonstrate that\nSubGCache consistently reduces inference latency with comparable and even\nimproved generation quality, achieving up to 6.68$\\times$ reduction in\ntime-to-first-token (TTFT)."
                },
                "authors": [
                    {
                        "name": "Qiuyu Zhu"
                    },
                    {
                        "name": "Liang Zhang"
                    },
                    {
                        "name": "Qianxiong Xu"
                    },
                    {
                        "name": "Cheng Long"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "author": "Jie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10951v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10951v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19460v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19460v3",
                "updated": "2025-05-19T17:50:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    50,
                    52,
                    0,
                    139,
                    0
                ],
                "published": "2025-02-26T10:28:44Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    10,
                    28,
                    44,
                    2,
                    57,
                    0
                ],
                "title": "Overcoming Dependent Censoring in the Evaluation of Survival Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Overcoming Dependent Censoring in the Evaluation of Survival Models"
                },
                "summary": "Conventional survival metrics, such as Harrell's concordance index (CI) and\nthe Brier Score, rely on the independent censoring assumption for valid\ninference with right-censored data. However, in the presence of so-called\ndependent censoring, where the probability of censoring is related to the event\nof interest, these metrics can give biased estimates of the underlying model\nerror. In this paper, we introduce three new evaluation metrics for survival\nanalysis based on Archimedean copulas that can account for dependent censoring.\nWe also develop a framework to generate realistic, semi-synthetic datasets with\ndependent censoring to facilitate the evaluation of the metrics. Our\nexperiments in synthetic and semi-synthetic data demonstrate that the proposed\nmetrics can provide more accurate estimates of the model error than\nconventional metrics under dependent censoring.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional survival metrics, such as Harrell's concordance index (CI) and\nthe Brier Score, rely on the independent censoring assumption for valid\ninference with right-censored data. However, in the presence of so-called\ndependent censoring, where the probability of censoring is related to the event\nof interest, these metrics can give biased estimates of the underlying model\nerror. In this paper, we introduce three new evaluation metrics for survival\nanalysis based on Archimedean copulas that can account for dependent censoring.\nWe also develop a framework to generate realistic, semi-synthetic datasets with\ndependent censoring to facilitate the evaluation of the metrics. Our\nexperiments in synthetic and semi-synthetic data demonstrate that the proposed\nmetrics can provide more accurate estimates of the model error than\nconventional metrics under dependent censoring."
                },
                "authors": [
                    {
                        "name": "Christian Marius Lillelund"
                    },
                    {
                        "name": "Shi-ang Qi"
                    },
                    {
                        "name": "Russell Greiner"
                    }
                ],
                "author_detail": {
                    "name": "Russell Greiner"
                },
                "author": "Russell Greiner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19460v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19460v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13417v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13417v1",
                "updated": "2025-05-19T17:50:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    50,
                    52,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T17:50:52Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    50,
                    52,
                    0,
                    139,
                    0
                ],
                "title": "AdaptThink: Reasoning Models Can Learn When to Think",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaptThink: Reasoning Models Can Learn When to Think"
                },
                "summary": "Recently, large reasoning models have achieved impressive performance on\nvarious tasks by employing human-like deep thinking. However, the lengthy\nthinking process substantially increases inference overhead, making efficiency\na critical bottleneck. In this work, we first demonstrate that NoThinking,\nwhich prompts the reasoning model to skip thinking and directly generate the\nfinal solution, is a better choice for relatively simple tasks in terms of both\nperformance and efficiency. Motivated by this, we propose AdaptThink, a novel\nRL algorithm to teach reasoning models to choose the optimal thinking mode\nadaptively based on problem difficulty. Specifically, AdaptThink features two\ncore components: (1) a constrained optimization objective that encourages the\nmodel to choose NoThinking while maintaining the overall performance; (2) an\nimportance sampling strategy that balances Thinking and NoThinking samples\nduring on-policy training, thereby enabling cold start and allowing the model\nto explore and exploit both thinking modes throughout the training process. Our\nexperiments indicate that AdaptThink significantly reduces the inference costs\nwhile further enhancing performance. Notably, on three math datasets,\nAdaptThink reduces the average response length of DeepSeek-R1-Distill-Qwen-1.5B\nby 53% and improves its accuracy by 2.4%, highlighting the promise of adaptive\nthinking-mode selection for optimizing the balance between reasoning quality\nand efficiency. Our codes and models are available at\nhttps://github.com/THU-KEG/AdaptThink.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large reasoning models have achieved impressive performance on\nvarious tasks by employing human-like deep thinking. However, the lengthy\nthinking process substantially increases inference overhead, making efficiency\na critical bottleneck. In this work, we first demonstrate that NoThinking,\nwhich prompts the reasoning model to skip thinking and directly generate the\nfinal solution, is a better choice for relatively simple tasks in terms of both\nperformance and efficiency. Motivated by this, we propose AdaptThink, a novel\nRL algorithm to teach reasoning models to choose the optimal thinking mode\nadaptively based on problem difficulty. Specifically, AdaptThink features two\ncore components: (1) a constrained optimization objective that encourages the\nmodel to choose NoThinking while maintaining the overall performance; (2) an\nimportance sampling strategy that balances Thinking and NoThinking samples\nduring on-policy training, thereby enabling cold start and allowing the model\nto explore and exploit both thinking modes throughout the training process. Our\nexperiments indicate that AdaptThink significantly reduces the inference costs\nwhile further enhancing performance. Notably, on three math datasets,\nAdaptThink reduces the average response length of DeepSeek-R1-Distill-Qwen-1.5B\nby 53% and improves its accuracy by 2.4%, highlighting the promise of adaptive\nthinking-mode selection for optimizing the balance between reasoning quality\nand efficiency. Our codes and models are available at\nhttps://github.com/THU-KEG/AdaptThink."
                },
                "authors": [
                    {
                        "name": "Jiajie Zhang"
                    },
                    {
                        "name": "Nianyi Lin"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Ling Feng"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13417v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13417v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13416v1",
                "updated": "2025-05-19T17:50:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    50,
                    45,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T17:50:45Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    50,
                    45,
                    0,
                    139,
                    0
                ],
                "title": "Gluon: Making Muon & Scion Great Again! (Bridging Theory and Practice of\n  LMO-based Optimizers for LLMs)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gluon: Making Muon & Scion Great Again! (Bridging Theory and Practice of\n  LMO-based Optimizers for LLMs)"
                },
                "summary": "Recent developments in deep learning optimization have brought about\nradically new algorithms based on the Linear Minimization Oracle (LMO)\nframework, such as $\\sf Muon$ and $\\sf Scion$. After over a decade of $\\sf\nAdam$'s dominance, these LMO-based methods are emerging as viable replacements,\noffering several practical advantages such as improved memory efficiency,\nbetter hyperparameter transferability, and most importantly, superior empirical\nperformance on large-scale tasks, including LLM training. However, a\nsignificant gap remains between their practical use and our current theoretical\nunderstanding: prior analyses (1) overlook the layer-wise LMO application of\nthese optimizers in practice, and (2) rely on an unrealistic smoothness\nassumption, leading to impractically small stepsizes. To address both, we\npropose a new LMO-based method called $\\sf Gluon$, capturing prior\ntheoretically analyzed methods as special cases, and introduce a new refined\ngeneralized smoothness model that captures the layer-wise geometry of neural\nnetworks, matches the layer-wise practical implementation of $\\sf Muon$ and\n$\\sf Scion$, and leads to convergence guarantees with strong practical\npredictive power. Unlike prior results, our theoretical stepsizes closely match\nthe fine-tuned values reported by Pethick et al. (2025). Our experiments with\nNanoGPT and CNN confirm that our assumption holds along the optimization\ntrajectory, ultimately closing the gap between theory and practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent developments in deep learning optimization have brought about\nradically new algorithms based on the Linear Minimization Oracle (LMO)\nframework, such as $\\sf Muon$ and $\\sf Scion$. After over a decade of $\\sf\nAdam$'s dominance, these LMO-based methods are emerging as viable replacements,\noffering several practical advantages such as improved memory efficiency,\nbetter hyperparameter transferability, and most importantly, superior empirical\nperformance on large-scale tasks, including LLM training. However, a\nsignificant gap remains between their practical use and our current theoretical\nunderstanding: prior analyses (1) overlook the layer-wise LMO application of\nthese optimizers in practice, and (2) rely on an unrealistic smoothness\nassumption, leading to impractically small stepsizes. To address both, we\npropose a new LMO-based method called $\\sf Gluon$, capturing prior\ntheoretically analyzed methods as special cases, and introduce a new refined\ngeneralized smoothness model that captures the layer-wise geometry of neural\nnetworks, matches the layer-wise practical implementation of $\\sf Muon$ and\n$\\sf Scion$, and leads to convergence guarantees with strong practical\npredictive power. Unlike prior results, our theoretical stepsizes closely match\nthe fine-tuned values reported by Pethick et al. (2025). Our experiments with\nNanoGPT and CNN confirm that our assumption holds along the optimization\ntrajectory, ultimately closing the gap between theory and practice."
                },
                "authors": [
                    {
                        "name": "Artem Riabinin"
                    },
                    {
                        "name": "Egor Shulgin"
                    },
                    {
                        "name": "Kaja Gruntkowska"
                    },
                    {
                        "name": "Peter Richtárik"
                    }
                ],
                "author_detail": {
                    "name": "Peter Richtárik"
                },
                "author": "Peter Richtárik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13406v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13406v1",
                "updated": "2025-05-19T17:41:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    41,
                    29,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T17:41:29Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    41,
                    29,
                    0,
                    139,
                    0
                ],
                "title": "AutoMathKG: The automated mathematical knowledge graph based on LLM and\n  vector database",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoMathKG: The automated mathematical knowledge graph based on LLM and\n  vector database"
                },
                "summary": "A mathematical knowledge graph (KG) presents knowledge within the field of\nmathematics in a structured manner. Constructing a math KG using natural\nlanguage is an essential but challenging task. There are two major limitations\nof existing works: first, they are constrained by corpus completeness, often\ndiscarding or manually supplementing incomplete knowledge; second, they\ntypically fail to fully automate the integration of diverse knowledge sources.\nThis paper proposes AutoMathKG, a high-quality, wide-coverage, and\nmulti-dimensional math KG capable of automatic updates. AutoMathKG regards\nmathematics as a vast directed graph composed of Definition, Theorem, and\nProblem entities, with their reference relationships as edges. It integrates\nknowledge from ProofWiki, textbooks, arXiv papers, and TheoremQA, enhancing\nentities and relationships with large language models (LLMs) via in-context\nlearning for data augmentation. To search for similar entities, MathVD, a\nvector database, is built through two designed embedding strategies using\nSBERT. To automatically update, two mechanisms are proposed. For knowledge\ncompletion mechanism, Math LLM is developed to interact with AutoMathKG,\nproviding missing proofs or solutions. For knowledge fusion mechanism, MathVD\nis used to retrieve similar entities, and LLM is used to determine whether to\nmerge with a candidate or add as a new entity. A wide range of experiments\ndemonstrate the advanced performance and broad applicability of the AutoMathKG\nsystem, including superior reachability query results in MathVD compared to\nfive baselines and robust mathematical reasoning capability in Math LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A mathematical knowledge graph (KG) presents knowledge within the field of\nmathematics in a structured manner. Constructing a math KG using natural\nlanguage is an essential but challenging task. There are two major limitations\nof existing works: first, they are constrained by corpus completeness, often\ndiscarding or manually supplementing incomplete knowledge; second, they\ntypically fail to fully automate the integration of diverse knowledge sources.\nThis paper proposes AutoMathKG, a high-quality, wide-coverage, and\nmulti-dimensional math KG capable of automatic updates. AutoMathKG regards\nmathematics as a vast directed graph composed of Definition, Theorem, and\nProblem entities, with their reference relationships as edges. It integrates\nknowledge from ProofWiki, textbooks, arXiv papers, and TheoremQA, enhancing\nentities and relationships with large language models (LLMs) via in-context\nlearning for data augmentation. To search for similar entities, MathVD, a\nvector database, is built through two designed embedding strategies using\nSBERT. To automatically update, two mechanisms are proposed. For knowledge\ncompletion mechanism, Math LLM is developed to interact with AutoMathKG,\nproviding missing proofs or solutions. For knowledge fusion mechanism, MathVD\nis used to retrieve similar entities, and LLM is used to determine whether to\nmerge with a candidate or add as a new entity. A wide range of experiments\ndemonstrate the advanced performance and broad applicability of the AutoMathKG\nsystem, including superior reachability query results in MathVD compared to\nfive baselines and robust mathematical reasoning capability in Math LLM."
                },
                "authors": [
                    {
                        "name": "Rong Bian"
                    },
                    {
                        "name": "Yu Geng"
                    },
                    {
                        "name": "Zijian Yang"
                    },
                    {
                        "name": "Bing Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Bing Cheng"
                },
                "author": "Bing Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13406v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13406v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13404v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13404v1",
                "updated": "2025-05-19T17:40:58Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    40,
                    58,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T17:40:58Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    40,
                    58,
                    0,
                    139,
                    0
                ],
                "title": "Granary: Speech Recognition and Translation Dataset in 25 European\n  Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Granary: Speech Recognition and Translation Dataset in 25 European\n  Languages"
                },
                "summary": "Multi-task and multilingual approaches benefit large models, yet speech\nprocessing for low-resource languages remains underexplored due to data\nscarcity. To address this, we present Granary, a large-scale collection of\nspeech datasets for recognition and translation across 25 European languages.\nThis is the first open-source effort at this scale for both transcription and\ntranslation. We enhance data quality using a pseudo-labeling pipeline with\nsegmentation, two-pass inference, hallucination filtering, and punctuation\nrestoration. We further generate translation pairs from pseudo-labeled\ntranscriptions using EuroLLM, followed by a data filtration pipeline. Designed\nfor efficiency, our pipeline processes vast amount of data within hours. We\nassess models trained on processed data by comparing their performance on\npreviously curated datasets for both high- and low-resource languages. Our\nfindings show that these models achieve similar performance using approx. 50%\nless data. Dataset will be made available at\nhttps://hf.co/datasets/nvidia/Granary",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-task and multilingual approaches benefit large models, yet speech\nprocessing for low-resource languages remains underexplored due to data\nscarcity. To address this, we present Granary, a large-scale collection of\nspeech datasets for recognition and translation across 25 European languages.\nThis is the first open-source effort at this scale for both transcription and\ntranslation. We enhance data quality using a pseudo-labeling pipeline with\nsegmentation, two-pass inference, hallucination filtering, and punctuation\nrestoration. We further generate translation pairs from pseudo-labeled\ntranscriptions using EuroLLM, followed by a data filtration pipeline. Designed\nfor efficiency, our pipeline processes vast amount of data within hours. We\nassess models trained on processed data by comparing their performance on\npreviously curated datasets for both high- and low-resource languages. Our\nfindings show that these models achieve similar performance using approx. 50%\nless data. Dataset will be made available at\nhttps://hf.co/datasets/nvidia/Granary"
                },
                "authors": [
                    {
                        "name": "Nithin Rao Koluguri"
                    },
                    {
                        "name": "Monica Sekoyan"
                    },
                    {
                        "name": "George Zelenfroynd"
                    },
                    {
                        "name": "Sasha Meister"
                    },
                    {
                        "name": "Shuoyang Ding"
                    },
                    {
                        "name": "Sofia Kostandian"
                    },
                    {
                        "name": "He Huang"
                    },
                    {
                        "name": "Nikolay Karpov"
                    },
                    {
                        "name": "Jagadeesh Balam"
                    },
                    {
                        "name": "Vitaly Lavrukhin"
                    },
                    {
                        "name": "Yifan Peng"
                    },
                    {
                        "name": "Sara Papi"
                    },
                    {
                        "name": "Marco Gaido"
                    },
                    {
                        "name": "Alessio Brutti"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "arxiv_comment": "Accepted at Interspeech 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13404v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13404v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13403v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13403v1",
                "updated": "2025-05-19T17:37:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    37,
                    39,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T17:37:39Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    37,
                    39,
                    0,
                    139,
                    0
                ],
                "title": "MR. Judge: Multimodal Reasoner as a Judge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MR. Judge: Multimodal Reasoner as a Judge"
                },
                "summary": "The paradigm of using Large Language Models (LLMs) and Multimodal Large\nLanguage Models (MLLMs) as evaluative judges has emerged as an effective\napproach in RLHF and inference-time scaling. In this work, we propose\nMultimodal Reasoner as a Judge (MR. Judge), a paradigm for empowering\ngeneral-purpose MLLMs judges with strong reasoning capabilities. Instead of\ndirectly assigning scores for each response, we formulate the judgement process\nas a reasoning-inspired multiple-choice problem. Specifically, the judge model\nfirst conducts deliberate reasoning covering different aspects of the responses\nand eventually selects the best response from them. This reasoning process not\nonly improves the interpretibility of the judgement, but also greatly enhances\nthe performance of MLLM judges. To cope with the lack of questions with scored\nresponses, we propose the following strategy to achieve automatic annotation:\n1) Reverse Response Candidates Synthesis: starting from a supervised\nfine-tuning (SFT) dataset, we treat the original response as the best candidate\nand prompt the MLLM to generate plausible but flawed negative candidates. 2)\nText-based reasoning extraction: we carefully design a data synthesis pipeline\nfor distilling the reasoning capability from a text-based reasoning model,\nwhich is adopted to enable the MLLM judges to regain complex reasoning ability\nvia warm up supervised fine-tuning. Experiments demonstrate that our MR. Judge\nis effective across a wide range of tasks. Specifically, our MR. Judge-7B\nsurpasses GPT-4o by 9.9% on VL-RewardBench, and improves performance on MM-Vet\nduring inference-time scaling by up to 7.7%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The paradigm of using Large Language Models (LLMs) and Multimodal Large\nLanguage Models (MLLMs) as evaluative judges has emerged as an effective\napproach in RLHF and inference-time scaling. In this work, we propose\nMultimodal Reasoner as a Judge (MR. Judge), a paradigm for empowering\ngeneral-purpose MLLMs judges with strong reasoning capabilities. Instead of\ndirectly assigning scores for each response, we formulate the judgement process\nas a reasoning-inspired multiple-choice problem. Specifically, the judge model\nfirst conducts deliberate reasoning covering different aspects of the responses\nand eventually selects the best response from them. This reasoning process not\nonly improves the interpretibility of the judgement, but also greatly enhances\nthe performance of MLLM judges. To cope with the lack of questions with scored\nresponses, we propose the following strategy to achieve automatic annotation:\n1) Reverse Response Candidates Synthesis: starting from a supervised\nfine-tuning (SFT) dataset, we treat the original response as the best candidate\nand prompt the MLLM to generate plausible but flawed negative candidates. 2)\nText-based reasoning extraction: we carefully design a data synthesis pipeline\nfor distilling the reasoning capability from a text-based reasoning model,\nwhich is adopted to enable the MLLM judges to regain complex reasoning ability\nvia warm up supervised fine-tuning. Experiments demonstrate that our MR. Judge\nis effective across a wide range of tasks. Specifically, our MR. Judge-7B\nsurpasses GPT-4o by 9.9% on VL-RewardBench, and improves performance on MM-Vet\nduring inference-time scaling by up to 7.7%."
                },
                "authors": [
                    {
                        "name": "Renjie Pi"
                    },
                    {
                        "name": "Felix Bai"
                    },
                    {
                        "name": "Qibin Chen"
                    },
                    {
                        "name": "Simon Wang"
                    },
                    {
                        "name": "Jiulong Shan"
                    },
                    {
                        "name": "Kieran Liu"
                    },
                    {
                        "name": "Meng Cao"
                    }
                ],
                "author_detail": {
                    "name": "Meng Cao"
                },
                "author": "Meng Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13403v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13403v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10573v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10573v2",
                "updated": "2025-05-19T17:36:27Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    36,
                    27,
                    0,
                    139,
                    0
                ],
                "published": "2025-03-13T17:23:45Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    23,
                    45,
                    3,
                    72,
                    0
                ],
                "title": "Evaluating Mathematical Reasoning Across Large Language Models: A\n  Fine-Grained Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Mathematical Reasoning Across Large Language Models: A\n  Fine-Grained Approach"
                },
                "summary": "With the rapid advancement of Artificial Intelligence (AI), Large Language\nModels (LLMs) have significantly impacted a wide array of domains, including\nhealthcare, engineering, science, education, and mathematical reasoning. Among\nthese, mathematical reasoning remains a particularly challenging capability,\noften requiring multi-step logic and abstract generalization. While prior work\nhas explored LLM performance on reasoning tasks, comprehensive evaluations that\nspan both depth and breadth across model families remain limited. In this\nstudy, we present a systematic evaluation of mathematical reasoning abilities\nacross eight leading LLMs, including two recent DeepSeek models, using three\nindependent benchmark datasets. Our analyses reveal several key findings: (1)\nDeepSeek-R1 performs competitively with o1 across most domains and achieves the\nhighest accuracy on the MMLU Formal Logic benchmark; (2) distilled variants,\nsuch as DeepSeek-1.5B, exhibit substantial performance degradation; and (3)\nGemini 2.0 Flash achieves the lowest response latency. Beyond quantitative\nmetrics, we explore how architectural choices, training paradigms, and\noptimization strategies contribute to variation in reasoning performance. These\nfindings provide new insights into the capabilities and limitations of current\nLLMs in mathematical domains, and offer guidance for the development of future\nmodels better aligned with rigorous reasoning demands.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of Artificial Intelligence (AI), Large Language\nModels (LLMs) have significantly impacted a wide array of domains, including\nhealthcare, engineering, science, education, and mathematical reasoning. Among\nthese, mathematical reasoning remains a particularly challenging capability,\noften requiring multi-step logic and abstract generalization. While prior work\nhas explored LLM performance on reasoning tasks, comprehensive evaluations that\nspan both depth and breadth across model families remain limited. In this\nstudy, we present a systematic evaluation of mathematical reasoning abilities\nacross eight leading LLMs, including two recent DeepSeek models, using three\nindependent benchmark datasets. Our analyses reveal several key findings: (1)\nDeepSeek-R1 performs competitively with o1 across most domains and achieves the\nhighest accuracy on the MMLU Formal Logic benchmark; (2) distilled variants,\nsuch as DeepSeek-1.5B, exhibit substantial performance degradation; and (3)\nGemini 2.0 Flash achieves the lowest response latency. Beyond quantitative\nmetrics, we explore how architectural choices, training paradigms, and\noptimization strategies contribute to variation in reasoning performance. These\nfindings provide new insights into the capabilities and limitations of current\nLLMs in mathematical domains, and offer guidance for the development of future\nmodels better aligned with rigorous reasoning demands."
                },
                "authors": [
                    {
                        "name": "Afrar Jahin"
                    },
                    {
                        "name": "Arif Hassan Zidan"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Yu Bao"
                    },
                    {
                        "name": "Tianming Liu"
                    }
                ],
                "author_detail": {
                    "name": "Tianming Liu"
                },
                "author": "Tianming Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10573v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10573v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13399v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13399v1",
                "updated": "2025-05-19T17:36:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    36,
                    1,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T17:36:01Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    36,
                    1,
                    0,
                    139,
                    0
                ],
                "title": "Filtering out large-scale noise for cluster weak-lensing mass estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Filtering out large-scale noise for cluster weak-lensing mass estimation"
                },
                "summary": "We present a new method for estimating galaxy cluster masses using\nweak-lensing magnification. The effect of weak-lensing magnification introduces\na correlation between the position of foreground galaxy clusters and the\ndensity of background sources. Therefore, cluster masses can be inferred\nthrough observations of these correlations. In this work, we introduce a method\nthat allows us to considerably reduce noise correlations between different\nradial bins of the cluster magnification signal via a Wiener filtering of our\nobserved magnification field on large scales. This method can reduce the\nuncertainty on the estimated galaxy cluster mass and it can also be applied to\ncluster mass estimation for weak-lensing shear. The method was applied to\nHyper-Suprime Cam galaxies and CAMIRA clusters detected within the\nHyper-Suprime Cam survey (HSC). With HSC data, we find that our filtering\nmethod significantly reduces the correlation of noise between radial\nmagnification bins. The estimated cluster mass is consistent between the\nfiltered and unfiltered methods, with similar errors between the two methods as\nour current measurement errors contain significant contributions from the\nirreducible shot-noise. For deeper surveys, the effects of shot noise will be\nless important and this method will lead to greater improvements on the\nestimated cluster mass.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a new method for estimating galaxy cluster masses using\nweak-lensing magnification. The effect of weak-lensing magnification introduces\na correlation between the position of foreground galaxy clusters and the\ndensity of background sources. Therefore, cluster masses can be inferred\nthrough observations of these correlations. In this work, we introduce a method\nthat allows us to considerably reduce noise correlations between different\nradial bins of the cluster magnification signal via a Wiener filtering of our\nobserved magnification field on large scales. This method can reduce the\nuncertainty on the estimated galaxy cluster mass and it can also be applied to\ncluster mass estimation for weak-lensing shear. The method was applied to\nHyper-Suprime Cam galaxies and CAMIRA clusters detected within the\nHyper-Suprime Cam survey (HSC). With HSC data, we find that our filtering\nmethod significantly reduces the correlation of noise between radial\nmagnification bins. The estimated cluster mass is consistent between the\nfiltered and unfiltered methods, with similar errors between the two methods as\nour current measurement errors contain significant contributions from the\nirreducible shot-noise. For deeper surveys, the effects of shot noise will be\nless important and this method will lead to greater improvements on the\nestimated cluster mass."
                },
                "authors": [
                    {
                        "name": "C. Murray"
                    },
                    {
                        "name": "C. Combet"
                    },
                    {
                        "name": "C. Payerne"
                    },
                    {
                        "name": "M. Ricci"
                    }
                ],
                "author_detail": {
                    "name": "M. Ricci"
                },
                "author": "M. Ricci",
                "arxiv_comment": "Comments and questions welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13399v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13399v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13389v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13389v1",
                "updated": "2025-05-19T17:30:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    30,
                    13,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T17:30:13Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    30,
                    13,
                    0,
                    139,
                    0
                ],
                "title": "Faster Video Diffusion with Trainable Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faster Video Diffusion with Trainable Sparse Attention"
                },
                "summary": "Scaling video diffusion transformers (DiTs) is limited by their quadratic 3D\nattention, even though most of the attention mass concentrates on a small\nsubset of positions. We turn this observation into VSA, a trainable,\nhardware-efficient sparse attention that replaces full attention at \\emph{both}\ntraining and inference. In VSA, a lightweight coarse stage pools tokens into\ntiles and identifies high-weight \\emph{critical tokens}; a fine stage computes\ntoken-level attention only inside those tiles subjecting to block computing\nlayout to ensure hard efficiency. This leads to a single differentiable kernel\nthat trains end-to-end, requires no post-hoc profiling, and sustains 85\\% of\nFlashAttention3 MFU. We perform a large sweep of ablation studies and\nscaling-law experiments by pretraining DiTs from 60M to 1.4B parameters. VSA\nreaches a Pareto point that cuts training FLOPS by 2.53$\\times$ with no drop in\ndiffusion loss. Retrofitting the open-source Wan-2.1 model speeds up attention\ntime by 6$\\times$ and lowers end-to-end generation time from 31s to 18s with\ncomparable quality. These results establish trainable sparse attention as a\npractical alternative to full attention and a key enabler for further scaling\nof video diffusion models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling video diffusion transformers (DiTs) is limited by their quadratic 3D\nattention, even though most of the attention mass concentrates on a small\nsubset of positions. We turn this observation into VSA, a trainable,\nhardware-efficient sparse attention that replaces full attention at \\emph{both}\ntraining and inference. In VSA, a lightweight coarse stage pools tokens into\ntiles and identifies high-weight \\emph{critical tokens}; a fine stage computes\ntoken-level attention only inside those tiles subjecting to block computing\nlayout to ensure hard efficiency. This leads to a single differentiable kernel\nthat trains end-to-end, requires no post-hoc profiling, and sustains 85\\% of\nFlashAttention3 MFU. We perform a large sweep of ablation studies and\nscaling-law experiments by pretraining DiTs from 60M to 1.4B parameters. VSA\nreaches a Pareto point that cuts training FLOPS by 2.53$\\times$ with no drop in\ndiffusion loss. Retrofitting the open-source Wan-2.1 model speeds up attention\ntime by 6$\\times$ and lowers end-to-end generation time from 31s to 18s with\ncomparable quality. These results establish trainable sparse attention as a\npractical alternative to full attention and a key enabler for further scaling\nof video diffusion models."
                },
                "authors": [
                    {
                        "name": "Peiyuan Zhang"
                    },
                    {
                        "name": "Haofeng Huang"
                    },
                    {
                        "name": "Yongqi Chen"
                    },
                    {
                        "name": "Will Lin"
                    },
                    {
                        "name": "Zhengzhong Liu"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Eric P. Xing"
                    },
                    {
                        "name": "Hao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Zhang"
                },
                "author": "Hao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13389v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13389v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13384v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13384v1",
                "updated": "2025-05-19T17:28:03Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    28,
                    3,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T17:28:03Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    28,
                    3,
                    0,
                    139,
                    0
                ],
                "title": "An Empirical Bayes approach to ARX Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Bayes approach to ARX Estimation"
                },
                "summary": "Empirical Bayes inference is based on estimation of the parameters of an a\npriori distribution from the observed data. The estimation technique of the\nparameters of the prior, called hyperparameters, is based on the marginal\ndistribution obtained by integrating the joint density of the model with\nrespect to the prior. This is a key step which needs to be properly adapted to\nthe problem at hand. In this paper we study Empirical Bayes inference of linear\nautoregressive models with inputs (ARX models) for time series and compare the\nperformance of the marginal parametric estimator with that a full Empirical\nBayesian analysis based on the estimated prior. Such a comparison, can only\nmake sense for a (realistic) finite data length. In this setting, we propose a\nnew estimation technique of the hyperparameters by a sequential Bayes procedure\nwhich is essentially a backward Kalman filter. It turns out that for finite\ndata length the marginal Bayes tends to behave slightly better than the full\nEmpirical Bayesian parameter estimator and so also in the case of slowly\nvarying random parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empirical Bayes inference is based on estimation of the parameters of an a\npriori distribution from the observed data. The estimation technique of the\nparameters of the prior, called hyperparameters, is based on the marginal\ndistribution obtained by integrating the joint density of the model with\nrespect to the prior. This is a key step which needs to be properly adapted to\nthe problem at hand. In this paper we study Empirical Bayes inference of linear\nautoregressive models with inputs (ARX models) for time series and compare the\nperformance of the marginal parametric estimator with that a full Empirical\nBayesian analysis based on the estimated prior. Such a comparison, can only\nmake sense for a (realistic) finite data length. In this setting, we propose a\nnew estimation technique of the hyperparameters by a sequential Bayes procedure\nwhich is essentially a backward Kalman filter. It turns out that for finite\ndata length the marginal Bayes tends to behave slightly better than the full\nEmpirical Bayesian parameter estimator and so also in the case of slowly\nvarying random parameters."
                },
                "authors": [
                    {
                        "name": "Timofei Leahu"
                    },
                    {
                        "name": "Giorgio Picci"
                    }
                ],
                "author_detail": {
                    "name": "Giorgio Picci"
                },
                "author": "Giorgio Picci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13384v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13384v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13379v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13379v1",
                "updated": "2025-05-19T17:24:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    24,
                    16,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T17:24:16Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    24,
                    16,
                    0,
                    139,
                    0
                ],
                "title": "Thinkless: LLM Learns When to Think",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thinkless: LLM Learns When to Think"
                },
                "summary": "Reasoning Language Models, capable of extended chain-of-thought reasoning,\nhave demonstrated remarkable performance on tasks requiring complex logical\ninference. However, applying elaborate reasoning for all queries often results\nin substantial computational inefficiencies, particularly when many problems\nadmit straightforward solutions. This motivates an open question: Can LLMs\nlearn when to think? To answer this, we propose Thinkless, a learnable\nframework that empowers an LLM to adaptively select between short-form and\nlong-form reasoning, based on both task complexity and the model's ability.\nThinkless is trained under a reinforcement learning paradigm and employs two\ncontrol tokens, <short> for concise responses and <think> for detailed\nreasoning. At the core of our method is a Decoupled Group Relative Policy\nOptimization (DeGRPO) algorithm, which decomposes the learning objective of\nhybrid reasoning into two components: (1) a control token loss that governs the\nselection of the reasoning mode, and (2) a response loss that improves the\naccuracy of the generated answers. This decoupled formulation enables\nfine-grained control over the contributions of each objective, stabilizing\ntraining and effectively preventing collapse observed in vanilla GRPO.\nEmpirically, on several benchmarks such as Minerva Algebra, MATH-500, and\nGSM8K, Thinkless is able to reduce the usage of long-chain thinking by 50% -\n90%, significantly improving the efficiency of Reasoning Language Models. The\ncode is available at https://github.com/VainF/Thinkless",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Language Models, capable of extended chain-of-thought reasoning,\nhave demonstrated remarkable performance on tasks requiring complex logical\ninference. However, applying elaborate reasoning for all queries often results\nin substantial computational inefficiencies, particularly when many problems\nadmit straightforward solutions. This motivates an open question: Can LLMs\nlearn when to think? To answer this, we propose Thinkless, a learnable\nframework that empowers an LLM to adaptively select between short-form and\nlong-form reasoning, based on both task complexity and the model's ability.\nThinkless is trained under a reinforcement learning paradigm and employs two\ncontrol tokens, <short> for concise responses and <think> for detailed\nreasoning. At the core of our method is a Decoupled Group Relative Policy\nOptimization (DeGRPO) algorithm, which decomposes the learning objective of\nhybrid reasoning into two components: (1) a control token loss that governs the\nselection of the reasoning mode, and (2) a response loss that improves the\naccuracy of the generated answers. This decoupled formulation enables\nfine-grained control over the contributions of each objective, stabilizing\ntraining and effectively preventing collapse observed in vanilla GRPO.\nEmpirically, on several benchmarks such as Minerva Algebra, MATH-500, and\nGSM8K, Thinkless is able to reduce the usage of long-chain thinking by 50% -\n90%, significantly improving the efficiency of Reasoning Language Models. The\ncode is available at https://github.com/VainF/Thinkless"
                },
                "authors": [
                    {
                        "name": "Gongfan Fang"
                    },
                    {
                        "name": "Xinyin Ma"
                    },
                    {
                        "name": "Xinchao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinchao Wang"
                },
                "author": "Xinchao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13379v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13379v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10872v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10872v2",
                "updated": "2025-05-19T17:21:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    21,
                    49,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-16T05:27:15Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    5,
                    27,
                    15,
                    4,
                    136,
                    0
                ],
                "title": "REI-Bench: Can Embodied Agents Understand Vague Human Instructions in\n  Task Planning?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REI-Bench: Can Embodied Agents Understand Vague Human Instructions in\n  Task Planning?"
                },
                "summary": "Robot task planning decomposes human instructions into executable action\nsequences that enable robots to complete a series of complex tasks. Although\nrecent large language model (LLM)-based task planners achieve amazing\nperformance, they assume that human instructions are clear and straightforward.\nHowever, real-world users are not experts, and their instructions to robots\noften contain significant vagueness. Linguists suggest that such vagueness\nfrequently arises from referring expressions (REs), whose meanings depend\nheavily on dialogue context and environment. This vagueness is even more\nprevalent among the elderly and children, who robots should serve more. This\npaper studies how such vagueness in REs within human instructions affects\nLLM-based robot task planning and how to overcome this issue. To this end, we\npropose the first robot task planning benchmark with vague REs (REI-Bench),\nwhere we discover that the vagueness of REs can severely degrade robot planning\nperformance, leading to success rate drops of up to 77.9%. We also observe that\nmost failure cases stem from missing objects in planners. To mitigate the REs\nissue, we propose a simple yet effective approach: task-oriented context\ncognition, which generates clear instructions for robots, achieving\nstate-of-the-art performance compared to aware prompt and chains of thought.\nThis work contributes to the research community of human-robot interaction\n(HRI) by making robot task planning more practical, particularly for non-expert\nusers, e.g., the elderly and children.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robot task planning decomposes human instructions into executable action\nsequences that enable robots to complete a series of complex tasks. Although\nrecent large language model (LLM)-based task planners achieve amazing\nperformance, they assume that human instructions are clear and straightforward.\nHowever, real-world users are not experts, and their instructions to robots\noften contain significant vagueness. Linguists suggest that such vagueness\nfrequently arises from referring expressions (REs), whose meanings depend\nheavily on dialogue context and environment. This vagueness is even more\nprevalent among the elderly and children, who robots should serve more. This\npaper studies how such vagueness in REs within human instructions affects\nLLM-based robot task planning and how to overcome this issue. To this end, we\npropose the first robot task planning benchmark with vague REs (REI-Bench),\nwhere we discover that the vagueness of REs can severely degrade robot planning\nperformance, leading to success rate drops of up to 77.9%. We also observe that\nmost failure cases stem from missing objects in planners. To mitigate the REs\nissue, we propose a simple yet effective approach: task-oriented context\ncognition, which generates clear instructions for robots, achieving\nstate-of-the-art performance compared to aware prompt and chains of thought.\nThis work contributes to the research community of human-robot interaction\n(HRI) by making robot task planning more practical, particularly for non-expert\nusers, e.g., the elderly and children."
                },
                "authors": [
                    {
                        "name": "Chenxi Jiang"
                    },
                    {
                        "name": "Chuhao Zhou"
                    },
                    {
                        "name": "Jianfei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jianfei Yang"
                },
                "author": "Jianfei Yang",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10872v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10872v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13376v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13376v1",
                "updated": "2025-05-19T17:19:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    19,
                    43,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T17:19:43Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    19,
                    43,
                    0,
                    139,
                    0
                ],
                "title": "Seeing, Saying, Solving: An LLM-to-TL Framework for Cooperative Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seeing, Saying, Solving: An LLM-to-TL Framework for Cooperative Robots"
                },
                "summary": "Increased robot deployment, such as in warehousing, has revealed a need for\nseamless collaboration among heterogeneous robot teams to resolve unforeseen\nconflicts. To address this challenge, we propose a novel, decentralized\nframework for robots to request and provide help. The framework begins with\nrobots detecting conflicts using a Vision Language Model (VLM), then reasoning\nover whether help is needed. If so, it crafts and broadcasts a natural language\n(NL) help request using a Large Language Model (LLM). Potential helper robots\nreason over the request and offer help (if able), along with information about\nimpact to their current tasks. Helper reasoning is implemented via an LLM\ngrounded in Signal Temporal Logic (STL) using a Backus-Naur Form (BNF) grammar\nto guarantee syntactically valid NL-to-STL translations, which are then solved\nas a Mixed Integer Linear Program (MILP). Finally, the requester robot chooses\na helper by reasoning over impact on the overall system. We evaluate our system\nvia experiments considering different strategies for choosing a helper, and\nfind that a requester robot can minimize overall time impact on the system by\nconsidering multiple help offers versus simple heuristics (e.g., selecting the\nnearest robot to help).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Increased robot deployment, such as in warehousing, has revealed a need for\nseamless collaboration among heterogeneous robot teams to resolve unforeseen\nconflicts. To address this challenge, we propose a novel, decentralized\nframework for robots to request and provide help. The framework begins with\nrobots detecting conflicts using a Vision Language Model (VLM), then reasoning\nover whether help is needed. If so, it crafts and broadcasts a natural language\n(NL) help request using a Large Language Model (LLM). Potential helper robots\nreason over the request and offer help (if able), along with information about\nimpact to their current tasks. Helper reasoning is implemented via an LLM\ngrounded in Signal Temporal Logic (STL) using a Backus-Naur Form (BNF) grammar\nto guarantee syntactically valid NL-to-STL translations, which are then solved\nas a Mixed Integer Linear Program (MILP). Finally, the requester robot chooses\na helper by reasoning over impact on the overall system. We evaluate our system\nvia experiments considering different strategies for choosing a helper, and\nfind that a requester robot can minimize overall time impact on the system by\nconsidering multiple help offers versus simple heuristics (e.g., selecting the\nnearest robot to help)."
                },
                "authors": [
                    {
                        "name": "Dan BW Choe"
                    },
                    {
                        "name": "Sundhar Vinodh Sangeetha"
                    },
                    {
                        "name": "Steven Emanuel"
                    },
                    {
                        "name": "Chih-Yuan Chiu"
                    },
                    {
                        "name": "Samuel Coogan"
                    },
                    {
                        "name": "Shreyas Kousik"
                    }
                ],
                "author_detail": {
                    "name": "Shreyas Kousik"
                },
                "author": "Shreyas Kousik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13376v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13376v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11380v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11380v2",
                "updated": "2025-05-19T17:17:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    17,
                    0,
                    0,
                    139,
                    0
                ],
                "published": "2025-02-17T02:52:07Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    2,
                    52,
                    7,
                    0,
                    48,
                    0
                ],
                "title": "From the New World of Word Embeddings: A Comparative Study of\n  Small-World Lexico-Semantic Networks in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From the New World of Word Embeddings: A Comparative Study of\n  Small-World Lexico-Semantic Networks in LLMs"
                },
                "summary": "Lexico-semantic networks represent words as nodes and their semantic\nrelatedness as edges. While such networks are traditionally constructed using\nembeddings from encoder-based models or static vectors, embeddings from\ndecoder-only large language models (LLMs) remain underexplored. Unlike encoder\nmodels, LLMs are trained with a next-token prediction objective, which does not\ndirectly encode the meaning of the current token. In this paper, we construct\nlexico-semantic networks from the input embeddings of LLMs with varying\nparameter scales and conduct a comparative analysis of their global and local\nstructures. Our results show that these networks exhibit small-world\nproperties, characterized by high clustering and short path lengths. Moreover,\nlarger LLMs yield more intricate networks with less small-world effects and\nlonger paths, reflecting richer semantic structures and relations. We further\nvalidate our approach through analyses of common conceptual pairs, structured\nlexical relations derived from WordNet, and a cross-lingual semantic network\nfor qualitative words.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lexico-semantic networks represent words as nodes and their semantic\nrelatedness as edges. While such networks are traditionally constructed using\nembeddings from encoder-based models or static vectors, embeddings from\ndecoder-only large language models (LLMs) remain underexplored. Unlike encoder\nmodels, LLMs are trained with a next-token prediction objective, which does not\ndirectly encode the meaning of the current token. In this paper, we construct\nlexico-semantic networks from the input embeddings of LLMs with varying\nparameter scales and conduct a comparative analysis of their global and local\nstructures. Our results show that these networks exhibit small-world\nproperties, characterized by high clustering and short path lengths. Moreover,\nlarger LLMs yield more intricate networks with less small-world effects and\nlonger paths, reflecting richer semantic structures and relations. We further\nvalidate our approach through analyses of common conceptual pairs, structured\nlexical relations derived from WordNet, and a cross-lingual semantic network\nfor qualitative words."
                },
                "authors": [
                    {
                        "name": "Zhu Liu"
                    },
                    {
                        "name": "Ying Liu"
                    },
                    {
                        "name": "KangYang Luo"
                    },
                    {
                        "name": "Cunliang Kong"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "arxiv_comment": "Paper under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11380v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11380v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14281v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14281v3",
                "updated": "2025-05-20T05:55:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    5,
                    55,
                    59,
                    1,
                    140,
                    0
                ],
                "published": "2025-03-18T14:20:54Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    14,
                    20,
                    54,
                    1,
                    77,
                    0
                ],
                "title": "XOXO: Stealthy Cross-Origin Context Poisoning Attacks against AI Coding\n  Assistants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XOXO: Stealthy Cross-Origin Context Poisoning Attacks against AI Coding\n  Assistants"
                },
                "summary": "AI coding assistants are widely used for tasks like code generation. These\ntools now require large and complex contexts, automatically sourced from\nvarious origins$\\unicode{x2014}$across files, projects, and\ncontributors$\\unicode{x2014}$forming part of the prompt fed to underlying LLMs.\nThis automatic context-gathering introduces new vulnerabilities, allowing\nattackers to subtly poison input to compromise the assistant's outputs,\npotentially generating vulnerable code or introducing critical errors. We\npropose a novel attack, Cross-Origin Context Poisoning (XOXO), that is\nchallenging to detect as it relies on adversarial code modifications that are\nsemantically equivalent. Traditional program analysis techniques struggle to\nidentify these perturbations since the semantics of the code remains correct,\nmaking it appear legitimate. This allows attackers to manipulate coding\nassistants into producing incorrect outputs, while shifting the blame to the\nvictim developer. We introduce a novel, task-agnostic, black-box attack\nalgorithm GCGS that systematically searches the transformation space using a\nCayley Graph, achieving a 75.72% attack success rate on average across five\ntasks and eleven models, including GPT 4.1 and Claude 3.5 Sonnet v2 used by\npopular AI coding assistants. Furthermore, defenses like adversarial\nfine-tuning are ineffective against our attack, underscoring the need for new\nsecurity measures in LLM-powered coding tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI coding assistants are widely used for tasks like code generation. These\ntools now require large and complex contexts, automatically sourced from\nvarious origins$\\unicode{x2014}$across files, projects, and\ncontributors$\\unicode{x2014}$forming part of the prompt fed to underlying LLMs.\nThis automatic context-gathering introduces new vulnerabilities, allowing\nattackers to subtly poison input to compromise the assistant's outputs,\npotentially generating vulnerable code or introducing critical errors. We\npropose a novel attack, Cross-Origin Context Poisoning (XOXO), that is\nchallenging to detect as it relies on adversarial code modifications that are\nsemantically equivalent. Traditional program analysis techniques struggle to\nidentify these perturbations since the semantics of the code remains correct,\nmaking it appear legitimate. This allows attackers to manipulate coding\nassistants into producing incorrect outputs, while shifting the blame to the\nvictim developer. We introduce a novel, task-agnostic, black-box attack\nalgorithm GCGS that systematically searches the transformation space using a\nCayley Graph, achieving a 75.72% attack success rate on average across five\ntasks and eleven models, including GPT 4.1 and Claude 3.5 Sonnet v2 used by\npopular AI coding assistants. Furthermore, defenses like adversarial\nfine-tuning are ineffective against our attack, underscoring the need for new\nsecurity measures in LLM-powered coding tools."
                },
                "authors": [
                    {
                        "name": "Adam Štorek"
                    },
                    {
                        "name": "Mukur Gupta"
                    },
                    {
                        "name": "Noopur Bhatt"
                    },
                    {
                        "name": "Aditya Gupta"
                    },
                    {
                        "name": "Janie Kim"
                    },
                    {
                        "name": "Prashast Srivastava"
                    },
                    {
                        "name": "Suman Jana"
                    }
                ],
                "author_detail": {
                    "name": "Suman Jana"
                },
                "author": "Suman Jana",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14281v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14281v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13364v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13364v1",
                "updated": "2025-05-19T17:10:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    10,
                    22,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T17:10:22Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    10,
                    22,
                    0,
                    139,
                    0
                ],
                "title": "Modeling Innovation Ecosystem Dynamics through Interacting Reinforced\n  Bernoulli Processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling Innovation Ecosystem Dynamics through Interacting Reinforced\n  Bernoulli Processes"
                },
                "summary": "Understanding how capabilities evolve into core capabilities-and how core\ncapabilities may ossify into rigidities-is central to innovation strategy\n[https://www.jstor.org/stable/2486355,\nhttps://www.barnesandnoble.com/w/dynamic-capabilities-and-strategic-management-david-j-teece/1102436798].\n  To address this, we propose a novel formal model based on interacting\nreinforced Bernoulli processes. This framework captures how patent successes\npropagate across technological categories and how these categories co-evolve.\nThe model is able to jointly account for several stylized facts in the\nempirical innovation literature, including sublinear success growth\n(success-probability decay), convergence of success shares across fields, and\ndiminishing cross-category correlations over time.\n  Empirical validation using GLOBAL PATSTAT (1980-2018) supports the\ntheoretical predictions. We estimate the structural parameters of the\ninteraction matrix and we also propose a statistical procedure to make\ninference on the intensity of cross-category interactions under the mean-field\nassumption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding how capabilities evolve into core capabilities-and how core\ncapabilities may ossify into rigidities-is central to innovation strategy\n[https://www.jstor.org/stable/2486355,\nhttps://www.barnesandnoble.com/w/dynamic-capabilities-and-strategic-management-david-j-teece/1102436798].\n  To address this, we propose a novel formal model based on interacting\nreinforced Bernoulli processes. This framework captures how patent successes\npropagate across technological categories and how these categories co-evolve.\nThe model is able to jointly account for several stylized facts in the\nempirical innovation literature, including sublinear success growth\n(success-probability decay), convergence of success shares across fields, and\ndiminishing cross-category correlations over time.\n  Empirical validation using GLOBAL PATSTAT (1980-2018) supports the\ntheoretical predictions. We estimate the structural parameters of the\ninteraction matrix and we also propose a statistical procedure to make\ninference on the intensity of cross-category interactions under the mean-field\nassumption."
                },
                "authors": [
                    {
                        "name": "Giacomo Aletti"
                    },
                    {
                        "name": "Irene Crimaldi"
                    },
                    {
                        "name": "Andrea Ghiglietti"
                    },
                    {
                        "name": "Federico Nutarelli"
                    }
                ],
                "author_detail": {
                    "name": "Federico Nutarelli"
                },
                "author": "Federico Nutarelli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13364v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13364v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13362v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13362v1",
                "updated": "2025-05-19T17:07:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    7,
                    0,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T17:07:00Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    7,
                    0,
                    0,
                    139,
                    0
                ],
                "title": "DynaNoise: Dynamic Probabilistic Noise Injection for Defending Against\n  Membership Inference Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynaNoise: Dynamic Probabilistic Noise Injection for Defending Against\n  Membership Inference Attacks"
                },
                "summary": "Membership Inference Attacks (MIAs) pose a significant risk to the privacy of\ntraining datasets by exploiting subtle differences in model outputs to\ndetermine whether a particular data sample was used during training. These\nattacks can compromise sensitive information, especially in domains such as\nhealthcare and finance, where data privacy is paramount. Traditional mitigation\ntechniques, such as static differential privacy, rely on injecting a fixed\namount of noise during training or inference. However, this approach often\nleads to a detrimental trade-off: the noise may be insufficient to counter\nsophisticated attacks or, when increased, may substantially degrade model\nperformance. In this paper, we present DynaNoise, an adaptive approach that\ndynamically modulates noise injection based on query sensitivity. Our approach\nperforms sensitivity analysis using measures such as Shannon entropy to\nevaluate the risk associated with each query and adjusts the noise variance\naccordingly. A probabilistic smoothing step is then applied to renormalize the\nperturbed outputs, ensuring that the model maintains high accuracy while\neffectively obfuscating membership signals. We further propose an empirical\nmetric, the Membership Inference Defense Privacy-Utility Tradeoff (MIDPUT),\nwhich quantifies the balance between reducing attack success rates and\npreserving the target model's accuracy. Our extensive evaluation on several\nbenchmark datasets demonstrates that DynaNoise not only significantly reduces\nMIA success rates but also achieves up to a fourfold improvement in the MIDPUT\nmetric compared to the state-of-the-art. Moreover, DynaNoise maintains\ncompetitive model accuracy while imposing only marginal inference overhead,\nhighlighting its potential as an effective and efficient privacy defense\nagainst MIAs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Membership Inference Attacks (MIAs) pose a significant risk to the privacy of\ntraining datasets by exploiting subtle differences in model outputs to\ndetermine whether a particular data sample was used during training. These\nattacks can compromise sensitive information, especially in domains such as\nhealthcare and finance, where data privacy is paramount. Traditional mitigation\ntechniques, such as static differential privacy, rely on injecting a fixed\namount of noise during training or inference. However, this approach often\nleads to a detrimental trade-off: the noise may be insufficient to counter\nsophisticated attacks or, when increased, may substantially degrade model\nperformance. In this paper, we present DynaNoise, an adaptive approach that\ndynamically modulates noise injection based on query sensitivity. Our approach\nperforms sensitivity analysis using measures such as Shannon entropy to\nevaluate the risk associated with each query and adjusts the noise variance\naccordingly. A probabilistic smoothing step is then applied to renormalize the\nperturbed outputs, ensuring that the model maintains high accuracy while\neffectively obfuscating membership signals. We further propose an empirical\nmetric, the Membership Inference Defense Privacy-Utility Tradeoff (MIDPUT),\nwhich quantifies the balance between reducing attack success rates and\npreserving the target model's accuracy. Our extensive evaluation on several\nbenchmark datasets demonstrates that DynaNoise not only significantly reduces\nMIA success rates but also achieves up to a fourfold improvement in the MIDPUT\nmetric compared to the state-of-the-art. Moreover, DynaNoise maintains\ncompetitive model accuracy while imposing only marginal inference overhead,\nhighlighting its potential as an effective and efficient privacy defense\nagainst MIAs."
                },
                "authors": [
                    {
                        "name": "Javad Forough"
                    },
                    {
                        "name": "Hamed Haddadi"
                    }
                ],
                "author_detail": {
                    "name": "Hamed Haddadi"
                },
                "author": "Hamed Haddadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13362v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13362v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13361v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13361v1",
                "updated": "2025-05-19T17:06:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    6,
                    21,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T17:06:21Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    6,
                    21,
                    0,
                    139,
                    0
                ],
                "title": "Feedback-Driven Dynamical Model for Axonal Extension on Parallel\n  Micropatterns",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feedback-Driven Dynamical Model for Axonal Extension on Parallel\n  Micropatterns"
                },
                "summary": "Despite significant advances in understanding neuronal development, a fully\nquantitative framework that integrates intracellular mechanisms with\nenvironmental cues during axonal growth remains incomplete. Here, we present a\nunified biophysical model that captures key mechanochemical processes governing\naxonal extension on micropatterned substrates. In these environments, axons\npreferentially align with the pattern direction, form bundles, and advance at\nconstant speed. The model integrates four core components: (i) actin-adhesion\ntraction coupling, (ii) lateral inhibition between neighboring axons, (iii)\ntubulin transport from soma to the growth cone, and (4) orientation dynamics\nguided by the substrate anisotropy. Dynamical systems analysis reveals that the\nsaddle-node bifurcation in the actin adhesion subsystem drives a transition to\na high-traction motile state, while traction feedback shifts a pitchfork\nbifurcation in the signaling loop, promoting symmetry breaking and robust\nalignment. An exact linear solution in the tubulin transport subsystem\nfunctions as a built-in speed regulator, ensuring stable elongation rates.\nSimulations using experimentally inferred parameters accurately reproduce\nelongation speed, alignment variance, and bundle spacing. The model provides\nexplicit design rules for enhancing axonal alignment through modulation of\nsubstrate stiffness and adhesion dynamics. By identifying key control\nparameters, this work enables rational design of biomaterials for neural repair\nand engineered tissue systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite significant advances in understanding neuronal development, a fully\nquantitative framework that integrates intracellular mechanisms with\nenvironmental cues during axonal growth remains incomplete. Here, we present a\nunified biophysical model that captures key mechanochemical processes governing\naxonal extension on micropatterned substrates. In these environments, axons\npreferentially align with the pattern direction, form bundles, and advance at\nconstant speed. The model integrates four core components: (i) actin-adhesion\ntraction coupling, (ii) lateral inhibition between neighboring axons, (iii)\ntubulin transport from soma to the growth cone, and (4) orientation dynamics\nguided by the substrate anisotropy. Dynamical systems analysis reveals that the\nsaddle-node bifurcation in the actin adhesion subsystem drives a transition to\na high-traction motile state, while traction feedback shifts a pitchfork\nbifurcation in the signaling loop, promoting symmetry breaking and robust\nalignment. An exact linear solution in the tubulin transport subsystem\nfunctions as a built-in speed regulator, ensuring stable elongation rates.\nSimulations using experimentally inferred parameters accurately reproduce\nelongation speed, alignment variance, and bundle spacing. The model provides\nexplicit design rules for enhancing axonal alignment through modulation of\nsubstrate stiffness and adhesion dynamics. By identifying key control\nparameters, this work enables rational design of biomaterials for neural repair\nand engineered tissue systems."
                },
                "authors": [
                    {
                        "name": "Kyle Cheng"
                    },
                    {
                        "name": "Udathari Kumarasinghe"
                    },
                    {
                        "name": "Cristian Staii"
                    }
                ],
                "author_detail": {
                    "name": "Cristian Staii"
                },
                "author": "Cristian Staii",
                "arxiv_comment": "13 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13361v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nlin.AO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.CB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13360v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13360v1",
                "updated": "2025-05-19T17:03:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    3,
                    42,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T17:03:42Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    3,
                    42,
                    0,
                    139,
                    0
                ],
                "title": "What Prompts Don't Say: Understanding and Managing Underspecification in\n  LLM Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Prompts Don't Say: Understanding and Managing Underspecification in\n  LLM Prompts"
                },
                "summary": "Building LLM-powered software requires developers to communicate their\nrequirements through natural language, but developer prompts are frequently\nunderspecified, failing to fully capture many user-important requirements. In\nthis paper, we present an in-depth analysis of prompt underspecification,\nshowing that while LLMs can often (41.1%) guess unspecified requirements by\ndefault, such behavior is less robust: Underspecified prompts are 2x more\nlikely to regress over model or prompt changes, sometimes with accuracy drops\nby more than 20%. We then demonstrate that simply adding more requirements to a\nprompt does not reliably improve performance, due to LLMs' limited\ninstruction-following capabilities and competing constraints, and standard\nprompt optimizers do not offer much help. To address this, we introduce novel\nrequirements-aware prompt optimization mechanisms that can improve performance\nby 4.8% on average over baselines that naively specify everything in the\nprompt. Beyond prompt optimization, we envision that effectively managing\nprompt underspecification requires a broader process, including proactive\nrequirements discovery, evaluation, and monitoring.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building LLM-powered software requires developers to communicate their\nrequirements through natural language, but developer prompts are frequently\nunderspecified, failing to fully capture many user-important requirements. In\nthis paper, we present an in-depth analysis of prompt underspecification,\nshowing that while LLMs can often (41.1%) guess unspecified requirements by\ndefault, such behavior is less robust: Underspecified prompts are 2x more\nlikely to regress over model or prompt changes, sometimes with accuracy drops\nby more than 20%. We then demonstrate that simply adding more requirements to a\nprompt does not reliably improve performance, due to LLMs' limited\ninstruction-following capabilities and competing constraints, and standard\nprompt optimizers do not offer much help. To address this, we introduce novel\nrequirements-aware prompt optimization mechanisms that can improve performance\nby 4.8% on average over baselines that naively specify everything in the\nprompt. Beyond prompt optimization, we envision that effectively managing\nprompt underspecification requires a broader process, including proactive\nrequirements discovery, evaluation, and monitoring."
                },
                "authors": [
                    {
                        "name": "Chenyang Yang"
                    },
                    {
                        "name": "Yike Shi"
                    },
                    {
                        "name": "Qianou Ma"
                    },
                    {
                        "name": "Michael Xieyang Liu"
                    },
                    {
                        "name": "Christian Kästner"
                    },
                    {
                        "name": "Tongshuang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Tongshuang Wu"
                },
                "author": "Tongshuang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13360v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13360v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11114v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11114v2",
                "updated": "2025-05-19T17:02:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    2,
                    39,
                    0,
                    139,
                    0
                ],
                "published": "2025-02-16T13:06:50Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    13,
                    6,
                    50,
                    6,
                    47,
                    0
                ],
                "title": "Beyond Pairwise: Global Zero-shot Temporal Graph Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Pairwise: Global Zero-shot Temporal Graph Generation"
                },
                "summary": "Temporal relation extraction (TRE) is a fundamental task in natural language\nprocessing (NLP) that involves identifying the temporal relationships between\nevents in a document. Despite the advances in large language models (LLMs),\ntheir application to TRE remains limited. Most existing approaches rely on\npairwise classification, where event pairs are classified in isolation, leading\nto computational inefficiency and a lack of global consistency in the resulting\ntemporal graph. In this work, we propose a novel zero-shot method for TRE that\ngenerates a document's complete temporal graph in a single step, followed by\ntemporal constraint optimization to refine predictions and enforce temporal\nconsistency across relations. Additionally, we introduce OmniTemp, a new\ndataset with complete annotations for all pairs of targeted events within a\ndocument. Through experiments and analyses, we demonstrate that our method\noutperforms existing zero-shot approaches and offers a competitive alternative\nto supervised TRE models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal relation extraction (TRE) is a fundamental task in natural language\nprocessing (NLP) that involves identifying the temporal relationships between\nevents in a document. Despite the advances in large language models (LLMs),\ntheir application to TRE remains limited. Most existing approaches rely on\npairwise classification, where event pairs are classified in isolation, leading\nto computational inefficiency and a lack of global consistency in the resulting\ntemporal graph. In this work, we propose a novel zero-shot method for TRE that\ngenerates a document's complete temporal graph in a single step, followed by\ntemporal constraint optimization to refine predictions and enforce temporal\nconsistency across relations. Additionally, we introduce OmniTemp, a new\ndataset with complete annotations for all pairs of targeted events within a\ndocument. Through experiments and analyses, we demonstrate that our method\noutperforms existing zero-shot approaches and offers a competitive alternative\nto supervised TRE models."
                },
                "authors": [
                    {
                        "name": "Alon Eirew"
                    },
                    {
                        "name": "Kfir Bar"
                    },
                    {
                        "name": "Ido Dagan"
                    }
                ],
                "author_detail": {
                    "name": "Ido Dagan"
                },
                "author": "Ido Dagan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11114v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11114v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13355v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13355v1",
                "updated": "2025-05-19T16:57:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    57,
                    57,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T16:57:57Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    57,
                    57,
                    0,
                    139,
                    0
                ],
                "title": "Multi-Armed Bandits Meet Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Armed Bandits Meet Large Language Models"
                },
                "summary": "Bandit algorithms and Large Language Models (LLMs) have emerged as powerful\ntools in artificial intelligence, each addressing distinct yet complementary\nchallenges in decision-making and natural language processing. This survey\nexplores the synergistic potential between these two fields, highlighting how\nbandit algorithms can enhance the performance of LLMs and how LLMs, in turn,\ncan provide novel insights for improving bandit-based decision-making. We first\nexamine the role of bandit algorithms in optimizing LLM fine-tuning, prompt\nengineering, and adaptive response generation, focusing on their ability to\nbalance exploration and exploitation in large-scale learning tasks.\nSubsequently, we explore how LLMs can augment bandit algorithms through\nadvanced contextual understanding, dynamic adaptation, and improved policy\nselection using natural language reasoning. By providing a comprehensive review\nof existing research and identifying key challenges and opportunities, this\nsurvey aims to bridge the gap between bandit algorithms and LLMs, paving the\nway for innovative applications and interdisciplinary research in AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bandit algorithms and Large Language Models (LLMs) have emerged as powerful\ntools in artificial intelligence, each addressing distinct yet complementary\nchallenges in decision-making and natural language processing. This survey\nexplores the synergistic potential between these two fields, highlighting how\nbandit algorithms can enhance the performance of LLMs and how LLMs, in turn,\ncan provide novel insights for improving bandit-based decision-making. We first\nexamine the role of bandit algorithms in optimizing LLM fine-tuning, prompt\nengineering, and adaptive response generation, focusing on their ability to\nbalance exploration and exploitation in large-scale learning tasks.\nSubsequently, we explore how LLMs can augment bandit algorithms through\nadvanced contextual understanding, dynamic adaptation, and improved policy\nselection using natural language reasoning. By providing a comprehensive review\nof existing research and identifying key challenges and opportunities, this\nsurvey aims to bridge the gap between bandit algorithms and LLMs, paving the\nway for innovative applications and interdisciplinary research in AI."
                },
                "authors": [
                    {
                        "name": "Djallel Bouneffouf"
                    },
                    {
                        "name": "Raphael Feraud"
                    }
                ],
                "author_detail": {
                    "name": "Raphael Feraud"
                },
                "author": "Raphael Feraud",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13355v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13355v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13353v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13353v2",
                "updated": "2025-05-20T05:45:55Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    5,
                    45,
                    55,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-19T16:56:31Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    56,
                    31,
                    0,
                    139,
                    0
                ],
                "title": "Sense and Sensitivity: Examining the Influence of Semantic Recall on\n  Long Context Code Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sense and Sensitivity: Examining the Influence of Semantic Recall on\n  Long Context Code Reasoning"
                },
                "summary": "Although modern Large Language Models (LLMs) support extremely large\ncontexts, their effectiveness in utilizing long context for code reasoning\nremains unclear. This paper investigates LLM reasoning ability over code\nsnippets within large repositories and how it relates to their recall ability.\nSpecifically, we differentiate between lexical code recall (verbatim retrieval)\nand semantic code recall (remembering what the code does). To measure semantic\nrecall, we propose SemTrace, a code reasoning technique where the impact of\nspecific statements on output is attributable and unpredictable. We also\npresent a method to quantify semantic recall sensitivity in existing\nbenchmarks. Our evaluation of state-of-the-art LLMs reveals a significant drop\nin code reasoning accuracy as a code snippet approaches the middle of the input\ncontext, particularly with techniques requiring high semantic recall like\nSemTrace. Moreover, we find that lexical recall varies by granularity, with\nmodels excelling at function retrieval but struggling with line-by-line recall.\nNotably, a disconnect exists between lexical and semantic recall, suggesting\ndifferent underlying mechanisms. Finally, our findings indicate that current\ncode reasoning benchmarks may exhibit low semantic recall sensitivity,\npotentially underestimating LLM challenges in leveraging in-context\ninformation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although modern Large Language Models (LLMs) support extremely large\ncontexts, their effectiveness in utilizing long context for code reasoning\nremains unclear. This paper investigates LLM reasoning ability over code\nsnippets within large repositories and how it relates to their recall ability.\nSpecifically, we differentiate between lexical code recall (verbatim retrieval)\nand semantic code recall (remembering what the code does). To measure semantic\nrecall, we propose SemTrace, a code reasoning technique where the impact of\nspecific statements on output is attributable and unpredictable. We also\npresent a method to quantify semantic recall sensitivity in existing\nbenchmarks. Our evaluation of state-of-the-art LLMs reveals a significant drop\nin code reasoning accuracy as a code snippet approaches the middle of the input\ncontext, particularly with techniques requiring high semantic recall like\nSemTrace. Moreover, we find that lexical recall varies by granularity, with\nmodels excelling at function retrieval but struggling with line-by-line recall.\nNotably, a disconnect exists between lexical and semantic recall, suggesting\ndifferent underlying mechanisms. Finally, our findings indicate that current\ncode reasoning benchmarks may exhibit low semantic recall sensitivity,\npotentially underestimating LLM challenges in leveraging in-context\ninformation."
                },
                "authors": [
                    {
                        "name": "Adam Štorek"
                    },
                    {
                        "name": "Mukur Gupta"
                    },
                    {
                        "name": "Samira Hajizadeh"
                    },
                    {
                        "name": "Prashast Srivastava"
                    },
                    {
                        "name": "Suman Jana"
                    }
                ],
                "author_detail": {
                    "name": "Suman Jana"
                },
                "author": "Suman Jana",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13353v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13353v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14268v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14268v3",
                "updated": "2025-05-19T16:54:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    54,
                    52,
                    0,
                    139,
                    0
                ],
                "published": "2025-04-19T11:35:03Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    11,
                    35,
                    3,
                    5,
                    109,
                    0
                ],
                "title": "Mixed-Precision Conjugate Gradient Solvers with RL-Driven Precision\n  Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixed-Precision Conjugate Gradient Solvers with RL-Driven Precision\n  Tuning"
                },
                "summary": "This paper presents a novel reinforcement learning (RL) framework for\ndynamically optimizing numerical precision in the preconditioned conjugate\ngradient (CG) method. By modeling precision selection as a Markov Decision\nProcess (MDP), we employ Q-learning to adaptively assign precision levels to\nkey operations, striking an optimal balance between computational efficiency\nand numerical accuracy, while ensuring stability through double-precision\nscalar computations and residual computing. In practice, the algorithm is\ntrained on a set of data and subsequently performs inference for precision\nselection on out-of-sample data, without requiring re-analysis or retraining\nfor new datasets. This enables the method to adapt seamlessly to new problem\ninstances without the computational overhead of recalibration. Our results\ndemonstrate the effectiveness of RL in enhancing solver's performance, marking\nthe first application of RL to mixed-precision numerical methods. The findings\nhighlight the approach's practical advantages, robustness, and scalability,\nproviding valuable insights into its integration with iterative solvers and\npaving the way for AI-driven advancements in scientific computing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a novel reinforcement learning (RL) framework for\ndynamically optimizing numerical precision in the preconditioned conjugate\ngradient (CG) method. By modeling precision selection as a Markov Decision\nProcess (MDP), we employ Q-learning to adaptively assign precision levels to\nkey operations, striking an optimal balance between computational efficiency\nand numerical accuracy, while ensuring stability through double-precision\nscalar computations and residual computing. In practice, the algorithm is\ntrained on a set of data and subsequently performs inference for precision\nselection on out-of-sample data, without requiring re-analysis or retraining\nfor new datasets. This enables the method to adapt seamlessly to new problem\ninstances without the computational overhead of recalibration. Our results\ndemonstrate the effectiveness of RL in enhancing solver's performance, marking\nthe first application of RL to mixed-precision numerical methods. The findings\nhighlight the approach's practical advantages, robustness, and scalability,\nproviding valuable insights into its integration with iterative solvers and\npaving the way for AI-driven advancements in scientific computing."
                },
                "authors": [
                    {
                        "name": "Xinye Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xinye Chen"
                },
                "author": "Xinye Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14268v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14268v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13348v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13348v1",
                "updated": "2025-05-19T16:51:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    51,
                    12,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T16:51:12Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    51,
                    12,
                    0,
                    139,
                    0
                ],
                "title": "Investigating the Vulnerability of LLM-as-a-Judge Architectures to\n  Prompt-Injection Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating the Vulnerability of LLM-as-a-Judge Architectures to\n  Prompt-Injection Attacks"
                },
                "summary": "Large Language Models (LLMs) are increasingly employed as evaluators\n(LLM-as-a-Judge) for assessing the quality of machine-generated text. This\nparadigm offers scalability and cost-effectiveness compared to human\nannotation. However, the reliability and security of such systems, particularly\ntheir robustness against adversarial manipulations, remain critical concerns.\nThis paper investigates the vulnerability of LLM-as-a-Judge architectures to\nprompt-injection attacks, where malicious inputs are designed to compromise the\njudge's decision-making process. We formalize two primary attack strategies:\nComparative Undermining Attack (CUA), which directly targets the final decision\noutput, and Justification Manipulation Attack (JMA), which aims to alter the\nmodel's generated reasoning. Using the Greedy Coordinate Gradient (GCG)\noptimization method, we craft adversarial suffixes appended to one of the\nresponses being compared. Experiments conducted on the MT-Bench Human Judgments\ndataset with open-source instruction-tuned LLMs (Qwen2.5-3B-Instruct and\nFalcon3-3B-Instruct) demonstrate significant susceptibility. The CUA achieves\nan Attack Success Rate (ASR) exceeding 30\\%, while JMA also shows notable\neffectiveness. These findings highlight substantial vulnerabilities in current\nLLM-as-a-Judge systems, underscoring the need for robust defense mechanisms and\nfurther research into adversarial evaluation and trustworthiness in LLM-based\nassessment frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly employed as evaluators\n(LLM-as-a-Judge) for assessing the quality of machine-generated text. This\nparadigm offers scalability and cost-effectiveness compared to human\nannotation. However, the reliability and security of such systems, particularly\ntheir robustness against adversarial manipulations, remain critical concerns.\nThis paper investigates the vulnerability of LLM-as-a-Judge architectures to\nprompt-injection attacks, where malicious inputs are designed to compromise the\njudge's decision-making process. We formalize two primary attack strategies:\nComparative Undermining Attack (CUA), which directly targets the final decision\noutput, and Justification Manipulation Attack (JMA), which aims to alter the\nmodel's generated reasoning. Using the Greedy Coordinate Gradient (GCG)\noptimization method, we craft adversarial suffixes appended to one of the\nresponses being compared. Experiments conducted on the MT-Bench Human Judgments\ndataset with open-source instruction-tuned LLMs (Qwen2.5-3B-Instruct and\nFalcon3-3B-Instruct) demonstrate significant susceptibility. The CUA achieves\nan Attack Success Rate (ASR) exceeding 30\\%, while JMA also shows notable\neffectiveness. These findings highlight substantial vulnerabilities in current\nLLM-as-a-Judge systems, underscoring the need for robust defense mechanisms and\nfurther research into adversarial evaluation and trustworthiness in LLM-based\nassessment frameworks."
                },
                "authors": [
                    {
                        "name": "Narek Maloyan"
                    },
                    {
                        "name": "Bislan Ashinov"
                    },
                    {
                        "name": "Dmitry Namiot"
                    }
                ],
                "author_detail": {
                    "name": "Dmitry Namiot"
                },
                "author": "Dmitry Namiot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13348v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13348v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13346v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13346v1",
                "updated": "2025-05-19T16:50:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    50,
                    35,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T16:50:35Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    50,
                    35,
                    0,
                    139,
                    0
                ],
                "title": "J4R: Learning to Judge with Equivalent Initial State Group Relative\n  Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "J4R: Learning to Judge with Equivalent Initial State Group Relative\n  Preference Optimization"
                },
                "summary": "To keep pace with the increasing pace of large language models (LLM)\ndevelopment, model output evaluation has transitioned away from time-consuming\nhuman evaluation to automatic evaluation, where LLMs themselves are tasked with\nassessing and critiquing other model outputs. LLM-as-judge models are a class\nof generative evaluators that excel in evaluating relatively simple domains,\nlike chat quality, but struggle in reasoning intensive domains where model\nresponses contain more substantive and challenging content. To remedy existing\njudge shortcomings, we explore training judges with reinforcement learning\n(RL). We make three key contributions: (1) We propose the Equivalent Initial\nState Group Relative Policy Optimization (EIS-GRPO) algorithm, which allows us\nto train our judge to be robust to positional biases that arise in more complex\nevaluation settings. (2) We introduce ReasoningJudgeBench, a benchmark that\nevaluates judges in diverse reasoning settings not covered by prior work. (3)\nWe train Judge for Reasoning (J4R), a 7B judge trained with EIS-GRPO that\noutperforms GPT-4o and the next best small judge by 6.7% and 9%, matching or\nexceeding the performance of larger GRPO-trained judges on both JudgeBench and\nReasoningJudgeBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To keep pace with the increasing pace of large language models (LLM)\ndevelopment, model output evaluation has transitioned away from time-consuming\nhuman evaluation to automatic evaluation, where LLMs themselves are tasked with\nassessing and critiquing other model outputs. LLM-as-judge models are a class\nof generative evaluators that excel in evaluating relatively simple domains,\nlike chat quality, but struggle in reasoning intensive domains where model\nresponses contain more substantive and challenging content. To remedy existing\njudge shortcomings, we explore training judges with reinforcement learning\n(RL). We make three key contributions: (1) We propose the Equivalent Initial\nState Group Relative Policy Optimization (EIS-GRPO) algorithm, which allows us\nto train our judge to be robust to positional biases that arise in more complex\nevaluation settings. (2) We introduce ReasoningJudgeBench, a benchmark that\nevaluates judges in diverse reasoning settings not covered by prior work. (3)\nWe train Judge for Reasoning (J4R), a 7B judge trained with EIS-GRPO that\noutperforms GPT-4o and the next best small judge by 6.7% and 9%, matching or\nexceeding the performance of larger GRPO-trained judges on both JudgeBench and\nReasoningJudgeBench."
                },
                "authors": [
                    {
                        "name": "Austin Xu"
                    },
                    {
                        "name": "Yilun Zhou"
                    },
                    {
                        "name": "Xuan-Phi Nguyen"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Shafiq Joty"
                    }
                ],
                "author_detail": {
                    "name": "Shafiq Joty"
                },
                "author": "Shafiq Joty",
                "arxiv_comment": "25 pages, 4 figures, 6 tables. To be updated with links for\n  code/benchmark",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13346v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13346v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13345v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13345v1",
                "updated": "2025-05-19T16:50:27Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    50,
                    27,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T16:50:27Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    50,
                    27,
                    0,
                    139,
                    0
                ],
                "title": "Occult: Optimizing Collaborative Communication across Experts for\n  Accelerated Parallel MoE Training and Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Occult: Optimizing Collaborative Communication across Experts for\n  Accelerated Parallel MoE Training and Inference"
                },
                "summary": "Mixture-of-experts (MoE) architectures could achieve impressive computational\nefficiency with expert parallelism, which relies heavily on all-to-all\ncommunication across devices. Unfortunately, such communication overhead\ntypically constitutes a significant portion of the total runtime, hampering the\nscalability of distributed training and inference for modern MoE models\n(consuming over $40\\%$ runtime in large-scale training). In this paper, we\nfirst define collaborative communication to illustrate this intrinsic\nlimitation, and then propose system- and algorithm-level innovations to reduce\ncommunication costs. Specifically, given a pair of experts co-activated by one\ntoken, we call them \"collaborated\", which comprises $2$ cases as intra- and\ninter-collaboration, depending on whether they are kept on the same device. Our\npilot investigations reveal that augmenting the proportion of\nintra-collaboration can accelerate expert parallelism at scale. It motivates us\nto strategically optimize collaborative communication for accelerated MoE\ntraining and inference, dubbed Occult. Our designs are capable of either\ndelivering exact results with reduced communication cost or controllably\nminimizing the cost with collaboration pruning, materialized by modified\nfine-tuning. Comprehensive experiments on various MoE-LLMs demonstrate that\nOccult can be faster than popular state-of-the-art inference or training\nframeworks (more than $1.5\\times$ speed up across multiple tasks and models)\nwith comparable or superior quality compared to the standard fine-tuning. Code\nis available at\n$\\href{https://github.com/UNITES-Lab/Occult}{https://github.com/UNITES-Lab/Occult}$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-experts (MoE) architectures could achieve impressive computational\nefficiency with expert parallelism, which relies heavily on all-to-all\ncommunication across devices. Unfortunately, such communication overhead\ntypically constitutes a significant portion of the total runtime, hampering the\nscalability of distributed training and inference for modern MoE models\n(consuming over $40\\%$ runtime in large-scale training). In this paper, we\nfirst define collaborative communication to illustrate this intrinsic\nlimitation, and then propose system- and algorithm-level innovations to reduce\ncommunication costs. Specifically, given a pair of experts co-activated by one\ntoken, we call them \"collaborated\", which comprises $2$ cases as intra- and\ninter-collaboration, depending on whether they are kept on the same device. Our\npilot investigations reveal that augmenting the proportion of\nintra-collaboration can accelerate expert parallelism at scale. It motivates us\nto strategically optimize collaborative communication for accelerated MoE\ntraining and inference, dubbed Occult. Our designs are capable of either\ndelivering exact results with reduced communication cost or controllably\nminimizing the cost with collaboration pruning, materialized by modified\nfine-tuning. Comprehensive experiments on various MoE-LLMs demonstrate that\nOccult can be faster than popular state-of-the-art inference or training\nframeworks (more than $1.5\\times$ speed up across multiple tasks and models)\nwith comparable or superior quality compared to the standard fine-tuning. Code\nis available at\n$\\href{https://github.com/UNITES-Lab/Occult}{https://github.com/UNITES-Lab/Occult}$."
                },
                "authors": [
                    {
                        "name": "Shuqing Luo"
                    },
                    {
                        "name": "Pingzhi Li"
                    },
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Hanrui Wang"
                    },
                    {
                        "name": "Yang"
                    },
                    {
                        "name": "Zhao"
                    },
                    {
                        "name": "Yu"
                    },
                    {
                        "name": "Cao"
                    },
                    {
                        "name": "Yu Cheng"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "arxiv_affiliation": "Kevin",
                "author": "Tianlong Chen",
                "arxiv_comment": "Accepted by ICML2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13345v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13345v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13338v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13338v1",
                "updated": "2025-05-19T16:47:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    47,
                    46,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T16:47:46Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    47,
                    46,
                    0,
                    139,
                    0
                ],
                "title": "Contextual Paralinguistic Data Creation for Multi-Modal Speech-LLM: Data\n  Condensation and Spoken QA Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contextual Paralinguistic Data Creation for Multi-Modal Speech-LLM: Data\n  Condensation and Spoken QA Generation"
                },
                "summary": "Current speech-LLMs exhibit limited capability in contextual reasoning\nalongside paralinguistic understanding, primarily due to the lack of\nQuestion-Answer (QA) datasets that cover both aspects. We propose a novel\nframework for dataset generation from in-the-wild speech data, that integrates\ncontextual reasoning with paralinguistic information. It consists of a pseudo\nparalinguistic label-based data condensation of in-the-wild speech and\nLLM-based Contextual Paralinguistic QA (CPQA) generation. The effectiveness is\nvalidated by a strong correlation in evaluations of the Qwen2-Audio-7B-Instruct\nmodel on a dataset created by our framework and human-generated CPQA dataset.\nThe results also reveal the speech-LLM's limitations in handling empathetic\nreasoning tasks, highlighting the need for such datasets and more robust\nmodels. The proposed framework is first of its kind and has potential in\ntraining more robust speech-LLMs with paralinguistic reasoning capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current speech-LLMs exhibit limited capability in contextual reasoning\nalongside paralinguistic understanding, primarily due to the lack of\nQuestion-Answer (QA) datasets that cover both aspects. We propose a novel\nframework for dataset generation from in-the-wild speech data, that integrates\ncontextual reasoning with paralinguistic information. It consists of a pseudo\nparalinguistic label-based data condensation of in-the-wild speech and\nLLM-based Contextual Paralinguistic QA (CPQA) generation. The effectiveness is\nvalidated by a strong correlation in evaluations of the Qwen2-Audio-7B-Instruct\nmodel on a dataset created by our framework and human-generated CPQA dataset.\nThe results also reveal the speech-LLM's limitations in handling empathetic\nreasoning tasks, highlighting the need for such datasets and more robust\nmodels. The proposed framework is first of its kind and has potential in\ntraining more robust speech-LLMs with paralinguistic reasoning capabilities."
                },
                "authors": [
                    {
                        "name": "Qiongqiong Wang"
                    },
                    {
                        "name": "Hardik B. Sailor"
                    },
                    {
                        "name": "Tianchi Liu"
                    },
                    {
                        "name": "Ai Ti Aw"
                    }
                ],
                "author_detail": {
                    "name": "Ai Ti Aw"
                },
                "author": "Ai Ti Aw",
                "arxiv_comment": "Accepted at Interspeech 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13338v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13338v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08140v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08140v2",
                "updated": "2025-05-19T16:46:27Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    46,
                    27,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-13T00:25:23Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    0,
                    25,
                    23,
                    1,
                    133,
                    0
                ],
                "title": "Lost in Transmission: When and Why LLMs Fail to Reason Globally",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lost in Transmission: When and Why LLMs Fail to Reason Globally"
                },
                "summary": "Despite their many successes, transformer-based large language models (LLMs)\ncontinue to struggle with tasks that require complex reasoning over large parts\nof their input. We argue that these failures arise due to capacity limits on\nthe accurate flow of information within LLMs. To formalize this issue, we\nintroduce the bounded attention prefix oracle (BAPO) model, a new computational\nframework that models bandwidth constraints on attention heads, the mechanism\nfor internal communication in LLMs. We show that several important reasoning\nproblems like graph reachability require high communication bandwidth for BAPOs\nto solve; we call these problems BAPO-hard. Our experiments corroborate our\ntheoretical predictions: GPT-4o, Claude, and Gemini succeed on BAPO-easy tasks\nand fail even on relatively small BAPO-hard tasks. BAPOs also reveal another\nbenefit of chain of thought (CoT): we prove that breaking down a task using CoT\ncan turn any BAPO-hard problem into a BAPO-easy one. Our results offer\nprincipled explanations for key LLM failures and suggest directions for\narchitectures and inference methods that mitigate bandwidth limits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their many successes, transformer-based large language models (LLMs)\ncontinue to struggle with tasks that require complex reasoning over large parts\nof their input. We argue that these failures arise due to capacity limits on\nthe accurate flow of information within LLMs. To formalize this issue, we\nintroduce the bounded attention prefix oracle (BAPO) model, a new computational\nframework that models bandwidth constraints on attention heads, the mechanism\nfor internal communication in LLMs. We show that several important reasoning\nproblems like graph reachability require high communication bandwidth for BAPOs\nto solve; we call these problems BAPO-hard. Our experiments corroborate our\ntheoretical predictions: GPT-4o, Claude, and Gemini succeed on BAPO-easy tasks\nand fail even on relatively small BAPO-hard tasks. BAPOs also reveal another\nbenefit of chain of thought (CoT): we prove that breaking down a task using CoT\ncan turn any BAPO-hard problem into a BAPO-easy one. Our results offer\nprincipled explanations for key LLM failures and suggest directions for\narchitectures and inference methods that mitigate bandwidth limits."
                },
                "authors": [
                    {
                        "name": "Tobias Schnabel"
                    },
                    {
                        "name": "Kiran Tomlinson"
                    },
                    {
                        "name": "Adith Swaminathan"
                    },
                    {
                        "name": "Jennifer Neville"
                    }
                ],
                "author_detail": {
                    "name": "Jennifer Neville"
                },
                "author": "Jennifer Neville",
                "arxiv_comment": "28 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08140v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08140v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13328v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13328v1",
                "updated": "2025-05-19T16:36:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    36,
                    13,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T16:36:13Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    36,
                    13,
                    0,
                    139,
                    0
                ],
                "title": "Rethinking Stateful Tool Use in Multi-Turn Dialogues: Benchmarks and\n  Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Stateful Tool Use in Multi-Turn Dialogues: Benchmarks and\n  Challenges"
                },
                "summary": "Existing benchmarks that assess Language Models (LMs) as Language Agents\n(LAs) for tool use primarily focus on stateless, single-turn interactions or\npartial evaluations, such as tool selection in a single turn, overlooking the\ninherent stateful nature of interactions in multi-turn applications. To fulfill\nthis gap, we propose \\texttt{DialogTool}, a multi-turn dialogue dataset with\nstateful tool interactions considering the whole life cycle of tool use, across\nsix key tasks in three stages: 1) \\textit{tool creation}; 2) \\textit{tool\nutilization}: tool awareness, tool selection, tool execution; and 3)\n\\textit{role-consistent response}: response generation and role play.\nFurthermore, we build \\texttt{VirtualMobile} -- an embodied virtual mobile\nevaluation environment to simulate API calls and assess the robustness of the\ncreated APIs\\footnote{We will use tools and APIs alternatively, there are no\nsignificant differences between them in this paper.}. Taking advantage of these\nartifacts, we conduct comprehensive evaluation on 13 distinct open- and\nclosed-source LLMs and provide detailed analysis at each stage, revealing that\nthe existing state-of-the-art LLMs still cannot perform well to use tools over\nlong horizons.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing benchmarks that assess Language Models (LMs) as Language Agents\n(LAs) for tool use primarily focus on stateless, single-turn interactions or\npartial evaluations, such as tool selection in a single turn, overlooking the\ninherent stateful nature of interactions in multi-turn applications. To fulfill\nthis gap, we propose \\texttt{DialogTool}, a multi-turn dialogue dataset with\nstateful tool interactions considering the whole life cycle of tool use, across\nsix key tasks in three stages: 1) \\textit{tool creation}; 2) \\textit{tool\nutilization}: tool awareness, tool selection, tool execution; and 3)\n\\textit{role-consistent response}: response generation and role play.\nFurthermore, we build \\texttt{VirtualMobile} -- an embodied virtual mobile\nevaluation environment to simulate API calls and assess the robustness of the\ncreated APIs\\footnote{We will use tools and APIs alternatively, there are no\nsignificant differences between them in this paper.}. Taking advantage of these\nartifacts, we conduct comprehensive evaluation on 13 distinct open- and\nclosed-source LLMs and provide detailed analysis at each stage, revealing that\nthe existing state-of-the-art LLMs still cannot perform well to use tools over\nlong horizons."
                },
                "authors": [
                    {
                        "name": "Hongru Wang"
                    },
                    {
                        "name": "Wenyu Huang"
                    },
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Yuanhao Xi"
                    },
                    {
                        "name": "Jianqiao Lu"
                    },
                    {
                        "name": "Huan Zhang"
                    },
                    {
                        "name": "Nan Hu"
                    },
                    {
                        "name": "Zeming Liu"
                    },
                    {
                        "name": "Jeff Z. Pan"
                    },
                    {
                        "name": "Kam-Fai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kam-Fai Wong"
                },
                "author": "Kam-Fai Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13328v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13328v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13326v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13326v1",
                "updated": "2025-05-19T16:34:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    34,
                    56,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T16:34:56Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    34,
                    56,
                    0,
                    139,
                    0
                ],
                "title": "Thinking Short and Right Over Thinking Long: Serving LLM Reasoning\n  Efficiently and Accurately",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thinking Short and Right Over Thinking Long: Serving LLM Reasoning\n  Efficiently and Accurately"
                },
                "summary": "Recent advances in test-time scaling suggest that Large Language Models\n(LLMs) can gain better capabilities by generating Chain-of-Thought reasoning\n(analogous to human thinking) to respond a given request, and meanwhile\nexploring more reasoning branches (i.e., generating multiple responses and\nensembling them) can improve the final output quality. However, when\nincorporating the two scaling dimensions, we find that the system efficiency is\ndampened significantly for two reasons. Firstly, the time cost to generate the\nfinal output increases substantially as many reasoning branches would be\ntrapped in the over-thinking dilemma, producing excessively long responses.\nSecondly, generating multiple reasoning branches for each request increases\nmemory consumption, which is unsuitable for LLM serving since we can only batch\na limited number of requests to process simultaneously. To address this, we\npresent SART, a serving framework for efficient and accurate LLM reasoning. The\nessential idea is to manage the thinking to be short and right, rather than\nlong. For one thing, we devise a redundant sampling with early stopping\napproach based on empirical observations and theoretic analysis, which\nincreases the likelihood of obtaining short-thinking responses when sampling\nreasoning branches. For another, we propose to dynamically prune low-quality\nbranches so that only right-thinking branches are maintained, reducing the\nmemory consumption and allowing us to batch more requests. Experimental results\ndemonstrate that SART not only improves the accuracy of LLM reasoning but also\nenhances the serving efficiency, outperforming existing methods by up to 28.2\ntimes and on average 15.7 times in terms of efficiency when achieving the same\nlevel of accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in test-time scaling suggest that Large Language Models\n(LLMs) can gain better capabilities by generating Chain-of-Thought reasoning\n(analogous to human thinking) to respond a given request, and meanwhile\nexploring more reasoning branches (i.e., generating multiple responses and\nensembling them) can improve the final output quality. However, when\nincorporating the two scaling dimensions, we find that the system efficiency is\ndampened significantly for two reasons. Firstly, the time cost to generate the\nfinal output increases substantially as many reasoning branches would be\ntrapped in the over-thinking dilemma, producing excessively long responses.\nSecondly, generating multiple reasoning branches for each request increases\nmemory consumption, which is unsuitable for LLM serving since we can only batch\na limited number of requests to process simultaneously. To address this, we\npresent SART, a serving framework for efficient and accurate LLM reasoning. The\nessential idea is to manage the thinking to be short and right, rather than\nlong. For one thing, we devise a redundant sampling with early stopping\napproach based on empirical observations and theoretic analysis, which\nincreases the likelihood of obtaining short-thinking responses when sampling\nreasoning branches. For another, we propose to dynamically prune low-quality\nbranches so that only right-thinking branches are maintained, reducing the\nmemory consumption and allowing us to batch more requests. Experimental results\ndemonstrate that SART not only improves the accuracy of LLM reasoning but also\nenhances the serving efficiency, outperforming existing methods by up to 28.2\ntimes and on average 15.7 times in terms of efficiency when achieving the same\nlevel of accuracy."
                },
                "authors": [
                    {
                        "name": "Yuhang Wang"
                    },
                    {
                        "name": "Youhe Jiang"
                    },
                    {
                        "name": "Bin Cui"
                    },
                    {
                        "name": "Fangcheng Fu"
                    }
                ],
                "author_detail": {
                    "name": "Fangcheng Fu"
                },
                "author": "Fangcheng Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13326v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13326v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13324v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13324v1",
                "updated": "2025-05-19T16:34:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    34,
                    36,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T16:34:36Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    34,
                    36,
                    0,
                    139,
                    0
                ],
                "title": "From What Ifs to Insights: Counterfactuals in Causal Inference vs.\n  Explainable AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From What Ifs to Insights: Counterfactuals in Causal Inference vs.\n  Explainable AI"
                },
                "summary": "Counterfactuals play a pivotal role in the two distinct data science fields\nof causal inference (CI) and explainable artificial intelligence (XAI). While\nthe core idea behind counterfactuals remains the same in both fields--the\nexamination of what would have happened under different circumstances--there\nare key differences in how they are used and interpreted. We introduce a formal\ndefinition that encompasses the multi-faceted concept of the counterfactual in\nCI and XAI. We then discuss how counterfactuals are used, evaluated, generated,\nand operationalized in CI vs. XAI, highlighting conceptual and practical\ndifferences. By comparing and contrasting the two, we hope to identify\nopportunities for cross-fertilization across CI and XAI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Counterfactuals play a pivotal role in the two distinct data science fields\nof causal inference (CI) and explainable artificial intelligence (XAI). While\nthe core idea behind counterfactuals remains the same in both fields--the\nexamination of what would have happened under different circumstances--there\nare key differences in how they are used and interpreted. We introduce a formal\ndefinition that encompasses the multi-faceted concept of the counterfactual in\nCI and XAI. We then discuss how counterfactuals are used, evaluated, generated,\nand operationalized in CI vs. XAI, highlighting conceptual and practical\ndifferences. By comparing and contrasting the two, we hope to identify\nopportunities for cross-fertilization across CI and XAI."
                },
                "authors": [
                    {
                        "name": "Galit Shmueli"
                    },
                    {
                        "name": "David Martens"
                    },
                    {
                        "name": "Jaewon Yoo"
                    },
                    {
                        "name": "Travis Greene"
                    }
                ],
                "author_detail": {
                    "name": "Travis Greene"
                },
                "author": "Travis Greene",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13324v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13324v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01893v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01893v2",
                "updated": "2025-05-19T16:32:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    32,
                    11,
                    0,
                    139,
                    0
                ],
                "published": "2024-09-03T13:30:00Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    13,
                    30,
                    0,
                    1,
                    247,
                    0
                ],
                "title": "What are the Essential Factors in Crafting Effective Long Context\n  Multi-Hop Instruction Datasets? Insights and Best Practices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What are the Essential Factors in Crafting Effective Long Context\n  Multi-Hop Instruction Datasets? Insights and Best Practices"
                },
                "summary": "Recent advancements in large language models (LLMs) with extended context\nwindows have significantly improved tasks such as information extraction,\nquestion answering, and complex planning scenarios. In order to achieve success\nin long context tasks, a large amount of work has been done to enhance the long\ncontext capabilities of the model through synthetic data. Existing methods\ntypically utilize the Self-Instruct framework to generate instruction tuning\ndata for better long context capability improvement. However, our preliminary\nexperiments indicate that less than 35% of generated samples are multi-hop, and\nmore than 40% exhibit poor quality, limiting comprehensive understanding and\nfurther research. To improve the quality of synthetic data, we propose the\nMulti-agent Interactive Multi-hop Generation (MIMG) framework, incorporating a\nQuality Verification Agent, a Single-hop Question Generation Agent, a Multiple\nQuestion Sampling Strategy, and a Multi-hop Question Merger Agent. This\nframework improves the data quality, with the proportion of high-quality,\nmulti-hop, and diverse data exceeding 85%. Furthermore, we systematically\ninvestigate strategies for document selection, question merging, and validation\ntechniques through extensive experiments across various models. Our findings\nshow that our synthetic high-quality long-context instruction data\nsignificantly enhances model performance, even surpassing models trained on\nlarger amounts of human-annotated data. Our code is available at:\nhttps://github.com/WowCZ/LongMIT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) with extended context\nwindows have significantly improved tasks such as information extraction,\nquestion answering, and complex planning scenarios. In order to achieve success\nin long context tasks, a large amount of work has been done to enhance the long\ncontext capabilities of the model through synthetic data. Existing methods\ntypically utilize the Self-Instruct framework to generate instruction tuning\ndata for better long context capability improvement. However, our preliminary\nexperiments indicate that less than 35% of generated samples are multi-hop, and\nmore than 40% exhibit poor quality, limiting comprehensive understanding and\nfurther research. To improve the quality of synthetic data, we propose the\nMulti-agent Interactive Multi-hop Generation (MIMG) framework, incorporating a\nQuality Verification Agent, a Single-hop Question Generation Agent, a Multiple\nQuestion Sampling Strategy, and a Multi-hop Question Merger Agent. This\nframework improves the data quality, with the proportion of high-quality,\nmulti-hop, and diverse data exceeding 85%. Furthermore, we systematically\ninvestigate strategies for document selection, question merging, and validation\ntechniques through extensive experiments across various models. Our findings\nshow that our synthetic high-quality long-context instruction data\nsignificantly enhances model performance, even surpassing models trained on\nlarger amounts of human-annotated data. Our code is available at:\nhttps://github.com/WowCZ/LongMIT."
                },
                "authors": [
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Qiguang Chen"
                    },
                    {
                        "name": "Libo Qin"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Haijun Lv"
                    },
                    {
                        "name": "Yicheng Zou"
                    },
                    {
                        "name": "Wanxiang Che"
                    },
                    {
                        "name": "Hang Yan"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Dahua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Dahua Lin"
                },
                "author": "Dahua Lin",
                "arxiv_comment": "ACL 2025 Camera Ready. Code is available at:\n  https://github.com/WowCZ/LongMIT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01893v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01893v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13319v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13319v2",
                "updated": "2025-05-20T04:32:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    4,
                    32,
                    38,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-19T16:30:27Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    30,
                    27,
                    0,
                    139,
                    0
                ],
                "title": "SVAFD: A Secure and Verifiable Co-Aggregation Protocol for Federated\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SVAFD: A Secure and Verifiable Co-Aggregation Protocol for Federated\n  Distillation"
                },
                "summary": "Secure Aggregation (SA) is an indispensable component of Federated Learning\n(FL) that concentrates on privacy preservation while allowing for robust\naggregation. However, most SA designs rely heavily on the unrealistic\nassumption of homogeneous model architectures. Federated Distillation (FD),\nwhich aggregates locally computed logits instead of model parameters,\nintroduces a promising alternative for cooperative training in heterogeneous\nmodel settings. Nevertheless, we recognize two major challenges in implementing\nSA for FD. (i) Prior SA designs encourage a dominant server, who is solely\nresponsible for collecting, aggregating and distributing. Such central\nauthority facilitates server to forge aggregation proofs or collude to bypass\nthe claimed security guarantees; (ii) Existing SA, tailored for FL models,\noverlook the intrinsic properties of logits, making them unsuitable for FD.\n  To address these challenges, we propose SVAFD, the first SA protocol that is\nspecifically designed for FD. At a high level, SVAFD incorporates two\ninnovations: (i) a multilateral co-aggregation method tha redefines the\nresponsibilities of clients and server. Clients autonomously evaluate and\naggregate logits shares locally with a lightweight coding scheme, while the\nserver handles ciphertext decoding and performs the task of generating\nverification proofs; (ii) a quality-aware knowledge filtration method that\nfacilitates biased logits exclusion against poisoning attacks. Moreover, SVAFD\nis resilient to stragglers and colluding clients, making it well-suited for\ndynamic networks in real-world applications. We have implemented the SVAFD\nprototype over four emerging FD architectures and evaluated it against\npoisoning and inference attacks. Results demonstrate that SVAFD improves model\naccuracy, making it a significant step forward in secure and verifiable\naggregation for heterogeneous FL systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secure Aggregation (SA) is an indispensable component of Federated Learning\n(FL) that concentrates on privacy preservation while allowing for robust\naggregation. However, most SA designs rely heavily on the unrealistic\nassumption of homogeneous model architectures. Federated Distillation (FD),\nwhich aggregates locally computed logits instead of model parameters,\nintroduces a promising alternative for cooperative training in heterogeneous\nmodel settings. Nevertheless, we recognize two major challenges in implementing\nSA for FD. (i) Prior SA designs encourage a dominant server, who is solely\nresponsible for collecting, aggregating and distributing. Such central\nauthority facilitates server to forge aggregation proofs or collude to bypass\nthe claimed security guarantees; (ii) Existing SA, tailored for FL models,\noverlook the intrinsic properties of logits, making them unsuitable for FD.\n  To address these challenges, we propose SVAFD, the first SA protocol that is\nspecifically designed for FD. At a high level, SVAFD incorporates two\ninnovations: (i) a multilateral co-aggregation method tha redefines the\nresponsibilities of clients and server. Clients autonomously evaluate and\naggregate logits shares locally with a lightweight coding scheme, while the\nserver handles ciphertext decoding and performs the task of generating\nverification proofs; (ii) a quality-aware knowledge filtration method that\nfacilitates biased logits exclusion against poisoning attacks. Moreover, SVAFD\nis resilient to stragglers and colluding clients, making it well-suited for\ndynamic networks in real-world applications. We have implemented the SVAFD\nprototype over four emerging FD architectures and evaluated it against\npoisoning and inference attacks. Results demonstrate that SVAFD improves model\naccuracy, making it a significant step forward in secure and verifiable\naggregation for heterogeneous FL systems."
                },
                "authors": [
                    {
                        "name": "Tian Wen"
                    },
                    {
                        "name": "Sheng Sun"
                    },
                    {
                        "name": "Yuwei Wang"
                    },
                    {
                        "name": "Peiyan Chen"
                    },
                    {
                        "name": "Zhiyuan Wu"
                    },
                    {
                        "name": "Min Liu"
                    },
                    {
                        "name": "Bo Gao"
                    }
                ],
                "author_detail": {
                    "name": "Bo Gao"
                },
                "author": "Bo Gao",
                "arxiv_comment": "15 pages, 16 figures, 3 tables, 27 equations",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13319v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13319v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13315v1",
                "updated": "2025-05-19T16:29:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    29,
                    7,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T16:29:07Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    29,
                    7,
                    0,
                    139,
                    0
                ],
                "title": "KHRONOS: a Kernel-Based Neural Architecture for Rapid,\n  Resource-Efficient Scientific Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KHRONOS: a Kernel-Based Neural Architecture for Rapid,\n  Resource-Efficient Scientific Computation"
                },
                "summary": "Contemporary models of high dimensional physical systems are constrained by\nthe curse of dimensionality and a reliance on dense data. We introduce KHRONOS\n(Kernel Expansion Hierarchy for Reduced Order, Neural Optimized Surrogates), an\nAI framework for model based, model free and model inversion tasks. KHRONOS\nconstructs continuously differentiable target fields with a hierarchical\ncomposition of per-dimension kernel expansions, which are tensorized into modes\nand then superposed. We evaluate KHRONOS on a canonical 2D, Poisson equation\nbenchmark: across 16 to 512 degrees of freedom (DoFs), it obtained L2 square\nerrors of 5e-4 down to 6e-10. This represents a 100 time gain over Kolmogorov\nArnold Networks (which itself reports a 100 times improvement on MLPs/PINNs\nwith 100 times fewer parameters) when controlling for the number of parameters.\nThis also represents a 1e4 times improvement in L2 square error compared to\nstandard linear FEM at comparable DoFs. Inference complexity is dominated by\ninner products, yielding sub-millisecond full-field predictions that scale to\nan arbitrary resolution. For inverse problems, KHRONOS facilitates rapid,\niterative level set recovery in only a few forward evaluations, with\nsub-microsecond per sample latency. KHRONOS scalability, expressivity, and\ninterpretability open new avenues in constrained edge computing, online\ncontrol, computer vision, and beyond.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary models of high dimensional physical systems are constrained by\nthe curse of dimensionality and a reliance on dense data. We introduce KHRONOS\n(Kernel Expansion Hierarchy for Reduced Order, Neural Optimized Surrogates), an\nAI framework for model based, model free and model inversion tasks. KHRONOS\nconstructs continuously differentiable target fields with a hierarchical\ncomposition of per-dimension kernel expansions, which are tensorized into modes\nand then superposed. We evaluate KHRONOS on a canonical 2D, Poisson equation\nbenchmark: across 16 to 512 degrees of freedom (DoFs), it obtained L2 square\nerrors of 5e-4 down to 6e-10. This represents a 100 time gain over Kolmogorov\nArnold Networks (which itself reports a 100 times improvement on MLPs/PINNs\nwith 100 times fewer parameters) when controlling for the number of parameters.\nThis also represents a 1e4 times improvement in L2 square error compared to\nstandard linear FEM at comparable DoFs. Inference complexity is dominated by\ninner products, yielding sub-millisecond full-field predictions that scale to\nan arbitrary resolution. For inverse problems, KHRONOS facilitates rapid,\niterative level set recovery in only a few forward evaluations, with\nsub-microsecond per sample latency. KHRONOS scalability, expressivity, and\ninterpretability open new avenues in constrained edge computing, online\ncontrol, computer vision, and beyond."
                },
                "authors": [
                    {
                        "name": "Reza T. Batley"
                    },
                    {
                        "name": "Sourav Saha"
                    }
                ],
                "author_detail": {
                    "name": "Sourav Saha"
                },
                "author": "Sourav Saha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13312v1",
                "updated": "2025-05-19T16:26:58Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    26,
                    58,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T16:26:58Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    26,
                    58,
                    0,
                    139,
                    0
                ],
                "title": "GUARD: Generation-time LLM Unlearning via Adaptive Restriction and\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GUARD: Generation-time LLM Unlearning via Adaptive Restriction and\n  Detection"
                },
                "summary": "Large Language Models (LLMs) have demonstrated strong capabilities in\nmemorizing vast amounts of knowledge across diverse domains. However, the\nability to selectively forget specific knowledge is critical for ensuring the\nsafety and compliance of deployed models. Existing unlearning efforts typically\nfine-tune the model with resources such as forget data, retain data, and a\ncalibration model. These additional gradient steps blur the decision boundary\nbetween forget and retain knowledge, making unlearning often at the expense of\noverall performance. To avoid the negative impact of fine-tuning, it would be\nbetter to unlearn solely at inference time by safely guarding the model against\ngenerating responses related to the forget target, without destroying the\nfluency of text generation. In this work, we propose Generation-time Unlearning\nvia Adaptive Restriction and Detection (GUARD), a framework that enables\ndynamic unlearning during LLM generation. Specifically, we first employ a\nprompt classifier to detect unlearning targets and extract the corresponding\nforbidden token. We then dynamically penalize and filter candidate tokens\nduring generation using a combination of token matching and semantic matching,\neffectively preventing the model from leaking the forgotten content.\nExperimental results on copyright content unlearning tasks over the Harry\nPotter dataset and the MUSE benchmark, as well as entity unlearning tasks on\nthe TOFU dataset, demonstrate that GUARD achieves strong forget quality across\nvarious tasks while causing almost no degradation to the LLM's general\ncapabilities, striking an excellent trade-off between forgetting and utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated strong capabilities in\nmemorizing vast amounts of knowledge across diverse domains. However, the\nability to selectively forget specific knowledge is critical for ensuring the\nsafety and compliance of deployed models. Existing unlearning efforts typically\nfine-tune the model with resources such as forget data, retain data, and a\ncalibration model. These additional gradient steps blur the decision boundary\nbetween forget and retain knowledge, making unlearning often at the expense of\noverall performance. To avoid the negative impact of fine-tuning, it would be\nbetter to unlearn solely at inference time by safely guarding the model against\ngenerating responses related to the forget target, without destroying the\nfluency of text generation. In this work, we propose Generation-time Unlearning\nvia Adaptive Restriction and Detection (GUARD), a framework that enables\ndynamic unlearning during LLM generation. Specifically, we first employ a\nprompt classifier to detect unlearning targets and extract the corresponding\nforbidden token. We then dynamically penalize and filter candidate tokens\nduring generation using a combination of token matching and semantic matching,\neffectively preventing the model from leaking the forgotten content.\nExperimental results on copyright content unlearning tasks over the Harry\nPotter dataset and the MUSE benchmark, as well as entity unlearning tasks on\nthe TOFU dataset, demonstrate that GUARD achieves strong forget quality across\nvarious tasks while causing almost no degradation to the LLM's general\ncapabilities, striking an excellent trade-off between forgetting and utility."
                },
                "authors": [
                    {
                        "name": "Zhijie Deng"
                    },
                    {
                        "name": "Chris Yuhao Liu"
                    },
                    {
                        "name": "Zirui Pang"
                    },
                    {
                        "name": "Xinlei He"
                    },
                    {
                        "name": "Lei Feng"
                    },
                    {
                        "name": "Qi Xuan"
                    },
                    {
                        "name": "Zhaowei Zhu"
                    },
                    {
                        "name": "Jiaheng Wei"
                    }
                ],
                "author_detail": {
                    "name": "Jiaheng Wei"
                },
                "author": "Jiaheng Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13308v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13308v1",
                "updated": "2025-05-19T16:26:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    26,
                    2,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T16:26:02Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    26,
                    2,
                    0,
                    139,
                    0
                ],
                "title": "Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient\n  in Latent Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient\n  in Latent Space"
                },
                "summary": "Reasoning ability, a core component of human intelligence, continues to pose\na significant challenge for Large Language Models (LLMs) in the pursuit of AGI.\nAlthough model performance has improved under the training scaling law,\nsignificant challenges remain, particularly with respect to training\nalgorithms, such as catastrophic forgetting, and the limited availability of\nnovel training data. As an alternative, test-time scaling enhances reasoning\nperformance by increasing test-time computation without parameter updating.\nUnlike prior methods in this paradigm focused on token space, we propose\nleveraging latent space for more effective reasoning and better adherence to\nthe test-time scaling law. We introduce LatentSeek, a novel framework that\nenhances LLM reasoning through Test-Time Instance-level Adaptation (TTIA)\nwithin the model's latent space. Specifically, LatentSeek leverages policy\ngradient to iteratively update latent representations, guided by self-generated\nreward signals. LatentSeek is evaluated on a range of reasoning benchmarks,\nincluding GSM8K, MATH-500, and AIME2024, across multiple LLM architectures.\nResults show that LatentSeek consistently outperforms strong baselines, such as\nChain-of-Thought prompting and fine-tuning-based methods. Furthermore, our\nanalysis demonstrates that LatentSeek is highly efficient, typically converging\nwithin a few iterations for problems of average complexity, while also\nbenefiting from additional iterations, thereby highlighting the potential of\ntest-time scaling in the latent space. These findings position LatentSeek as a\nlightweight, scalable, and effective solution for enhancing the reasoning\ncapabilities of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning ability, a core component of human intelligence, continues to pose\na significant challenge for Large Language Models (LLMs) in the pursuit of AGI.\nAlthough model performance has improved under the training scaling law,\nsignificant challenges remain, particularly with respect to training\nalgorithms, such as catastrophic forgetting, and the limited availability of\nnovel training data. As an alternative, test-time scaling enhances reasoning\nperformance by increasing test-time computation without parameter updating.\nUnlike prior methods in this paradigm focused on token space, we propose\nleveraging latent space for more effective reasoning and better adherence to\nthe test-time scaling law. We introduce LatentSeek, a novel framework that\nenhances LLM reasoning through Test-Time Instance-level Adaptation (TTIA)\nwithin the model's latent space. Specifically, LatentSeek leverages policy\ngradient to iteratively update latent representations, guided by self-generated\nreward signals. LatentSeek is evaluated on a range of reasoning benchmarks,\nincluding GSM8K, MATH-500, and AIME2024, across multiple LLM architectures.\nResults show that LatentSeek consistently outperforms strong baselines, such as\nChain-of-Thought prompting and fine-tuning-based methods. Furthermore, our\nanalysis demonstrates that LatentSeek is highly efficient, typically converging\nwithin a few iterations for problems of average complexity, while also\nbenefiting from additional iterations, thereby highlighting the potential of\ntest-time scaling in the latent space. These findings position LatentSeek as a\nlightweight, scalable, and effective solution for enhancing the reasoning\ncapabilities of LLMs."
                },
                "authors": [
                    {
                        "name": "Hengli Li"
                    },
                    {
                        "name": "Chenxi Li"
                    },
                    {
                        "name": "Tong Wu"
                    },
                    {
                        "name": "Xuekai Zhu"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Zhaoxin Yu"
                    },
                    {
                        "name": "Eric Hanchen Jiang"
                    },
                    {
                        "name": "Song-Chun Zhu"
                    },
                    {
                        "name": "Zixia Jia"
                    },
                    {
                        "name": "Ying Nian Wu"
                    },
                    {
                        "name": "Zilong Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zilong Zheng"
                },
                "author": "Zilong Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13308v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13308v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13306v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13306v1",
                "updated": "2025-05-19T16:25:55Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    25,
                    55,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T16:25:55Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    25,
                    55,
                    0,
                    139,
                    0
                ],
                "title": "GMM-Based Comprehensive Feature Extraction and Relative Distance\n  Preservation For Few-Shot Cross-Modal Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GMM-Based Comprehensive Feature Extraction and Relative Distance\n  Preservation For Few-Shot Cross-Modal Retrieval"
                },
                "summary": "Few-shot cross-modal retrieval focuses on learning cross-modal\nrepresentations with limited training samples, enabling the model to handle\nunseen classes during inference. Unlike traditional cross-modal retrieval\ntasks, which assume that both training and testing data share the same class\ndistribution, few-shot retrieval involves data with sparse representations\nacross modalities. Existing methods often fail to adequately model the\nmulti-peak distribution of few-shot cross-modal data, resulting in two main\nbiases in the latent semantic space: intra-modal bias, where sparse samples\nfail to capture intra-class diversity, and inter-modal bias, where\nmisalignments between image and text distributions exacerbate the semantic gap.\nThese biases hinder retrieval accuracy. To address these issues, we propose a\nnovel method, GCRDP, for few-shot cross-modal retrieval. This approach\neffectively captures the complex multi-peak distribution of data using a\nGaussian Mixture Model (GMM) and incorporates a multi-positive sample\ncontrastive learning mechanism for comprehensive feature modeling.\nAdditionally, we introduce a new strategy for cross-modal semantic alignment,\nwhich constrains the relative distances between image and text feature\ndistributions, thereby improving the accuracy of cross-modal representations.\nWe validate our approach through extensive experiments on four benchmark\ndatasets, demonstrating superior performance over six state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-shot cross-modal retrieval focuses on learning cross-modal\nrepresentations with limited training samples, enabling the model to handle\nunseen classes during inference. Unlike traditional cross-modal retrieval\ntasks, which assume that both training and testing data share the same class\ndistribution, few-shot retrieval involves data with sparse representations\nacross modalities. Existing methods often fail to adequately model the\nmulti-peak distribution of few-shot cross-modal data, resulting in two main\nbiases in the latent semantic space: intra-modal bias, where sparse samples\nfail to capture intra-class diversity, and inter-modal bias, where\nmisalignments between image and text distributions exacerbate the semantic gap.\nThese biases hinder retrieval accuracy. To address these issues, we propose a\nnovel method, GCRDP, for few-shot cross-modal retrieval. This approach\neffectively captures the complex multi-peak distribution of data using a\nGaussian Mixture Model (GMM) and incorporates a multi-positive sample\ncontrastive learning mechanism for comprehensive feature modeling.\nAdditionally, we introduce a new strategy for cross-modal semantic alignment,\nwhich constrains the relative distances between image and text feature\ndistributions, thereby improving the accuracy of cross-modal representations.\nWe validate our approach through extensive experiments on four benchmark\ndatasets, demonstrating superior performance over six state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Chengsong Sun"
                    },
                    {
                        "name": "Weiping Li"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Yuankun Liu"
                    },
                    {
                        "name": "Lianlei Shan"
                    }
                ],
                "author_detail": {
                    "name": "Lianlei Shan"
                },
                "author": "Lianlei Shan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13306v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13306v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13307v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13307v1",
                "updated": "2025-05-19T16:25:55Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    25,
                    55,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T16:25:55Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    25,
                    55,
                    0,
                    139,
                    0
                ],
                "title": "RBF++: Quantifying and Optimizing Reasoning Boundaries across Measurable\n  and Unmeasurable Capabilities for Chain-of-Thought Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RBF++: Quantifying and Optimizing Reasoning Boundaries across Measurable\n  and Unmeasurable Capabilities for Chain-of-Thought Reasoning"
                },
                "summary": "Chain-of-Thought (CoT) reasoning has proven effective in enhancing large\nlanguage models (LLMs) on complex tasks, spurring research into its underlying\nmechanisms. However, two primary challenges remain for real-world applications:\n(1) the lack of quantitative metrics and actionable guidelines for evaluating\nand optimizing measurable boundaries of CoT capability, and (2) the absence of\nmethods to assess boundaries of unmeasurable CoT capability, such as multimodal\nperception. To address these gaps, we introduce the Reasoning Boundary\nFramework++ (RBF++). To tackle the first challenge, we define the reasoning\nboundary (RB) as the maximum limit of CoT performance. We also propose a\ncombination law for RBs, enabling quantitative analysis and offering actionable\nguidance across various CoT tasks. For the second challenge, particularly in\nmultimodal scenarios, we introduce a constant assumption, which replaces\nunmeasurable RBs with scenario-specific constants. Additionally, we propose the\nreasoning boundary division mechanism, which divides unmeasurable RBs into two\nsub-boundaries, facilitating the quantification and optimization of both\nunmeasurable domain knowledge and multimodal perception capabilities. Extensive\nexperiments involving 38 models across 13 tasks validate the feasibility of our\nframework in cross-modal settings. Additionally, we evaluate 10 CoT strategies,\noffer insights into optimization and decay from two complementary perspectives,\nand expand evaluation benchmarks for measuring RBs in LLM reasoning. We hope\nthis work advances the understanding of RBs and optimization strategies in\nLLMs. Code and data are available at\nhttps://github.com/LightChen233/reasoning-boundary.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) reasoning has proven effective in enhancing large\nlanguage models (LLMs) on complex tasks, spurring research into its underlying\nmechanisms. However, two primary challenges remain for real-world applications:\n(1) the lack of quantitative metrics and actionable guidelines for evaluating\nand optimizing measurable boundaries of CoT capability, and (2) the absence of\nmethods to assess boundaries of unmeasurable CoT capability, such as multimodal\nperception. To address these gaps, we introduce the Reasoning Boundary\nFramework++ (RBF++). To tackle the first challenge, we define the reasoning\nboundary (RB) as the maximum limit of CoT performance. We also propose a\ncombination law for RBs, enabling quantitative analysis and offering actionable\nguidance across various CoT tasks. For the second challenge, particularly in\nmultimodal scenarios, we introduce a constant assumption, which replaces\nunmeasurable RBs with scenario-specific constants. Additionally, we propose the\nreasoning boundary division mechanism, which divides unmeasurable RBs into two\nsub-boundaries, facilitating the quantification and optimization of both\nunmeasurable domain knowledge and multimodal perception capabilities. Extensive\nexperiments involving 38 models across 13 tasks validate the feasibility of our\nframework in cross-modal settings. Additionally, we evaluate 10 CoT strategies,\noffer insights into optimization and decay from two complementary perspectives,\nand expand evaluation benchmarks for measuring RBs in LLM reasoning. We hope\nthis work advances the understanding of RBs and optimization strategies in\nLLMs. Code and data are available at\nhttps://github.com/LightChen233/reasoning-boundary."
                },
                "authors": [
                    {
                        "name": "Qiguang Chen"
                    },
                    {
                        "name": "Libo Qin"
                    },
                    {
                        "name": "Jinhao Liu"
                    },
                    {
                        "name": "Yue Liao"
                    },
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Jingxuan Zhou"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "arxiv_comment": "Manuscript",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13307v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13307v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04475v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04475v3",
                "updated": "2025-05-19T16:19:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    19,
                    9,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-07T14:46:22Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    14,
                    46,
                    22,
                    2,
                    127,
                    0
                ],
                "title": "Mass Modeling the Andromeda Dwarf Galaxies: Andromeda VI and Andromeda\n  XXIII",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mass Modeling the Andromeda Dwarf Galaxies: Andromeda VI and Andromeda\n  XXIII"
                },
                "summary": "Accurately mapping the mass profiles of low mass dwarf spheroidal (dSph)\ngalaxies allows us to test predictions made by dark matter (DM) models. To\ndate, such analyses have primarily been performed on Milky Way (MW) satellites.\nMeanwhile, the Andromeda Galaxy (M31) is home to 35 known dwarf galaxies, yet\nonly two have been successfully mass-modeled so far. A more comprehensive study\nof Local Group dwarfs is necessary to better understand the nature of dark\nmatter. In this study, we have undertaken a dynamical study of two\nhigher-luminosity Andromeda dwarf galaxies: Andromeda VI (And VI) and Andromeda\nXXIII (And XXIII). We infer an enclosed mass for And VI of M(r $<$ r$_{h}$) =\n(4.9 $\\pm$ 1.5) $\\times$ 10$^{7}$ M$_{\\odot}$, corresponding to a mass-to-light\nratio of $[M/L]_{r_{\\rm{h}}}$ = (27.1 $\\pm$ 8.2) M$_{\\odot}$/L$_{\\odot}$. We\ninfer an enclosed mass for And XXIII of M(r $<$ r$_{h}$) = (3.1 $\\pm$ 1.9)\n$\\times$ 10$^{7}$ M$_{\\odot}$, corresponding to a mass-to-light ratio of\n$[M/L]_{r_{\\rm{h}}}$ = (90.2 $\\pm$ 53.9) M$_{\\odot}$/L$_{\\odot}$. Using the\ndynamical Jeans modeling tool, \\gravsphere, we determine And VI and And XXIII's\ndark matter density at 150 pc, finding $\\rho_{\\rm{DM,VI}}$(150 pc) = (1.4 $\\pm$\n0.5) $\\times 10^{8}$ M$_{\\odot}$ kpc$^{-3}$ and $\\rho_{\\rm{DM,XXIII}}$(150 pc)\n= 0.5$\\substack{+0.4 \\\\ -0.3} \\times 10^{8}$ M$_{\\odot}$ kpc$^{-3}$. Our\nresults make And VI the first mass-modeled M31 satellite to fall into the cuspy\nregime. And XXIII has a lower density, implying either a more cored central\ndark matter density, or a lowering of the density through tides. This adds And\nXXIII to a growing list of M31 dwarfs with a central density lower than most MW\ndwarfs and lower than expected for isolated dwarfs in the Standard Cosmology.\nThis could be explained by the M31 dwarfs having experienced stronger tides\nthan their MW counterparts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately mapping the mass profiles of low mass dwarf spheroidal (dSph)\ngalaxies allows us to test predictions made by dark matter (DM) models. To\ndate, such analyses have primarily been performed on Milky Way (MW) satellites.\nMeanwhile, the Andromeda Galaxy (M31) is home to 35 known dwarf galaxies, yet\nonly two have been successfully mass-modeled so far. A more comprehensive study\nof Local Group dwarfs is necessary to better understand the nature of dark\nmatter. In this study, we have undertaken a dynamical study of two\nhigher-luminosity Andromeda dwarf galaxies: Andromeda VI (And VI) and Andromeda\nXXIII (And XXIII). We infer an enclosed mass for And VI of M(r $<$ r$_{h}$) =\n(4.9 $\\pm$ 1.5) $\\times$ 10$^{7}$ M$_{\\odot}$, corresponding to a mass-to-light\nratio of $[M/L]_{r_{\\rm{h}}}$ = (27.1 $\\pm$ 8.2) M$_{\\odot}$/L$_{\\odot}$. We\ninfer an enclosed mass for And XXIII of M(r $<$ r$_{h}$) = (3.1 $\\pm$ 1.9)\n$\\times$ 10$^{7}$ M$_{\\odot}$, corresponding to a mass-to-light ratio of\n$[M/L]_{r_{\\rm{h}}}$ = (90.2 $\\pm$ 53.9) M$_{\\odot}$/L$_{\\odot}$. Using the\ndynamical Jeans modeling tool, \\gravsphere, we determine And VI and And XXIII's\ndark matter density at 150 pc, finding $\\rho_{\\rm{DM,VI}}$(150 pc) = (1.4 $\\pm$\n0.5) $\\times 10^{8}$ M$_{\\odot}$ kpc$^{-3}$ and $\\rho_{\\rm{DM,XXIII}}$(150 pc)\n= 0.5$\\substack{+0.4 \\\\ -0.3} \\times 10^{8}$ M$_{\\odot}$ kpc$^{-3}$. Our\nresults make And VI the first mass-modeled M31 satellite to fall into the cuspy\nregime. And XXIII has a lower density, implying either a more cored central\ndark matter density, or a lowering of the density through tides. This adds And\nXXIII to a growing list of M31 dwarfs with a central density lower than most MW\ndwarfs and lower than expected for isolated dwarfs in the Standard Cosmology.\nThis could be explained by the M31 dwarfs having experienced stronger tides\nthan their MW counterparts."
                },
                "authors": [
                    {
                        "name": "Connor S. Pickett"
                    },
                    {
                        "name": "Michelle L. M. Collins"
                    },
                    {
                        "name": "R. Michael Rich"
                    },
                    {
                        "name": "Justin I. Read"
                    },
                    {
                        "name": "Emily J. E. Charles"
                    },
                    {
                        "name": "Nicolas Martin"
                    },
                    {
                        "name": "Scott Chapman"
                    },
                    {
                        "name": "Alan McConnachie"
                    },
                    {
                        "name": "Alessandro Savino"
                    },
                    {
                        "name": "Daniel R. Weisz"
                    }
                ],
                "author_detail": {
                    "name": "Daniel R. Weisz"
                },
                "author": "Daniel R. Weisz",
                "arxiv_doi": "10.1093/mnras/staf796",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/mnras/staf796",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.04475v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04475v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "19 pages, 16 figures. Accepted to MNRAS",
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13292v1",
                "updated": "2025-05-19T16:14:27Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    14,
                    27,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T16:14:27Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    14,
                    27,
                    0,
                    139,
                    0
                ],
                "title": "Cross-Cloud Data Privacy Protection: Optimizing Collaborative Mechanisms\n  of AI Systems by Integrating Federated Learning and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Cloud Data Privacy Protection: Optimizing Collaborative Mechanisms\n  of AI Systems by Integrating Federated Learning and LLMs"
                },
                "summary": "In the age of cloud computing, data privacy protection has become a major\nchallenge, especially when sharing sensitive data across cloud environments.\nHowever, how to optimize collaboration across cloud environments remains an\nunresolved problem. In this paper, we combine federated learning with\nlarge-scale language models to optimize the collaborative mechanism of AI\nsystems. Based on the existing federated learning framework, we introduce a\ncross-cloud architecture in which federated learning works by aggregating model\nupdates from decentralized nodes without exposing the original data. At the\nsame time, combined with large-scale language models, its powerful context and\nsemantic understanding capabilities are used to improve model training\nefficiency and decision-making ability. We've further innovated by introducing\na secure communication layer to ensure the privacy and integrity of model\nupdates and training data. The model enables continuous model adaptation and\nfine-tuning across different cloud environments while protecting sensitive\ndata. Experimental results show that the proposed method is significantly\nbetter than the traditional federated learning model in terms of accuracy,\nconvergence speed and data privacy protection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the age of cloud computing, data privacy protection has become a major\nchallenge, especially when sharing sensitive data across cloud environments.\nHowever, how to optimize collaboration across cloud environments remains an\nunresolved problem. In this paper, we combine federated learning with\nlarge-scale language models to optimize the collaborative mechanism of AI\nsystems. Based on the existing federated learning framework, we introduce a\ncross-cloud architecture in which federated learning works by aggregating model\nupdates from decentralized nodes without exposing the original data. At the\nsame time, combined with large-scale language models, its powerful context and\nsemantic understanding capabilities are used to improve model training\nefficiency and decision-making ability. We've further innovated by introducing\na secure communication layer to ensure the privacy and integrity of model\nupdates and training data. The model enables continuous model adaptation and\nfine-tuning across different cloud environments while protecting sensitive\ndata. Experimental results show that the proposed method is significantly\nbetter than the traditional federated learning model in terms of accuracy,\nconvergence speed and data privacy protection."
                },
                "authors": [
                    {
                        "name": "Huaiying Luo"
                    },
                    {
                        "name": "Cheng Ji"
                    }
                ],
                "author_detail": {
                    "name": "Cheng Ji"
                },
                "author": "Cheng Ji",
                "arxiv_comment": "Accepted by 2025 IEEE 7th International Conference on Communications,\n  Information System and Computer Engineering",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17140v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17140v2",
                "updated": "2025-05-19T16:12:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    12,
                    40,
                    0,
                    139,
                    0
                ],
                "published": "2024-09-25T17:58:08Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    58,
                    8,
                    2,
                    269,
                    0
                ],
                "title": "AXIS: Efficient Human-Agent-Computer Interaction with API-First\n  LLM-Based Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AXIS: Efficient Human-Agent-Computer Interaction with API-First\n  LLM-Based Agents"
                },
                "summary": "Multimodal large language models (MLLMs) have enabled LLM-based agents to\ndirectly interact with application user interfaces (UIs), enhancing agents'\nperformance in complex tasks. However, these agents often suffer from high\nlatency and low reliability due to the extensive sequential UI interactions. To\naddress this issue, we propose AXIS, a novel LLM-based agents framework that\nprioritize actions through application programming interfaces (APIs) over UI\nactions. This framework also facilitates the creation and expansion of APIs\nthrough automated exploration of applications. Our experiments on Microsoft\nWord demonstrate that AXIS reduces task completion time by 65%-70% and\ncognitive workload by 38%-53%, while maintaining accuracy of 97%-98% compared\nto humans. Our work contributes to a new human-agent-computer interaction\n(HACI) framework and explores a fresh UI design principle for application\nproviders to turn applications into agents in the era of LLMs, paving the way\ntowards an agent-centric operating system (Agent OS).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have enabled LLM-based agents to\ndirectly interact with application user interfaces (UIs), enhancing agents'\nperformance in complex tasks. However, these agents often suffer from high\nlatency and low reliability due to the extensive sequential UI interactions. To\naddress this issue, we propose AXIS, a novel LLM-based agents framework that\nprioritize actions through application programming interfaces (APIs) over UI\nactions. This framework also facilitates the creation and expansion of APIs\nthrough automated exploration of applications. Our experiments on Microsoft\nWord demonstrate that AXIS reduces task completion time by 65%-70% and\ncognitive workload by 38%-53%, while maintaining accuracy of 97%-98% compared\nto humans. Our work contributes to a new human-agent-computer interaction\n(HACI) framework and explores a fresh UI design principle for application\nproviders to turn applications into agents in the era of LLMs, paving the way\ntowards an agent-centric operating system (Agent OS)."
                },
                "authors": [
                    {
                        "name": "Junting Lu"
                    },
                    {
                        "name": "Zhiyang Zhang"
                    },
                    {
                        "name": "Fangkai Yang"
                    },
                    {
                        "name": "Jue Zhang"
                    },
                    {
                        "name": "Lu Wang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Qingwei Lin"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    },
                    {
                        "name": "Dongmei Zhang"
                    },
                    {
                        "name": "Qi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qi Zhang"
                },
                "author": "Qi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17140v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17140v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13291v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13291v1",
                "updated": "2025-05-19T16:11:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    11,
                    23,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T16:11:23Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    11,
                    23,
                    0,
                    139,
                    0
                ],
                "title": "TimeSeriesGym: A Scalable Benchmark for (Time Series) Machine Learning\n  Engineering Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TimeSeriesGym: A Scalable Benchmark for (Time Series) Machine Learning\n  Engineering Agents"
                },
                "summary": "We introduce TimeSeriesGym, a scalable benchmarking framework for evaluating\nArtificial Intelligence (AI) agents on time series machine learning engineering\nchallenges. Existing benchmarks lack scalability, focus narrowly on model\nbuilding in well-defined settings, and evaluate only a limited set of research\nartifacts (e.g., CSV submission files). To make AI agent benchmarking more\nrelevant to the practice of machine learning engineering, our framework scales\nalong two critical dimensions. First, recognizing that effective ML engineering\nrequires a range of diverse skills, TimeSeriesGym incorporates challenges from\ndiverse sources spanning multiple domains and tasks. We design challenges to\nevaluate both isolated capabilities (including data handling, understanding\nresearch repositories, and code translation) and their combinations, and rather\nthan addressing each challenge independently, we develop tools that support\ndesigning multiple challenges at scale. Second, we implement evaluation\nmechanisms for multiple research artifacts, including submission files, code,\nand models, using both precise numeric measures and more flexible LLM-based\nevaluation approaches. This dual strategy balances objective assessment with\ncontextual judgment. Although our initial focus is on time series applications,\nour framework can be readily extended to other data modalities, broadly\nenhancing the comprehensiveness and practical utility of agentic AI evaluation.\nWe open-source our benchmarking framework to facilitate future research on the\nML engineering capabilities of AI agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce TimeSeriesGym, a scalable benchmarking framework for evaluating\nArtificial Intelligence (AI) agents on time series machine learning engineering\nchallenges. Existing benchmarks lack scalability, focus narrowly on model\nbuilding in well-defined settings, and evaluate only a limited set of research\nartifacts (e.g., CSV submission files). To make AI agent benchmarking more\nrelevant to the practice of machine learning engineering, our framework scales\nalong two critical dimensions. First, recognizing that effective ML engineering\nrequires a range of diverse skills, TimeSeriesGym incorporates challenges from\ndiverse sources spanning multiple domains and tasks. We design challenges to\nevaluate both isolated capabilities (including data handling, understanding\nresearch repositories, and code translation) and their combinations, and rather\nthan addressing each challenge independently, we develop tools that support\ndesigning multiple challenges at scale. Second, we implement evaluation\nmechanisms for multiple research artifacts, including submission files, code,\nand models, using both precise numeric measures and more flexible LLM-based\nevaluation approaches. This dual strategy balances objective assessment with\ncontextual judgment. Although our initial focus is on time series applications,\nour framework can be readily extended to other data modalities, broadly\nenhancing the comprehensiveness and practical utility of agentic AI evaluation.\nWe open-source our benchmarking framework to facilitate future research on the\nML engineering capabilities of AI agents."
                },
                "authors": [
                    {
                        "name": "Yifu Cai"
                    },
                    {
                        "name": "Xinyu Li"
                    },
                    {
                        "name": "Mononito Goswami"
                    },
                    {
                        "name": "Michał Wiliński"
                    },
                    {
                        "name": "Gus Welter"
                    },
                    {
                        "name": "Artur Dubrawski"
                    }
                ],
                "author_detail": {
                    "name": "Artur Dubrawski"
                },
                "author": "Artur Dubrawski",
                "arxiv_comment": "Open source code available at\n  https://github.com/moment-timeseries-foundation-model/TimeSeriesGym. YC, XL,\n  MG and MW contributed equally, and should be considered joint first authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13291v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09569v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09569v2",
                "updated": "2025-05-19T16:10:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    10,
                    21,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-14T17:11:23Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    17,
                    11,
                    23,
                    2,
                    134,
                    0
                ],
                "title": "MigrationBench: Repository-Level Code Migration Benchmark from Java 8",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MigrationBench: Repository-Level Code Migration Benchmark from Java 8"
                },
                "summary": "With the rapid advancement of powerful large language models (LLMs) in recent\nyears, a wide range of software engineering tasks can now be addressed using\nLLMs, significantly enhancing productivity and scalability. Numerous benchmark\ndatasets have been developed to evaluate the coding capabilities of these\nmodels, while they primarily focus on code generation and issue-resolution\ntasks. In contrast, we introduce a new coding benchmark MigrationBench with a\ndistinct focus: code migration. MigrationBench aims to serve as a comprehensive\nbenchmark for migration from Java $8$ to the latest long-term support (LTS)\nversions (Java $17$, $21$), including a full dataset and its subset selected\nwith $5,102$ and $300$ repositories respectively. Selected is a representative\nsubset curated for complexity and difficulty, offering a versatile resource to\nsupport research in the field of code migration. Additionally, we provide a\ncomprehensive evaluation framework to facilitate rigorous and standardized\nassessment of LLMs on this challenging task. We further propose SD-Feedback and\ndemonstrate that LLMs can effectively tackle repository-level code migration to\nJava $17$. For the selected subset with Claude-3.5-Sonnet-v2, SD-Feedback\nachieves $62.33\\%$ and $27.33\\%$ success rate (pass@1) for minimal and maximal\nmigration respectively. The benchmark dataset and source code are available at:\nhttps://huggingface.co/collections/AmazonScience/migrationbench-68125452fc21a4564b92b6c3\nand https://github.com/amazon-science/MigrationBench respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of powerful large language models (LLMs) in recent\nyears, a wide range of software engineering tasks can now be addressed using\nLLMs, significantly enhancing productivity and scalability. Numerous benchmark\ndatasets have been developed to evaluate the coding capabilities of these\nmodels, while they primarily focus on code generation and issue-resolution\ntasks. In contrast, we introduce a new coding benchmark MigrationBench with a\ndistinct focus: code migration. MigrationBench aims to serve as a comprehensive\nbenchmark for migration from Java $8$ to the latest long-term support (LTS)\nversions (Java $17$, $21$), including a full dataset and its subset selected\nwith $5,102$ and $300$ repositories respectively. Selected is a representative\nsubset curated for complexity and difficulty, offering a versatile resource to\nsupport research in the field of code migration. Additionally, we provide a\ncomprehensive evaluation framework to facilitate rigorous and standardized\nassessment of LLMs on this challenging task. We further propose SD-Feedback and\ndemonstrate that LLMs can effectively tackle repository-level code migration to\nJava $17$. For the selected subset with Claude-3.5-Sonnet-v2, SD-Feedback\nachieves $62.33\\%$ and $27.33\\%$ success rate (pass@1) for minimal and maximal\nmigration respectively. The benchmark dataset and source code are available at:\nhttps://huggingface.co/collections/AmazonScience/migrationbench-68125452fc21a4564b92b6c3\nand https://github.com/amazon-science/MigrationBench respectively."
                },
                "authors": [
                    {
                        "name": "Linbo Liu"
                    },
                    {
                        "name": "Xinle Liu"
                    },
                    {
                        "name": "Qiang Zhou"
                    },
                    {
                        "name": "Lin Chen"
                    },
                    {
                        "name": "Yihan Liu"
                    },
                    {
                        "name": "Hoan Nguyen"
                    },
                    {
                        "name": "Behrooz Omidvar-Tehrani"
                    },
                    {
                        "name": "Xi Shen"
                    },
                    {
                        "name": "Jun Huan"
                    },
                    {
                        "name": "Omer Tripp"
                    },
                    {
                        "name": "Anoop Deoras"
                    }
                ],
                "author_detail": {
                    "name": "Anoop Deoras"
                },
                "author": "Anoop Deoras",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09569v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09569v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13284v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13284v1",
                "updated": "2025-05-19T16:07:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    7,
                    16,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T16:07:16Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    7,
                    16,
                    0,
                    139,
                    0
                ],
                "title": "Intuitionistic BV (Extended version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intuitionistic BV (Extended version)"
                },
                "summary": "We present the logic IBV, which is an intuitionistic version of BV, in the\nsense that its restriction to the MLL connectives is exactly IMLL, the\nintuitionistic version of MLL. For this logic we give a deep inference proof\nsystem and show cut elimination. We also show that the logic obtained from IBV\nby dropping the associativity of the new non-commutative seq-connective is an\nintuitionistic variant of the recently introduced logic NML. For this logic,\ncalled INML, we give a cut-free sequent calculus.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the logic IBV, which is an intuitionistic version of BV, in the\nsense that its restriction to the MLL connectives is exactly IMLL, the\nintuitionistic version of MLL. For this logic we give a deep inference proof\nsystem and show cut elimination. We also show that the logic obtained from IBV\nby dropping the associativity of the new non-commutative seq-connective is an\nintuitionistic variant of the recently introduced logic NML. For this logic,\ncalled INML, we give a cut-free sequent calculus."
                },
                "authors": [
                    {
                        "name": "Matteo Acclavio"
                    },
                    {
                        "name": "Lutz Strassburger"
                    }
                ],
                "author_detail": {
                    "name": "Lutz Strassburger"
                },
                "author": "Lutz Strassburger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13284v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13284v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13280v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13280v1",
                "updated": "2025-05-19T16:04:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    4,
                    43,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T16:04:43Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    4,
                    43,
                    0,
                    139,
                    0
                ],
                "title": "FlowPure: Continuous Normalizing Flows for Adversarial Purification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlowPure: Continuous Normalizing Flows for Adversarial Purification"
                },
                "summary": "Despite significant advancements in the area, adversarial robustness remains\na critical challenge in systems employing machine learning models. The removal\nof adversarial perturbations at inference time, known as adversarial\npurification, has emerged as a promising defense strategy. To achieve this,\nstate-of-the-art methods leverage diffusion models that inject Gaussian noise\nduring a forward process to dilute adversarial perturbations, followed by a\ndenoising step to restore clean samples before classification. In this work, we\npropose FlowPure, a novel purification method based on Continuous Normalizing\nFlows (CNFs) trained with Conditional Flow Matching (CFM) to learn mappings\nfrom adversarial examples to their clean counterparts. Unlike prior\ndiffusion-based approaches that rely on fixed noise processes, FlowPure can\nleverage specific attack knowledge to improve robustness under known threats,\nwhile also supporting a more general stochastic variant trained on Gaussian\nperturbations for settings where such knowledge is unavailable. Experiments on\nCIFAR-10 and CIFAR-100 demonstrate that our method outperforms state-of-the-art\npurification-based defenses in preprocessor-blind and white-box scenarios, and\ncan do so while fully preserving benign accuracy in the former. Moreover, our\nresults show that not only is FlowPure a highly effective purifier but it also\nholds a strong potential for adversarial detection, identifying\npreprocessor-blind PGD samples with near-perfect accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite significant advancements in the area, adversarial robustness remains\na critical challenge in systems employing machine learning models. The removal\nof adversarial perturbations at inference time, known as adversarial\npurification, has emerged as a promising defense strategy. To achieve this,\nstate-of-the-art methods leverage diffusion models that inject Gaussian noise\nduring a forward process to dilute adversarial perturbations, followed by a\ndenoising step to restore clean samples before classification. In this work, we\npropose FlowPure, a novel purification method based on Continuous Normalizing\nFlows (CNFs) trained with Conditional Flow Matching (CFM) to learn mappings\nfrom adversarial examples to their clean counterparts. Unlike prior\ndiffusion-based approaches that rely on fixed noise processes, FlowPure can\nleverage specific attack knowledge to improve robustness under known threats,\nwhile also supporting a more general stochastic variant trained on Gaussian\nperturbations for settings where such knowledge is unavailable. Experiments on\nCIFAR-10 and CIFAR-100 demonstrate that our method outperforms state-of-the-art\npurification-based defenses in preprocessor-blind and white-box scenarios, and\ncan do so while fully preserving benign accuracy in the former. Moreover, our\nresults show that not only is FlowPure a highly effective purifier but it also\nholds a strong potential for adversarial detection, identifying\npreprocessor-blind PGD samples with near-perfect accuracy."
                },
                "authors": [
                    {
                        "name": "Elias Collaert"
                    },
                    {
                        "name": "Abel Rodríguez"
                    },
                    {
                        "name": "Sander Joos"
                    },
                    {
                        "name": "Lieven Desmet"
                    },
                    {
                        "name": "Vera Rimmer"
                    }
                ],
                "author_detail": {
                    "name": "Vera Rimmer"
                },
                "author": "Vera Rimmer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13280v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13280v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13278v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13278v1",
                "updated": "2025-05-19T16:01:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    1,
                    36,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T16:01:36Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    1,
                    36,
                    0,
                    139,
                    0
                ],
                "title": "Hybrid Voting-Based Task Assignment in Modular Construction Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Voting-Based Task Assignment in Modular Construction Scenarios"
                },
                "summary": "Modular construction, involving off-site prefabrication and on-site assembly,\noffers significant advantages but presents complex coordination challenges for\nrobotic automation. Effective task allocation is critical for leveraging\nmulti-agent systems (MAS) in these structured environments. This paper\nintroduces the Hybrid Voting-Based Task Assignment (HVBTA) framework, a novel\napproach to optimizing collaboration between heterogeneous multi-agent\nconstruction teams. Inspired by human reasoning in task delegation, HVBTA\nuniquely integrates multiple voting mechanisms with the capabilities of a Large\nLanguage Model (LLM) for nuanced suitability assessment between agent\ncapabilities and task requirements. The framework operates by assigning\nCapability Profiles to agents and detailed requirement lists called Task\nDescriptions to construction tasks, subsequently generating a quantitative\nSuitability Matrix. Six distinct voting methods, augmented by a pre-trained\nLLM, analyze this matrix to robustly identify the optimal agent for each task.\nConflict-Based Search (CBS) is integrated for decentralized, collision-free\npath planning, ensuring efficient and safe spatio-temporal coordination of the\nrobotic team during assembly operations. HVBTA enables efficient, conflict-free\nassignment and coordination, facilitating potentially faster and more accurate\nmodular assembly. Current work is evaluating HVBTA's performance across various\nsimulated construction scenarios involving diverse robotic platforms and task\ncomplexities. While designed as a generalizable framework for any domain with\nclearly definable tasks and capabilities, HVBTA will be particularly effective\nfor addressing the demanding coordination requirements of multi-agent\ncollaborative robotics in modular construction due to the predetermined\nconstruction planning involved.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modular construction, involving off-site prefabrication and on-site assembly,\noffers significant advantages but presents complex coordination challenges for\nrobotic automation. Effective task allocation is critical for leveraging\nmulti-agent systems (MAS) in these structured environments. This paper\nintroduces the Hybrid Voting-Based Task Assignment (HVBTA) framework, a novel\napproach to optimizing collaboration between heterogeneous multi-agent\nconstruction teams. Inspired by human reasoning in task delegation, HVBTA\nuniquely integrates multiple voting mechanisms with the capabilities of a Large\nLanguage Model (LLM) for nuanced suitability assessment between agent\ncapabilities and task requirements. The framework operates by assigning\nCapability Profiles to agents and detailed requirement lists called Task\nDescriptions to construction tasks, subsequently generating a quantitative\nSuitability Matrix. Six distinct voting methods, augmented by a pre-trained\nLLM, analyze this matrix to robustly identify the optimal agent for each task.\nConflict-Based Search (CBS) is integrated for decentralized, collision-free\npath planning, ensuring efficient and safe spatio-temporal coordination of the\nrobotic team during assembly operations. HVBTA enables efficient, conflict-free\nassignment and coordination, facilitating potentially faster and more accurate\nmodular assembly. Current work is evaluating HVBTA's performance across various\nsimulated construction scenarios involving diverse robotic platforms and task\ncomplexities. While designed as a generalizable framework for any domain with\nclearly definable tasks and capabilities, HVBTA will be particularly effective\nfor addressing the demanding coordination requirements of multi-agent\ncollaborative robotics in modular construction due to the predetermined\nconstruction planning involved."
                },
                "authors": [
                    {
                        "name": "Daniel Weiner"
                    },
                    {
                        "name": "Raj Korpan"
                    }
                ],
                "author_detail": {
                    "name": "Raj Korpan"
                },
                "author": "Raj Korpan",
                "arxiv_comment": "Accepted to Block by Block workshop at ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13278v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13278v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09493v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09493v2",
                "updated": "2025-05-19T15:57:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    57,
                    5,
                    0,
                    139,
                    0
                ],
                "published": "2024-09-14T17:40:35Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    17,
                    40,
                    35,
                    5,
                    258,
                    0
                ],
                "title": "Hacking, The Lazy Way: LLM Augmented Pentesting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hacking, The Lazy Way: LLM Augmented Pentesting"
                },
                "summary": "In our research, we introduce a new concept called \"LLM Augmented Pentesting\"\ndemonstrated with a tool named \"Pentest Copilot,\" that revolutionizes the field\nof ethical hacking by integrating Large Language Models (LLMs) into penetration\ntesting workflows, leveraging the advanced GPT-4-turbo model. Our approach\nfocuses on overcoming the traditional resistance to automation in penetration\ntesting by employing LLMs to automate specific sub-tasks while ensuring a\ncomprehensive understanding of the overall testing process.\n  Pentest Copilot showcases remarkable proficiency in tasks such as utilizing\ntesting tools, interpreting outputs, and suggesting follow-up actions,\nefficiently bridging the gap between automated systems and human expertise. By\nintegrating a \"chain of thought\" mechanism, Pentest Copilot optimizes token\nusage and enhances decision-making processes, leading to more accurate and\ncontext-aware outputs. Additionally, our implementation of Retrieval-Augmented\nGeneration (RAG) minimizes hallucinations and ensures the tool remains aligned\nwith the latest cybersecurity techniques and knowledge. We also highlight a\nunique infrastructure system that supports in-browser penetration testing,\nproviding a robust platform for cybersecurity professionals. Our findings\ndemonstrate that LLM Augmented Pentesting can not only significantly enhance\ntask completion rates in penetration testing but also effectively addresses\nreal-world challenges, marking a substantial advancement in the cybersecurity\ndomain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In our research, we introduce a new concept called \"LLM Augmented Pentesting\"\ndemonstrated with a tool named \"Pentest Copilot,\" that revolutionizes the field\nof ethical hacking by integrating Large Language Models (LLMs) into penetration\ntesting workflows, leveraging the advanced GPT-4-turbo model. Our approach\nfocuses on overcoming the traditional resistance to automation in penetration\ntesting by employing LLMs to automate specific sub-tasks while ensuring a\ncomprehensive understanding of the overall testing process.\n  Pentest Copilot showcases remarkable proficiency in tasks such as utilizing\ntesting tools, interpreting outputs, and suggesting follow-up actions,\nefficiently bridging the gap between automated systems and human expertise. By\nintegrating a \"chain of thought\" mechanism, Pentest Copilot optimizes token\nusage and enhances decision-making processes, leading to more accurate and\ncontext-aware outputs. Additionally, our implementation of Retrieval-Augmented\nGeneration (RAG) minimizes hallucinations and ensures the tool remains aligned\nwith the latest cybersecurity techniques and knowledge. We also highlight a\nunique infrastructure system that supports in-browser penetration testing,\nproviding a robust platform for cybersecurity professionals. Our findings\ndemonstrate that LLM Augmented Pentesting can not only significantly enhance\ntask completion rates in penetration testing but also effectively addresses\nreal-world challenges, marking a substantial advancement in the cybersecurity\ndomain."
                },
                "authors": [
                    {
                        "name": "Dhruva Goyal"
                    },
                    {
                        "name": "Sitaraman Subramanian"
                    },
                    {
                        "name": "Aditya Peela"
                    },
                    {
                        "name": "Nisha P. Shetty"
                    }
                ],
                "author_detail": {
                    "name": "Nisha P. Shetty"
                },
                "author": "Nisha P. Shetty",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication.\n  Nisha P. Shetty has been added as an author as the new version includes work\n  under her supervision, enhancing the research. Significant changes have been\n  made in the methodology, survey, and introduction sections",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09493v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09493v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13271v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13271v1",
                "updated": "2025-05-19T15:52:19Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    52,
                    19,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T15:52:19Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    52,
                    19,
                    0,
                    139,
                    0
                ],
                "title": "CSC-SQL: Corrective Self-Consistency in Text-to-SQL via Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSC-SQL: Corrective Self-Consistency in Text-to-SQL via Reinforcement\n  Learning"
                },
                "summary": "Large language models (LLMs) have demonstrated strong capabilities in\ntranslating natural language questions about relational databases into SQL\nqueries. In particular, test-time scaling techniques such as Self-Consistency\nand Self-Correction can enhance SQL generation accuracy by increasing\ncomputational effort during inference. However, these methods have notable\nlimitations: Self-Consistency may select suboptimal outputs despite majority\nvotes, while Self-Correction typically addresses only syntactic errors. To\nleverage the strengths of both approaches, we propose CSC-SQL, a novel method\nthat integrates Self-Consistency and Self-Correction. CSC-SQL selects the two\nmost frequently occurring outputs from parallel sampling and feeds them into a\nmerge revision model for correction. Additionally, we employ the Group Relative\nPolicy Optimization (GRPO) algorithm to fine-tune both the SQL generation and\nrevision models via reinforcement learning, significantly enhancing output\nquality. Experimental results confirm the effectiveness and generalizability of\nCSC-SQL. On the BIRD development set, our 3B model achieves 65.28% execution\naccuracy, while the 7B model achieves 69.19%. The code will be open sourced at\nhttps://github.com/CycloneBoy/csc_sql.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated strong capabilities in\ntranslating natural language questions about relational databases into SQL\nqueries. In particular, test-time scaling techniques such as Self-Consistency\nand Self-Correction can enhance SQL generation accuracy by increasing\ncomputational effort during inference. However, these methods have notable\nlimitations: Self-Consistency may select suboptimal outputs despite majority\nvotes, while Self-Correction typically addresses only syntactic errors. To\nleverage the strengths of both approaches, we propose CSC-SQL, a novel method\nthat integrates Self-Consistency and Self-Correction. CSC-SQL selects the two\nmost frequently occurring outputs from parallel sampling and feeds them into a\nmerge revision model for correction. Additionally, we employ the Group Relative\nPolicy Optimization (GRPO) algorithm to fine-tune both the SQL generation and\nrevision models via reinforcement learning, significantly enhancing output\nquality. Experimental results confirm the effectiveness and generalizability of\nCSC-SQL. On the BIRD development set, our 3B model achieves 65.28% execution\naccuracy, while the 7B model achieves 69.19%. The code will be open sourced at\nhttps://github.com/CycloneBoy/csc_sql."
                },
                "authors": [
                    {
                        "name": "Lei Sheng"
                    },
                    {
                        "name": "Shuai-Shuai Xu"
                    }
                ],
                "author_detail": {
                    "name": "Shuai-Shuai Xu"
                },
                "author": "Shuai-Shuai Xu",
                "arxiv_comment": "11 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13271v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13271v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16707v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16707v3",
                "updated": "2025-05-19T15:51:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    51,
                    40,
                    0,
                    139,
                    0
                ],
                "published": "2024-11-21T19:01:07Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    19,
                    1,
                    7,
                    3,
                    326,
                    0
                ],
                "title": "Enhancing LLMs for Power System Simulations: A Feedback-driven\n  Multi-agent Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLMs for Power System Simulations: A Feedback-driven\n  Multi-agent Framework"
                },
                "summary": "The integration of experimental technologies with large language models\n(LLMs) is transforming scientific research. It positions AI as a versatile\nresearch assistant rather than a mere problem-solving tool. In the field of\npower systems, however, managing simulations -- one of the essential\nexperimental technologies -- remains a challenge for LLMs due to their limited\ndomain-specific knowledge, restricted reasoning capabilities, and imprecise\nhandling of simulation parameters. To address these limitations, this paper\nproposes a feedback-driven, multi-agent framework. It incorporates three\nproposed modules: an enhanced retrieval-augmented generation (RAG) module, an\nimproved reasoning module, and a dynamic environmental acting module with an\nerror-feedback mechanism. Validated on 69 diverse tasks from Daline and\nMATPOWER, this framework achieves success rates of 93.13% and 96.85%,\nrespectively. It significantly outperforms ChatGPT 4o, o1-preview, and the\nfine-tuned GPT-4o, which all achieved a success rate lower than 30% on complex\ntasks. Additionally, the proposed framework also supports rapid, cost-effective\ntask execution, completing each simulation in approximately 30 seconds at an\naverage cost of 0.014 USD for tokens. Overall, this adaptable framework lays a\nfoundation for developing intelligent LLM-based assistants for human\nresearchers, facilitating power system research and beyond.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of experimental technologies with large language models\n(LLMs) is transforming scientific research. It positions AI as a versatile\nresearch assistant rather than a mere problem-solving tool. In the field of\npower systems, however, managing simulations -- one of the essential\nexperimental technologies -- remains a challenge for LLMs due to their limited\ndomain-specific knowledge, restricted reasoning capabilities, and imprecise\nhandling of simulation parameters. To address these limitations, this paper\nproposes a feedback-driven, multi-agent framework. It incorporates three\nproposed modules: an enhanced retrieval-augmented generation (RAG) module, an\nimproved reasoning module, and a dynamic environmental acting module with an\nerror-feedback mechanism. Validated on 69 diverse tasks from Daline and\nMATPOWER, this framework achieves success rates of 93.13% and 96.85%,\nrespectively. It significantly outperforms ChatGPT 4o, o1-preview, and the\nfine-tuned GPT-4o, which all achieved a success rate lower than 30% on complex\ntasks. Additionally, the proposed framework also supports rapid, cost-effective\ntask execution, completing each simulation in approximately 30 seconds at an\naverage cost of 0.014 USD for tokens. Overall, this adaptable framework lays a\nfoundation for developing intelligent LLM-based assistants for human\nresearchers, facilitating power system research and beyond."
                },
                "authors": [
                    {
                        "name": "Mengshuo Jia"
                    },
                    {
                        "name": "Zeyu Cui"
                    },
                    {
                        "name": "Gabriela Hug"
                    }
                ],
                "author_detail": {
                    "name": "Gabriela Hug"
                },
                "author": "Gabriela Hug",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16707v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16707v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16565v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16565v2",
                "updated": "2025-05-19T15:45:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    45,
                    13,
                    0,
                    139,
                    0
                ],
                "published": "2025-02-23T13:12:53Z",
                "published_parsed": [
                    2025,
                    2,
                    23,
                    13,
                    12,
                    53,
                    6,
                    54,
                    0
                ],
                "title": "The Hidden Strength of Disagreement: Unraveling the Consensus-Diversity\n  Tradeoff in Adaptive Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Hidden Strength of Disagreement: Unraveling the Consensus-Diversity\n  Tradeoff in Adaptive Multi-Agent Systems"
                },
                "summary": "Consensus formation is pivotal in multi-agent systems (MAS), balancing\ncollective coherence with individual diversity. Conventional LLM-based MAS\nprimarily rely on explicit coordination, e.g., prompts or voting, risking\npremature homogenization. We argue that implicit consensus, where agents\nexchange information yet independently form decisions via in-context learning,\ncan be more effective in dynamic environments that require long-horizon\nadaptability. By retaining partial diversity, systems can better explore novel\nstrategies and cope with external shocks. We formalize a consensus-diversity\ntradeoff, showing conditions where implicit methods outperform explicit ones.\nExperiments on three scenarios -- Dynamic Disaster Response, Information Spread\nand Manipulation, and Dynamic Public-Goods Provision -- confirm partial\ndeviation from group norms boosts exploration, robustness, and performance. We\nhighlight emergent coordination via in-context learning, underscoring the value\nof preserving diversity for resilient decision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Consensus formation is pivotal in multi-agent systems (MAS), balancing\ncollective coherence with individual diversity. Conventional LLM-based MAS\nprimarily rely on explicit coordination, e.g., prompts or voting, risking\npremature homogenization. We argue that implicit consensus, where agents\nexchange information yet independently form decisions via in-context learning,\ncan be more effective in dynamic environments that require long-horizon\nadaptability. By retaining partial diversity, systems can better explore novel\nstrategies and cope with external shocks. We formalize a consensus-diversity\ntradeoff, showing conditions where implicit methods outperform explicit ones.\nExperiments on three scenarios -- Dynamic Disaster Response, Information Spread\nand Manipulation, and Dynamic Public-Goods Provision -- confirm partial\ndeviation from group norms boosts exploration, robustness, and performance. We\nhighlight emergent coordination via in-context learning, underscoring the value\nof preserving diversity for resilient decision-making."
                },
                "authors": [
                    {
                        "name": "Zengqing Wu"
                    },
                    {
                        "name": "Takayuki Ito"
                    }
                ],
                "author_detail": {
                    "name": "Takayuki Ito"
                },
                "author": "Takayuki Ito",
                "arxiv_comment": "Source codes are available at\n  https://github.com/wuzengqing001225/ConsensusDiversityTradeoffMAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16565v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16565v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13263v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13263v1",
                "updated": "2025-05-19T15:44:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    44,
                    24,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T15:44:24Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    44,
                    24,
                    0,
                    139,
                    0
                ],
                "title": "Are requirements really all you need? A case study of LLM-driven\n  configuration code generation for automotive simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are requirements really all you need? A case study of LLM-driven\n  configuration code generation for automotive simulations"
                },
                "summary": "Large Language Models (LLMs) are taking many industries by storm. They\npossess impressive reasoning capabilities and are capable of handling complex\nproblems, as shown by their steadily improving scores on coding and\nmathematical benchmarks. However, are the models currently available truly\ncapable of addressing real-world challenges, such as those found in the\nautomotive industry? How well can they understand high-level, abstract\ninstructions? Can they translate these instructions directly into functional\ncode, or do they still need help and supervision? In this work, we put one of\nthe current state-of-the-art models to the test. We evaluate its performance in\nthe task of translating abstract requirements, extracted from automotive\nstandards and documents, into configuration code for CARLA simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are taking many industries by storm. They\npossess impressive reasoning capabilities and are capable of handling complex\nproblems, as shown by their steadily improving scores on coding and\nmathematical benchmarks. However, are the models currently available truly\ncapable of addressing real-world challenges, such as those found in the\nautomotive industry? How well can they understand high-level, abstract\ninstructions? Can they translate these instructions directly into functional\ncode, or do they still need help and supervision? In this work, we put one of\nthe current state-of-the-art models to the test. We evaluate its performance in\nthe task of translating abstract requirements, extracted from automotive\nstandards and documents, into configuration code for CARLA simulations."
                },
                "authors": [
                    {
                        "name": "Krzysztof Lebioda"
                    },
                    {
                        "name": "Nenad Petrovic"
                    },
                    {
                        "name": "Fengjunjie Pan"
                    },
                    {
                        "name": "Vahid Zolfaghari"
                    },
                    {
                        "name": "Andre Schamschurko"
                    },
                    {
                        "name": "Alois Knoll"
                    }
                ],
                "author_detail": {
                    "name": "Alois Knoll"
                },
                "author": "Alois Knoll",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13263v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13263v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13259v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13259v1",
                "updated": "2025-05-19T15:41:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    41,
                    32,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T15:41:32Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    41,
                    32,
                    0,
                    139,
                    0
                ],
                "title": "From Automation to Autonomy: A Survey on Large Language Models in\n  Scientific Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Automation to Autonomy: A Survey on Large Language Models in\n  Scientific Discovery"
                },
                "summary": "Large Language Models (LLMs) are catalyzing a paradigm shift in scientific\ndiscovery, evolving from task-specific automation tools into increasingly\nautonomous agents and fundamentally redefining research processes and human-AI\ncollaboration. This survey systematically charts this burgeoning field, placing\na central focus on the changing roles and escalating capabilities of LLMs in\nscience. Through the lens of the scientific method, we introduce a foundational\nthree-level taxonomy-Tool, Analyst, and Scientist-to delineate their escalating\nautonomy and evolving responsibilities within the research lifecycle. We\nfurther identify pivotal challenges and future research trajectories such as\nrobotic automation, self-improvement, and ethical governance. Overall, this\nsurvey provides a conceptual architecture and strategic foresight to navigate\nand shape the future of AI-driven scientific discovery, fostering both rapid\ninnovation and responsible advancement. Github Repository:\nhttps://github.com/HKUST-KnowComp/Awesome-LLM-Scientific-Discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are catalyzing a paradigm shift in scientific\ndiscovery, evolving from task-specific automation tools into increasingly\nautonomous agents and fundamentally redefining research processes and human-AI\ncollaboration. This survey systematically charts this burgeoning field, placing\na central focus on the changing roles and escalating capabilities of LLMs in\nscience. Through the lens of the scientific method, we introduce a foundational\nthree-level taxonomy-Tool, Analyst, and Scientist-to delineate their escalating\nautonomy and evolving responsibilities within the research lifecycle. We\nfurther identify pivotal challenges and future research trajectories such as\nrobotic automation, self-improvement, and ethical governance. Overall, this\nsurvey provides a conceptual architecture and strategic foresight to navigate\nand shape the future of AI-driven scientific discovery, fostering both rapid\ninnovation and responsible advancement. Github Repository:\nhttps://github.com/HKUST-KnowComp/Awesome-LLM-Scientific-Discovery."
                },
                "authors": [
                    {
                        "name": "Tianshi Zheng"
                    },
                    {
                        "name": "Zheye Deng"
                    },
                    {
                        "name": "Hong Ting Tsang"
                    },
                    {
                        "name": "Weiqi Wang"
                    },
                    {
                        "name": "Jiaxin Bai"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Yangqiu Song"
                    }
                ],
                "author_detail": {
                    "name": "Yangqiu Song"
                },
                "author": "Yangqiu Song",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13259v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13259v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13258v1",
                "updated": "2025-05-19T15:40:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    40,
                    29,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T15:40:29Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    40,
                    29,
                    0,
                    139,
                    0
                ],
                "title": "Effective and Transparent RAG: Adaptive-Reward Reinforcement Learning\n  for Decision Traceability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective and Transparent RAG: Adaptive-Reward Reinforcement Learning\n  for Decision Traceability"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has significantly improved the\nperformance of large language models (LLMs) on knowledge-intensive domains.\nHowever, although RAG achieved successes across distinct domains, there are\nstill some unsolved challenges: 1) Effectiveness. Existing research mainly\nfocuses on developing more powerful RAG retrievers, but how to enhance the\ngenerator's (LLM's) ability to utilize the retrieved information for reasoning\nand generation? 2) Transparency. Most RAG methods ignore which retrieved\ncontent actually contributes to the reasoning process, resulting in a lack of\ninterpretability and visibility. To address this, we propose ARENA\n(Adaptive-Rewarded Evidence Navigation Agent), a transparent RAG generator\nframework trained via reinforcement learning (RL) with our proposed rewards.\nBased on the structured generation and adaptive reward calculation, our\nRL-based training enables the model to identify key evidence, perform\nstructured reasoning, and generate answers with interpretable decision traces.\nApplied to Qwen2.5-7B-Instruct and Llama3.1-8B-Instruct, abundant experiments\nwith various RAG baselines demonstrate that our model achieves 10-30%\nimprovements on all multi-hop QA datasets, which is comparable with the SOTA\nCommercially-developed LLMs (e.g., OpenAI-o1, DeepSeek-R1). Further analyses\nshow that ARENA has strong flexibility to be adopted on new datasets without\nextra training. Our models and codes are publicly released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has significantly improved the\nperformance of large language models (LLMs) on knowledge-intensive domains.\nHowever, although RAG achieved successes across distinct domains, there are\nstill some unsolved challenges: 1) Effectiveness. Existing research mainly\nfocuses on developing more powerful RAG retrievers, but how to enhance the\ngenerator's (LLM's) ability to utilize the retrieved information for reasoning\nand generation? 2) Transparency. Most RAG methods ignore which retrieved\ncontent actually contributes to the reasoning process, resulting in a lack of\ninterpretability and visibility. To address this, we propose ARENA\n(Adaptive-Rewarded Evidence Navigation Agent), a transparent RAG generator\nframework trained via reinforcement learning (RL) with our proposed rewards.\nBased on the structured generation and adaptive reward calculation, our\nRL-based training enables the model to identify key evidence, perform\nstructured reasoning, and generate answers with interpretable decision traces.\nApplied to Qwen2.5-7B-Instruct and Llama3.1-8B-Instruct, abundant experiments\nwith various RAG baselines demonstrate that our model achieves 10-30%\nimprovements on all multi-hop QA datasets, which is comparable with the SOTA\nCommercially-developed LLMs (e.g., OpenAI-o1, DeepSeek-R1). Further analyses\nshow that ARENA has strong flexibility to be adopted on new datasets without\nextra training. Our models and codes are publicly released."
                },
                "authors": [
                    {
                        "name": "Jingyi Ren"
                    },
                    {
                        "name": "Yekun Xu"
                    },
                    {
                        "name": "Xiaolong Wang"
                    },
                    {
                        "name": "Weitao Li"
                    },
                    {
                        "name": "Weizhi Ma"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13254v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13254v1",
                "updated": "2025-05-19T15:38:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    38,
                    40,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T15:38:40Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    38,
                    40,
                    0,
                    139,
                    0
                ],
                "title": "HeteroSpec: Leveraging Contextual Heterogeneity for Efficient\n  Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HeteroSpec: Leveraging Contextual Heterogeneity for Efficient\n  Speculative Decoding"
                },
                "summary": "Autoregressive decoding, the standard approach for Large Language Model (LLM)\ninference, remains a significant bottleneck due to its sequential nature. While\nspeculative decoding algorithms mitigate this inefficiency through parallel\nverification, they fail to exploit the inherent heterogeneity in linguistic\ncomplexity, a key factor leading to suboptimal resource allocation. We address\nthis by proposing HeteroSpec, a heterogeneity-adaptive speculative decoding\nframework that dynamically optimizes computational resource allocation based on\nlinguistic context complexity. HeteroSpec introduces two key mechanisms: (1) A\nnovel cumulative meta-path Top-$K$ entropy metric for efficiently identifying\npredictable contexts. (2) A dynamic resource allocation strategy based on\ndata-driven entropy partitioning, enabling adaptive speculative expansion and\npruning tailored to local context difficulty. Evaluated on five public\nbenchmarks and four models, HeteroSpec achieves an average speedup of\n4.26$\\times$. It consistently outperforms state-of-the-art EAGLE-3 across\nspeedup rates, average acceptance length, and verification cost. Notably,\nHeteroSpec requires no draft model retraining, incurs minimal overhead, and is\northogonal to other acceleration techniques. It demonstrates enhanced\nacceleration with stronger draft models, establishing a new paradigm for\ncontext-aware LLM inference acceleration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive decoding, the standard approach for Large Language Model (LLM)\ninference, remains a significant bottleneck due to its sequential nature. While\nspeculative decoding algorithms mitigate this inefficiency through parallel\nverification, they fail to exploit the inherent heterogeneity in linguistic\ncomplexity, a key factor leading to suboptimal resource allocation. We address\nthis by proposing HeteroSpec, a heterogeneity-adaptive speculative decoding\nframework that dynamically optimizes computational resource allocation based on\nlinguistic context complexity. HeteroSpec introduces two key mechanisms: (1) A\nnovel cumulative meta-path Top-$K$ entropy metric for efficiently identifying\npredictable contexts. (2) A dynamic resource allocation strategy based on\ndata-driven entropy partitioning, enabling adaptive speculative expansion and\npruning tailored to local context difficulty. Evaluated on five public\nbenchmarks and four models, HeteroSpec achieves an average speedup of\n4.26$\\times$. It consistently outperforms state-of-the-art EAGLE-3 across\nspeedup rates, average acceptance length, and verification cost. Notably,\nHeteroSpec requires no draft model retraining, incurs minimal overhead, and is\northogonal to other acceleration techniques. It demonstrates enhanced\nacceleration with stronger draft models, establishing a new paradigm for\ncontext-aware LLM inference acceleration."
                },
                "authors": [
                    {
                        "name": "Siran Liu"
                    },
                    {
                        "name": "Yang Ye"
                    },
                    {
                        "name": "Qianchao Zhu"
                    },
                    {
                        "name": "Zheng Cao"
                    },
                    {
                        "name": "Yongchao He"
                    }
                ],
                "author_detail": {
                    "name": "Yongchao He"
                },
                "author": "Yongchao He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13254v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13254v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13252v1",
                "updated": "2025-05-19T15:35:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    35,
                    17,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T15:35:17Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    35,
                    17,
                    0,
                    139,
                    0
                ],
                "title": "Natural Language Planning via Coding and Inference Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language Planning via Coding and Inference Scaling"
                },
                "summary": "Real-life textual planning tasks such as meeting scheduling have posed much\nchallenge to LLMs especially when the complexity is high. While previous work\nprimarily studied auto-regressive generation of plans with closed-source\nmodels, we systematically evaluate both closed- and open-source models,\nincluding those that scales output length with complexity during inference, in\ngenerating programs, which are executed to output the plan. We consider not\nonly standard Python code, but also the code to a constraint satisfaction\nproblem solver. Despite the algorithmic nature of the task, we show that\nprogramming often but not always outperforms planning. Our detailed error\nanalysis also indicates a lack of robustness and efficiency in the generated\ncode that hinders generalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-life textual planning tasks such as meeting scheduling have posed much\nchallenge to LLMs especially when the complexity is high. While previous work\nprimarily studied auto-regressive generation of plans with closed-source\nmodels, we systematically evaluate both closed- and open-source models,\nincluding those that scales output length with complexity during inference, in\ngenerating programs, which are executed to output the plan. We consider not\nonly standard Python code, but also the code to a constraint satisfaction\nproblem solver. Despite the algorithmic nature of the task, we show that\nprogramming often but not always outperforms planning. Our detailed error\nanalysis also indicates a lack of robustness and efficiency in the generated\ncode that hinders generalization."
                },
                "authors": [
                    {
                        "name": "Rikhil Amonkar"
                    },
                    {
                        "name": "Ronan Le Bras"
                    },
                    {
                        "name": "Li Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Li Zhang"
                },
                "author": "Li Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13249v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13249v1",
                "updated": "2025-05-19T15:32:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    32,
                    49,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T15:32:49Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    32,
                    49,
                    0,
                    139,
                    0
                ],
                "title": "RN-F: A Novel Approach for Mitigating Contaminated Data in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RN-F: A Novel Approach for Mitigating Contaminated Data in Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) have become foundational in modern artificial\nintelligence, powering a wide range of applications from code generation and\nvirtual assistants to scientific research and enterprise automation. However,\nconcerns about data contamination--where test data overlaps with training\ndata--have raised serious questions about the reliability of these\napplications. Despite awareness of this issue, existing methods fall short in\neffectively identifying or mitigating contamination. In this paper, we propose\nResidual-Noise Fingerprinting (RN-F), a novel framework for detecting\ncontaminated data in LLMs. RN-F is a single-pass, gradient-free detection\nmethod that leverages residual signal patterns without introducing additional\nfloating-point operations. Our approach is lightweight, model-agnostic, and\nefficient. We evaluate RN-F on multiple LLMs across various contaminated\ndatasets and show that it consistently outperforms existing state-of-the-art\nmethods, achieving performance improvements of up to 10.5% in contamination\ndetection metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become foundational in modern artificial\nintelligence, powering a wide range of applications from code generation and\nvirtual assistants to scientific research and enterprise automation. However,\nconcerns about data contamination--where test data overlaps with training\ndata--have raised serious questions about the reliability of these\napplications. Despite awareness of this issue, existing methods fall short in\neffectively identifying or mitigating contamination. In this paper, we propose\nResidual-Noise Fingerprinting (RN-F), a novel framework for detecting\ncontaminated data in LLMs. RN-F is a single-pass, gradient-free detection\nmethod that leverages residual signal patterns without introducing additional\nfloating-point operations. Our approach is lightweight, model-agnostic, and\nefficient. We evaluate RN-F on multiple LLMs across various contaminated\ndatasets and show that it consistently outperforms existing state-of-the-art\nmethods, achieving performance improvements of up to 10.5% in contamination\ndetection metrics."
                },
                "authors": [
                    {
                        "name": "Le Vu Anh"
                    },
                    {
                        "name": "Dinh Duc Nha Nguyen"
                    },
                    {
                        "name": "Phi Long Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Phi Long Nguyen"
                },
                "author": "Phi Long Nguyen",
                "arxiv_comment": "12 pages, 4 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13249v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13249v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05687v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05687v3",
                "updated": "2025-05-19T15:31:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    31,
                    45,
                    0,
                    139,
                    0
                ],
                "published": "2024-06-09T08:17:13Z",
                "published_parsed": [
                    2024,
                    6,
                    9,
                    8,
                    17,
                    13,
                    6,
                    161,
                    0
                ],
                "title": "FlightBench: Benchmarking Learning-based Methods for Ego-vision-based\n  Quadrotors Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlightBench: Benchmarking Learning-based Methods for Ego-vision-based\n  Quadrotors Navigation"
                },
                "summary": "Ego-vision-based navigation in cluttered environments is crucial for mobile\nsystems, particularly agile quadrotors. While learning-based methods have shown\npromise recently, head-to-head comparisons with cutting-edge optimization-based\napproaches are scarce, leaving open the question of where and to what extent\nthey truly excel. In this paper, we introduce FlightBench, the first\ncomprehensive benchmark that implements various learning-based methods for\nego-vision-based navigation and evaluates them against mainstream\noptimization-based baselines using a broad set of performance metrics. More\nimportantly, we develop a suite of criteria to assess scenario difficulty and\ndesign test cases that span different levels of difficulty based on these\ncriteria. Our results show that while learning-based methods excel in\nhigh-speed flight and faster inference, they struggle with challenging\nscenarios like sharp corners or view occlusion. Analytical experiments validate\nthe correlation between our difficulty criteria and flight performance.\nMoreover, we verify the trend in flight performance within real-world\nenvironments through full-pipeline and hardware-in-the-loop experiments. We\nhope this benchmark and these criteria will drive future advancements in\nlearning-based navigation for ego-vision quadrotors. Code and documentation are\navailable at https://github.com/thu-uav/FlightBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ego-vision-based navigation in cluttered environments is crucial for mobile\nsystems, particularly agile quadrotors. While learning-based methods have shown\npromise recently, head-to-head comparisons with cutting-edge optimization-based\napproaches are scarce, leaving open the question of where and to what extent\nthey truly excel. In this paper, we introduce FlightBench, the first\ncomprehensive benchmark that implements various learning-based methods for\nego-vision-based navigation and evaluates them against mainstream\noptimization-based baselines using a broad set of performance metrics. More\nimportantly, we develop a suite of criteria to assess scenario difficulty and\ndesign test cases that span different levels of difficulty based on these\ncriteria. Our results show that while learning-based methods excel in\nhigh-speed flight and faster inference, they struggle with challenging\nscenarios like sharp corners or view occlusion. Analytical experiments validate\nthe correlation between our difficulty criteria and flight performance.\nMoreover, we verify the trend in flight performance within real-world\nenvironments through full-pipeline and hardware-in-the-loop experiments. We\nhope this benchmark and these criteria will drive future advancements in\nlearning-based navigation for ego-vision quadrotors. Code and documentation are\navailable at https://github.com/thu-uav/FlightBench."
                },
                "authors": [
                    {
                        "name": "Shu-Ang Yu"
                    },
                    {
                        "name": "Chao Yu"
                    },
                    {
                        "name": "Feng Gao"
                    },
                    {
                        "name": "Yi Wu"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "arxiv_comment": "The first three authors contribute equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05687v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05687v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13246v1",
                "updated": "2025-05-19T15:28:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    28,
                    10,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T15:28:10Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    28,
                    10,
                    0,
                    139,
                    0
                ],
                "title": "Agentic Publications: An LLM-Driven Framework for Interactive Scientific\n  Publishing, Supplementing Traditional Papers with AI-Powered Knowledge\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic Publications: An LLM-Driven Framework for Interactive Scientific\n  Publishing, Supplementing Traditional Papers with AI-Powered Knowledge\n  Systems"
                },
                "summary": "The exponential growth of scientific literature presents significant\nchallenges for researchers navigating the complex knowledge landscape. We\npropose \"Agentic Publications\", a novel LLM-driven framework complementing\ntraditional publishing by transforming papers into interactive knowledge\nsystems. Our architecture integrates structured data with unstructured content\nthrough retrieval-augmented generation and multi-agent verification. The\nframework offers interfaces for both humans and machines, combining narrative\nexplanations with machine-readable outputs while addressing ethical\nconsiderations through automated validation and transparent governance. Key\nfeatures include continuous knowledge updates, automatic integration of new\nfindings, and customizable detail levels. Our proof-of-concept demonstrates\nmultilingual interaction, API accessibility, and structured knowledge\nrepresentation through vector databases, knowledge graphs, and verification\nagents. This approach enhances scientific communication across disciplines,\nimproving efficiency and collaboration while preserving traditional publishing\npathways, particularly valuable for interdisciplinary fields where knowledge\nintegration remains challenging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential growth of scientific literature presents significant\nchallenges for researchers navigating the complex knowledge landscape. We\npropose \"Agentic Publications\", a novel LLM-driven framework complementing\ntraditional publishing by transforming papers into interactive knowledge\nsystems. Our architecture integrates structured data with unstructured content\nthrough retrieval-augmented generation and multi-agent verification. The\nframework offers interfaces for both humans and machines, combining narrative\nexplanations with machine-readable outputs while addressing ethical\nconsiderations through automated validation and transparent governance. Key\nfeatures include continuous knowledge updates, automatic integration of new\nfindings, and customizable detail levels. Our proof-of-concept demonstrates\nmultilingual interaction, API accessibility, and structured knowledge\nrepresentation through vector databases, knowledge graphs, and verification\nagents. This approach enhances scientific communication across disciplines,\nimproving efficiency and collaboration while preserving traditional publishing\npathways, particularly valuable for interdisciplinary fields where knowledge\nintegration remains challenging."
                },
                "authors": [
                    {
                        "name": "Roberto Pugliese"
                    },
                    {
                        "name": "George Kourousias"
                    },
                    {
                        "name": "Francesco Venier"
                    },
                    {
                        "name": "Grazia Garlatti Costa"
                    }
                ],
                "author_detail": {
                    "name": "Grazia Garlatti Costa"
                },
                "author": "Grazia Garlatti Costa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19722v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19722v2",
                "updated": "2025-05-19T15:26:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    26,
                    21,
                    0,
                    139,
                    0
                ],
                "published": "2024-11-29T14:14:59Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    14,
                    14,
                    59,
                    4,
                    334,
                    0
                ],
                "title": "JetFormer: An Autoregressive Generative Model of Raw Images and Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JetFormer: An Autoregressive Generative Model of Raw Images and Text"
                },
                "summary": "Removing modeling constraints and unifying architectures across domains has\nbeen a key driver of the recent progress in training large multimodal models.\nHowever, most of these models still rely on many separately trained components\nsuch as modality-specific encoders and decoders. In this work, we further\nstreamline joint generative modeling of images and text. We propose an\nautoregressive decoder-only transformer - JetFormer - which is trained to\ndirectly maximize the likelihood of raw data, without relying on any separately\npretrained components, and can understand and generate both text and images.\nSpecifically, we leverage a normalizing flow model to obtain a soft-token image\nrepresentation that is jointly trained with an autoregressive multimodal\ntransformer. The normalizing flow model serves as both an image encoder for\nperception tasks and an image decoder for image generation tasks during\ninference. JetFormer achieves text-to-image generation quality competitive with\nrecent VQ-VAE- and VAE-based baselines. These baselines rely on pretrained\nimage autoencoders, which are trained with a complex mixture of losses,\nincluding perceptual ones. At the same time, JetFormer demonstrates robust\nimage understanding capabilities. To the best of our knowledge, JetFormer is\nthe first model that is capable of generating high-fidelity images and\nproducing strong log-likelihood bounds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Removing modeling constraints and unifying architectures across domains has\nbeen a key driver of the recent progress in training large multimodal models.\nHowever, most of these models still rely on many separately trained components\nsuch as modality-specific encoders and decoders. In this work, we further\nstreamline joint generative modeling of images and text. We propose an\nautoregressive decoder-only transformer - JetFormer - which is trained to\ndirectly maximize the likelihood of raw data, without relying on any separately\npretrained components, and can understand and generate both text and images.\nSpecifically, we leverage a normalizing flow model to obtain a soft-token image\nrepresentation that is jointly trained with an autoregressive multimodal\ntransformer. The normalizing flow model serves as both an image encoder for\nperception tasks and an image decoder for image generation tasks during\ninference. JetFormer achieves text-to-image generation quality competitive with\nrecent VQ-VAE- and VAE-based baselines. These baselines rely on pretrained\nimage autoencoders, which are trained with a complex mixture of losses,\nincluding perceptual ones. At the same time, JetFormer demonstrates robust\nimage understanding capabilities. To the best of our knowledge, JetFormer is\nthe first model that is capable of generating high-fidelity images and\nproducing strong log-likelihood bounds."
                },
                "authors": [
                    {
                        "name": "Michael Tschannen"
                    },
                    {
                        "name": "André Susano Pinto"
                    },
                    {
                        "name": "Alexander Kolesnikov"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Kolesnikov"
                },
                "author": "Alexander Kolesnikov",
                "arxiv_comment": "ICLR 2025. Code available at\n  https://github.com/google-research/big_vision",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19722v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19722v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13244v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13244v1",
                "updated": "2025-05-19T15:24:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    24,
                    53,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T15:24:53Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    24,
                    53,
                    0,
                    139,
                    0
                ],
                "title": "JNLP at SemEval-2025 Task 11: Cross-Lingual Multi-Label Emotion\n  Detection Using Generative Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JNLP at SemEval-2025 Task 11: Cross-Lingual Multi-Label Emotion\n  Detection Using Generative Models"
                },
                "summary": "With the rapid advancement of global digitalization, users from different\ncountries increasingly rely on social media for information exchange. In this\ncontext, multilingual multi-label emotion detection has emerged as a critical\nresearch area. This study addresses SemEval-2025 Task 11: Bridging the Gap in\nText-Based Emotion Detection. Our paper focuses on two sub-tracks of this task:\n(1) Track A: Multi-label emotion detection, and (2) Track B: Emotion intensity.\nTo tackle multilingual challenges, we leverage pre-trained multilingual models\nand focus on two architectures: (1) a fine-tuned BERT-based classification\nmodel and (2) an instruction-tuned generative LLM. Additionally, we propose two\nmethods for handling multi-label classification: the base method, which maps an\ninput directly to all its corresponding emotion labels, and the pairwise\nmethod, which models the relationship between the input text and each emotion\ncategory individually. Experimental results demonstrate the strong\ngeneralization ability of our approach in multilingual emotion recognition. In\nTrack A, our method achieved Top 4 performance across 10 languages, ranking 1st\nin Hindi. In Track B, our approach also secured Top 5 performance in 7\nlanguages, highlighting its simplicity and effectiveness\\footnote{Our code is\navailable at https://github.com/yingjie7/mlingual_multilabel_emo_detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of global digitalization, users from different\ncountries increasingly rely on social media for information exchange. In this\ncontext, multilingual multi-label emotion detection has emerged as a critical\nresearch area. This study addresses SemEval-2025 Task 11: Bridging the Gap in\nText-Based Emotion Detection. Our paper focuses on two sub-tracks of this task:\n(1) Track A: Multi-label emotion detection, and (2) Track B: Emotion intensity.\nTo tackle multilingual challenges, we leverage pre-trained multilingual models\nand focus on two architectures: (1) a fine-tuned BERT-based classification\nmodel and (2) an instruction-tuned generative LLM. Additionally, we propose two\nmethods for handling multi-label classification: the base method, which maps an\ninput directly to all its corresponding emotion labels, and the pairwise\nmethod, which models the relationship between the input text and each emotion\ncategory individually. Experimental results demonstrate the strong\ngeneralization ability of our approach in multilingual emotion recognition. In\nTrack A, our method achieved Top 4 performance across 10 languages, ranking 1st\nin Hindi. In Track B, our approach also secured Top 5 performance in 7\nlanguages, highlighting its simplicity and effectiveness\\footnote{Our code is\navailable at https://github.com/yingjie7/mlingual_multilabel_emo_detection."
                },
                "authors": [
                    {
                        "name": "Jieying Xue"
                    },
                    {
                        "name": "Phuong Minh Nguyen"
                    },
                    {
                        "name": "Minh Le Nguyen"
                    },
                    {
                        "name": "Xin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xin Liu"
                },
                "author": "Xin Liu",
                "arxiv_comment": "Published in The 19th International Workshop on Semantic Evaluation\n  (SemEval-2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13244v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13244v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05677v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05677v2",
                "updated": "2025-05-19T15:19:03Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    19,
                    3,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-08T22:27:38Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    22,
                    27,
                    38,
                    3,
                    128,
                    0
                ],
                "title": "Conditional Front-door Adjustment for Heterogeneous Treatment Assignment\n  Effect Estimation Under Non-adherence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conditional Front-door Adjustment for Heterogeneous Treatment Assignment\n  Effect Estimation Under Non-adherence"
                },
                "summary": "Estimates of heterogeneous treatment assignment effects can inform treatment\ndecisions. Under the presence of non-adherence (e.g., patients do not adhere to\ntheir assigned treatment), both the standard backdoor adjustment (SBD) and the\nconditional front-door adjustment (CFD) can recover unbiased estimates of the\ntreatment assignment effects. However, the estimation variance of these\napproaches may vary widely across settings, which remains underexplored in the\nliterature. In this work, we demonstrate theoretically and empirically that CFD\nyields lower-variance estimates than SBD when the true effect of treatment\nassignment is small (i.e., assigning an intervention leads to small changes in\npatients' future outcome). Additionally, since CFD requires estimating multiple\nnuisance parameters, we introduce LobsterNet, a multi-task neural network that\nimplements CFD with joint modeling of the nuisance parameters. Empirically,\nLobsterNet reduces estimation error across several semi-synthetic and\nreal-world datasets compared to baselines. Our findings suggest CFD with shared\nnuisance parameter modeling can improve treatment assignment effect estimation\nunder non-adherence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimates of heterogeneous treatment assignment effects can inform treatment\ndecisions. Under the presence of non-adherence (e.g., patients do not adhere to\ntheir assigned treatment), both the standard backdoor adjustment (SBD) and the\nconditional front-door adjustment (CFD) can recover unbiased estimates of the\ntreatment assignment effects. However, the estimation variance of these\napproaches may vary widely across settings, which remains underexplored in the\nliterature. In this work, we demonstrate theoretically and empirically that CFD\nyields lower-variance estimates than SBD when the true effect of treatment\nassignment is small (i.e., assigning an intervention leads to small changes in\npatients' future outcome). Additionally, since CFD requires estimating multiple\nnuisance parameters, we introduce LobsterNet, a multi-task neural network that\nimplements CFD with joint modeling of the nuisance parameters. Empirically,\nLobsterNet reduces estimation error across several semi-synthetic and\nreal-world datasets compared to baselines. Our findings suggest CFD with shared\nnuisance parameter modeling can improve treatment assignment effect estimation\nunder non-adherence."
                },
                "authors": [
                    {
                        "name": "Winston Chen"
                    },
                    {
                        "name": "Trenton Chang"
                    },
                    {
                        "name": "Jenna Wiens"
                    }
                ],
                "author_detail": {
                    "name": "Jenna Wiens"
                },
                "author": "Jenna Wiens",
                "arxiv_comment": "Accepted by Conference on Health, Inference, and Learning (CHIL) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05677v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05677v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17061v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17061v2",
                "updated": "2025-05-19T15:18:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    18,
                    10,
                    0,
                    139,
                    0
                ],
                "published": "2024-12-22T15:16:44Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    15,
                    16,
                    44,
                    6,
                    357,
                    0
                ],
                "title": "Multi-Agent Sampling: Scaling Inference Compute for Data Synthesis with\n  Tree Search-Based Agentic Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Sampling: Scaling Inference Compute for Data Synthesis with\n  Tree Search-Based Agentic Collaboration"
                },
                "summary": "Scaling laws for inference compute in multi-agent systems remain\nunder-explored compared to single-agent scenarios. This work aims to bridge\nthis gap by investigating the problem of data synthesis through multi-agent\nsampling, where synthetic responses are generated by sampling from multiple\ndistinct language models. Effective model coordination is crucial for\nsuccessful multi-agent collaboration. Unlike previous approaches that rely on\nfixed workflows, we treat model coordination as a multi-step decision-making\nprocess, optimizing generation structures dynamically for each input question.\nWe introduce Tree Search-based Orchestrated Agents~(TOA), where the workflow\nevolves iteratively during the sequential sampling process. To achieve this, we\nleverage Monte Carlo Tree Search (MCTS), integrating a reward model to provide\nreal-time feedback and accelerate exploration. Our experiments on alignment,\nmachine translation, and mathematical reasoning demonstrate that multi-agent\nsampling significantly outperforms single-agent sampling as inference compute\nscales. TOA is the most compute-efficient approach, achieving SOTA performance\non WMT and a 72.2\\% LC win rate on AlpacaEval. Moreover, fine-tuning with our\nsynthesized alignment data surpasses strong preference learning methods on\nchallenging benchmarks such as Arena-Hard and AlpacaEval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling laws for inference compute in multi-agent systems remain\nunder-explored compared to single-agent scenarios. This work aims to bridge\nthis gap by investigating the problem of data synthesis through multi-agent\nsampling, where synthetic responses are generated by sampling from multiple\ndistinct language models. Effective model coordination is crucial for\nsuccessful multi-agent collaboration. Unlike previous approaches that rely on\nfixed workflows, we treat model coordination as a multi-step decision-making\nprocess, optimizing generation structures dynamically for each input question.\nWe introduce Tree Search-based Orchestrated Agents~(TOA), where the workflow\nevolves iteratively during the sequential sampling process. To achieve this, we\nleverage Monte Carlo Tree Search (MCTS), integrating a reward model to provide\nreal-time feedback and accelerate exploration. Our experiments on alignment,\nmachine translation, and mathematical reasoning demonstrate that multi-agent\nsampling significantly outperforms single-agent sampling as inference compute\nscales. TOA is the most compute-efficient approach, achieving SOTA performance\non WMT and a 72.2\\% LC win rate on AlpacaEval. Moreover, fine-tuning with our\nsynthesized alignment data surpasses strong preference learning methods on\nchallenging benchmarks such as Arena-Hard and AlpacaEval."
                },
                "authors": [
                    {
                        "name": "Hai Ye"
                    },
                    {
                        "name": "Mingbao Lin"
                    },
                    {
                        "name": "Hwee Tou Ng"
                    },
                    {
                        "name": "Shuicheng Yan"
                    }
                ],
                "author_detail": {
                    "name": "Shuicheng Yan"
                },
                "author": "Shuicheng Yan",
                "arxiv_comment": "In submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17061v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17061v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08638v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08638v2",
                "updated": "2025-05-19T15:15:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    15,
                    46,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-13T14:55:31Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    14,
                    55,
                    31,
                    1,
                    133,
                    0
                ],
                "title": "TRAIL: Trace Reasoning and Agentic Issue Localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRAIL: Trace Reasoning and Agentic Issue Localization"
                },
                "summary": "The increasing adoption of agentic workflows across diverse domains brings a\ncritical need to scalably and systematically evaluate the complex traces these\nsystems generate. Current evaluation methods depend on manual, domain-specific\nhuman analysis of lengthy workflow traces - an approach that does not scale\nwith the growing complexity and volume of agentic outputs. Error analysis in\nthese settings is further complicated by the interplay of external tool outputs\nand language model reasoning, making it more challenging than traditional\nsoftware debugging. In this work, we (1) articulate the need for robust and\ndynamic evaluation methods for agentic workflow traces, (2) introduce a formal\ntaxonomy of error types encountered in agentic systems, and (3) present a set\nof 148 large human-annotated traces (TRAIL) constructed using this taxonomy and\ngrounded in established agentic benchmarks. To ensure ecological validity, we\ncurate traces from both single and multi-agent systems, focusing on real-world\napplications such as software engineering and open-world information retrieval.\nOur evaluations reveal that modern long context LLMs perform poorly at trace\ndebugging, with the best Gemini-2.5-pro model scoring a mere 11% on TRAIL. Our\ndataset and code are made publicly available to support and accelerate future\nresearch in scalable evaluation for agentic workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing adoption of agentic workflows across diverse domains brings a\ncritical need to scalably and systematically evaluate the complex traces these\nsystems generate. Current evaluation methods depend on manual, domain-specific\nhuman analysis of lengthy workflow traces - an approach that does not scale\nwith the growing complexity and volume of agentic outputs. Error analysis in\nthese settings is further complicated by the interplay of external tool outputs\nand language model reasoning, making it more challenging than traditional\nsoftware debugging. In this work, we (1) articulate the need for robust and\ndynamic evaluation methods for agentic workflow traces, (2) introduce a formal\ntaxonomy of error types encountered in agentic systems, and (3) present a set\nof 148 large human-annotated traces (TRAIL) constructed using this taxonomy and\ngrounded in established agentic benchmarks. To ensure ecological validity, we\ncurate traces from both single and multi-agent systems, focusing on real-world\napplications such as software engineering and open-world information retrieval.\nOur evaluations reveal that modern long context LLMs perform poorly at trace\ndebugging, with the best Gemini-2.5-pro model scoring a mere 11% on TRAIL. Our\ndataset and code are made publicly available to support and accelerate future\nresearch in scalable evaluation for agentic workflows."
                },
                "authors": [
                    {
                        "name": "Darshan Deshpande"
                    },
                    {
                        "name": "Varun Gangal"
                    },
                    {
                        "name": "Hersh Mehta"
                    },
                    {
                        "name": "Jitin Krishnan"
                    },
                    {
                        "name": "Anand Kannappan"
                    },
                    {
                        "name": "Rebecca Qian"
                    }
                ],
                "author_detail": {
                    "name": "Rebecca Qian"
                },
                "author": "Rebecca Qian",
                "arxiv_comment": "Dataset: https://huggingface.co/datasets/PatronusAI/TRAIL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08638v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08638v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13233v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13233v1",
                "updated": "2025-05-19T15:15:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    15,
                    37,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T15:15:37Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    15,
                    37,
                    0,
                    139,
                    0
                ],
                "title": "From Local Details to Global Context: Advancing Vision-Language Models\n  with Attention-Based Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Local Details to Global Context: Advancing Vision-Language Models\n  with Attention-Based Selection"
                },
                "summary": "Pretrained vision-language models (VLMs), e.g., CLIP, demonstrate impressive\nzero-shot capabilities on downstream tasks. Prior research highlights the\ncrucial role of visual augmentation techniques, like random cropping, in\nalignment with fine-grained class descriptions generated by large language\nmodels (LLMs), significantly enhancing zero-shot performance by incorporating\nmulti-view information. However, the inherent randomness of these augmentations\ncan inevitably introduce background artifacts and cause models to overly focus\non local details, compromising global semantic understanding. To address these\nissues, we propose an \\textbf{A}ttention-\\textbf{B}ased \\textbf{S}election\n(\\textbf{ABS}) method from local details to global context, which applies\nattention-guided cropping in both raw images and feature space, supplement\nglobal semantic information through strategic feature selection. Additionally,\nwe introduce a soft matching technique to effectively filter LLM descriptions\nfor better alignment. \\textbf{ABS} achieves state-of-the-art performance on\nout-of-distribution generalization and zero-shot classification tasks. Notably,\n\\textbf{ABS} is training-free and even rivals few-shot and test-time adaptation\nmethods. Our code is available at\n\\href{https://github.com/BIT-DA/ABS}{\\textcolor{darkgreen}{https://github.com/BIT-DA/ABS}}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pretrained vision-language models (VLMs), e.g., CLIP, demonstrate impressive\nzero-shot capabilities on downstream tasks. Prior research highlights the\ncrucial role of visual augmentation techniques, like random cropping, in\nalignment with fine-grained class descriptions generated by large language\nmodels (LLMs), significantly enhancing zero-shot performance by incorporating\nmulti-view information. However, the inherent randomness of these augmentations\ncan inevitably introduce background artifacts and cause models to overly focus\non local details, compromising global semantic understanding. To address these\nissues, we propose an \\textbf{A}ttention-\\textbf{B}ased \\textbf{S}election\n(\\textbf{ABS}) method from local details to global context, which applies\nattention-guided cropping in both raw images and feature space, supplement\nglobal semantic information through strategic feature selection. Additionally,\nwe introduce a soft matching technique to effectively filter LLM descriptions\nfor better alignment. \\textbf{ABS} achieves state-of-the-art performance on\nout-of-distribution generalization and zero-shot classification tasks. Notably,\n\\textbf{ABS} is training-free and even rivals few-shot and test-time adaptation\nmethods. Our code is available at\n\\href{https://github.com/BIT-DA/ABS}{\\textcolor{darkgreen}{https://github.com/BIT-DA/ABS}}."
                },
                "authors": [
                    {
                        "name": "Lincan Cai"
                    },
                    {
                        "name": "Jingxuan Kang"
                    },
                    {
                        "name": "Shuang Li"
                    },
                    {
                        "name": "Wenxuan Ma"
                    },
                    {
                        "name": "Binhui Xie"
                    },
                    {
                        "name": "Zhida Qin"
                    },
                    {
                        "name": "Jian Liang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Liang"
                },
                "author": "Jian Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13233v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13233v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05265v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05265v3",
                "updated": "2025-05-19T15:12:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    12,
                    39,
                    0,
                    139,
                    0
                ],
                "published": "2024-12-06T18:53:49Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    18,
                    53,
                    49,
                    4,
                    341,
                    0
                ],
                "title": "Reinforcement Learning: An Overview",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning: An Overview"
                },
                "summary": "This manuscript gives a big-picture, up-to-date overview of the field of\n(deep) reinforcement learning and sequential decision making, covering\nvalue-based methods, policy-based methods, model-based methods, multi-agent RL,\nLLMs and RL, and various other topics (e.g., offline RL, hierarchical RL,\nintrinsic reward).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This manuscript gives a big-picture, up-to-date overview of the field of\n(deep) reinforcement learning and sequential decision making, covering\nvalue-based methods, policy-based methods, model-based methods, multi-agent RL,\nLLMs and RL, and various other topics (e.g., offline RL, hierarchical RL,\nintrinsic reward)."
                },
                "authors": [
                    {
                        "name": "Kevin Murphy"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Murphy"
                },
                "author": "Kevin Murphy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05265v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05265v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13220v1",
                "updated": "2025-05-19T15:02:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    2,
                    59,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T15:02:59Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    2,
                    59,
                    0,
                    139,
                    0
                ],
                "title": "SeedBench: A Multi-task Benchmark for Evaluating Large Language Models\n  in Seed Science",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SeedBench: A Multi-task Benchmark for Evaluating Large Language Models\n  in Seed Science"
                },
                "summary": "Seed science is essential for modern agriculture, directly influencing crop\nyields and global food security. However, challenges such as interdisciplinary\ncomplexity and high costs with limited returns hinder progress, leading to a\nshortage of experts and insufficient technological support. While large\nlanguage models (LLMs) have shown promise across various fields, their\napplication in seed science remains limited due to the scarcity of digital\nresources, complex gene-trait relationships, and the lack of standardized\nbenchmarks. To address this gap, we introduce SeedBench -- the first multi-task\nbenchmark specifically designed for seed science. Developed in collaboration\nwith domain experts, SeedBench focuses on seed breeding and simulates key\naspects of modern breeding processes. We conduct a comprehensive evaluation of\n26 leading LLMs, encompassing proprietary, open-source, and domain-specific\nfine-tuned models. Our findings not only highlight the substantial gaps between\nthe power of LLMs and the real-world seed science problems, but also make a\nfoundational step for research on LLMs for seed design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seed science is essential for modern agriculture, directly influencing crop\nyields and global food security. However, challenges such as interdisciplinary\ncomplexity and high costs with limited returns hinder progress, leading to a\nshortage of experts and insufficient technological support. While large\nlanguage models (LLMs) have shown promise across various fields, their\napplication in seed science remains limited due to the scarcity of digital\nresources, complex gene-trait relationships, and the lack of standardized\nbenchmarks. To address this gap, we introduce SeedBench -- the first multi-task\nbenchmark specifically designed for seed science. Developed in collaboration\nwith domain experts, SeedBench focuses on seed breeding and simulates key\naspects of modern breeding processes. We conduct a comprehensive evaluation of\n26 leading LLMs, encompassing proprietary, open-source, and domain-specific\nfine-tuned models. Our findings not only highlight the substantial gaps between\nthe power of LLMs and the real-world seed science problems, but also make a\nfoundational step for research on LLMs for seed design."
                },
                "authors": [
                    {
                        "name": "Jie Ying"
                    },
                    {
                        "name": "Zihong Chen"
                    },
                    {
                        "name": "Zhefan Wang"
                    },
                    {
                        "name": "Wanli Jiang"
                    },
                    {
                        "name": "Chenyang Wang"
                    },
                    {
                        "name": "Zhonghang Yuan"
                    },
                    {
                        "name": "Haoyang Su"
                    },
                    {
                        "name": "Huanjun Kong"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Nanqing Dong"
                    }
                ],
                "author_detail": {
                    "name": "Nanqing Dong"
                },
                "author": "Nanqing Dong",
                "arxiv_comment": "Accepted by ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13211v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13211v1",
                "updated": "2025-05-19T14:58:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    58,
                    50,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T14:58:50Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    58,
                    50,
                    0,
                    139,
                    0
                ],
                "title": "MAGI-1: Autoregressive Video Generation at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAGI-1: Autoregressive Video Generation at Scale"
                },
                "summary": "We present MAGI-1, a world model that generates videos by autoregressively\npredicting a sequence of video chunks, defined as fixed-length segments of\nconsecutive frames. Trained to denoise per-chunk noise that increases\nmonotonically over time, MAGI-1 enables causal temporal modeling and naturally\nsupports streaming generation. It achieves strong performance on image-to-video\n(I2V) tasks conditioned on text instructions, providing high temporal\nconsistency and scalability, which are made possible by several algorithmic\ninnovations and a dedicated infrastructure stack. MAGI-1 facilitates\ncontrollable generation via chunk-wise prompting and supports real-time,\nmemory-efficient deployment by maintaining constant peak inference cost,\nregardless of video length. The largest variant of MAGI-1 comprises 24 billion\nparameters and supports context lengths of up to 4 million tokens,\ndemonstrating the scalability and robustness of our approach. The code and\nmodels are available at https://github.com/SandAI-org/MAGI-1 and\nhttps://github.com/SandAI-org/MagiAttention. The product can be accessed at\nhttps://sand.ai.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present MAGI-1, a world model that generates videos by autoregressively\npredicting a sequence of video chunks, defined as fixed-length segments of\nconsecutive frames. Trained to denoise per-chunk noise that increases\nmonotonically over time, MAGI-1 enables causal temporal modeling and naturally\nsupports streaming generation. It achieves strong performance on image-to-video\n(I2V) tasks conditioned on text instructions, providing high temporal\nconsistency and scalability, which are made possible by several algorithmic\ninnovations and a dedicated infrastructure stack. MAGI-1 facilitates\ncontrollable generation via chunk-wise prompting and supports real-time,\nmemory-efficient deployment by maintaining constant peak inference cost,\nregardless of video length. The largest variant of MAGI-1 comprises 24 billion\nparameters and supports context lengths of up to 4 million tokens,\ndemonstrating the scalability and robustness of our approach. The code and\nmodels are available at https://github.com/SandAI-org/MAGI-1 and\nhttps://github.com/SandAI-org/MagiAttention. The product can be accessed at\nhttps://sand.ai."
                },
                "authors": [
                    {
                        "name": "Sand. ai"
                    },
                    {
                        "name": "Hansi Teng"
                    },
                    {
                        "name": "Hongyu Jia"
                    },
                    {
                        "name": "Lei Sun"
                    },
                    {
                        "name": "Lingzhi Li"
                    },
                    {
                        "name": "Maolin Li"
                    },
                    {
                        "name": "Mingqiu Tang"
                    },
                    {
                        "name": "Shuai Han"
                    },
                    {
                        "name": "Tianning Zhang"
                    },
                    {
                        "name": "W. Q. Zhang"
                    },
                    {
                        "name": "Weifeng Luo"
                    },
                    {
                        "name": "Xiaoyang Kang"
                    },
                    {
                        "name": "Yuchen Sun"
                    },
                    {
                        "name": "Yue Cao"
                    },
                    {
                        "name": "Yunpeng Huang"
                    },
                    {
                        "name": "Yutong Lin"
                    },
                    {
                        "name": "Yuxin Fang"
                    },
                    {
                        "name": "Zewei Tao"
                    },
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Zhongshu Wang"
                    },
                    {
                        "name": "Zixun Liu"
                    },
                    {
                        "name": "Dai Shi"
                    },
                    {
                        "name": "Guoli Su"
                    },
                    {
                        "name": "Hanwen Sun"
                    },
                    {
                        "name": "Hong Pan"
                    },
                    {
                        "name": "Jie Wang"
                    },
                    {
                        "name": "Jiexin Sheng"
                    },
                    {
                        "name": "Min Cui"
                    },
                    {
                        "name": "Min Hu"
                    },
                    {
                        "name": "Ming Yan"
                    },
                    {
                        "name": "Shucheng Yin"
                    },
                    {
                        "name": "Siran Zhang"
                    },
                    {
                        "name": "Tingting Liu"
                    },
                    {
                        "name": "Xianping Yin"
                    },
                    {
                        "name": "Xiaoyu Yang"
                    },
                    {
                        "name": "Xin Song"
                    },
                    {
                        "name": "Xuan Hu"
                    },
                    {
                        "name": "Yankai Zhang"
                    },
                    {
                        "name": "Yuqiao Li"
                    }
                ],
                "author_detail": {
                    "name": "Yuqiao Li"
                },
                "author": "Yuqiao Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13211v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13211v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13210v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13210v1",
                "updated": "2025-05-19T14:58:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    58,
                    44,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T14:58:44Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    58,
                    44,
                    0,
                    139,
                    0
                ],
                "title": "Picturized and Recited with Dialects: A Multimodal Chinese\n  Representation Framework for Sentiment Analysis of Classical Chinese Poetry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Picturized and Recited with Dialects: A Multimodal Chinese\n  Representation Framework for Sentiment Analysis of Classical Chinese Poetry"
                },
                "summary": "Classical Chinese poetry is a vital and enduring part of Chinese literature,\nconveying profound emotional resonance. Existing studies analyze sentiment\nbased on textual meanings, overlooking the unique rhythmic and visual features\ninherent in poetry,especially since it is often recited and accompanied by\nChinese paintings. In this work, we propose a dialect-enhanced multimodal\nframework for classical Chinese poetry sentiment analysis. We extract\nsentence-level audio features from the poetry and incorporate audio from\nmultiple dialects,which may retain regional ancient Chinese phonetic features,\nenriching the phonetic representation. Additionally, we generate sentence-level\nvisual features, and the multimodal features are fused with textual features\nenhanced by LLM translation through multimodal contrastive representation\nlearning. Our framework outperforms state-of-the-art methods on two public\ndatasets, achieving at least 2.51% improvement in accuracy and 1.63% in macro\nF1. We open-source the code to facilitate research in this area and provide\ninsights for general multimodal Chinese representation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Classical Chinese poetry is a vital and enduring part of Chinese literature,\nconveying profound emotional resonance. Existing studies analyze sentiment\nbased on textual meanings, overlooking the unique rhythmic and visual features\ninherent in poetry,especially since it is often recited and accompanied by\nChinese paintings. In this work, we propose a dialect-enhanced multimodal\nframework for classical Chinese poetry sentiment analysis. We extract\nsentence-level audio features from the poetry and incorporate audio from\nmultiple dialects,which may retain regional ancient Chinese phonetic features,\nenriching the phonetic representation. Additionally, we generate sentence-level\nvisual features, and the multimodal features are fused with textual features\nenhanced by LLM translation through multimodal contrastive representation\nlearning. Our framework outperforms state-of-the-art methods on two public\ndatasets, achieving at least 2.51% improvement in accuracy and 1.63% in macro\nF1. We open-source the code to facilitate research in this area and provide\ninsights for general multimodal Chinese representation."
                },
                "authors": [
                    {
                        "name": "Xiaocong Du"
                    },
                    {
                        "name": "Haoyu Pei"
                    },
                    {
                        "name": "Haipeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Haipeng Zhang"
                },
                "author": "Haipeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13210v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13210v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13206v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13206v1",
                "updated": "2025-05-19T14:57:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    57,
                    12,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T14:57:12Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    57,
                    12,
                    0,
                    139,
                    0
                ],
                "title": "Rapidly Varying Completely Random Measures for Modeling Extremely Sparse\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapidly Varying Completely Random Measures for Modeling Extremely Sparse\n  Networks"
                },
                "summary": "Completely random measures (CRMs) are fundamental to Bayesian nonparametric\nmodels, with applications in clustering, feature allocation, and network\nanalysis. A key quantity of interest is the Laplace exponent, whose asymptotic\nbehavior determines how the random structures scale. When the Laplace exponent\ngrows nearly linearly - known as rapid variation - the induced models exhibit\napproximately linear growth in the number of clusters, features, or edges with\nsample size or network nodes. This regime is especially relevant for modeling\nsparse networks, yet existing CRM constructions lack tractability under rapid\nvariation. We address this by introducing a new class of CRMs with index of\nvariation $\\alpha\\in(0,1]$, defined as mixtures of stable or generalized gamma\nprocesses. These models offer interpretable parameters, include well-known CRMs\nas limiting cases, and retain analytical tractability through a tractable\nLaplace exponent and simple size-biased representation. We analyze the\nasymptotic properties of this CRM class and apply it to the Caron-Fox framework\nfor sparse graphs. The resulting models produce networks with near-linear edge\ngrowth, aligning with empirical evidence from large-scale networks.\nAdditionally, we present efficient algorithms for simulation and posterior\ninference, demonstrating practical advantages through experiments on real-world\nsparse network datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Completely random measures (CRMs) are fundamental to Bayesian nonparametric\nmodels, with applications in clustering, feature allocation, and network\nanalysis. A key quantity of interest is the Laplace exponent, whose asymptotic\nbehavior determines how the random structures scale. When the Laplace exponent\ngrows nearly linearly - known as rapid variation - the induced models exhibit\napproximately linear growth in the number of clusters, features, or edges with\nsample size or network nodes. This regime is especially relevant for modeling\nsparse networks, yet existing CRM constructions lack tractability under rapid\nvariation. We address this by introducing a new class of CRMs with index of\nvariation $\\alpha\\in(0,1]$, defined as mixtures of stable or generalized gamma\nprocesses. These models offer interpretable parameters, include well-known CRMs\nas limiting cases, and retain analytical tractability through a tractable\nLaplace exponent and simple size-biased representation. We analyze the\nasymptotic properties of this CRM class and apply it to the Caron-Fox framework\nfor sparse graphs. The resulting models produce networks with near-linear edge\ngrowth, aligning with empirical evidence from large-scale networks.\nAdditionally, we present efficient algorithms for simulation and posterior\ninference, demonstrating practical advantages through experiments on real-world\nsparse network datasets."
                },
                "authors": [
                    {
                        "name": "Valentin Kilian"
                    },
                    {
                        "name": "Benjamin Guedj"
                    },
                    {
                        "name": "François Caron"
                    }
                ],
                "author_detail": {
                    "name": "François Caron"
                },
                "author": "François Caron",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13206v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13206v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13205v1",
                "updated": "2025-05-19T14:56:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    56,
                    24,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T14:56:24Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    56,
                    24,
                    0,
                    139,
                    0
                ],
                "title": "Quantum Knowledge Distillation for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Knowledge Distillation for Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are integral to advancing natural language\nprocessing, used extensively from machine translation to content creation.\nHowever, as these models scale to billions of parameters, their resource\ndemands increase dramatically. Meanwhile, quantum computing is recognized for\nefficiently solving complex problems with quantum characteristics like\nsuperposition and entanglement, providing a novel approach to these challenges.\nThis paper attempts to combine quantum computing with LLMs and proposes a\nQuantum knowledge Distillation algorithm for LLMs (QD-LLM), aimed at reducing\nthe computational and memory overhead required for model loading and inference.\nSpecifically, during the distillation stage, data is fed simultaneously into\nboth the LLMs and the designed quantum student model to initially quantify the\ndifference between their outputs; subsequently, with the help of the true\nlabel, the optimization of the quantum student model is executed to minimize\nthe difference with the LLM's output. Throughout this process, only the\nparameters of the quantum student network are updated to make its output closer\nto that of the LLMs, thereby achieving the purpose of distillation. Finally,\nthe optimized student model obtained by QD-LLM can efficiently solve\ndomain-specific tasks during inference without the usage of the original LLMs.\nExperimental results show that, compared to mainstream compression methods,\nQD-LLM significantly reduces the number of training parameters, memory\nconsumption, training time, and inference time while maintaining performance.\nMoreover, the optimized student model obtained by QD-LLM surpasses specific\nmodels designed for these tasks. We believe that QD-LLM can lay the groundwork\nfor exploring the utilization of quantum computing in model compression and its\npotential extension to other natural language processing challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are integral to advancing natural language\nprocessing, used extensively from machine translation to content creation.\nHowever, as these models scale to billions of parameters, their resource\ndemands increase dramatically. Meanwhile, quantum computing is recognized for\nefficiently solving complex problems with quantum characteristics like\nsuperposition and entanglement, providing a novel approach to these challenges.\nThis paper attempts to combine quantum computing with LLMs and proposes a\nQuantum knowledge Distillation algorithm for LLMs (QD-LLM), aimed at reducing\nthe computational and memory overhead required for model loading and inference.\nSpecifically, during the distillation stage, data is fed simultaneously into\nboth the LLMs and the designed quantum student model to initially quantify the\ndifference between their outputs; subsequently, with the help of the true\nlabel, the optimization of the quantum student model is executed to minimize\nthe difference with the LLM's output. Throughout this process, only the\nparameters of the quantum student network are updated to make its output closer\nto that of the LLMs, thereby achieving the purpose of distillation. Finally,\nthe optimized student model obtained by QD-LLM can efficiently solve\ndomain-specific tasks during inference without the usage of the original LLMs.\nExperimental results show that, compared to mainstream compression methods,\nQD-LLM significantly reduces the number of training parameters, memory\nconsumption, training time, and inference time while maintaining performance.\nMoreover, the optimized student model obtained by QD-LLM surpasses specific\nmodels designed for these tasks. We believe that QD-LLM can lay the groundwork\nfor exploring the utilization of quantum computing in model compression and its\npotential extension to other natural language processing challenges."
                },
                "authors": [
                    {
                        "name": "Lingxiao Li"
                    },
                    {
                        "name": "Yihao Wang"
                    },
                    {
                        "name": "Jiacheng Fan"
                    },
                    {
                        "name": "Jing Li"
                    },
                    {
                        "name": "Sujuan Qin"
                    },
                    {
                        "name": "Qiaoyan Wen"
                    },
                    {
                        "name": "Fei Gao"
                    }
                ],
                "author_detail": {
                    "name": "Fei Gao"
                },
                "author": "Fei Gao",
                "arxiv_comment": "Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13204v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13204v1",
                "updated": "2025-05-19T14:55:41Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    55,
                    41,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T14:55:41Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    55,
                    41,
                    0,
                    139,
                    0
                ],
                "title": "Alignment-Augmented Speculative Decoding with Alignment Sampling and\n  Conditional Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alignment-Augmented Speculative Decoding with Alignment Sampling and\n  Conditional Verification"
                },
                "summary": "Recent works have revealed the great potential of speculative decoding in\naccelerating the autoregressive generation process of large language models.\nThe success of these methods relies on the alignment between draft candidates\nand the sampled outputs of the target model. Existing methods mainly achieve\ndraft-target alignment with training-based methods, e.g., EAGLE, Medusa,\ninvolving considerable training costs. In this paper, we present a\ntraining-free alignment-augmented speculative decoding algorithm. We propose\nalignment sampling, which leverages output distribution obtained in the\nprefilling phase to provide more aligned draft candidates. To further benefit\nfrom high-quality but non-aligned draft candidates, we also introduce a simple\nyet effective flexible verification strategy. Through an adaptive probability\nthreshold, our approach can improve generation accuracy while further improving\ninference efficiency. Experiments on 8 datasets (including question answering,\nsummarization and code completion tasks) show that our approach increases the\naverage generation score by 3.3 points for the LLaMA3 model. Our method\nachieves a mean acceptance length up to 2.39 and speed up generation by 2.23.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent works have revealed the great potential of speculative decoding in\naccelerating the autoregressive generation process of large language models.\nThe success of these methods relies on the alignment between draft candidates\nand the sampled outputs of the target model. Existing methods mainly achieve\ndraft-target alignment with training-based methods, e.g., EAGLE, Medusa,\ninvolving considerable training costs. In this paper, we present a\ntraining-free alignment-augmented speculative decoding algorithm. We propose\nalignment sampling, which leverages output distribution obtained in the\nprefilling phase to provide more aligned draft candidates. To further benefit\nfrom high-quality but non-aligned draft candidates, we also introduce a simple\nyet effective flexible verification strategy. Through an adaptive probability\nthreshold, our approach can improve generation accuracy while further improving\ninference efficiency. Experiments on 8 datasets (including question answering,\nsummarization and code completion tasks) show that our approach increases the\naverage generation score by 3.3 points for the LLaMA3 model. Our method\nachieves a mean acceptance length up to 2.39 and speed up generation by 2.23."
                },
                "authors": [
                    {
                        "name": "Jikai Wang"
                    },
                    {
                        "name": "Zhenxu Tian"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Qingrong Xia"
                    },
                    {
                        "name": "Xinyu Duan"
                    },
                    {
                        "name": "Zhefeng Wang"
                    },
                    {
                        "name": "Baoxing Huai"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "Pre-print",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13204v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13204v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13201v1",
                "updated": "2025-05-19T14:54:04Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    54,
                    4,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T14:54:04Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    54,
                    4,
                    0,
                    139,
                    0
                ],
                "title": "MatPredict: a dataset and benchmark for learning material properties of\n  diverse indoor objects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MatPredict: a dataset and benchmark for learning material properties of\n  diverse indoor objects"
                },
                "summary": "Determining material properties from camera images can expand the ability to\nidentify complex objects in indoor environments, which is valuable for consumer\nrobotics applications. To support this, we introduce MatPredict, a dataset that\ncombines the high-quality synthetic objects from Replica dataset with MatSynth\ndataset's material properties classes - to create objects with diverse material\nproperties. We select 3D meshes of specific foreground objects and render them\nwith different material properties. In total, we generate \\textbf{18} commonly\noccurring objects with \\textbf{14} different materials. We showcase how we\nprovide variability in terms of lighting and camera placement for these\nobjects. Next, we provide a benchmark for inferring material properties from\nvisual images using these perturbed models in the scene, discussing the\nspecific neural network models involved and their performance based on\ndifferent image comparison metrics. By accurately simulating light interactions\nwith different materials, we can enhance realism, which is crucial for training\nmodels effectively through large-scale simulations. This research aims to\nrevolutionize perception in consumer robotics. The dataset is provided\n\\href{https://huggingface.co/datasets/UMTRI/MatPredict}{here} and the code is\nprovided \\href{https://github.com/arpan-kusari/MatPredict}{here}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Determining material properties from camera images can expand the ability to\nidentify complex objects in indoor environments, which is valuable for consumer\nrobotics applications. To support this, we introduce MatPredict, a dataset that\ncombines the high-quality synthetic objects from Replica dataset with MatSynth\ndataset's material properties classes - to create objects with diverse material\nproperties. We select 3D meshes of specific foreground objects and render them\nwith different material properties. In total, we generate \\textbf{18} commonly\noccurring objects with \\textbf{14} different materials. We showcase how we\nprovide variability in terms of lighting and camera placement for these\nobjects. Next, we provide a benchmark for inferring material properties from\nvisual images using these perturbed models in the scene, discussing the\nspecific neural network models involved and their performance based on\ndifferent image comparison metrics. By accurately simulating light interactions\nwith different materials, we can enhance realism, which is crucial for training\nmodels effectively through large-scale simulations. This research aims to\nrevolutionize perception in consumer robotics. The dataset is provided\n\\href{https://huggingface.co/datasets/UMTRI/MatPredict}{here} and the code is\nprovided \\href{https://github.com/arpan-kusari/MatPredict}{here}."
                },
                "authors": [
                    {
                        "name": "Yuzhen Chen"
                    },
                    {
                        "name": "Hojun Son"
                    },
                    {
                        "name": "Arpan Kusari"
                    }
                ],
                "author_detail": {
                    "name": "Arpan Kusari"
                },
                "author": "Arpan Kusari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.04447v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.04447v3",
                "updated": "2025-05-19T14:53:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    53,
                    9,
                    0,
                    139,
                    0
                ],
                "published": "2024-06-06T19:05:53Z",
                "published_parsed": [
                    2024,
                    6,
                    6,
                    19,
                    5,
                    53,
                    3,
                    158,
                    0
                ],
                "title": "PbTe/SnTe heterostructures -- candidate platform for studying\n  spin-triplet superconductivity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PbTe/SnTe heterostructures -- candidate platform for studying\n  spin-triplet superconductivity"
                },
                "summary": "This paper explores the potential for spin-triplet superconductivity in\nmolecular beam epitaxy grown PbTe/SnTe semiconductor heterostructures. We\npresent convincing evidence for spin-triplet pairing by soft point-contact\nspectroscopy experiments, using both spin-polarized and unpolarized electrons\nand additionally, by detailed analysis of the upper critical field, as inferred\nfrom the four probe resistance measurements. The experimental data are\ndescribed in terms of the Anderson-Brinkman-Morel model of p-wave electron\npairing. Our results confirm the predictions on strain-induced topological\nsuperconductivity by E.Tang and L. Fu (Nature Physics, 10, 964, 2014).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the potential for spin-triplet superconductivity in\nmolecular beam epitaxy grown PbTe/SnTe semiconductor heterostructures. We\npresent convincing evidence for spin-triplet pairing by soft point-contact\nspectroscopy experiments, using both spin-polarized and unpolarized electrons\nand additionally, by detailed analysis of the upper critical field, as inferred\nfrom the four probe resistance measurements. The experimental data are\ndescribed in terms of the Anderson-Brinkman-Morel model of p-wave electron\npairing. Our results confirm the predictions on strain-induced topological\nsuperconductivity by E.Tang and L. Fu (Nature Physics, 10, 964, 2014)."
                },
                "authors": [
                    {
                        "name": "P. Sidorczak"
                    },
                    {
                        "name": "W. Wołkanowicz"
                    },
                    {
                        "name": "A. Kaleta"
                    },
                    {
                        "name": "M. Wójcik"
                    },
                    {
                        "name": "S. Gierałtowska"
                    },
                    {
                        "name": "K. Gas"
                    },
                    {
                        "name": "T. Płociński"
                    },
                    {
                        "name": "R. Minikayev"
                    },
                    {
                        "name": "S. Kret"
                    },
                    {
                        "name": "M. Sawicki"
                    },
                    {
                        "name": "T. Wojtowicz"
                    },
                    {
                        "name": "D. Wasik"
                    },
                    {
                        "name": "M. Gryglas-Borysiewicz"
                    },
                    {
                        "name": "K. Dybko"
                    }
                ],
                "author_detail": {
                    "name": "K. Dybko"
                },
                "author": "K. Dybko",
                "arxiv_comment": "23 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.04447v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.04447v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13197v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13197v1",
                "updated": "2025-05-19T14:51:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    51,
                    47,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T14:51:47Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    51,
                    47,
                    0,
                    139,
                    0
                ],
                "title": "Inferring stochastic dynamics with growth from cross-sectional data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring stochastic dynamics with growth from cross-sectional data"
                },
                "summary": "Time-resolved single-cell omics data offers high-throughput, genome-wide\nmeasurements of cellular states, which are instrumental to reverse-engineer the\nprocesses underpinning cell fate. Such technologies are inherently destructive,\nallowing only cross-sectional measurements of the underlying stochastic\ndynamical system. Furthermore, cells may divide or die in addition to changing\ntheir molecular state. Collectively these present a major challenge to\ninferring realistic biophysical models. We present a novel approach,\n\\emph{unbalanced} probability flow inference, that addresses this challenge for\nbiological processes modelled as stochastic dynamics with growth. By leveraging\na Lagrangian formulation of the Fokker-Planck equation, our method accurately\ndisentangles drift from intrinsic noise and growth. We showcase the\napplicability of our approach through evaluation on a range of simulated and\nreal single-cell RNA-seq datasets. Comparing to several existing methods, we\nfind our method achieves higher accuracy while enjoying a simple two-step\ntraining scheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-resolved single-cell omics data offers high-throughput, genome-wide\nmeasurements of cellular states, which are instrumental to reverse-engineer the\nprocesses underpinning cell fate. Such technologies are inherently destructive,\nallowing only cross-sectional measurements of the underlying stochastic\ndynamical system. Furthermore, cells may divide or die in addition to changing\ntheir molecular state. Collectively these present a major challenge to\ninferring realistic biophysical models. We present a novel approach,\n\\emph{unbalanced} probability flow inference, that addresses this challenge for\nbiological processes modelled as stochastic dynamics with growth. By leveraging\na Lagrangian formulation of the Fokker-Planck equation, our method accurately\ndisentangles drift from intrinsic noise and growth. We showcase the\napplicability of our approach through evaluation on a range of simulated and\nreal single-cell RNA-seq datasets. Comparing to several existing methods, we\nfind our method achieves higher accuracy while enjoying a simple two-step\ntraining scheme."
                },
                "authors": [
                    {
                        "name": "Stephen Zhang"
                    },
                    {
                        "name": "Suryanarayana Maddu"
                    },
                    {
                        "name": "Xiaoje Qiu"
                    },
                    {
                        "name": "Victor Chardès"
                    }
                ],
                "author_detail": {
                    "name": "Victor Chardès"
                },
                "author": "Victor Chardès",
                "arxiv_comment": "9 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13197v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13197v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11266v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11266v5",
                "updated": "2025-05-19T14:51:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    51,
                    44,
                    0,
                    139,
                    0
                ],
                "published": "2024-11-18T03:45:34Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    3,
                    45,
                    34,
                    0,
                    323,
                    0
                ],
                "title": "VersaTune: An Efficient Data Composition Framework for Training\n  Multi-Capability LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VersaTune: An Efficient Data Composition Framework for Training\n  Multi-Capability LLMs"
                },
                "summary": "As demonstrated by the proprietary Large Language Models (LLMs) such as GPT\nand Claude series, LLMs have the potential to achieve remarkable proficiency\nacross a wide range of domains, including law, medicine, finance, science,\ncode, etc., all within a single model. These capabilities are further augmented\nduring the Supervised Fine-Tuning (SFT) phase. Despite their potential,\nexisting work mainly focuses on domain-specific enhancements during\nfine-tuning, the challenge of which lies in catastrophic forgetting of\nknowledge across other domains. In this study, we introduce **VersaTune**, a\nnovel data composition framework designed for enhancing LLMs' overall\nmulti-domain capabilities during training. We begin with detecting the\ndistribution of domain-specific knowledge within the base model, followed by\nthe training data composition that aligns with the model's existing knowledge\ndistribution. During the subsequent training process, domain weights are\ndynamically adjusted based on their learnable potential and forgetting degree.\nExperimental results indicate that VersaTune is effective in multi-domain\nfostering, with an improvement of 35.21\\% in the overall multi-ability\nperformances compared to uniform domain weights. Furthermore, we find that\nQwen-2.5-32B + VersaTune even surpasses frontier models, including GPT-4o,\nClaude3.5-Sonnet and DeepSeek-V3 by 0.86\\%, 4.76\\% and 4.60\\%. Additionally, in\nscenarios where flexible expansion of a specific domain is required, VersaTune\nreduces the performance degradation in other domains by 38.77\\%, while\npreserving the training efficacy of the target domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As demonstrated by the proprietary Large Language Models (LLMs) such as GPT\nand Claude series, LLMs have the potential to achieve remarkable proficiency\nacross a wide range of domains, including law, medicine, finance, science,\ncode, etc., all within a single model. These capabilities are further augmented\nduring the Supervised Fine-Tuning (SFT) phase. Despite their potential,\nexisting work mainly focuses on domain-specific enhancements during\nfine-tuning, the challenge of which lies in catastrophic forgetting of\nknowledge across other domains. In this study, we introduce **VersaTune**, a\nnovel data composition framework designed for enhancing LLMs' overall\nmulti-domain capabilities during training. We begin with detecting the\ndistribution of domain-specific knowledge within the base model, followed by\nthe training data composition that aligns with the model's existing knowledge\ndistribution. During the subsequent training process, domain weights are\ndynamically adjusted based on their learnable potential and forgetting degree.\nExperimental results indicate that VersaTune is effective in multi-domain\nfostering, with an improvement of 35.21\\% in the overall multi-ability\nperformances compared to uniform domain weights. Furthermore, we find that\nQwen-2.5-32B + VersaTune even surpasses frontier models, including GPT-4o,\nClaude3.5-Sonnet and DeepSeek-V3 by 0.86\\%, 4.76\\% and 4.60\\%. Additionally, in\nscenarios where flexible expansion of a specific domain is required, VersaTune\nreduces the performance degradation in other domains by 38.77\\%, while\npreserving the training efficacy of the target domain."
                },
                "authors": [
                    {
                        "name": "Keer Lu"
                    },
                    {
                        "name": "Keshi Zhao"
                    },
                    {
                        "name": "Zhuoran Zhang"
                    },
                    {
                        "name": "Zheng Liang"
                    },
                    {
                        "name": "Da Pan"
                    },
                    {
                        "name": "Shusen Zhang"
                    },
                    {
                        "name": "Xin Wu"
                    },
                    {
                        "name": "Guosheng Dong"
                    },
                    {
                        "name": "Bin Cui"
                    },
                    {
                        "name": "Tengjiao Wang"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11266v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11266v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13195v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13195v1",
                "updated": "2025-05-19T14:50:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    50,
                    44,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T14:50:44Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    50,
                    44,
                    0,
                    139,
                    0
                ],
                "title": "Adversarial Testing in LLMs: Insights into Decision-Making\n  Vulnerabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial Testing in LLMs: Insights into Decision-Making\n  Vulnerabilities"
                },
                "summary": "As Large Language Models (LLMs) become increasingly integrated into\nreal-world decision-making systems, understanding their behavioural\nvulnerabilities remains a critical challenge for AI safety and alignment. While\nexisting evaluation metrics focus primarily on reasoning accuracy or factual\ncorrectness, they often overlook whether LLMs are robust to adversarial\nmanipulation or capable of using adaptive strategy in dynamic environments.\nThis paper introduces an adversarial evaluation framework designed to\nsystematically stress-test the decision-making processes of LLMs under\ninteractive and adversarial conditions. Drawing on methodologies from cognitive\npsychology and game theory, our framework probes how models respond in two\ncanonical tasks: the two-armed bandit task and the Multi-Round Trust Task.\nThese tasks capture key aspects of exploration-exploitation trade-offs, social\ncooperation, and strategic flexibility. We apply this framework to several\nstate-of-the-art LLMs, including GPT-3.5, GPT-4, Gemini-1.5, and DeepSeek-V3,\nrevealing model-specific susceptibilities to manipulation and rigidity in\nstrategy adaptation. Our findings highlight distinct behavioral patterns across\nmodels and emphasize the importance of adaptability and fairness recognition\nfor trustworthy AI deployment. Rather than offering a performance benchmark,\nthis work proposes a methodology for diagnosing decision-making weaknesses in\nLLM-based agents, providing actionable insights for alignment and safety\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) become increasingly integrated into\nreal-world decision-making systems, understanding their behavioural\nvulnerabilities remains a critical challenge for AI safety and alignment. While\nexisting evaluation metrics focus primarily on reasoning accuracy or factual\ncorrectness, they often overlook whether LLMs are robust to adversarial\nmanipulation or capable of using adaptive strategy in dynamic environments.\nThis paper introduces an adversarial evaluation framework designed to\nsystematically stress-test the decision-making processes of LLMs under\ninteractive and adversarial conditions. Drawing on methodologies from cognitive\npsychology and game theory, our framework probes how models respond in two\ncanonical tasks: the two-armed bandit task and the Multi-Round Trust Task.\nThese tasks capture key aspects of exploration-exploitation trade-offs, social\ncooperation, and strategic flexibility. We apply this framework to several\nstate-of-the-art LLMs, including GPT-3.5, GPT-4, Gemini-1.5, and DeepSeek-V3,\nrevealing model-specific susceptibilities to manipulation and rigidity in\nstrategy adaptation. Our findings highlight distinct behavioral patterns across\nmodels and emphasize the importance of adaptability and fairness recognition\nfor trustworthy AI deployment. Rather than offering a performance benchmark,\nthis work proposes a methodology for diagnosing decision-making weaknesses in\nLLM-based agents, providing actionable insights for alignment and safety\nresearch."
                },
                "authors": [
                    {
                        "name": "Lili Zhang"
                    },
                    {
                        "name": "Haomiaomiao Wang"
                    },
                    {
                        "name": "Long Cheng"
                    },
                    {
                        "name": "Libao Deng"
                    },
                    {
                        "name": "Tomas Ward"
                    }
                ],
                "author_detail": {
                    "name": "Tomas Ward"
                },
                "author": "Tomas Ward",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13195v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13195v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13192v1",
                "updated": "2025-05-19T14:49:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    49,
                    10,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T14:49:10Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    49,
                    10,
                    0,
                    139,
                    0
                ],
                "title": "True Zero-Shot Inference of Dynamical Systems Preserving Long-Term\n  Statistics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "True Zero-Shot Inference of Dynamical Systems Preserving Long-Term\n  Statistics"
                },
                "summary": "Complex, temporally evolving phenomena, from climate to brain activity, are\ngoverned by dynamical systems (DS). DS reconstruction (DSR) seeks to infer\ngenerative surrogate models of these from observed data, reproducing their\nlong-term behavior. Existing DSR approaches require purpose-training for any\nnew system observed, lacking the zero-shot and in-context inference\ncapabilities known from LLMs. Here we introduce DynaMix, a novel multivariate\nALRNN-based mixture-of-experts architecture pre-trained for DSR, the first DSR\nmodel able to generalize zero-shot to out-of-domain DS. Just from a provided\ncontext signal, without any re-training, DynaMix faithfully forecasts the\nlong-term evolution of novel DS where existing time series (TS) foundation\nmodels, like Chronos, fail -- at a fraction of the number of parameters and\norders of magnitude faster inference times. DynaMix outperforms TS foundation\nmodels in terms of long-term statistics, and often also short-term forecasts,\neven on real-world time series, like traffic or weather data, typically used\nfor training and evaluating TS models, but not at all part of DynaMix' training\ncorpus. We illustrate some of the failure modes of TS models for DSR problems,\nand conclude that models built on DS principles may bear a huge potential also\nfor advancing the TS prediction field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Complex, temporally evolving phenomena, from climate to brain activity, are\ngoverned by dynamical systems (DS). DS reconstruction (DSR) seeks to infer\ngenerative surrogate models of these from observed data, reproducing their\nlong-term behavior. Existing DSR approaches require purpose-training for any\nnew system observed, lacking the zero-shot and in-context inference\ncapabilities known from LLMs. Here we introduce DynaMix, a novel multivariate\nALRNN-based mixture-of-experts architecture pre-trained for DSR, the first DSR\nmodel able to generalize zero-shot to out-of-domain DS. Just from a provided\ncontext signal, without any re-training, DynaMix faithfully forecasts the\nlong-term evolution of novel DS where existing time series (TS) foundation\nmodels, like Chronos, fail -- at a fraction of the number of parameters and\norders of magnitude faster inference times. DynaMix outperforms TS foundation\nmodels in terms of long-term statistics, and often also short-term forecasts,\neven on real-world time series, like traffic or weather data, typically used\nfor training and evaluating TS models, but not at all part of DynaMix' training\ncorpus. We illustrate some of the failure modes of TS models for DSR problems,\nand conclude that models built on DS principles may bear a huge potential also\nfor advancing the TS prediction field."
                },
                "authors": [
                    {
                        "name": "Christoph Jürgen Hemmer"
                    },
                    {
                        "name": "Daniel Durstewitz"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Durstewitz"
                },
                "author": "Daniel Durstewitz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nlin.CD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13181v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13181v1",
                "updated": "2025-05-19T14:38:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    38,
                    59,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T14:38:59Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    38,
                    59,
                    0,
                    139,
                    0
                ],
                "title": "Efficient Speech Language Modeling via Energy Distance in Continuous\n  Latent Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Speech Language Modeling via Energy Distance in Continuous\n  Latent Space"
                },
                "summary": "We introduce SLED, an alternative approach to speech language modeling by\nencoding speech waveforms into sequences of continuous latent representations\nand modeling them autoregressively using an energy distance objective. The\nenergy distance offers an analytical measure of the distributional gap by\ncontrasting simulated and target samples, enabling efficient training to\ncapture the underlying continuous autoregressive distribution. By bypassing\nreliance on residual vector quantization, SLED avoids discretization errors and\neliminates the need for the complicated hierarchical architectures common in\nexisting speech language models. It simplifies the overall modeling pipeline\nwhile preserving the richness of speech information and maintaining inference\nefficiency. Empirical results demonstrate that SLED achieves strong performance\nin both zero-shot and streaming speech synthesis, showing its potential for\nbroader applications in general-purpose speech language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce SLED, an alternative approach to speech language modeling by\nencoding speech waveforms into sequences of continuous latent representations\nand modeling them autoregressively using an energy distance objective. The\nenergy distance offers an analytical measure of the distributional gap by\ncontrasting simulated and target samples, enabling efficient training to\ncapture the underlying continuous autoregressive distribution. By bypassing\nreliance on residual vector quantization, SLED avoids discretization errors and\neliminates the need for the complicated hierarchical architectures common in\nexisting speech language models. It simplifies the overall modeling pipeline\nwhile preserving the richness of speech information and maintaining inference\nefficiency. Empirical results demonstrate that SLED achieves strong performance\nin both zero-shot and streaming speech synthesis, showing its potential for\nbroader applications in general-purpose speech language models."
                },
                "authors": [
                    {
                        "name": "Zhengrui Ma"
                    },
                    {
                        "name": "Yang Feng"
                    },
                    {
                        "name": "Chenze Shao"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "Demos and code are available at https://github.com/ictnlp/SLED-TTS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13181v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13181v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15654v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15654v5",
                "updated": "2025-05-19T14:34:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    34,
                    42,
                    0,
                    139,
                    0
                ],
                "published": "2025-02-21T18:22:36Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    18,
                    22,
                    36,
                    4,
                    52,
                    0
                ],
                "title": "Machine-generated text detection prevents language model collapse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine-generated text detection prevents language model collapse"
                },
                "summary": "As Large Language Models (LLMs) become increasingly prevalent, their\ngenerated outputs are proliferating across the web, risking a future where\nmachine-generated content dilutes human-authored text. Since online data is the\nprimary resource for LLM pre-training, subsequent models could be trained on an\nunknown portion of synthetic samples. This will lead to model collapse, a\ndegenerative process whereby LLMs reinforce their own errors, converge to a low\nvariance output distribution, and ultimately yield a declining performance. In\nthis study, we investigate the impact of decoding strategy on model collapse,\nanalysing the text characteristics at each model generation, the similarity to\nhuman references, and the resulting model performance. Using the decoding\nstrategies that lead to the most significant degradation, we evaluate model\ncollapse in more realistic scenarios where the origin of the data (human or\nsynthetic) is unknown. We train a machine-generated text detector and propose\nan importance sampling approach to alleviate model collapse. Our method is\nvalidated on two LLM variants (GPT-2 and SmolLM2), across a range of model\nsizes (124M to 1.7B), on the open-ended text generation task. We demonstrate\nthat it can not only prevent model collapse but also improve performance when\nsufficient human-authored samples are present. Source code:\ngithub.com/GeorgeDrayson/model_collapse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) become increasingly prevalent, their\ngenerated outputs are proliferating across the web, risking a future where\nmachine-generated content dilutes human-authored text. Since online data is the\nprimary resource for LLM pre-training, subsequent models could be trained on an\nunknown portion of synthetic samples. This will lead to model collapse, a\ndegenerative process whereby LLMs reinforce their own errors, converge to a low\nvariance output distribution, and ultimately yield a declining performance. In\nthis study, we investigate the impact of decoding strategy on model collapse,\nanalysing the text characteristics at each model generation, the similarity to\nhuman references, and the resulting model performance. Using the decoding\nstrategies that lead to the most significant degradation, we evaluate model\ncollapse in more realistic scenarios where the origin of the data (human or\nsynthetic) is unknown. We train a machine-generated text detector and propose\nan importance sampling approach to alleviate model collapse. Our method is\nvalidated on two LLM variants (GPT-2 and SmolLM2), across a range of model\nsizes (124M to 1.7B), on the open-ended text generation task. We demonstrate\nthat it can not only prevent model collapse but also improve performance when\nsufficient human-authored samples are present. Source code:\ngithub.com/GeorgeDrayson/model_collapse."
                },
                "authors": [
                    {
                        "name": "George Drayson"
                    },
                    {
                        "name": "Emine Yilmaz"
                    },
                    {
                        "name": "Vasileios Lampos"
                    }
                ],
                "author_detail": {
                    "name": "Vasileios Lampos"
                },
                "author": "Vasileios Lampos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15654v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15654v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13176v1",
                "updated": "2025-05-19T14:30:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    30,
                    46,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T14:30:46Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    30,
                    46,
                    0,
                    139,
                    0
                ],
                "title": "ToolSpectrum : Towards Personalized Tool Utilization for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ToolSpectrum : Towards Personalized Tool Utilization for Large Language\n  Models"
                },
                "summary": "While integrating external tools into large language models (LLMs) enhances\ntheir ability to access real-time information and domain-specific services,\nexisting approaches focus narrowly on functional tool selection following user\ninstructions, overlooking the context-aware personalization in tool selection.\nThis oversight leads to suboptimal user satisfaction and inefficient tool\nutilization, particularly when overlapping toolsets require nuanced selection\nbased on contextual factors. To bridge this gap, we introduce ToolSpectrum, a\nbenchmark designed to evaluate LLMs' capabilities in personalized tool\nutilization. Specifically, we formalize two key dimensions of personalization,\nuser profile and environmental factors, and analyze their individual and\nsynergistic impacts on tool utilization. Through extensive experiments on\nToolSpectrum, we demonstrate that personalized tool utilization significantly\nimproves user experience across diverse scenarios. However, even\nstate-of-the-art LLMs exhibit the limited ability to reason jointly about user\nprofiles and environmental factors, often prioritizing one dimension at the\nexpense of the other. Our findings underscore the necessity of context-aware\npersonalization in tool-augmented LLMs and reveal critical limitations for\ncurrent models. Our data and code are available at\nhttps://github.com/Chengziha0/ToolSpectrum.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While integrating external tools into large language models (LLMs) enhances\ntheir ability to access real-time information and domain-specific services,\nexisting approaches focus narrowly on functional tool selection following user\ninstructions, overlooking the context-aware personalization in tool selection.\nThis oversight leads to suboptimal user satisfaction and inefficient tool\nutilization, particularly when overlapping toolsets require nuanced selection\nbased on contextual factors. To bridge this gap, we introduce ToolSpectrum, a\nbenchmark designed to evaluate LLMs' capabilities in personalized tool\nutilization. Specifically, we formalize two key dimensions of personalization,\nuser profile and environmental factors, and analyze their individual and\nsynergistic impacts on tool utilization. Through extensive experiments on\nToolSpectrum, we demonstrate that personalized tool utilization significantly\nimproves user experience across diverse scenarios. However, even\nstate-of-the-art LLMs exhibit the limited ability to reason jointly about user\nprofiles and environmental factors, often prioritizing one dimension at the\nexpense of the other. Our findings underscore the necessity of context-aware\npersonalization in tool-augmented LLMs and reveal critical limitations for\ncurrent models. Our data and code are available at\nhttps://github.com/Chengziha0/ToolSpectrum."
                },
                "authors": [
                    {
                        "name": "Zihao Cheng"
                    },
                    {
                        "name": "Hongru Wang"
                    },
                    {
                        "name": "Zeming Liu"
                    },
                    {
                        "name": "Yuhang Guo"
                    },
                    {
                        "name": "Yuanfang Guo"
                    },
                    {
                        "name": "Yunhong Wang"
                    },
                    {
                        "name": "Haifeng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haifeng Wang"
                },
                "author": "Haifeng Wang",
                "arxiv_comment": "Accepted by ACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13175v1",
                "updated": "2025-05-19T14:30:41Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    30,
                    41,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T14:30:41Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    30,
                    41,
                    0,
                    139,
                    0
                ],
                "title": "Enhancing LLMs for Time Series Forecasting via Structure-Guided\n  Cross-Modal Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLMs for Time Series Forecasting via Structure-Guided\n  Cross-Modal Alignment"
                },
                "summary": "The emerging paradigm of leveraging pretrained large language models (LLMs)\nfor time series forecasting has predominantly employed linguistic-temporal\nmodality alignment strategies through token-level or layer-wise feature\nmapping. However, these approaches fundamentally neglect a critical insight:\nthe core competency of LLMs resides not merely in processing localized token\nfeatures but in their inherent capacity to model holistic sequence structures.\nThis paper posits that effective cross-modal alignment necessitates structural\nconsistency at the sequence level. We propose the Structure-Guided Cross-Modal\nAlignment (SGCMA), a framework that fully exploits and aligns the\nstate-transition graph structures shared by time-series and linguistic data as\nsequential modalities, thereby endowing time series with language-like\nproperties and delivering stronger generalization after modality alignment.\nSGCMA consists of two key components, namely Structure Alignment and Semantic\nAlignment. In Structure Alignment, a state transition matrix is learned from\ntext data through Hidden Markov Models (HMMs), and a shallow transformer-based\nMaximum Entropy Markov Model (MEMM) receives the hot-start transition matrix\nand annotates each temporal patch into state probability, ensuring that the\ntemporal representation sequence inherits language-like sequential dynamics. In\nSemantic Alignment, cross-attention is applied between temporal patches and the\ntop-k tokens within each state, and the ultimate temporal embeddings are\nderived by the expected value of these embeddings using a weighted average\nbased on state probabilities. Experiments on multiple benchmarks demonstrate\nthat SGCMA achieves state-of-the-art performance, offering a novel approach to\ncross-modal alignment in time series forecasting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emerging paradigm of leveraging pretrained large language models (LLMs)\nfor time series forecasting has predominantly employed linguistic-temporal\nmodality alignment strategies through token-level or layer-wise feature\nmapping. However, these approaches fundamentally neglect a critical insight:\nthe core competency of LLMs resides not merely in processing localized token\nfeatures but in their inherent capacity to model holistic sequence structures.\nThis paper posits that effective cross-modal alignment necessitates structural\nconsistency at the sequence level. We propose the Structure-Guided Cross-Modal\nAlignment (SGCMA), a framework that fully exploits and aligns the\nstate-transition graph structures shared by time-series and linguistic data as\nsequential modalities, thereby endowing time series with language-like\nproperties and delivering stronger generalization after modality alignment.\nSGCMA consists of two key components, namely Structure Alignment and Semantic\nAlignment. In Structure Alignment, a state transition matrix is learned from\ntext data through Hidden Markov Models (HMMs), and a shallow transformer-based\nMaximum Entropy Markov Model (MEMM) receives the hot-start transition matrix\nand annotates each temporal patch into state probability, ensuring that the\ntemporal representation sequence inherits language-like sequential dynamics. In\nSemantic Alignment, cross-attention is applied between temporal patches and the\ntop-k tokens within each state, and the ultimate temporal embeddings are\nderived by the expected value of these embeddings using a weighted average\nbased on state probabilities. Experiments on multiple benchmarks demonstrate\nthat SGCMA achieves state-of-the-art performance, offering a novel approach to\ncross-modal alignment in time series forecasting."
                },
                "authors": [
                    {
                        "name": "Siming Sun"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Xuejun Jiang"
                    },
                    {
                        "name": "Wenchao Meng"
                    },
                    {
                        "name": "Qinmin Yang"
                    }
                ],
                "author_detail": {
                    "name": "Qinmin Yang"
                },
                "author": "Qinmin Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13173v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13173v1",
                "updated": "2025-05-19T14:30:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    30,
                    10,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T14:30:10Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    30,
                    10,
                    0,
                    139,
                    0
                ],
                "title": "A Case Study of Cross-Lingual Zero-Shot Generalization for Classical\n  Languages in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Case Study of Cross-Lingual Zero-Shot Generalization for Classical\n  Languages in LLMs"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable generalization\ncapabilities across diverse tasks and languages. In this study, we focus on\nnatural language understanding in three classical languages -- Sanskrit,\nAncient Greek and Latin -- to investigate the factors affecting cross-lingual\nzero-shot generalization. First, we explore named entity recognition and\nmachine translation into English. While LLMs perform equal to or better than\nfine-tuned baselines on out-of-domain data, smaller models often struggle,\nespecially with niche or abstract entity types. In addition, we concentrate on\nSanskrit by presenting a factoid question-answering (QA) dataset and show that\nincorporating context via retrieval-augmented generation approach significantly\nboosts performance. In contrast, we observe pronounced performance drops for\nsmaller LLMs across these QA tasks. These results suggest model scale as an\nimportant factor influencing cross-lingual generalization. Assuming that models\nused such as GPT-4o and Llama-3.1 are not instruction fine-tuned on classical\nlanguages, our findings provide insights into how LLMs may generalize on these\nlanguages and their consequent utility in classical studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable generalization\ncapabilities across diverse tasks and languages. In this study, we focus on\nnatural language understanding in three classical languages -- Sanskrit,\nAncient Greek and Latin -- to investigate the factors affecting cross-lingual\nzero-shot generalization. First, we explore named entity recognition and\nmachine translation into English. While LLMs perform equal to or better than\nfine-tuned baselines on out-of-domain data, smaller models often struggle,\nespecially with niche or abstract entity types. In addition, we concentrate on\nSanskrit by presenting a factoid question-answering (QA) dataset and show that\nincorporating context via retrieval-augmented generation approach significantly\nboosts performance. In contrast, we observe pronounced performance drops for\nsmaller LLMs across these QA tasks. These results suggest model scale as an\nimportant factor influencing cross-lingual generalization. Assuming that models\nused such as GPT-4o and Llama-3.1 are not instruction fine-tuned on classical\nlanguages, our findings provide insights into how LLMs may generalize on these\nlanguages and their consequent utility in classical studies."
                },
                "authors": [
                    {
                        "name": "V. S. D. S. Mahesh Akavarapu"
                    },
                    {
                        "name": "Hrishikesh Terdalkar"
                    },
                    {
                        "name": "Pramit Bhattacharyya"
                    },
                    {
                        "name": "Shubhangi Agarwal"
                    },
                    {
                        "name": "Vishakha Deulgaonkar"
                    },
                    {
                        "name": "Pralay Manna"
                    },
                    {
                        "name": "Chaitali Dangarikar"
                    },
                    {
                        "name": "Arnab Bhattacharya"
                    }
                ],
                "author_detail": {
                    "name": "Arnab Bhattacharya"
                },
                "author": "Arnab Bhattacharya",
                "arxiv_comment": "Accepted to ACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13173v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13173v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13171v1",
                "updated": "2025-05-19T14:28:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    28,
                    35,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T14:28:35Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    28,
                    35,
                    0,
                    139,
                    0
                ],
                "title": "Positional Fragility in LLMs: How Offset Effects Reshape Our\n  Understanding of Memorization Risks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Positional Fragility in LLMs: How Offset Effects Reshape Our\n  Understanding of Memorization Risks"
                },
                "summary": "Large language models are known to memorize parts of their training data,\nposing risk of copyright violations. To systematically examine this risk, we\npretrain language models (1B/3B/8B) from scratch on 83B tokens, mixing\nweb-scale data with public domain books used to simulate copyrighted content at\ncontrolled frequencies at lengths at least ten times longer than prior work. We\nthereby identified the offset effect, a phenomenon characterized by two key\nfindings: (1) verbatim memorization is most strongly triggered by short\nprefixes drawn from the beginning of the context window, with memorization\ndecreasing counterintuitively as prefix length increases; and (2) a sharp\ndecline in verbatim recall when prefix begins offset from the initial tokens of\nthe context window. We attribute this to positional fragility: models rely\ndisproportionately on the earliest tokens in their context window as retrieval\nanchors, making them sensitive to even slight shifts. We further observe that\nwhen the model fails to retrieve memorized content, it often produces\ndegenerated text. Leveraging these findings, we show that shifting sensitive\ndata deeper into the context window suppresses both extractable memorization\nand degeneration. Our results suggest that positional offset is a critical and\npreviously overlooked axis for evaluating memorization risks, since prior work\nimplicitly assumed uniformity by probing only from the beginning of training\nsequences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are known to memorize parts of their training data,\nposing risk of copyright violations. To systematically examine this risk, we\npretrain language models (1B/3B/8B) from scratch on 83B tokens, mixing\nweb-scale data with public domain books used to simulate copyrighted content at\ncontrolled frequencies at lengths at least ten times longer than prior work. We\nthereby identified the offset effect, a phenomenon characterized by two key\nfindings: (1) verbatim memorization is most strongly triggered by short\nprefixes drawn from the beginning of the context window, with memorization\ndecreasing counterintuitively as prefix length increases; and (2) a sharp\ndecline in verbatim recall when prefix begins offset from the initial tokens of\nthe context window. We attribute this to positional fragility: models rely\ndisproportionately on the earliest tokens in their context window as retrieval\nanchors, making them sensitive to even slight shifts. We further observe that\nwhen the model fails to retrieve memorized content, it often produces\ndegenerated text. Leveraging these findings, we show that shifting sensitive\ndata deeper into the context window suppresses both extractable memorization\nand degeneration. Our results suggest that positional offset is a critical and\npreviously overlooked axis for evaluating memorization risks, since prior work\nimplicitly assumed uniformity by probing only from the beginning of training\nsequences."
                },
                "authors": [
                    {
                        "name": "Yixuan Xu"
                    },
                    {
                        "name": "Antoine Bosselut"
                    },
                    {
                        "name": "Imanol Schlag"
                    }
                ],
                "author_detail": {
                    "name": "Imanol Schlag"
                },
                "author": "Imanol Schlag",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02506v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02506v3",
                "updated": "2025-05-19T14:26:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    26,
                    34,
                    0,
                    139,
                    0
                ],
                "published": "2025-01-05T11:06:55Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    11,
                    6,
                    55,
                    6,
                    5,
                    0
                ],
                "title": "ToolHop: A Query-Driven Benchmark for Evaluating Large Language Models\n  in Multi-Hop Tool Use",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ToolHop: A Query-Driven Benchmark for Evaluating Large Language Models\n  in Multi-Hop Tool Use"
                },
                "summary": "Effective evaluation of multi-hop tool use is critical for analyzing the\nunderstanding, reasoning, and function-calling capabilities of large language\nmodels (LLMs). However, progress has been hindered by a lack of reliable\nevaluation datasets. To address this, we present ToolHop, a dataset comprising\n995 user queries and 3,912 associated tools, specifically designed for rigorous\nevaluation of multi-hop tool use. ToolHop ensures diverse queries, meaningful\ninterdependencies, locally executable tools, detailed feedback, and verifiable\nanswers through a novel query-driven data construction approach that includes\ntool creation, document refinement, and code generation. We evaluate 14 LLMs\nacross five model families (i.e., LLaMA3.1, Qwen2.5, Gemini1.5, Claude3.5, and\nGPT), uncovering significant challenges in handling multi-hop tool-use\nscenarios. The leading model, GPT-4o, achieves an accuracy of 49.04%,\nunderscoring substantial room for improvement. Further analysis reveals\nvariations in tool-use strategies for various families, offering actionable\ninsights to guide the development of more effective approaches. Code and data\ncan be found in https://huggingface.co/datasets/bytedance-research/ToolHop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective evaluation of multi-hop tool use is critical for analyzing the\nunderstanding, reasoning, and function-calling capabilities of large language\nmodels (LLMs). However, progress has been hindered by a lack of reliable\nevaluation datasets. To address this, we present ToolHop, a dataset comprising\n995 user queries and 3,912 associated tools, specifically designed for rigorous\nevaluation of multi-hop tool use. ToolHop ensures diverse queries, meaningful\ninterdependencies, locally executable tools, detailed feedback, and verifiable\nanswers through a novel query-driven data construction approach that includes\ntool creation, document refinement, and code generation. We evaluate 14 LLMs\nacross five model families (i.e., LLaMA3.1, Qwen2.5, Gemini1.5, Claude3.5, and\nGPT), uncovering significant challenges in handling multi-hop tool-use\nscenarios. The leading model, GPT-4o, achieves an accuracy of 49.04%,\nunderscoring substantial room for improvement. Further analysis reveals\nvariations in tool-use strategies for various families, offering actionable\ninsights to guide the development of more effective approaches. Code and data\ncan be found in https://huggingface.co/datasets/bytedance-research/ToolHop."
                },
                "authors": [
                    {
                        "name": "Junjie Ye"
                    },
                    {
                        "name": "Zhengyin Du"
                    },
                    {
                        "name": "Xuesong Yao"
                    },
                    {
                        "name": "Weijian Lin"
                    },
                    {
                        "name": "Yufei Xu"
                    },
                    {
                        "name": "Zehui Chen"
                    },
                    {
                        "name": "Zaiyuan Wang"
                    },
                    {
                        "name": "Sining Zhu"
                    },
                    {
                        "name": "Zhiheng Xi"
                    },
                    {
                        "name": "Siyu Yuan"
                    },
                    {
                        "name": "Tao Gui"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Jiecao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jiecao Chen"
                },
                "author": "Jiecao Chen",
                "arxiv_comment": "Accepted by ACL 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02506v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02506v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10849v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10849v2",
                "updated": "2025-05-19T14:26:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    26,
                    7,
                    0,
                    139,
                    0
                ],
                "published": "2024-12-14T14:46:18Z",
                "published_parsed": [
                    2024,
                    12,
                    14,
                    14,
                    46,
                    18,
                    5,
                    349,
                    0
                ],
                "title": "Superhuman performance of a large language model on the reasoning tasks\n  of a physician",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Superhuman performance of a large language model on the reasoning tasks\n  of a physician"
                },
                "summary": "A seminal paper published by Ledley and Lusted in 1959 introduced complex\nclinical diagnostic reasoning cases as the gold standard for the evaluation of\nexpert medical computing systems, a standard that has held ever since. Here, we\nreport the results of a physician evaluation of a large language model (LLM) on\nchallenging clinical cases against a baseline of hundreds of physicians. We\nconduct five experiments to measure clinical reasoning across differential\ndiagnosis generation, display of diagnostic reasoning, triage differential\ndiagnosis, probabilistic reasoning, and management reasoning, all adjudicated\nby physician experts with validated psychometrics. We then report a real-world\nstudy comparing human expert and AI second opinions in randomly-selected\npatients in the emergency room of a major tertiary academic medical center in\nBoston, MA. We compared LLMs and board-certified physicians at three predefined\ndiagnostic touchpoints: triage in the emergency room, initial evaluation by a\nphysician, and admission to the hospital or intensive care unit. In all\nexperiments--both vignettes and emergency room second opinions--the LLM\ndisplayed superhuman diagnostic and reasoning abilities, as well as continued\nimprovement from prior generations of AI clinical decision support. Our study\nsuggests that LLMs have achieved superhuman performance on general medical\ndiagnostic and management reasoning, fulfilling the vision put forth by Ledley\nand Lusted, and motivating the urgent need for prospective trials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A seminal paper published by Ledley and Lusted in 1959 introduced complex\nclinical diagnostic reasoning cases as the gold standard for the evaluation of\nexpert medical computing systems, a standard that has held ever since. Here, we\nreport the results of a physician evaluation of a large language model (LLM) on\nchallenging clinical cases against a baseline of hundreds of physicians. We\nconduct five experiments to measure clinical reasoning across differential\ndiagnosis generation, display of diagnostic reasoning, triage differential\ndiagnosis, probabilistic reasoning, and management reasoning, all adjudicated\nby physician experts with validated psychometrics. We then report a real-world\nstudy comparing human expert and AI second opinions in randomly-selected\npatients in the emergency room of a major tertiary academic medical center in\nBoston, MA. We compared LLMs and board-certified physicians at three predefined\ndiagnostic touchpoints: triage in the emergency room, initial evaluation by a\nphysician, and admission to the hospital or intensive care unit. In all\nexperiments--both vignettes and emergency room second opinions--the LLM\ndisplayed superhuman diagnostic and reasoning abilities, as well as continued\nimprovement from prior generations of AI clinical decision support. Our study\nsuggests that LLMs have achieved superhuman performance on general medical\ndiagnostic and management reasoning, fulfilling the vision put forth by Ledley\nand Lusted, and motivating the urgent need for prospective trials."
                },
                "authors": [
                    {
                        "name": "Peter G. Brodeur"
                    },
                    {
                        "name": "Thomas A. Buckley"
                    },
                    {
                        "name": "Zahir Kanjee"
                    },
                    {
                        "name": "Ethan Goh"
                    },
                    {
                        "name": "Evelyn Bin Ling"
                    },
                    {
                        "name": "Priyank Jain"
                    },
                    {
                        "name": "Stephanie Cabral"
                    },
                    {
                        "name": "Raja-Elie Abdulnour"
                    },
                    {
                        "name": "Adrian D. Haimovich"
                    },
                    {
                        "name": "Jason A. Freed"
                    },
                    {
                        "name": "Andrew Olson"
                    },
                    {
                        "name": "Daniel J. Morgan"
                    },
                    {
                        "name": "Jason Hom"
                    },
                    {
                        "name": "Robert Gallo"
                    },
                    {
                        "name": "Liam G. McCoy"
                    },
                    {
                        "name": "Haadi Mombini"
                    },
                    {
                        "name": "Christopher Lucas"
                    },
                    {
                        "name": "Misha Fotoohi"
                    },
                    {
                        "name": "Matthew Gwiazdon"
                    },
                    {
                        "name": "Daniele Restifo"
                    },
                    {
                        "name": "Daniel Restrepo"
                    },
                    {
                        "name": "Eric Horvitz"
                    },
                    {
                        "name": "Jonathan Chen"
                    },
                    {
                        "name": "Arjun K. Manrai"
                    },
                    {
                        "name": "Adam Rodman"
                    }
                ],
                "author_detail": {
                    "name": "Adam Rodman"
                },
                "author": "Adam Rodman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10849v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10849v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2505.13446v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13446v1",
                "updated": "2025-05-19T17:59:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    59,
                    35,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T17:59:35Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    59,
                    35,
                    0,
                    139,
                    0
                ],
                "title": "Unlocking Non-Invasive Brain-to-Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking Non-Invasive Brain-to-Text"
                },
                "summary": "Despite major advances in surgical brain-to-text (B2T), i.e. transcribing\nspeech from invasive brain recordings, non-invasive alternatives have yet to\nsurpass even chance on standard metrics. This remains a barrier to building a\nnon-invasive brain-computer interface (BCI) capable of restoring communication\nin paralysed individuals without surgery. Here, we present the first\nnon-invasive B2T result that significantly exceeds these critical baselines,\nraising BLEU by $1.4\\mathrm{-}2.6\\times$ over prior work. This result is driven\nby three contributions: (1) we extend recent word-classification models with\nLLM-based rescoring, transforming single-word predictors into closed-vocabulary\nB2T systems; (2) we introduce a predictive in-filling approach to handle\nout-of-vocabulary (OOV) words, substantially expanding the effective\nvocabulary; and (3) we demonstrate, for the first time, how to scale\nnon-invasive B2T models across datasets, unlocking deep learning at scale and\nimproving accuracy by $2.1\\mathrm{-}2.3\\times$. Through these contributions, we\noffer new insights into the roles of data quality and vocabulary size.\nTogether, our results remove a major obstacle to realising practical\nnon-invasive B2T systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite major advances in surgical brain-to-text (B2T), i.e. transcribing\nspeech from invasive brain recordings, non-invasive alternatives have yet to\nsurpass even chance on standard metrics. This remains a barrier to building a\nnon-invasive brain-computer interface (BCI) capable of restoring communication\nin paralysed individuals without surgery. Here, we present the first\nnon-invasive B2T result that significantly exceeds these critical baselines,\nraising BLEU by $1.4\\mathrm{-}2.6\\times$ over prior work. This result is driven\nby three contributions: (1) we extend recent word-classification models with\nLLM-based rescoring, transforming single-word predictors into closed-vocabulary\nB2T systems; (2) we introduce a predictive in-filling approach to handle\nout-of-vocabulary (OOV) words, substantially expanding the effective\nvocabulary; and (3) we demonstrate, for the first time, how to scale\nnon-invasive B2T models across datasets, unlocking deep learning at scale and\nimproving accuracy by $2.1\\mathrm{-}2.3\\times$. Through these contributions, we\noffer new insights into the roles of data quality and vocabulary size.\nTogether, our results remove a major obstacle to realising practical\nnon-invasive B2T systems."
                },
                "authors": [
                    {
                        "name": "Dulhan Jayalath"
                    },
                    {
                        "name": "Gilad Landau"
                    },
                    {
                        "name": "Oiwi Parker Jones"
                    }
                ],
                "author_detail": {
                    "name": "Oiwi Parker Jones"
                },
                "author": "Oiwi Parker Jones",
                "arxiv_comment": "27 pages, 10 figures, 10 tables. Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13446v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13446v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13445v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13445v1",
                "updated": "2025-05-19T17:59:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    59,
                    31,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T17:59:31Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    59,
                    31,
                    0,
                    139,
                    0
                ],
                "title": "Trust, But Verify: A Self-Verification Approach to Reinforcement\n  Learning with Verifiable Rewards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trust, But Verify: A Self-Verification Approach to Reinforcement\n  Learning with Verifiable Rewards"
                },
                "summary": "Large Language Models (LLMs) show great promise in complex reasoning, with\nReinforcement Learning with Verifiable Rewards (RLVR) being a key enhancement\nstrategy. However, a prevalent issue is ``superficial self-reflection'', where\nmodels fail to robustly verify their own outputs. We introduce RISE\n(Reinforcing Reasoning with Self-Verification), a novel online RL framework\ndesigned to tackle this. RISE explicitly and simultaneously trains an LLM to\nimprove both its problem-solving and self-verification abilities within a\nsingle, integrated RL process. The core mechanism involves leveraging\nverifiable rewards from an outcome verifier to provide on-the-fly feedback for\nboth solution generation and self-verification tasks. In each iteration, the\nmodel generates solutions, then critiques its own on-policy generated\nsolutions, with both trajectories contributing to the policy update. Extensive\nexperiments on diverse mathematical reasoning benchmarks show that RISE\nconsistently improves model's problem-solving accuracy while concurrently\nfostering strong self-verification skills. Our analyses highlight the\nadvantages of online verification and the benefits of increased verification\ncompute. Additionally, RISE models exhibit more frequent and accurate\nself-verification behaviors during reasoning. These advantages reinforce RISE\nas a flexible and effective path towards developing more robust and self-aware\nreasoners.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) show great promise in complex reasoning, with\nReinforcement Learning with Verifiable Rewards (RLVR) being a key enhancement\nstrategy. However, a prevalent issue is ``superficial self-reflection'', where\nmodels fail to robustly verify their own outputs. We introduce RISE\n(Reinforcing Reasoning with Self-Verification), a novel online RL framework\ndesigned to tackle this. RISE explicitly and simultaneously trains an LLM to\nimprove both its problem-solving and self-verification abilities within a\nsingle, integrated RL process. The core mechanism involves leveraging\nverifiable rewards from an outcome verifier to provide on-the-fly feedback for\nboth solution generation and self-verification tasks. In each iteration, the\nmodel generates solutions, then critiques its own on-policy generated\nsolutions, with both trajectories contributing to the policy update. Extensive\nexperiments on diverse mathematical reasoning benchmarks show that RISE\nconsistently improves model's problem-solving accuracy while concurrently\nfostering strong self-verification skills. Our analyses highlight the\nadvantages of online verification and the benefits of increased verification\ncompute. Additionally, RISE models exhibit more frequent and accurate\nself-verification behaviors during reasoning. These advantages reinforce RISE\nas a flexible and effective path towards developing more robust and self-aware\nreasoners."
                },
                "authors": [
                    {
                        "name": "Xiaoyuan Liu"
                    },
                    {
                        "name": "Tian Liang"
                    },
                    {
                        "name": "Zhiwei He"
                    },
                    {
                        "name": "Jiahao Xu"
                    },
                    {
                        "name": "Wenxuan Wang"
                    },
                    {
                        "name": "Pinjia He"
                    },
                    {
                        "name": "Zhaopeng Tu"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Dong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yu"
                },
                "author": "Dong Yu",
                "arxiv_comment": "code available at https://github.com/xyliu-cs/RISE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13445v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13445v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15175v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15175v3",
                "updated": "2025-05-19T17:58:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    58,
                    53,
                    0,
                    139,
                    0
                ],
                "published": "2025-01-25T11:06:37Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    11,
                    6,
                    37,
                    5,
                    25,
                    0
                ],
                "title": "Option-ID Based Elimination For Multiple Choice Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Option-ID Based Elimination For Multiple Choice Questions"
                },
                "summary": "Multiple choice questions (MCQs) are a popular and important task for\nevaluating large language models (LLMs). Based on common strategies people use\nwhen answering MCQs, the process of elimination (PoE) has been proposed as an\neffective problem-solving method. Existing PoE methods typically either have\nLLMs directly identify incorrect options or score options and replace\nlower-scoring ones with [MASK]. However, both methods suffer from\ninapplicability or suboptimal performance. To address these issues, this paper\nproposes a novel option-ID based PoE ($\\text{PoE}_{\\text{ID}}$).\n$\\text{PoE}_{\\text{ID}}$ critically incorporates a debiasing technique to\ncounteract LLMs token bias, enhancing robustness over naive ID-based\nelimination. It features two strategies: $\\text{PoE}_{\\text{ID}}^{\\text{log}}$,\nwhich eliminates options whose IDs have log probabilities below the average\nthreshold, and $\\text{PoE}_{\\text{ID}}^{\\text{seq}}$, which iteratively removes\nthe option with the lowest ID probability. We conduct extensive experiments\nwith 6 different LLMs on 4 diverse datasets. The results demonstrate that\n$\\text{PoE}_{\\text{ID}}$, especially $\\text{PoE}_{\\text{ID}}^{\\text{log}}$,\nsignificantly improves zero-shot and few-shot MCQs performance, particularly in\ndatasets with more options. Our analyses demonstrate that\n$\\text{PoE}_{\\text{ID}}^{\\text{log}}$ enhances the LLMs' confidence in\nselecting the correct option, and the option elimination strategy outperforms\nmethods relying on [MASK] replacement. We further investigate the limitations\nof LLMs in directly identifying incorrect options, which stem from their\ninherent deficiencies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiple choice questions (MCQs) are a popular and important task for\nevaluating large language models (LLMs). Based on common strategies people use\nwhen answering MCQs, the process of elimination (PoE) has been proposed as an\neffective problem-solving method. Existing PoE methods typically either have\nLLMs directly identify incorrect options or score options and replace\nlower-scoring ones with [MASK]. However, both methods suffer from\ninapplicability or suboptimal performance. To address these issues, this paper\nproposes a novel option-ID based PoE ($\\text{PoE}_{\\text{ID}}$).\n$\\text{PoE}_{\\text{ID}}$ critically incorporates a debiasing technique to\ncounteract LLMs token bias, enhancing robustness over naive ID-based\nelimination. It features two strategies: $\\text{PoE}_{\\text{ID}}^{\\text{log}}$,\nwhich eliminates options whose IDs have log probabilities below the average\nthreshold, and $\\text{PoE}_{\\text{ID}}^{\\text{seq}}$, which iteratively removes\nthe option with the lowest ID probability. We conduct extensive experiments\nwith 6 different LLMs on 4 diverse datasets. The results demonstrate that\n$\\text{PoE}_{\\text{ID}}$, especially $\\text{PoE}_{\\text{ID}}^{\\text{log}}$,\nsignificantly improves zero-shot and few-shot MCQs performance, particularly in\ndatasets with more options. Our analyses demonstrate that\n$\\text{PoE}_{\\text{ID}}^{\\text{log}}$ enhances the LLMs' confidence in\nselecting the correct option, and the option elimination strategy outperforms\nmethods relying on [MASK] replacement. We further investigate the limitations\nof LLMs in directly identifying incorrect options, which stem from their\ninherent deficiencies."
                },
                "authors": [
                    {
                        "name": "Zhenhao Zhu"
                    },
                    {
                        "name": "Bulou Liu"
                    },
                    {
                        "name": "Qingyao Ai"
                    },
                    {
                        "name": "Yiqun Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yiqun Liu"
                },
                "author": "Yiqun Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15175v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15175v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13438v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13438v1",
                "updated": "2025-05-19T17:58:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    58,
                    44,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T17:58:44Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    58,
                    44,
                    0,
                    139,
                    0
                ],
                "title": "Optimizing Anytime Reasoning via Budget Relative Policy Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Anytime Reasoning via Budget Relative Policy Optimization"
                },
                "summary": "Scaling test-time compute is crucial for enhancing the reasoning capabilities\nof large language models (LLMs). Existing approaches typically employ\nreinforcement learning (RL) to maximize a verifiable reward obtained at the end\nof reasoning traces. However, such methods optimize only the final performance\nunder a large and fixed token budget, which hinders efficiency in both training\nand deployment. In this work, we present a novel framework, AnytimeReasoner, to\noptimize anytime reasoning performance, which aims to improve token efficiency\nand the flexibility of reasoning under varying token budget constraints. To\nachieve this, we truncate the complete thinking process to fit within sampled\ntoken budgets from a prior distribution, compelling the model to summarize the\noptimal answer for each truncated thinking for verification. This introduces\nverifiable dense rewards into the reasoning process, facilitating more\neffective credit assignment in RL optimization. We then optimize the thinking\nand summary policies in a decoupled manner to maximize the cumulative reward.\nAdditionally, we introduce a novel variance reduction technique, Budget\nRelative Policy Optimization (BRPO), to enhance the robustness and efficiency\nof the learning process when reinforcing the thinking policy. Empirical results\nin mathematical reasoning tasks demonstrate that our method consistently\noutperforms GRPO across all thinking budgets under various prior distributions,\nenhancing both training and token efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling test-time compute is crucial for enhancing the reasoning capabilities\nof large language models (LLMs). Existing approaches typically employ\nreinforcement learning (RL) to maximize a verifiable reward obtained at the end\nof reasoning traces. However, such methods optimize only the final performance\nunder a large and fixed token budget, which hinders efficiency in both training\nand deployment. In this work, we present a novel framework, AnytimeReasoner, to\noptimize anytime reasoning performance, which aims to improve token efficiency\nand the flexibility of reasoning under varying token budget constraints. To\nachieve this, we truncate the complete thinking process to fit within sampled\ntoken budgets from a prior distribution, compelling the model to summarize the\noptimal answer for each truncated thinking for verification. This introduces\nverifiable dense rewards into the reasoning process, facilitating more\neffective credit assignment in RL optimization. We then optimize the thinking\nand summary policies in a decoupled manner to maximize the cumulative reward.\nAdditionally, we introduce a novel variance reduction technique, Budget\nRelative Policy Optimization (BRPO), to enhance the robustness and efficiency\nof the learning process when reinforcing the thinking policy. Empirical results\nin mathematical reasoning tasks demonstrate that our method consistently\noutperforms GRPO across all thinking budgets under various prior distributions,\nenhancing both training and token efficiency."
                },
                "authors": [
                    {
                        "name": "Penghui Qi"
                    },
                    {
                        "name": "Zichen Liu"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Wee Sun Lee"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13438v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13438v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14234v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14234v3",
                "updated": "2025-05-19T17:56:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    56,
                    42,
                    0,
                    139,
                    0
                ],
                "published": "2025-03-18T13:11:43Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    13,
                    11,
                    43,
                    1,
                    77,
                    0
                ],
                "title": "Beyond Single Pass, Looping Through Time: KG-IRAG with Iterative\n  Knowledge Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Single Pass, Looping Through Time: KG-IRAG with Iterative\n  Knowledge Retrieval"
                },
                "summary": "Graph Retrieval-Augmented Generation (GraphRAG) has proven highly effective\nin enhancing the performance of Large Language Models (LLMs) on tasks that\nrequire external knowledge. By leveraging Knowledge Graphs (KGs), GraphRAG\nimproves information retrieval for complex reasoning tasks, providing more\nprecise and comprehensive retrieval and generating more accurate responses to\nQAs. However, most RAG methods fall short in addressing multi-step reasoning,\nparticularly when both information extraction and inference are necessary. To\naddress this limitation, this paper presents Knowledge Graph-Based Iterative\nRetrieval-Augmented Generation (KG-IRAG), a novel framework that integrates KGs\nwith iterative reasoning to improve LLMs' ability to handle queries involving\ntemporal and logical dependencies. Through iterative retrieval steps, KG-IRAG\nincrementally gathers relevant data from external KGs, enabling step-by-step\nreasoning. The proposed approach is particularly suited for scenarios where\nreasoning is required alongside dynamic temporal data extraction, such as\ndetermining optimal travel times based on weather conditions or traffic\npatterns. Experimental results show that KG-IRAG improves accuracy in complex\nreasoning tasks by effectively integrating external knowledge with iterative,\nlogic-based retrieval. Additionally, three new datasets: weatherQA-Irish,\nweatherQA-Sydney, and trafficQA-TFNSW, are formed to evaluate KG-IRAG's\nperformance, demonstrating its potential beyond traditional RAG applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Retrieval-Augmented Generation (GraphRAG) has proven highly effective\nin enhancing the performance of Large Language Models (LLMs) on tasks that\nrequire external knowledge. By leveraging Knowledge Graphs (KGs), GraphRAG\nimproves information retrieval for complex reasoning tasks, providing more\nprecise and comprehensive retrieval and generating more accurate responses to\nQAs. However, most RAG methods fall short in addressing multi-step reasoning,\nparticularly when both information extraction and inference are necessary. To\naddress this limitation, this paper presents Knowledge Graph-Based Iterative\nRetrieval-Augmented Generation (KG-IRAG), a novel framework that integrates KGs\nwith iterative reasoning to improve LLMs' ability to handle queries involving\ntemporal and logical dependencies. Through iterative retrieval steps, KG-IRAG\nincrementally gathers relevant data from external KGs, enabling step-by-step\nreasoning. The proposed approach is particularly suited for scenarios where\nreasoning is required alongside dynamic temporal data extraction, such as\ndetermining optimal travel times based on weather conditions or traffic\npatterns. Experimental results show that KG-IRAG improves accuracy in complex\nreasoning tasks by effectively integrating external knowledge with iterative,\nlogic-based retrieval. Additionally, three new datasets: weatherQA-Irish,\nweatherQA-Sydney, and trafficQA-TFNSW, are formed to evaluate KG-IRAG's\nperformance, demonstrating its potential beyond traditional RAG applications."
                },
                "authors": [
                    {
                        "name": "Ruiyi Yang"
                    },
                    {
                        "name": "Hao Xue"
                    },
                    {
                        "name": "Imran Razzak"
                    },
                    {
                        "name": "Hakim Hacid"
                    },
                    {
                        "name": "Flora D. Salim"
                    }
                ],
                "author_detail": {
                    "name": "Flora D. Salim"
                },
                "author": "Flora D. Salim",
                "arxiv_comment": "15 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14234v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14234v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13430v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13430v1",
                "updated": "2025-05-19T17:55:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    55,
                    15,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T17:55:15Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    55,
                    15,
                    0,
                    139,
                    0
                ],
                "title": "Fine-tuning Quantized Neural Networks with Zeroth-order Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning Quantized Neural Networks with Zeroth-order Optimization"
                },
                "summary": "As the size of large language models grows exponentially, GPU memory has\nbecome a bottleneck for adapting these models to downstream tasks. In this\npaper, we aim to push the limits of memory-efficient training by minimizing\nmemory usage on model weights, gradients, and optimizer states, within a\nunified framework. Our idea is to eliminate both gradients and optimizer states\nusing zeroth-order optimization, which approximates gradients by perturbing\nweights during forward passes to identify gradient directions. To minimize\nmemory usage on weights, we employ model quantization, e.g., converting from\nbfloat16 to int4. However, directly applying zeroth-order optimization to\nquantized weights is infeasible due to the precision gap between discrete\nweights and continuous gradients, which would otherwise require de-quantization\nand re-quantization. To overcome this challenge, we propose Quantized\nZeroth-order Optimization (QZO), a novel approach that perturbs the continuous\nquantization scale for gradient estimation and uses a directional derivative\nclipping method to stabilize training. QZO is orthogonal to both scalar-based\nand codebook-based post-training quantization methods. Compared to\nfull-parameter fine-tuning in bfloat16, QZO can reduce the total memory cost by\nmore than 18$\\times$ for 4-bit LLMs, and enables fine-tuning Llama-2-13B and\nStable Diffusion 3.5 Large within a single 24GB GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the size of large language models grows exponentially, GPU memory has\nbecome a bottleneck for adapting these models to downstream tasks. In this\npaper, we aim to push the limits of memory-efficient training by minimizing\nmemory usage on model weights, gradients, and optimizer states, within a\nunified framework. Our idea is to eliminate both gradients and optimizer states\nusing zeroth-order optimization, which approximates gradients by perturbing\nweights during forward passes to identify gradient directions. To minimize\nmemory usage on weights, we employ model quantization, e.g., converting from\nbfloat16 to int4. However, directly applying zeroth-order optimization to\nquantized weights is infeasible due to the precision gap between discrete\nweights and continuous gradients, which would otherwise require de-quantization\nand re-quantization. To overcome this challenge, we propose Quantized\nZeroth-order Optimization (QZO), a novel approach that perturbs the continuous\nquantization scale for gradient estimation and uses a directional derivative\nclipping method to stabilize training. QZO is orthogonal to both scalar-based\nand codebook-based post-training quantization methods. Compared to\nfull-parameter fine-tuning in bfloat16, QZO can reduce the total memory cost by\nmore than 18$\\times$ for 4-bit LLMs, and enables fine-tuning Llama-2-13B and\nStable Diffusion 3.5 Large within a single 24GB GPU."
                },
                "authors": [
                    {
                        "name": "Sifeng Shang"
                    },
                    {
                        "name": "Jiayi Zhou"
                    },
                    {
                        "name": "Chenyu Lin"
                    },
                    {
                        "name": "Minxian Li"
                    },
                    {
                        "name": "Kaiyang Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Kaiyang Zhou"
                },
                "author": "Kaiyang Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13430v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13430v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13425v1",
                "updated": "2025-05-19T17:54:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    54,
                    35,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T17:54:35Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    54,
                    35,
                    0,
                    139,
                    0
                ],
                "title": "Learnware of Language Models: Specialized Small Language Models Can Do\n  Big",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learnware of Language Models: Specialized Small Language Models Can Do\n  Big"
                },
                "summary": "The learnware paradigm offers a novel approach to machine learning by\nenabling users to reuse a set of well-trained models for tasks beyond the\nmodels' original purposes. It eliminates the need to build models from scratch,\ninstead relying on specifications (representations of a model's capabilities)\nto identify and leverage the most suitable models for new tasks. While\nlearnware has proven effective in many scenarios, its application to language\nmodels has remained largely unexplored. At the same time, large language models\n(LLMs) have demonstrated remarkable universal question-answering abilities, yet\nthey face challenges in specialized scenarios due to data scarcity, privacy\nconcerns, and high computational costs, thus more and more specialized small\nlanguage models (SLMs) are being trained for specific domains. To address these\nlimitations systematically, the learnware paradigm provides a promising\nsolution by enabling maximum utilization of specialized SLMs, and allowing\nusers to identify and reuse them in a collaborative and privacy-preserving\nmanner.\n  This paper presents a preliminary attempt to apply the learnware paradigm to\nlanguage models. We simulated a learnware system comprising approximately 100\nlearnwares of specialized SLMs with 8B parameters, fine-tuned across finance,\nhealthcare, and mathematics domains. Each learnware contains an SLM and a\nspecification, which enables users to identify the most relevant models without\nexposing their own data. Experimental results demonstrate promising\nperformance: by selecting one suitable learnware for each task-specific\ninference, the system outperforms the base SLMs on all benchmarks. Compared to\nLLMs, the system outperforms Qwen1.5-110B, Qwen2.5-72B, and\nLlama3.1-70B-Instruct by at least 14% in finance domain tasks, and surpasses\nFlan-PaLM-540B (ranked 7th on the Open Medical LLM Leaderboard) in medical\ndomain tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The learnware paradigm offers a novel approach to machine learning by\nenabling users to reuse a set of well-trained models for tasks beyond the\nmodels' original purposes. It eliminates the need to build models from scratch,\ninstead relying on specifications (representations of a model's capabilities)\nto identify and leverage the most suitable models for new tasks. While\nlearnware has proven effective in many scenarios, its application to language\nmodels has remained largely unexplored. At the same time, large language models\n(LLMs) have demonstrated remarkable universal question-answering abilities, yet\nthey face challenges in specialized scenarios due to data scarcity, privacy\nconcerns, and high computational costs, thus more and more specialized small\nlanguage models (SLMs) are being trained for specific domains. To address these\nlimitations systematically, the learnware paradigm provides a promising\nsolution by enabling maximum utilization of specialized SLMs, and allowing\nusers to identify and reuse them in a collaborative and privacy-preserving\nmanner.\n  This paper presents a preliminary attempt to apply the learnware paradigm to\nlanguage models. We simulated a learnware system comprising approximately 100\nlearnwares of specialized SLMs with 8B parameters, fine-tuned across finance,\nhealthcare, and mathematics domains. Each learnware contains an SLM and a\nspecification, which enables users to identify the most relevant models without\nexposing their own data. Experimental results demonstrate promising\nperformance: by selecting one suitable learnware for each task-specific\ninference, the system outperforms the base SLMs on all benchmarks. Compared to\nLLMs, the system outperforms Qwen1.5-110B, Qwen2.5-72B, and\nLlama3.1-70B-Instruct by at least 14% in finance domain tasks, and surpasses\nFlan-PaLM-540B (ranked 7th on the Open Medical LLM Leaderboard) in medical\ndomain tasks."
                },
                "authors": [
                    {
                        "name": "Zhi-Hao Tan"
                    },
                    {
                        "name": "Zi-Chen Zhao"
                    },
                    {
                        "name": "Hao-Yu Shi"
                    },
                    {
                        "name": "Xin-Yu Zhang"
                    },
                    {
                        "name": "Peng Tan"
                    },
                    {
                        "name": "Yang Yu"
                    },
                    {
                        "name": "Zhi-Hua Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zhi-Hua Zhou"
                },
                "author": "Zhi-Hua Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13421v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13421v1",
                "updated": "2025-05-19T17:52:58Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    52,
                    58,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T17:52:58Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    52,
                    58,
                    0,
                    139,
                    0
                ],
                "title": "Make Still Further Progress: Chain of Thoughts for Tabular Data\n  Leaderboard",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Make Still Further Progress: Chain of Thoughts for Tabular Data\n  Leaderboard"
                },
                "summary": "Tabular data, a fundamental data format in machine learning, is predominantly\nutilized in competitions and real-world applications. The performance of\ntabular models--such as gradient boosted decision trees and neural\nnetworks--can vary significantly across datasets due to differences in feature\ndistributions and task characteristics. Achieving top performance on each\ndataset often requires specialized expert knowledge. To address this\nvariability, practitioners often aggregate the predictions of multiple models.\nHowever, conventional aggregation strategies typically rely on static\ncombination rules and lack instance-level adaptability. In this work, we\npropose an in-context ensemble framework for tabular prediction that leverages\nlarge language models (LLMs) to perform dynamic, instance-specific integration\nof external model predictions. Without access to raw tabular features or\nsemantic information, our method constructs a context around each test instance\nusing its nearest neighbors and the predictions from a pool of external models.\nWithin this enriched context, we introduce Chain of Tabular Thoughts (CoT$^2$),\na prompting strategy that guides LLMs through multi-step, interpretable\nreasoning, making still further progress toward expert-level decision-making.\nExperimental results show that our method outperforms well-tuned baselines and\nstandard ensemble techniques across a wide range of tabular datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tabular data, a fundamental data format in machine learning, is predominantly\nutilized in competitions and real-world applications. The performance of\ntabular models--such as gradient boosted decision trees and neural\nnetworks--can vary significantly across datasets due to differences in feature\ndistributions and task characteristics. Achieving top performance on each\ndataset often requires specialized expert knowledge. To address this\nvariability, practitioners often aggregate the predictions of multiple models.\nHowever, conventional aggregation strategies typically rely on static\ncombination rules and lack instance-level adaptability. In this work, we\npropose an in-context ensemble framework for tabular prediction that leverages\nlarge language models (LLMs) to perform dynamic, instance-specific integration\nof external model predictions. Without access to raw tabular features or\nsemantic information, our method constructs a context around each test instance\nusing its nearest neighbors and the predictions from a pool of external models.\nWithin this enriched context, we introduce Chain of Tabular Thoughts (CoT$^2$),\na prompting strategy that guides LLMs through multi-step, interpretable\nreasoning, making still further progress toward expert-level decision-making.\nExperimental results show that our method outperforms well-tuned baselines and\nstandard ensemble techniques across a wide range of tabular datasets."
                },
                "authors": [
                    {
                        "name": "Si-Yang Liu"
                    },
                    {
                        "name": "Qile Zhou"
                    },
                    {
                        "name": "Han-Jia Ye"
                    }
                ],
                "author_detail": {
                    "name": "Han-Jia Ye"
                },
                "author": "Han-Jia Ye",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13421v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13421v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13418v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13418v1",
                "updated": "2025-05-19T17:51:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    51,
                    35,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T17:51:35Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    51,
                    35,
                    0,
                    139,
                    0
                ],
                "title": "Dementia Through Different Eyes: Explainable Modeling of Human and LLM\n  Perceptions for Early Awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dementia Through Different Eyes: Explainable Modeling of Human and LLM\n  Perceptions for Early Awareness"
                },
                "summary": "Cognitive decline often surfaces in language years before diagnosis. It is\nfrequently non-experts, such as those closest to the patient, who first sense a\nchange and raise concern. As LLMs become integrated into daily communication\nand used over prolonged periods, it may even be an LLM that notices something\nis off. But what exactly do they notice--and should be noticing--when making\nthat judgment? This paper investigates how dementia is perceived through\nlanguage by non-experts. We presented transcribed picture descriptions to\nnon-expert humans and LLMs, asking them to intuitively judge whether each text\nwas produced by someone healthy or with dementia. We introduce an explainable\nmethod that uses LLMs to extract high-level, expert-guided features\nrepresenting these picture descriptions, and use logistic regression to model\nhuman and LLM perceptions and compare with clinical diagnoses. Our analysis\nreveals that human perception of dementia is inconsistent and relies on a\nnarrow, and sometimes misleading, set of cues. LLMs, by contrast, draw on a\nricher, more nuanced feature set that aligns more closely with clinical\npatterns. Still, both groups show a tendency toward false negatives, frequently\noverlooking dementia cases. Through our interpretable framework and the\ninsights it provides, we hope to help non-experts better recognize the\nlinguistic signs that matter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive decline often surfaces in language years before diagnosis. It is\nfrequently non-experts, such as those closest to the patient, who first sense a\nchange and raise concern. As LLMs become integrated into daily communication\nand used over prolonged periods, it may even be an LLM that notices something\nis off. But what exactly do they notice--and should be noticing--when making\nthat judgment? This paper investigates how dementia is perceived through\nlanguage by non-experts. We presented transcribed picture descriptions to\nnon-expert humans and LLMs, asking them to intuitively judge whether each text\nwas produced by someone healthy or with dementia. We introduce an explainable\nmethod that uses LLMs to extract high-level, expert-guided features\nrepresenting these picture descriptions, and use logistic regression to model\nhuman and LLM perceptions and compare with clinical diagnoses. Our analysis\nreveals that human perception of dementia is inconsistent and relies on a\nnarrow, and sometimes misleading, set of cues. LLMs, by contrast, draw on a\nricher, more nuanced feature set that aligns more closely with clinical\npatterns. Still, both groups show a tendency toward false negatives, frequently\noverlooking dementia cases. Through our interpretable framework and the\ninsights it provides, we hope to help non-experts better recognize the\nlinguistic signs that matter."
                },
                "authors": [
                    {
                        "name": "Lotem Peled-Cohen"
                    },
                    {
                        "name": "Maya Zadok"
                    },
                    {
                        "name": "Nitay Calderon"
                    },
                    {
                        "name": "Hila Gonen"
                    },
                    {
                        "name": "Roi Reichart"
                    }
                ],
                "author_detail": {
                    "name": "Roi Reichart"
                },
                "author": "Roi Reichart",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13418v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13418v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10951v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10951v2",
                "updated": "2025-05-19T17:51:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    51,
                    26,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-16T07:39:41Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    7,
                    39,
                    41,
                    4,
                    136,
                    0
                ],
                "title": "SubGCache: Accelerating Graph-based RAG with Subgraph-level KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SubGCache: Accelerating Graph-based RAG with Subgraph-level KV Cache"
                },
                "summary": "Graph-based retrieval-augmented generation (RAG) enables large language\nmodels (LLMs) to incorporate structured knowledge via graph retrieval as\ncontextual input, enhancing more accurate and context-aware reasoning. We\nobserve that for different queries, it could retrieve similar subgraphs as\nprompts, and thus we propose SubGCache, which aims to reduce inference latency\nby reusing computation across queries with similar structural prompts (i.e.,\nsubgraphs). Specifically, SubGCache clusters queries based on subgraph\nembeddings, constructs a representative subgraph for each cluster, and\npre-computes the key-value (KV) cache of the representative subgraph. For each\nquery with its retrieved subgraph within a cluster, it reuses the pre-computed\nKV cache of the representative subgraph of the cluster without computing the KV\ntensors again for saving computation. Experiments on two new datasets across\nmultiple LLM backbones and graph-based RAG frameworks demonstrate that\nSubGCache consistently reduces inference latency with comparable and even\nimproved generation quality, achieving up to 6.68$\\times$ reduction in\ntime-to-first-token (TTFT).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-based retrieval-augmented generation (RAG) enables large language\nmodels (LLMs) to incorporate structured knowledge via graph retrieval as\ncontextual input, enhancing more accurate and context-aware reasoning. We\nobserve that for different queries, it could retrieve similar subgraphs as\nprompts, and thus we propose SubGCache, which aims to reduce inference latency\nby reusing computation across queries with similar structural prompts (i.e.,\nsubgraphs). Specifically, SubGCache clusters queries based on subgraph\nembeddings, constructs a representative subgraph for each cluster, and\npre-computes the key-value (KV) cache of the representative subgraph. For each\nquery with its retrieved subgraph within a cluster, it reuses the pre-computed\nKV cache of the representative subgraph of the cluster without computing the KV\ntensors again for saving computation. Experiments on two new datasets across\nmultiple LLM backbones and graph-based RAG frameworks demonstrate that\nSubGCache consistently reduces inference latency with comparable and even\nimproved generation quality, achieving up to 6.68$\\times$ reduction in\ntime-to-first-token (TTFT)."
                },
                "authors": [
                    {
                        "name": "Qiuyu Zhu"
                    },
                    {
                        "name": "Liang Zhang"
                    },
                    {
                        "name": "Qianxiong Xu"
                    },
                    {
                        "name": "Cheng Long"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "author": "Jie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10951v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10951v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13416v1",
                "updated": "2025-05-19T17:50:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    50,
                    45,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T17:50:45Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    50,
                    45,
                    0,
                    139,
                    0
                ],
                "title": "Gluon: Making Muon & Scion Great Again! (Bridging Theory and Practice of\n  LMO-based Optimizers for LLMs)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gluon: Making Muon & Scion Great Again! (Bridging Theory and Practice of\n  LMO-based Optimizers for LLMs)"
                },
                "summary": "Recent developments in deep learning optimization have brought about\nradically new algorithms based on the Linear Minimization Oracle (LMO)\nframework, such as $\\sf Muon$ and $\\sf Scion$. After over a decade of $\\sf\nAdam$'s dominance, these LMO-based methods are emerging as viable replacements,\noffering several practical advantages such as improved memory efficiency,\nbetter hyperparameter transferability, and most importantly, superior empirical\nperformance on large-scale tasks, including LLM training. However, a\nsignificant gap remains between their practical use and our current theoretical\nunderstanding: prior analyses (1) overlook the layer-wise LMO application of\nthese optimizers in practice, and (2) rely on an unrealistic smoothness\nassumption, leading to impractically small stepsizes. To address both, we\npropose a new LMO-based method called $\\sf Gluon$, capturing prior\ntheoretically analyzed methods as special cases, and introduce a new refined\ngeneralized smoothness model that captures the layer-wise geometry of neural\nnetworks, matches the layer-wise practical implementation of $\\sf Muon$ and\n$\\sf Scion$, and leads to convergence guarantees with strong practical\npredictive power. Unlike prior results, our theoretical stepsizes closely match\nthe fine-tuned values reported by Pethick et al. (2025). Our experiments with\nNanoGPT and CNN confirm that our assumption holds along the optimization\ntrajectory, ultimately closing the gap between theory and practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent developments in deep learning optimization have brought about\nradically new algorithms based on the Linear Minimization Oracle (LMO)\nframework, such as $\\sf Muon$ and $\\sf Scion$. After over a decade of $\\sf\nAdam$'s dominance, these LMO-based methods are emerging as viable replacements,\noffering several practical advantages such as improved memory efficiency,\nbetter hyperparameter transferability, and most importantly, superior empirical\nperformance on large-scale tasks, including LLM training. However, a\nsignificant gap remains between their practical use and our current theoretical\nunderstanding: prior analyses (1) overlook the layer-wise LMO application of\nthese optimizers in practice, and (2) rely on an unrealistic smoothness\nassumption, leading to impractically small stepsizes. To address both, we\npropose a new LMO-based method called $\\sf Gluon$, capturing prior\ntheoretically analyzed methods as special cases, and introduce a new refined\ngeneralized smoothness model that captures the layer-wise geometry of neural\nnetworks, matches the layer-wise practical implementation of $\\sf Muon$ and\n$\\sf Scion$, and leads to convergence guarantees with strong practical\npredictive power. Unlike prior results, our theoretical stepsizes closely match\nthe fine-tuned values reported by Pethick et al. (2025). Our experiments with\nNanoGPT and CNN confirm that our assumption holds along the optimization\ntrajectory, ultimately closing the gap between theory and practice."
                },
                "authors": [
                    {
                        "name": "Artem Riabinin"
                    },
                    {
                        "name": "Egor Shulgin"
                    },
                    {
                        "name": "Kaja Gruntkowska"
                    },
                    {
                        "name": "Peter Richtárik"
                    }
                ],
                "author_detail": {
                    "name": "Peter Richtárik"
                },
                "author": "Peter Richtárik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13406v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13406v1",
                "updated": "2025-05-19T17:41:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    41,
                    29,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T17:41:29Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    41,
                    29,
                    0,
                    139,
                    0
                ],
                "title": "AutoMathKG: The automated mathematical knowledge graph based on LLM and\n  vector database",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoMathKG: The automated mathematical knowledge graph based on LLM and\n  vector database"
                },
                "summary": "A mathematical knowledge graph (KG) presents knowledge within the field of\nmathematics in a structured manner. Constructing a math KG using natural\nlanguage is an essential but challenging task. There are two major limitations\nof existing works: first, they are constrained by corpus completeness, often\ndiscarding or manually supplementing incomplete knowledge; second, they\ntypically fail to fully automate the integration of diverse knowledge sources.\nThis paper proposes AutoMathKG, a high-quality, wide-coverage, and\nmulti-dimensional math KG capable of automatic updates. AutoMathKG regards\nmathematics as a vast directed graph composed of Definition, Theorem, and\nProblem entities, with their reference relationships as edges. It integrates\nknowledge from ProofWiki, textbooks, arXiv papers, and TheoremQA, enhancing\nentities and relationships with large language models (LLMs) via in-context\nlearning for data augmentation. To search for similar entities, MathVD, a\nvector database, is built through two designed embedding strategies using\nSBERT. To automatically update, two mechanisms are proposed. For knowledge\ncompletion mechanism, Math LLM is developed to interact with AutoMathKG,\nproviding missing proofs or solutions. For knowledge fusion mechanism, MathVD\nis used to retrieve similar entities, and LLM is used to determine whether to\nmerge with a candidate or add as a new entity. A wide range of experiments\ndemonstrate the advanced performance and broad applicability of the AutoMathKG\nsystem, including superior reachability query results in MathVD compared to\nfive baselines and robust mathematical reasoning capability in Math LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A mathematical knowledge graph (KG) presents knowledge within the field of\nmathematics in a structured manner. Constructing a math KG using natural\nlanguage is an essential but challenging task. There are two major limitations\nof existing works: first, they are constrained by corpus completeness, often\ndiscarding or manually supplementing incomplete knowledge; second, they\ntypically fail to fully automate the integration of diverse knowledge sources.\nThis paper proposes AutoMathKG, a high-quality, wide-coverage, and\nmulti-dimensional math KG capable of automatic updates. AutoMathKG regards\nmathematics as a vast directed graph composed of Definition, Theorem, and\nProblem entities, with their reference relationships as edges. It integrates\nknowledge from ProofWiki, textbooks, arXiv papers, and TheoremQA, enhancing\nentities and relationships with large language models (LLMs) via in-context\nlearning for data augmentation. To search for similar entities, MathVD, a\nvector database, is built through two designed embedding strategies using\nSBERT. To automatically update, two mechanisms are proposed. For knowledge\ncompletion mechanism, Math LLM is developed to interact with AutoMathKG,\nproviding missing proofs or solutions. For knowledge fusion mechanism, MathVD\nis used to retrieve similar entities, and LLM is used to determine whether to\nmerge with a candidate or add as a new entity. A wide range of experiments\ndemonstrate the advanced performance and broad applicability of the AutoMathKG\nsystem, including superior reachability query results in MathVD compared to\nfive baselines and robust mathematical reasoning capability in Math LLM."
                },
                "authors": [
                    {
                        "name": "Rong Bian"
                    },
                    {
                        "name": "Yu Geng"
                    },
                    {
                        "name": "Zijian Yang"
                    },
                    {
                        "name": "Bing Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Bing Cheng"
                },
                "author": "Bing Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13406v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13406v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13403v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13403v1",
                "updated": "2025-05-19T17:37:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    37,
                    39,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T17:37:39Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    37,
                    39,
                    0,
                    139,
                    0
                ],
                "title": "MR. Judge: Multimodal Reasoner as a Judge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MR. Judge: Multimodal Reasoner as a Judge"
                },
                "summary": "The paradigm of using Large Language Models (LLMs) and Multimodal Large\nLanguage Models (MLLMs) as evaluative judges has emerged as an effective\napproach in RLHF and inference-time scaling. In this work, we propose\nMultimodal Reasoner as a Judge (MR. Judge), a paradigm for empowering\ngeneral-purpose MLLMs judges with strong reasoning capabilities. Instead of\ndirectly assigning scores for each response, we formulate the judgement process\nas a reasoning-inspired multiple-choice problem. Specifically, the judge model\nfirst conducts deliberate reasoning covering different aspects of the responses\nand eventually selects the best response from them. This reasoning process not\nonly improves the interpretibility of the judgement, but also greatly enhances\nthe performance of MLLM judges. To cope with the lack of questions with scored\nresponses, we propose the following strategy to achieve automatic annotation:\n1) Reverse Response Candidates Synthesis: starting from a supervised\nfine-tuning (SFT) dataset, we treat the original response as the best candidate\nand prompt the MLLM to generate plausible but flawed negative candidates. 2)\nText-based reasoning extraction: we carefully design a data synthesis pipeline\nfor distilling the reasoning capability from a text-based reasoning model,\nwhich is adopted to enable the MLLM judges to regain complex reasoning ability\nvia warm up supervised fine-tuning. Experiments demonstrate that our MR. Judge\nis effective across a wide range of tasks. Specifically, our MR. Judge-7B\nsurpasses GPT-4o by 9.9% on VL-RewardBench, and improves performance on MM-Vet\nduring inference-time scaling by up to 7.7%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The paradigm of using Large Language Models (LLMs) and Multimodal Large\nLanguage Models (MLLMs) as evaluative judges has emerged as an effective\napproach in RLHF and inference-time scaling. In this work, we propose\nMultimodal Reasoner as a Judge (MR. Judge), a paradigm for empowering\ngeneral-purpose MLLMs judges with strong reasoning capabilities. Instead of\ndirectly assigning scores for each response, we formulate the judgement process\nas a reasoning-inspired multiple-choice problem. Specifically, the judge model\nfirst conducts deliberate reasoning covering different aspects of the responses\nand eventually selects the best response from them. This reasoning process not\nonly improves the interpretibility of the judgement, but also greatly enhances\nthe performance of MLLM judges. To cope with the lack of questions with scored\nresponses, we propose the following strategy to achieve automatic annotation:\n1) Reverse Response Candidates Synthesis: starting from a supervised\nfine-tuning (SFT) dataset, we treat the original response as the best candidate\nand prompt the MLLM to generate plausible but flawed negative candidates. 2)\nText-based reasoning extraction: we carefully design a data synthesis pipeline\nfor distilling the reasoning capability from a text-based reasoning model,\nwhich is adopted to enable the MLLM judges to regain complex reasoning ability\nvia warm up supervised fine-tuning. Experiments demonstrate that our MR. Judge\nis effective across a wide range of tasks. Specifically, our MR. Judge-7B\nsurpasses GPT-4o by 9.9% on VL-RewardBench, and improves performance on MM-Vet\nduring inference-time scaling by up to 7.7%."
                },
                "authors": [
                    {
                        "name": "Renjie Pi"
                    },
                    {
                        "name": "Felix Bai"
                    },
                    {
                        "name": "Qibin Chen"
                    },
                    {
                        "name": "Simon Wang"
                    },
                    {
                        "name": "Jiulong Shan"
                    },
                    {
                        "name": "Kieran Liu"
                    },
                    {
                        "name": "Meng Cao"
                    }
                ],
                "author_detail": {
                    "name": "Meng Cao"
                },
                "author": "Meng Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13403v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13403v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10573v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10573v2",
                "updated": "2025-05-19T17:36:27Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    36,
                    27,
                    0,
                    139,
                    0
                ],
                "published": "2025-03-13T17:23:45Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    23,
                    45,
                    3,
                    72,
                    0
                ],
                "title": "Evaluating Mathematical Reasoning Across Large Language Models: A\n  Fine-Grained Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Mathematical Reasoning Across Large Language Models: A\n  Fine-Grained Approach"
                },
                "summary": "With the rapid advancement of Artificial Intelligence (AI), Large Language\nModels (LLMs) have significantly impacted a wide array of domains, including\nhealthcare, engineering, science, education, and mathematical reasoning. Among\nthese, mathematical reasoning remains a particularly challenging capability,\noften requiring multi-step logic and abstract generalization. While prior work\nhas explored LLM performance on reasoning tasks, comprehensive evaluations that\nspan both depth and breadth across model families remain limited. In this\nstudy, we present a systematic evaluation of mathematical reasoning abilities\nacross eight leading LLMs, including two recent DeepSeek models, using three\nindependent benchmark datasets. Our analyses reveal several key findings: (1)\nDeepSeek-R1 performs competitively with o1 across most domains and achieves the\nhighest accuracy on the MMLU Formal Logic benchmark; (2) distilled variants,\nsuch as DeepSeek-1.5B, exhibit substantial performance degradation; and (3)\nGemini 2.0 Flash achieves the lowest response latency. Beyond quantitative\nmetrics, we explore how architectural choices, training paradigms, and\noptimization strategies contribute to variation in reasoning performance. These\nfindings provide new insights into the capabilities and limitations of current\nLLMs in mathematical domains, and offer guidance for the development of future\nmodels better aligned with rigorous reasoning demands.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of Artificial Intelligence (AI), Large Language\nModels (LLMs) have significantly impacted a wide array of domains, including\nhealthcare, engineering, science, education, and mathematical reasoning. Among\nthese, mathematical reasoning remains a particularly challenging capability,\noften requiring multi-step logic and abstract generalization. While prior work\nhas explored LLM performance on reasoning tasks, comprehensive evaluations that\nspan both depth and breadth across model families remain limited. In this\nstudy, we present a systematic evaluation of mathematical reasoning abilities\nacross eight leading LLMs, including two recent DeepSeek models, using three\nindependent benchmark datasets. Our analyses reveal several key findings: (1)\nDeepSeek-R1 performs competitively with o1 across most domains and achieves the\nhighest accuracy on the MMLU Formal Logic benchmark; (2) distilled variants,\nsuch as DeepSeek-1.5B, exhibit substantial performance degradation; and (3)\nGemini 2.0 Flash achieves the lowest response latency. Beyond quantitative\nmetrics, we explore how architectural choices, training paradigms, and\noptimization strategies contribute to variation in reasoning performance. These\nfindings provide new insights into the capabilities and limitations of current\nLLMs in mathematical domains, and offer guidance for the development of future\nmodels better aligned with rigorous reasoning demands."
                },
                "authors": [
                    {
                        "name": "Afrar Jahin"
                    },
                    {
                        "name": "Arif Hassan Zidan"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Yu Bao"
                    },
                    {
                        "name": "Tianming Liu"
                    }
                ],
                "author_detail": {
                    "name": "Tianming Liu"
                },
                "author": "Tianming Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10573v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10573v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13379v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13379v1",
                "updated": "2025-05-19T17:24:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    24,
                    16,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T17:24:16Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    24,
                    16,
                    0,
                    139,
                    0
                ],
                "title": "Thinkless: LLM Learns When to Think",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thinkless: LLM Learns When to Think"
                },
                "summary": "Reasoning Language Models, capable of extended chain-of-thought reasoning,\nhave demonstrated remarkable performance on tasks requiring complex logical\ninference. However, applying elaborate reasoning for all queries often results\nin substantial computational inefficiencies, particularly when many problems\nadmit straightforward solutions. This motivates an open question: Can LLMs\nlearn when to think? To answer this, we propose Thinkless, a learnable\nframework that empowers an LLM to adaptively select between short-form and\nlong-form reasoning, based on both task complexity and the model's ability.\nThinkless is trained under a reinforcement learning paradigm and employs two\ncontrol tokens, <short> for concise responses and <think> for detailed\nreasoning. At the core of our method is a Decoupled Group Relative Policy\nOptimization (DeGRPO) algorithm, which decomposes the learning objective of\nhybrid reasoning into two components: (1) a control token loss that governs the\nselection of the reasoning mode, and (2) a response loss that improves the\naccuracy of the generated answers. This decoupled formulation enables\nfine-grained control over the contributions of each objective, stabilizing\ntraining and effectively preventing collapse observed in vanilla GRPO.\nEmpirically, on several benchmarks such as Minerva Algebra, MATH-500, and\nGSM8K, Thinkless is able to reduce the usage of long-chain thinking by 50% -\n90%, significantly improving the efficiency of Reasoning Language Models. The\ncode is available at https://github.com/VainF/Thinkless",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Language Models, capable of extended chain-of-thought reasoning,\nhave demonstrated remarkable performance on tasks requiring complex logical\ninference. However, applying elaborate reasoning for all queries often results\nin substantial computational inefficiencies, particularly when many problems\nadmit straightforward solutions. This motivates an open question: Can LLMs\nlearn when to think? To answer this, we propose Thinkless, a learnable\nframework that empowers an LLM to adaptively select between short-form and\nlong-form reasoning, based on both task complexity and the model's ability.\nThinkless is trained under a reinforcement learning paradigm and employs two\ncontrol tokens, <short> for concise responses and <think> for detailed\nreasoning. At the core of our method is a Decoupled Group Relative Policy\nOptimization (DeGRPO) algorithm, which decomposes the learning objective of\nhybrid reasoning into two components: (1) a control token loss that governs the\nselection of the reasoning mode, and (2) a response loss that improves the\naccuracy of the generated answers. This decoupled formulation enables\nfine-grained control over the contributions of each objective, stabilizing\ntraining and effectively preventing collapse observed in vanilla GRPO.\nEmpirically, on several benchmarks such as Minerva Algebra, MATH-500, and\nGSM8K, Thinkless is able to reduce the usage of long-chain thinking by 50% -\n90%, significantly improving the efficiency of Reasoning Language Models. The\ncode is available at https://github.com/VainF/Thinkless"
                },
                "authors": [
                    {
                        "name": "Gongfan Fang"
                    },
                    {
                        "name": "Xinyin Ma"
                    },
                    {
                        "name": "Xinchao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinchao Wang"
                },
                "author": "Xinchao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13379v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13379v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10872v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10872v2",
                "updated": "2025-05-19T17:21:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    21,
                    49,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-16T05:27:15Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    5,
                    27,
                    15,
                    4,
                    136,
                    0
                ],
                "title": "REI-Bench: Can Embodied Agents Understand Vague Human Instructions in\n  Task Planning?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REI-Bench: Can Embodied Agents Understand Vague Human Instructions in\n  Task Planning?"
                },
                "summary": "Robot task planning decomposes human instructions into executable action\nsequences that enable robots to complete a series of complex tasks. Although\nrecent large language model (LLM)-based task planners achieve amazing\nperformance, they assume that human instructions are clear and straightforward.\nHowever, real-world users are not experts, and their instructions to robots\noften contain significant vagueness. Linguists suggest that such vagueness\nfrequently arises from referring expressions (REs), whose meanings depend\nheavily on dialogue context and environment. This vagueness is even more\nprevalent among the elderly and children, who robots should serve more. This\npaper studies how such vagueness in REs within human instructions affects\nLLM-based robot task planning and how to overcome this issue. To this end, we\npropose the first robot task planning benchmark with vague REs (REI-Bench),\nwhere we discover that the vagueness of REs can severely degrade robot planning\nperformance, leading to success rate drops of up to 77.9%. We also observe that\nmost failure cases stem from missing objects in planners. To mitigate the REs\nissue, we propose a simple yet effective approach: task-oriented context\ncognition, which generates clear instructions for robots, achieving\nstate-of-the-art performance compared to aware prompt and chains of thought.\nThis work contributes to the research community of human-robot interaction\n(HRI) by making robot task planning more practical, particularly for non-expert\nusers, e.g., the elderly and children.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robot task planning decomposes human instructions into executable action\nsequences that enable robots to complete a series of complex tasks. Although\nrecent large language model (LLM)-based task planners achieve amazing\nperformance, they assume that human instructions are clear and straightforward.\nHowever, real-world users are not experts, and their instructions to robots\noften contain significant vagueness. Linguists suggest that such vagueness\nfrequently arises from referring expressions (REs), whose meanings depend\nheavily on dialogue context and environment. This vagueness is even more\nprevalent among the elderly and children, who robots should serve more. This\npaper studies how such vagueness in REs within human instructions affects\nLLM-based robot task planning and how to overcome this issue. To this end, we\npropose the first robot task planning benchmark with vague REs (REI-Bench),\nwhere we discover that the vagueness of REs can severely degrade robot planning\nperformance, leading to success rate drops of up to 77.9%. We also observe that\nmost failure cases stem from missing objects in planners. To mitigate the REs\nissue, we propose a simple yet effective approach: task-oriented context\ncognition, which generates clear instructions for robots, achieving\nstate-of-the-art performance compared to aware prompt and chains of thought.\nThis work contributes to the research community of human-robot interaction\n(HRI) by making robot task planning more practical, particularly for non-expert\nusers, e.g., the elderly and children."
                },
                "authors": [
                    {
                        "name": "Chenxi Jiang"
                    },
                    {
                        "name": "Chuhao Zhou"
                    },
                    {
                        "name": "Jianfei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jianfei Yang"
                },
                "author": "Jianfei Yang",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10872v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10872v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13376v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13376v1",
                "updated": "2025-05-19T17:19:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    19,
                    43,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T17:19:43Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    19,
                    43,
                    0,
                    139,
                    0
                ],
                "title": "Seeing, Saying, Solving: An LLM-to-TL Framework for Cooperative Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seeing, Saying, Solving: An LLM-to-TL Framework for Cooperative Robots"
                },
                "summary": "Increased robot deployment, such as in warehousing, has revealed a need for\nseamless collaboration among heterogeneous robot teams to resolve unforeseen\nconflicts. To address this challenge, we propose a novel, decentralized\nframework for robots to request and provide help. The framework begins with\nrobots detecting conflicts using a Vision Language Model (VLM), then reasoning\nover whether help is needed. If so, it crafts and broadcasts a natural language\n(NL) help request using a Large Language Model (LLM). Potential helper robots\nreason over the request and offer help (if able), along with information about\nimpact to their current tasks. Helper reasoning is implemented via an LLM\ngrounded in Signal Temporal Logic (STL) using a Backus-Naur Form (BNF) grammar\nto guarantee syntactically valid NL-to-STL translations, which are then solved\nas a Mixed Integer Linear Program (MILP). Finally, the requester robot chooses\na helper by reasoning over impact on the overall system. We evaluate our system\nvia experiments considering different strategies for choosing a helper, and\nfind that a requester robot can minimize overall time impact on the system by\nconsidering multiple help offers versus simple heuristics (e.g., selecting the\nnearest robot to help).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Increased robot deployment, such as in warehousing, has revealed a need for\nseamless collaboration among heterogeneous robot teams to resolve unforeseen\nconflicts. To address this challenge, we propose a novel, decentralized\nframework for robots to request and provide help. The framework begins with\nrobots detecting conflicts using a Vision Language Model (VLM), then reasoning\nover whether help is needed. If so, it crafts and broadcasts a natural language\n(NL) help request using a Large Language Model (LLM). Potential helper robots\nreason over the request and offer help (if able), along with information about\nimpact to their current tasks. Helper reasoning is implemented via an LLM\ngrounded in Signal Temporal Logic (STL) using a Backus-Naur Form (BNF) grammar\nto guarantee syntactically valid NL-to-STL translations, which are then solved\nas a Mixed Integer Linear Program (MILP). Finally, the requester robot chooses\na helper by reasoning over impact on the overall system. We evaluate our system\nvia experiments considering different strategies for choosing a helper, and\nfind that a requester robot can minimize overall time impact on the system by\nconsidering multiple help offers versus simple heuristics (e.g., selecting the\nnearest robot to help)."
                },
                "authors": [
                    {
                        "name": "Dan BW Choe"
                    },
                    {
                        "name": "Sundhar Vinodh Sangeetha"
                    },
                    {
                        "name": "Steven Emanuel"
                    },
                    {
                        "name": "Chih-Yuan Chiu"
                    },
                    {
                        "name": "Samuel Coogan"
                    },
                    {
                        "name": "Shreyas Kousik"
                    }
                ],
                "author_detail": {
                    "name": "Shreyas Kousik"
                },
                "author": "Shreyas Kousik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13376v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13376v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11380v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11380v2",
                "updated": "2025-05-19T17:17:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    17,
                    0,
                    0,
                    139,
                    0
                ],
                "published": "2025-02-17T02:52:07Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    2,
                    52,
                    7,
                    0,
                    48,
                    0
                ],
                "title": "From the New World of Word Embeddings: A Comparative Study of\n  Small-World Lexico-Semantic Networks in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From the New World of Word Embeddings: A Comparative Study of\n  Small-World Lexico-Semantic Networks in LLMs"
                },
                "summary": "Lexico-semantic networks represent words as nodes and their semantic\nrelatedness as edges. While such networks are traditionally constructed using\nembeddings from encoder-based models or static vectors, embeddings from\ndecoder-only large language models (LLMs) remain underexplored. Unlike encoder\nmodels, LLMs are trained with a next-token prediction objective, which does not\ndirectly encode the meaning of the current token. In this paper, we construct\nlexico-semantic networks from the input embeddings of LLMs with varying\nparameter scales and conduct a comparative analysis of their global and local\nstructures. Our results show that these networks exhibit small-world\nproperties, characterized by high clustering and short path lengths. Moreover,\nlarger LLMs yield more intricate networks with less small-world effects and\nlonger paths, reflecting richer semantic structures and relations. We further\nvalidate our approach through analyses of common conceptual pairs, structured\nlexical relations derived from WordNet, and a cross-lingual semantic network\nfor qualitative words.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lexico-semantic networks represent words as nodes and their semantic\nrelatedness as edges. While such networks are traditionally constructed using\nembeddings from encoder-based models or static vectors, embeddings from\ndecoder-only large language models (LLMs) remain underexplored. Unlike encoder\nmodels, LLMs are trained with a next-token prediction objective, which does not\ndirectly encode the meaning of the current token. In this paper, we construct\nlexico-semantic networks from the input embeddings of LLMs with varying\nparameter scales and conduct a comparative analysis of their global and local\nstructures. Our results show that these networks exhibit small-world\nproperties, characterized by high clustering and short path lengths. Moreover,\nlarger LLMs yield more intricate networks with less small-world effects and\nlonger paths, reflecting richer semantic structures and relations. We further\nvalidate our approach through analyses of common conceptual pairs, structured\nlexical relations derived from WordNet, and a cross-lingual semantic network\nfor qualitative words."
                },
                "authors": [
                    {
                        "name": "Zhu Liu"
                    },
                    {
                        "name": "Ying Liu"
                    },
                    {
                        "name": "KangYang Luo"
                    },
                    {
                        "name": "Cunliang Kong"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "arxiv_comment": "Paper under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11380v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11380v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14281v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14281v3",
                "updated": "2025-05-20T05:55:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    5,
                    55,
                    59,
                    1,
                    140,
                    0
                ],
                "published": "2025-03-18T14:20:54Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    14,
                    20,
                    54,
                    1,
                    77,
                    0
                ],
                "title": "XOXO: Stealthy Cross-Origin Context Poisoning Attacks against AI Coding\n  Assistants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XOXO: Stealthy Cross-Origin Context Poisoning Attacks against AI Coding\n  Assistants"
                },
                "summary": "AI coding assistants are widely used for tasks like code generation. These\ntools now require large and complex contexts, automatically sourced from\nvarious origins$\\unicode{x2014}$across files, projects, and\ncontributors$\\unicode{x2014}$forming part of the prompt fed to underlying LLMs.\nThis automatic context-gathering introduces new vulnerabilities, allowing\nattackers to subtly poison input to compromise the assistant's outputs,\npotentially generating vulnerable code or introducing critical errors. We\npropose a novel attack, Cross-Origin Context Poisoning (XOXO), that is\nchallenging to detect as it relies on adversarial code modifications that are\nsemantically equivalent. Traditional program analysis techniques struggle to\nidentify these perturbations since the semantics of the code remains correct,\nmaking it appear legitimate. This allows attackers to manipulate coding\nassistants into producing incorrect outputs, while shifting the blame to the\nvictim developer. We introduce a novel, task-agnostic, black-box attack\nalgorithm GCGS that systematically searches the transformation space using a\nCayley Graph, achieving a 75.72% attack success rate on average across five\ntasks and eleven models, including GPT 4.1 and Claude 3.5 Sonnet v2 used by\npopular AI coding assistants. Furthermore, defenses like adversarial\nfine-tuning are ineffective against our attack, underscoring the need for new\nsecurity measures in LLM-powered coding tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI coding assistants are widely used for tasks like code generation. These\ntools now require large and complex contexts, automatically sourced from\nvarious origins$\\unicode{x2014}$across files, projects, and\ncontributors$\\unicode{x2014}$forming part of the prompt fed to underlying LLMs.\nThis automatic context-gathering introduces new vulnerabilities, allowing\nattackers to subtly poison input to compromise the assistant's outputs,\npotentially generating vulnerable code or introducing critical errors. We\npropose a novel attack, Cross-Origin Context Poisoning (XOXO), that is\nchallenging to detect as it relies on adversarial code modifications that are\nsemantically equivalent. Traditional program analysis techniques struggle to\nidentify these perturbations since the semantics of the code remains correct,\nmaking it appear legitimate. This allows attackers to manipulate coding\nassistants into producing incorrect outputs, while shifting the blame to the\nvictim developer. We introduce a novel, task-agnostic, black-box attack\nalgorithm GCGS that systematically searches the transformation space using a\nCayley Graph, achieving a 75.72% attack success rate on average across five\ntasks and eleven models, including GPT 4.1 and Claude 3.5 Sonnet v2 used by\npopular AI coding assistants. Furthermore, defenses like adversarial\nfine-tuning are ineffective against our attack, underscoring the need for new\nsecurity measures in LLM-powered coding tools."
                },
                "authors": [
                    {
                        "name": "Adam Štorek"
                    },
                    {
                        "name": "Mukur Gupta"
                    },
                    {
                        "name": "Noopur Bhatt"
                    },
                    {
                        "name": "Aditya Gupta"
                    },
                    {
                        "name": "Janie Kim"
                    },
                    {
                        "name": "Prashast Srivastava"
                    },
                    {
                        "name": "Suman Jana"
                    }
                ],
                "author_detail": {
                    "name": "Suman Jana"
                },
                "author": "Suman Jana",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14281v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14281v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13360v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13360v1",
                "updated": "2025-05-19T17:03:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    3,
                    42,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T17:03:42Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    3,
                    42,
                    0,
                    139,
                    0
                ],
                "title": "What Prompts Don't Say: Understanding and Managing Underspecification in\n  LLM Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Prompts Don't Say: Understanding and Managing Underspecification in\n  LLM Prompts"
                },
                "summary": "Building LLM-powered software requires developers to communicate their\nrequirements through natural language, but developer prompts are frequently\nunderspecified, failing to fully capture many user-important requirements. In\nthis paper, we present an in-depth analysis of prompt underspecification,\nshowing that while LLMs can often (41.1%) guess unspecified requirements by\ndefault, such behavior is less robust: Underspecified prompts are 2x more\nlikely to regress over model or prompt changes, sometimes with accuracy drops\nby more than 20%. We then demonstrate that simply adding more requirements to a\nprompt does not reliably improve performance, due to LLMs' limited\ninstruction-following capabilities and competing constraints, and standard\nprompt optimizers do not offer much help. To address this, we introduce novel\nrequirements-aware prompt optimization mechanisms that can improve performance\nby 4.8% on average over baselines that naively specify everything in the\nprompt. Beyond prompt optimization, we envision that effectively managing\nprompt underspecification requires a broader process, including proactive\nrequirements discovery, evaluation, and monitoring.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building LLM-powered software requires developers to communicate their\nrequirements through natural language, but developer prompts are frequently\nunderspecified, failing to fully capture many user-important requirements. In\nthis paper, we present an in-depth analysis of prompt underspecification,\nshowing that while LLMs can often (41.1%) guess unspecified requirements by\ndefault, such behavior is less robust: Underspecified prompts are 2x more\nlikely to regress over model or prompt changes, sometimes with accuracy drops\nby more than 20%. We then demonstrate that simply adding more requirements to a\nprompt does not reliably improve performance, due to LLMs' limited\ninstruction-following capabilities and competing constraints, and standard\nprompt optimizers do not offer much help. To address this, we introduce novel\nrequirements-aware prompt optimization mechanisms that can improve performance\nby 4.8% on average over baselines that naively specify everything in the\nprompt. Beyond prompt optimization, we envision that effectively managing\nprompt underspecification requires a broader process, including proactive\nrequirements discovery, evaluation, and monitoring."
                },
                "authors": [
                    {
                        "name": "Chenyang Yang"
                    },
                    {
                        "name": "Yike Shi"
                    },
                    {
                        "name": "Qianou Ma"
                    },
                    {
                        "name": "Michael Xieyang Liu"
                    },
                    {
                        "name": "Christian Kästner"
                    },
                    {
                        "name": "Tongshuang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Tongshuang Wu"
                },
                "author": "Tongshuang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13360v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13360v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11114v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11114v2",
                "updated": "2025-05-19T17:02:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    2,
                    39,
                    0,
                    139,
                    0
                ],
                "published": "2025-02-16T13:06:50Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    13,
                    6,
                    50,
                    6,
                    47,
                    0
                ],
                "title": "Beyond Pairwise: Global Zero-shot Temporal Graph Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Pairwise: Global Zero-shot Temporal Graph Generation"
                },
                "summary": "Temporal relation extraction (TRE) is a fundamental task in natural language\nprocessing (NLP) that involves identifying the temporal relationships between\nevents in a document. Despite the advances in large language models (LLMs),\ntheir application to TRE remains limited. Most existing approaches rely on\npairwise classification, where event pairs are classified in isolation, leading\nto computational inefficiency and a lack of global consistency in the resulting\ntemporal graph. In this work, we propose a novel zero-shot method for TRE that\ngenerates a document's complete temporal graph in a single step, followed by\ntemporal constraint optimization to refine predictions and enforce temporal\nconsistency across relations. Additionally, we introduce OmniTemp, a new\ndataset with complete annotations for all pairs of targeted events within a\ndocument. Through experiments and analyses, we demonstrate that our method\noutperforms existing zero-shot approaches and offers a competitive alternative\nto supervised TRE models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal relation extraction (TRE) is a fundamental task in natural language\nprocessing (NLP) that involves identifying the temporal relationships between\nevents in a document. Despite the advances in large language models (LLMs),\ntheir application to TRE remains limited. Most existing approaches rely on\npairwise classification, where event pairs are classified in isolation, leading\nto computational inefficiency and a lack of global consistency in the resulting\ntemporal graph. In this work, we propose a novel zero-shot method for TRE that\ngenerates a document's complete temporal graph in a single step, followed by\ntemporal constraint optimization to refine predictions and enforce temporal\nconsistency across relations. Additionally, we introduce OmniTemp, a new\ndataset with complete annotations for all pairs of targeted events within a\ndocument. Through experiments and analyses, we demonstrate that our method\noutperforms existing zero-shot approaches and offers a competitive alternative\nto supervised TRE models."
                },
                "authors": [
                    {
                        "name": "Alon Eirew"
                    },
                    {
                        "name": "Kfir Bar"
                    },
                    {
                        "name": "Ido Dagan"
                    }
                ],
                "author_detail": {
                    "name": "Ido Dagan"
                },
                "author": "Ido Dagan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11114v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11114v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13355v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13355v1",
                "updated": "2025-05-19T16:57:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    57,
                    57,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T16:57:57Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    57,
                    57,
                    0,
                    139,
                    0
                ],
                "title": "Multi-Armed Bandits Meet Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Armed Bandits Meet Large Language Models"
                },
                "summary": "Bandit algorithms and Large Language Models (LLMs) have emerged as powerful\ntools in artificial intelligence, each addressing distinct yet complementary\nchallenges in decision-making and natural language processing. This survey\nexplores the synergistic potential between these two fields, highlighting how\nbandit algorithms can enhance the performance of LLMs and how LLMs, in turn,\ncan provide novel insights for improving bandit-based decision-making. We first\nexamine the role of bandit algorithms in optimizing LLM fine-tuning, prompt\nengineering, and adaptive response generation, focusing on their ability to\nbalance exploration and exploitation in large-scale learning tasks.\nSubsequently, we explore how LLMs can augment bandit algorithms through\nadvanced contextual understanding, dynamic adaptation, and improved policy\nselection using natural language reasoning. By providing a comprehensive review\nof existing research and identifying key challenges and opportunities, this\nsurvey aims to bridge the gap between bandit algorithms and LLMs, paving the\nway for innovative applications and interdisciplinary research in AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bandit algorithms and Large Language Models (LLMs) have emerged as powerful\ntools in artificial intelligence, each addressing distinct yet complementary\nchallenges in decision-making and natural language processing. This survey\nexplores the synergistic potential between these two fields, highlighting how\nbandit algorithms can enhance the performance of LLMs and how LLMs, in turn,\ncan provide novel insights for improving bandit-based decision-making. We first\nexamine the role of bandit algorithms in optimizing LLM fine-tuning, prompt\nengineering, and adaptive response generation, focusing on their ability to\nbalance exploration and exploitation in large-scale learning tasks.\nSubsequently, we explore how LLMs can augment bandit algorithms through\nadvanced contextual understanding, dynamic adaptation, and improved policy\nselection using natural language reasoning. By providing a comprehensive review\nof existing research and identifying key challenges and opportunities, this\nsurvey aims to bridge the gap between bandit algorithms and LLMs, paving the\nway for innovative applications and interdisciplinary research in AI."
                },
                "authors": [
                    {
                        "name": "Djallel Bouneffouf"
                    },
                    {
                        "name": "Raphael Feraud"
                    }
                ],
                "author_detail": {
                    "name": "Raphael Feraud"
                },
                "author": "Raphael Feraud",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13355v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13355v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13353v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13353v2",
                "updated": "2025-05-20T05:45:55Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    5,
                    45,
                    55,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-19T16:56:31Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    56,
                    31,
                    0,
                    139,
                    0
                ],
                "title": "Sense and Sensitivity: Examining the Influence of Semantic Recall on\n  Long Context Code Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sense and Sensitivity: Examining the Influence of Semantic Recall on\n  Long Context Code Reasoning"
                },
                "summary": "Although modern Large Language Models (LLMs) support extremely large\ncontexts, their effectiveness in utilizing long context for code reasoning\nremains unclear. This paper investigates LLM reasoning ability over code\nsnippets within large repositories and how it relates to their recall ability.\nSpecifically, we differentiate between lexical code recall (verbatim retrieval)\nand semantic code recall (remembering what the code does). To measure semantic\nrecall, we propose SemTrace, a code reasoning technique where the impact of\nspecific statements on output is attributable and unpredictable. We also\npresent a method to quantify semantic recall sensitivity in existing\nbenchmarks. Our evaluation of state-of-the-art LLMs reveals a significant drop\nin code reasoning accuracy as a code snippet approaches the middle of the input\ncontext, particularly with techniques requiring high semantic recall like\nSemTrace. Moreover, we find that lexical recall varies by granularity, with\nmodels excelling at function retrieval but struggling with line-by-line recall.\nNotably, a disconnect exists between lexical and semantic recall, suggesting\ndifferent underlying mechanisms. Finally, our findings indicate that current\ncode reasoning benchmarks may exhibit low semantic recall sensitivity,\npotentially underestimating LLM challenges in leveraging in-context\ninformation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although modern Large Language Models (LLMs) support extremely large\ncontexts, their effectiveness in utilizing long context for code reasoning\nremains unclear. This paper investigates LLM reasoning ability over code\nsnippets within large repositories and how it relates to their recall ability.\nSpecifically, we differentiate between lexical code recall (verbatim retrieval)\nand semantic code recall (remembering what the code does). To measure semantic\nrecall, we propose SemTrace, a code reasoning technique where the impact of\nspecific statements on output is attributable and unpredictable. We also\npresent a method to quantify semantic recall sensitivity in existing\nbenchmarks. Our evaluation of state-of-the-art LLMs reveals a significant drop\nin code reasoning accuracy as a code snippet approaches the middle of the input\ncontext, particularly with techniques requiring high semantic recall like\nSemTrace. Moreover, we find that lexical recall varies by granularity, with\nmodels excelling at function retrieval but struggling with line-by-line recall.\nNotably, a disconnect exists between lexical and semantic recall, suggesting\ndifferent underlying mechanisms. Finally, our findings indicate that current\ncode reasoning benchmarks may exhibit low semantic recall sensitivity,\npotentially underestimating LLM challenges in leveraging in-context\ninformation."
                },
                "authors": [
                    {
                        "name": "Adam Štorek"
                    },
                    {
                        "name": "Mukur Gupta"
                    },
                    {
                        "name": "Samira Hajizadeh"
                    },
                    {
                        "name": "Prashast Srivastava"
                    },
                    {
                        "name": "Suman Jana"
                    }
                ],
                "author_detail": {
                    "name": "Suman Jana"
                },
                "author": "Suman Jana",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13353v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13353v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13348v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13348v1",
                "updated": "2025-05-19T16:51:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    51,
                    12,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T16:51:12Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    51,
                    12,
                    0,
                    139,
                    0
                ],
                "title": "Investigating the Vulnerability of LLM-as-a-Judge Architectures to\n  Prompt-Injection Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating the Vulnerability of LLM-as-a-Judge Architectures to\n  Prompt-Injection Attacks"
                },
                "summary": "Large Language Models (LLMs) are increasingly employed as evaluators\n(LLM-as-a-Judge) for assessing the quality of machine-generated text. This\nparadigm offers scalability and cost-effectiveness compared to human\nannotation. However, the reliability and security of such systems, particularly\ntheir robustness against adversarial manipulations, remain critical concerns.\nThis paper investigates the vulnerability of LLM-as-a-Judge architectures to\nprompt-injection attacks, where malicious inputs are designed to compromise the\njudge's decision-making process. We formalize two primary attack strategies:\nComparative Undermining Attack (CUA), which directly targets the final decision\noutput, and Justification Manipulation Attack (JMA), which aims to alter the\nmodel's generated reasoning. Using the Greedy Coordinate Gradient (GCG)\noptimization method, we craft adversarial suffixes appended to one of the\nresponses being compared. Experiments conducted on the MT-Bench Human Judgments\ndataset with open-source instruction-tuned LLMs (Qwen2.5-3B-Instruct and\nFalcon3-3B-Instruct) demonstrate significant susceptibility. The CUA achieves\nan Attack Success Rate (ASR) exceeding 30\\%, while JMA also shows notable\neffectiveness. These findings highlight substantial vulnerabilities in current\nLLM-as-a-Judge systems, underscoring the need for robust defense mechanisms and\nfurther research into adversarial evaluation and trustworthiness in LLM-based\nassessment frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly employed as evaluators\n(LLM-as-a-Judge) for assessing the quality of machine-generated text. This\nparadigm offers scalability and cost-effectiveness compared to human\nannotation. However, the reliability and security of such systems, particularly\ntheir robustness against adversarial manipulations, remain critical concerns.\nThis paper investigates the vulnerability of LLM-as-a-Judge architectures to\nprompt-injection attacks, where malicious inputs are designed to compromise the\njudge's decision-making process. We formalize two primary attack strategies:\nComparative Undermining Attack (CUA), which directly targets the final decision\noutput, and Justification Manipulation Attack (JMA), which aims to alter the\nmodel's generated reasoning. Using the Greedy Coordinate Gradient (GCG)\noptimization method, we craft adversarial suffixes appended to one of the\nresponses being compared. Experiments conducted on the MT-Bench Human Judgments\ndataset with open-source instruction-tuned LLMs (Qwen2.5-3B-Instruct and\nFalcon3-3B-Instruct) demonstrate significant susceptibility. The CUA achieves\nan Attack Success Rate (ASR) exceeding 30\\%, while JMA also shows notable\neffectiveness. These findings highlight substantial vulnerabilities in current\nLLM-as-a-Judge systems, underscoring the need for robust defense mechanisms and\nfurther research into adversarial evaluation and trustworthiness in LLM-based\nassessment frameworks."
                },
                "authors": [
                    {
                        "name": "Narek Maloyan"
                    },
                    {
                        "name": "Bislan Ashinov"
                    },
                    {
                        "name": "Dmitry Namiot"
                    }
                ],
                "author_detail": {
                    "name": "Dmitry Namiot"
                },
                "author": "Dmitry Namiot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13348v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13348v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13346v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13346v1",
                "updated": "2025-05-19T16:50:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    50,
                    35,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T16:50:35Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    50,
                    35,
                    0,
                    139,
                    0
                ],
                "title": "J4R: Learning to Judge with Equivalent Initial State Group Relative\n  Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "J4R: Learning to Judge with Equivalent Initial State Group Relative\n  Preference Optimization"
                },
                "summary": "To keep pace with the increasing pace of large language models (LLM)\ndevelopment, model output evaluation has transitioned away from time-consuming\nhuman evaluation to automatic evaluation, where LLMs themselves are tasked with\nassessing and critiquing other model outputs. LLM-as-judge models are a class\nof generative evaluators that excel in evaluating relatively simple domains,\nlike chat quality, but struggle in reasoning intensive domains where model\nresponses contain more substantive and challenging content. To remedy existing\njudge shortcomings, we explore training judges with reinforcement learning\n(RL). We make three key contributions: (1) We propose the Equivalent Initial\nState Group Relative Policy Optimization (EIS-GRPO) algorithm, which allows us\nto train our judge to be robust to positional biases that arise in more complex\nevaluation settings. (2) We introduce ReasoningJudgeBench, a benchmark that\nevaluates judges in diverse reasoning settings not covered by prior work. (3)\nWe train Judge for Reasoning (J4R), a 7B judge trained with EIS-GRPO that\noutperforms GPT-4o and the next best small judge by 6.7% and 9%, matching or\nexceeding the performance of larger GRPO-trained judges on both JudgeBench and\nReasoningJudgeBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To keep pace with the increasing pace of large language models (LLM)\ndevelopment, model output evaluation has transitioned away from time-consuming\nhuman evaluation to automatic evaluation, where LLMs themselves are tasked with\nassessing and critiquing other model outputs. LLM-as-judge models are a class\nof generative evaluators that excel in evaluating relatively simple domains,\nlike chat quality, but struggle in reasoning intensive domains where model\nresponses contain more substantive and challenging content. To remedy existing\njudge shortcomings, we explore training judges with reinforcement learning\n(RL). We make three key contributions: (1) We propose the Equivalent Initial\nState Group Relative Policy Optimization (EIS-GRPO) algorithm, which allows us\nto train our judge to be robust to positional biases that arise in more complex\nevaluation settings. (2) We introduce ReasoningJudgeBench, a benchmark that\nevaluates judges in diverse reasoning settings not covered by prior work. (3)\nWe train Judge for Reasoning (J4R), a 7B judge trained with EIS-GRPO that\noutperforms GPT-4o and the next best small judge by 6.7% and 9%, matching or\nexceeding the performance of larger GRPO-trained judges on both JudgeBench and\nReasoningJudgeBench."
                },
                "authors": [
                    {
                        "name": "Austin Xu"
                    },
                    {
                        "name": "Yilun Zhou"
                    },
                    {
                        "name": "Xuan-Phi Nguyen"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Shafiq Joty"
                    }
                ],
                "author_detail": {
                    "name": "Shafiq Joty"
                },
                "author": "Shafiq Joty",
                "arxiv_comment": "25 pages, 4 figures, 6 tables. To be updated with links for\n  code/benchmark",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13346v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13346v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13345v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13345v1",
                "updated": "2025-05-19T16:50:27Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    50,
                    27,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T16:50:27Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    50,
                    27,
                    0,
                    139,
                    0
                ],
                "title": "Occult: Optimizing Collaborative Communication across Experts for\n  Accelerated Parallel MoE Training and Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Occult: Optimizing Collaborative Communication across Experts for\n  Accelerated Parallel MoE Training and Inference"
                },
                "summary": "Mixture-of-experts (MoE) architectures could achieve impressive computational\nefficiency with expert parallelism, which relies heavily on all-to-all\ncommunication across devices. Unfortunately, such communication overhead\ntypically constitutes a significant portion of the total runtime, hampering the\nscalability of distributed training and inference for modern MoE models\n(consuming over $40\\%$ runtime in large-scale training). In this paper, we\nfirst define collaborative communication to illustrate this intrinsic\nlimitation, and then propose system- and algorithm-level innovations to reduce\ncommunication costs. Specifically, given a pair of experts co-activated by one\ntoken, we call them \"collaborated\", which comprises $2$ cases as intra- and\ninter-collaboration, depending on whether they are kept on the same device. Our\npilot investigations reveal that augmenting the proportion of\nintra-collaboration can accelerate expert parallelism at scale. It motivates us\nto strategically optimize collaborative communication for accelerated MoE\ntraining and inference, dubbed Occult. Our designs are capable of either\ndelivering exact results with reduced communication cost or controllably\nminimizing the cost with collaboration pruning, materialized by modified\nfine-tuning. Comprehensive experiments on various MoE-LLMs demonstrate that\nOccult can be faster than popular state-of-the-art inference or training\nframeworks (more than $1.5\\times$ speed up across multiple tasks and models)\nwith comparable or superior quality compared to the standard fine-tuning. Code\nis available at\n$\\href{https://github.com/UNITES-Lab/Occult}{https://github.com/UNITES-Lab/Occult}$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-experts (MoE) architectures could achieve impressive computational\nefficiency with expert parallelism, which relies heavily on all-to-all\ncommunication across devices. Unfortunately, such communication overhead\ntypically constitutes a significant portion of the total runtime, hampering the\nscalability of distributed training and inference for modern MoE models\n(consuming over $40\\%$ runtime in large-scale training). In this paper, we\nfirst define collaborative communication to illustrate this intrinsic\nlimitation, and then propose system- and algorithm-level innovations to reduce\ncommunication costs. Specifically, given a pair of experts co-activated by one\ntoken, we call them \"collaborated\", which comprises $2$ cases as intra- and\ninter-collaboration, depending on whether they are kept on the same device. Our\npilot investigations reveal that augmenting the proportion of\nintra-collaboration can accelerate expert parallelism at scale. It motivates us\nto strategically optimize collaborative communication for accelerated MoE\ntraining and inference, dubbed Occult. Our designs are capable of either\ndelivering exact results with reduced communication cost or controllably\nminimizing the cost with collaboration pruning, materialized by modified\nfine-tuning. Comprehensive experiments on various MoE-LLMs demonstrate that\nOccult can be faster than popular state-of-the-art inference or training\nframeworks (more than $1.5\\times$ speed up across multiple tasks and models)\nwith comparable or superior quality compared to the standard fine-tuning. Code\nis available at\n$\\href{https://github.com/UNITES-Lab/Occult}{https://github.com/UNITES-Lab/Occult}$."
                },
                "authors": [
                    {
                        "name": "Shuqing Luo"
                    },
                    {
                        "name": "Pingzhi Li"
                    },
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Hanrui Wang"
                    },
                    {
                        "name": "Yang"
                    },
                    {
                        "name": "Zhao"
                    },
                    {
                        "name": "Yu"
                    },
                    {
                        "name": "Cao"
                    },
                    {
                        "name": "Yu Cheng"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "arxiv_affiliation": "Kevin",
                "author": "Tianlong Chen",
                "arxiv_comment": "Accepted by ICML2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13345v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13345v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13338v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13338v1",
                "updated": "2025-05-19T16:47:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    47,
                    46,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T16:47:46Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    47,
                    46,
                    0,
                    139,
                    0
                ],
                "title": "Contextual Paralinguistic Data Creation for Multi-Modal Speech-LLM: Data\n  Condensation and Spoken QA Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contextual Paralinguistic Data Creation for Multi-Modal Speech-LLM: Data\n  Condensation and Spoken QA Generation"
                },
                "summary": "Current speech-LLMs exhibit limited capability in contextual reasoning\nalongside paralinguistic understanding, primarily due to the lack of\nQuestion-Answer (QA) datasets that cover both aspects. We propose a novel\nframework for dataset generation from in-the-wild speech data, that integrates\ncontextual reasoning with paralinguistic information. It consists of a pseudo\nparalinguistic label-based data condensation of in-the-wild speech and\nLLM-based Contextual Paralinguistic QA (CPQA) generation. The effectiveness is\nvalidated by a strong correlation in evaluations of the Qwen2-Audio-7B-Instruct\nmodel on a dataset created by our framework and human-generated CPQA dataset.\nThe results also reveal the speech-LLM's limitations in handling empathetic\nreasoning tasks, highlighting the need for such datasets and more robust\nmodels. The proposed framework is first of its kind and has potential in\ntraining more robust speech-LLMs with paralinguistic reasoning capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current speech-LLMs exhibit limited capability in contextual reasoning\nalongside paralinguistic understanding, primarily due to the lack of\nQuestion-Answer (QA) datasets that cover both aspects. We propose a novel\nframework for dataset generation from in-the-wild speech data, that integrates\ncontextual reasoning with paralinguistic information. It consists of a pseudo\nparalinguistic label-based data condensation of in-the-wild speech and\nLLM-based Contextual Paralinguistic QA (CPQA) generation. The effectiveness is\nvalidated by a strong correlation in evaluations of the Qwen2-Audio-7B-Instruct\nmodel on a dataset created by our framework and human-generated CPQA dataset.\nThe results also reveal the speech-LLM's limitations in handling empathetic\nreasoning tasks, highlighting the need for such datasets and more robust\nmodels. The proposed framework is first of its kind and has potential in\ntraining more robust speech-LLMs with paralinguistic reasoning capabilities."
                },
                "authors": [
                    {
                        "name": "Qiongqiong Wang"
                    },
                    {
                        "name": "Hardik B. Sailor"
                    },
                    {
                        "name": "Tianchi Liu"
                    },
                    {
                        "name": "Ai Ti Aw"
                    }
                ],
                "author_detail": {
                    "name": "Ai Ti Aw"
                },
                "author": "Ai Ti Aw",
                "arxiv_comment": "Accepted at Interspeech 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13338v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13338v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08140v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08140v2",
                "updated": "2025-05-19T16:46:27Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    46,
                    27,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-13T00:25:23Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    0,
                    25,
                    23,
                    1,
                    133,
                    0
                ],
                "title": "Lost in Transmission: When and Why LLMs Fail to Reason Globally",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lost in Transmission: When and Why LLMs Fail to Reason Globally"
                },
                "summary": "Despite their many successes, transformer-based large language models (LLMs)\ncontinue to struggle with tasks that require complex reasoning over large parts\nof their input. We argue that these failures arise due to capacity limits on\nthe accurate flow of information within LLMs. To formalize this issue, we\nintroduce the bounded attention prefix oracle (BAPO) model, a new computational\nframework that models bandwidth constraints on attention heads, the mechanism\nfor internal communication in LLMs. We show that several important reasoning\nproblems like graph reachability require high communication bandwidth for BAPOs\nto solve; we call these problems BAPO-hard. Our experiments corroborate our\ntheoretical predictions: GPT-4o, Claude, and Gemini succeed on BAPO-easy tasks\nand fail even on relatively small BAPO-hard tasks. BAPOs also reveal another\nbenefit of chain of thought (CoT): we prove that breaking down a task using CoT\ncan turn any BAPO-hard problem into a BAPO-easy one. Our results offer\nprincipled explanations for key LLM failures and suggest directions for\narchitectures and inference methods that mitigate bandwidth limits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their many successes, transformer-based large language models (LLMs)\ncontinue to struggle with tasks that require complex reasoning over large parts\nof their input. We argue that these failures arise due to capacity limits on\nthe accurate flow of information within LLMs. To formalize this issue, we\nintroduce the bounded attention prefix oracle (BAPO) model, a new computational\nframework that models bandwidth constraints on attention heads, the mechanism\nfor internal communication in LLMs. We show that several important reasoning\nproblems like graph reachability require high communication bandwidth for BAPOs\nto solve; we call these problems BAPO-hard. Our experiments corroborate our\ntheoretical predictions: GPT-4o, Claude, and Gemini succeed on BAPO-easy tasks\nand fail even on relatively small BAPO-hard tasks. BAPOs also reveal another\nbenefit of chain of thought (CoT): we prove that breaking down a task using CoT\ncan turn any BAPO-hard problem into a BAPO-easy one. Our results offer\nprincipled explanations for key LLM failures and suggest directions for\narchitectures and inference methods that mitigate bandwidth limits."
                },
                "authors": [
                    {
                        "name": "Tobias Schnabel"
                    },
                    {
                        "name": "Kiran Tomlinson"
                    },
                    {
                        "name": "Adith Swaminathan"
                    },
                    {
                        "name": "Jennifer Neville"
                    }
                ],
                "author_detail": {
                    "name": "Jennifer Neville"
                },
                "author": "Jennifer Neville",
                "arxiv_comment": "28 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08140v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08140v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13328v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13328v1",
                "updated": "2025-05-19T16:36:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    36,
                    13,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T16:36:13Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    36,
                    13,
                    0,
                    139,
                    0
                ],
                "title": "Rethinking Stateful Tool Use in Multi-Turn Dialogues: Benchmarks and\n  Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Stateful Tool Use in Multi-Turn Dialogues: Benchmarks and\n  Challenges"
                },
                "summary": "Existing benchmarks that assess Language Models (LMs) as Language Agents\n(LAs) for tool use primarily focus on stateless, single-turn interactions or\npartial evaluations, such as tool selection in a single turn, overlooking the\ninherent stateful nature of interactions in multi-turn applications. To fulfill\nthis gap, we propose \\texttt{DialogTool}, a multi-turn dialogue dataset with\nstateful tool interactions considering the whole life cycle of tool use, across\nsix key tasks in three stages: 1) \\textit{tool creation}; 2) \\textit{tool\nutilization}: tool awareness, tool selection, tool execution; and 3)\n\\textit{role-consistent response}: response generation and role play.\nFurthermore, we build \\texttt{VirtualMobile} -- an embodied virtual mobile\nevaluation environment to simulate API calls and assess the robustness of the\ncreated APIs\\footnote{We will use tools and APIs alternatively, there are no\nsignificant differences between them in this paper.}. Taking advantage of these\nartifacts, we conduct comprehensive evaluation on 13 distinct open- and\nclosed-source LLMs and provide detailed analysis at each stage, revealing that\nthe existing state-of-the-art LLMs still cannot perform well to use tools over\nlong horizons.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing benchmarks that assess Language Models (LMs) as Language Agents\n(LAs) for tool use primarily focus on stateless, single-turn interactions or\npartial evaluations, such as tool selection in a single turn, overlooking the\ninherent stateful nature of interactions in multi-turn applications. To fulfill\nthis gap, we propose \\texttt{DialogTool}, a multi-turn dialogue dataset with\nstateful tool interactions considering the whole life cycle of tool use, across\nsix key tasks in three stages: 1) \\textit{tool creation}; 2) \\textit{tool\nutilization}: tool awareness, tool selection, tool execution; and 3)\n\\textit{role-consistent response}: response generation and role play.\nFurthermore, we build \\texttt{VirtualMobile} -- an embodied virtual mobile\nevaluation environment to simulate API calls and assess the robustness of the\ncreated APIs\\footnote{We will use tools and APIs alternatively, there are no\nsignificant differences between them in this paper.}. Taking advantage of these\nartifacts, we conduct comprehensive evaluation on 13 distinct open- and\nclosed-source LLMs and provide detailed analysis at each stage, revealing that\nthe existing state-of-the-art LLMs still cannot perform well to use tools over\nlong horizons."
                },
                "authors": [
                    {
                        "name": "Hongru Wang"
                    },
                    {
                        "name": "Wenyu Huang"
                    },
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Yuanhao Xi"
                    },
                    {
                        "name": "Jianqiao Lu"
                    },
                    {
                        "name": "Huan Zhang"
                    },
                    {
                        "name": "Nan Hu"
                    },
                    {
                        "name": "Zeming Liu"
                    },
                    {
                        "name": "Jeff Z. Pan"
                    },
                    {
                        "name": "Kam-Fai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kam-Fai Wong"
                },
                "author": "Kam-Fai Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13328v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13328v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13327v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13327v2",
                "updated": "2025-05-20T02:07:48Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    2,
                    7,
                    48,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-19T16:35:45Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    35,
                    45,
                    0,
                    139,
                    0
                ],
                "title": "Benchmarking Unified Face Attack Detection via Hierarchical Prompt\n  Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Unified Face Attack Detection via Hierarchical Prompt\n  Tuning"
                },
                "summary": "Presentation Attack Detection and Face Forgery Detection are designed to\nprotect face data from physical media-based Presentation Attacks and digital\nediting-based DeepFakes respectively. But separate training of these two models\nmakes them vulnerable to unknown attacks and burdens deployment environments.\nThe lack of a Unified Face Attack Detection model to handle both types of\nattacks is mainly due to two factors. First, there's a lack of adequate\nbenchmarks for models to explore. Existing UAD datasets have limited attack\ntypes and samples, restricting the model's ability to address advanced threats.\nTo address this, we propose UniAttackDataPlus (UniAttackData+), the most\nextensive and sophisticated collection of forgery techniques to date. It\nincludes 2,875 identities and their 54 kinds of falsified samples, totaling\n697,347 videos. Second, there's a lack of a reliable classification criterion.\nCurrent methods try to find an arbitrary criterion within the same semantic\nspace, which fails when encountering diverse attacks. So, we present a novel\nVisual-Language Model-based Hierarchical Prompt Tuning Framework (HiPTune) that\nadaptively explores multiple classification criteria from different semantic\nspaces. We build a Visual Prompt Tree to explore various classification rules\nhierarchically. Then, by adaptively pruning the prompts, the model can select\nthe most suitable prompts to guide the encoder to extract discriminative\nfeatures at different levels in a coarse-to-fine way. Finally, to help the\nmodel understand the classification criteria in visual space, we propose a\nDynamically Prompt Integration module to project the visual prompts to the text\nencoder for more accurate semantics. Experiments on 12 datasets have shown the\npotential to inspire further innovations in the UAD field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Presentation Attack Detection and Face Forgery Detection are designed to\nprotect face data from physical media-based Presentation Attacks and digital\nediting-based DeepFakes respectively. But separate training of these two models\nmakes them vulnerable to unknown attacks and burdens deployment environments.\nThe lack of a Unified Face Attack Detection model to handle both types of\nattacks is mainly due to two factors. First, there's a lack of adequate\nbenchmarks for models to explore. Existing UAD datasets have limited attack\ntypes and samples, restricting the model's ability to address advanced threats.\nTo address this, we propose UniAttackDataPlus (UniAttackData+), the most\nextensive and sophisticated collection of forgery techniques to date. It\nincludes 2,875 identities and their 54 kinds of falsified samples, totaling\n697,347 videos. Second, there's a lack of a reliable classification criterion.\nCurrent methods try to find an arbitrary criterion within the same semantic\nspace, which fails when encountering diverse attacks. So, we present a novel\nVisual-Language Model-based Hierarchical Prompt Tuning Framework (HiPTune) that\nadaptively explores multiple classification criteria from different semantic\nspaces. We build a Visual Prompt Tree to explore various classification rules\nhierarchically. Then, by adaptively pruning the prompts, the model can select\nthe most suitable prompts to guide the encoder to extract discriminative\nfeatures at different levels in a coarse-to-fine way. Finally, to help the\nmodel understand the classification criteria in visual space, we propose a\nDynamically Prompt Integration module to project the visual prompts to the text\nencoder for more accurate semantics. Experiments on 12 datasets have shown the\npotential to inspire further innovations in the UAD field."
                },
                "authors": [
                    {
                        "name": "Ajian Liu"
                    },
                    {
                        "name": "Haocheng Yuan"
                    },
                    {
                        "name": "Xiao Guo"
                    },
                    {
                        "name": "Hui Ma"
                    },
                    {
                        "name": "Wanyi Zhuang"
                    },
                    {
                        "name": "Changtao Miao"
                    },
                    {
                        "name": "Yan Hong"
                    },
                    {
                        "name": "Chuanbiao Song"
                    },
                    {
                        "name": "Jun Lan"
                    },
                    {
                        "name": "Qi Chu"
                    },
                    {
                        "name": "Tao Gong"
                    },
                    {
                        "name": "Yanyan Liang"
                    },
                    {
                        "name": "Weiqiang Wang"
                    },
                    {
                        "name": "Jun Wan"
                    },
                    {
                        "name": "Xiaoming Liu"
                    },
                    {
                        "name": "Zhen Lei"
                    }
                ],
                "author_detail": {
                    "name": "Zhen Lei"
                },
                "author": "Zhen Lei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13327v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13327v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13326v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13326v1",
                "updated": "2025-05-19T16:34:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    34,
                    56,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T16:34:56Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    34,
                    56,
                    0,
                    139,
                    0
                ],
                "title": "Thinking Short and Right Over Thinking Long: Serving LLM Reasoning\n  Efficiently and Accurately",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thinking Short and Right Over Thinking Long: Serving LLM Reasoning\n  Efficiently and Accurately"
                },
                "summary": "Recent advances in test-time scaling suggest that Large Language Models\n(LLMs) can gain better capabilities by generating Chain-of-Thought reasoning\n(analogous to human thinking) to respond a given request, and meanwhile\nexploring more reasoning branches (i.e., generating multiple responses and\nensembling them) can improve the final output quality. However, when\nincorporating the two scaling dimensions, we find that the system efficiency is\ndampened significantly for two reasons. Firstly, the time cost to generate the\nfinal output increases substantially as many reasoning branches would be\ntrapped in the over-thinking dilemma, producing excessively long responses.\nSecondly, generating multiple reasoning branches for each request increases\nmemory consumption, which is unsuitable for LLM serving since we can only batch\na limited number of requests to process simultaneously. To address this, we\npresent SART, a serving framework for efficient and accurate LLM reasoning. The\nessential idea is to manage the thinking to be short and right, rather than\nlong. For one thing, we devise a redundant sampling with early stopping\napproach based on empirical observations and theoretic analysis, which\nincreases the likelihood of obtaining short-thinking responses when sampling\nreasoning branches. For another, we propose to dynamically prune low-quality\nbranches so that only right-thinking branches are maintained, reducing the\nmemory consumption and allowing us to batch more requests. Experimental results\ndemonstrate that SART not only improves the accuracy of LLM reasoning but also\nenhances the serving efficiency, outperforming existing methods by up to 28.2\ntimes and on average 15.7 times in terms of efficiency when achieving the same\nlevel of accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in test-time scaling suggest that Large Language Models\n(LLMs) can gain better capabilities by generating Chain-of-Thought reasoning\n(analogous to human thinking) to respond a given request, and meanwhile\nexploring more reasoning branches (i.e., generating multiple responses and\nensembling them) can improve the final output quality. However, when\nincorporating the two scaling dimensions, we find that the system efficiency is\ndampened significantly for two reasons. Firstly, the time cost to generate the\nfinal output increases substantially as many reasoning branches would be\ntrapped in the over-thinking dilemma, producing excessively long responses.\nSecondly, generating multiple reasoning branches for each request increases\nmemory consumption, which is unsuitable for LLM serving since we can only batch\na limited number of requests to process simultaneously. To address this, we\npresent SART, a serving framework for efficient and accurate LLM reasoning. The\nessential idea is to manage the thinking to be short and right, rather than\nlong. For one thing, we devise a redundant sampling with early stopping\napproach based on empirical observations and theoretic analysis, which\nincreases the likelihood of obtaining short-thinking responses when sampling\nreasoning branches. For another, we propose to dynamically prune low-quality\nbranches so that only right-thinking branches are maintained, reducing the\nmemory consumption and allowing us to batch more requests. Experimental results\ndemonstrate that SART not only improves the accuracy of LLM reasoning but also\nenhances the serving efficiency, outperforming existing methods by up to 28.2\ntimes and on average 15.7 times in terms of efficiency when achieving the same\nlevel of accuracy."
                },
                "authors": [
                    {
                        "name": "Yuhang Wang"
                    },
                    {
                        "name": "Youhe Jiang"
                    },
                    {
                        "name": "Bin Cui"
                    },
                    {
                        "name": "Fangcheng Fu"
                    }
                ],
                "author_detail": {
                    "name": "Fangcheng Fu"
                },
                "author": "Fangcheng Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13326v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13326v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01893v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01893v2",
                "updated": "2025-05-19T16:32:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    32,
                    11,
                    0,
                    139,
                    0
                ],
                "published": "2024-09-03T13:30:00Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    13,
                    30,
                    0,
                    1,
                    247,
                    0
                ],
                "title": "What are the Essential Factors in Crafting Effective Long Context\n  Multi-Hop Instruction Datasets? Insights and Best Practices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What are the Essential Factors in Crafting Effective Long Context\n  Multi-Hop Instruction Datasets? Insights and Best Practices"
                },
                "summary": "Recent advancements in large language models (LLMs) with extended context\nwindows have significantly improved tasks such as information extraction,\nquestion answering, and complex planning scenarios. In order to achieve success\nin long context tasks, a large amount of work has been done to enhance the long\ncontext capabilities of the model through synthetic data. Existing methods\ntypically utilize the Self-Instruct framework to generate instruction tuning\ndata for better long context capability improvement. However, our preliminary\nexperiments indicate that less than 35% of generated samples are multi-hop, and\nmore than 40% exhibit poor quality, limiting comprehensive understanding and\nfurther research. To improve the quality of synthetic data, we propose the\nMulti-agent Interactive Multi-hop Generation (MIMG) framework, incorporating a\nQuality Verification Agent, a Single-hop Question Generation Agent, a Multiple\nQuestion Sampling Strategy, and a Multi-hop Question Merger Agent. This\nframework improves the data quality, with the proportion of high-quality,\nmulti-hop, and diverse data exceeding 85%. Furthermore, we systematically\ninvestigate strategies for document selection, question merging, and validation\ntechniques through extensive experiments across various models. Our findings\nshow that our synthetic high-quality long-context instruction data\nsignificantly enhances model performance, even surpassing models trained on\nlarger amounts of human-annotated data. Our code is available at:\nhttps://github.com/WowCZ/LongMIT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) with extended context\nwindows have significantly improved tasks such as information extraction,\nquestion answering, and complex planning scenarios. In order to achieve success\nin long context tasks, a large amount of work has been done to enhance the long\ncontext capabilities of the model through synthetic data. Existing methods\ntypically utilize the Self-Instruct framework to generate instruction tuning\ndata for better long context capability improvement. However, our preliminary\nexperiments indicate that less than 35% of generated samples are multi-hop, and\nmore than 40% exhibit poor quality, limiting comprehensive understanding and\nfurther research. To improve the quality of synthetic data, we propose the\nMulti-agent Interactive Multi-hop Generation (MIMG) framework, incorporating a\nQuality Verification Agent, a Single-hop Question Generation Agent, a Multiple\nQuestion Sampling Strategy, and a Multi-hop Question Merger Agent. This\nframework improves the data quality, with the proportion of high-quality,\nmulti-hop, and diverse data exceeding 85%. Furthermore, we systematically\ninvestigate strategies for document selection, question merging, and validation\ntechniques through extensive experiments across various models. Our findings\nshow that our synthetic high-quality long-context instruction data\nsignificantly enhances model performance, even surpassing models trained on\nlarger amounts of human-annotated data. Our code is available at:\nhttps://github.com/WowCZ/LongMIT."
                },
                "authors": [
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Qiguang Chen"
                    },
                    {
                        "name": "Libo Qin"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Haijun Lv"
                    },
                    {
                        "name": "Yicheng Zou"
                    },
                    {
                        "name": "Wanxiang Che"
                    },
                    {
                        "name": "Hang Yan"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Dahua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Dahua Lin"
                },
                "author": "Dahua Lin",
                "arxiv_comment": "ACL 2025 Camera Ready. Code is available at:\n  https://github.com/WowCZ/LongMIT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01893v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01893v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13312v1",
                "updated": "2025-05-19T16:26:58Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    26,
                    58,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T16:26:58Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    26,
                    58,
                    0,
                    139,
                    0
                ],
                "title": "GUARD: Generation-time LLM Unlearning via Adaptive Restriction and\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GUARD: Generation-time LLM Unlearning via Adaptive Restriction and\n  Detection"
                },
                "summary": "Large Language Models (LLMs) have demonstrated strong capabilities in\nmemorizing vast amounts of knowledge across diverse domains. However, the\nability to selectively forget specific knowledge is critical for ensuring the\nsafety and compliance of deployed models. Existing unlearning efforts typically\nfine-tune the model with resources such as forget data, retain data, and a\ncalibration model. These additional gradient steps blur the decision boundary\nbetween forget and retain knowledge, making unlearning often at the expense of\noverall performance. To avoid the negative impact of fine-tuning, it would be\nbetter to unlearn solely at inference time by safely guarding the model against\ngenerating responses related to the forget target, without destroying the\nfluency of text generation. In this work, we propose Generation-time Unlearning\nvia Adaptive Restriction and Detection (GUARD), a framework that enables\ndynamic unlearning during LLM generation. Specifically, we first employ a\nprompt classifier to detect unlearning targets and extract the corresponding\nforbidden token. We then dynamically penalize and filter candidate tokens\nduring generation using a combination of token matching and semantic matching,\neffectively preventing the model from leaking the forgotten content.\nExperimental results on copyright content unlearning tasks over the Harry\nPotter dataset and the MUSE benchmark, as well as entity unlearning tasks on\nthe TOFU dataset, demonstrate that GUARD achieves strong forget quality across\nvarious tasks while causing almost no degradation to the LLM's general\ncapabilities, striking an excellent trade-off between forgetting and utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated strong capabilities in\nmemorizing vast amounts of knowledge across diverse domains. However, the\nability to selectively forget specific knowledge is critical for ensuring the\nsafety and compliance of deployed models. Existing unlearning efforts typically\nfine-tune the model with resources such as forget data, retain data, and a\ncalibration model. These additional gradient steps blur the decision boundary\nbetween forget and retain knowledge, making unlearning often at the expense of\noverall performance. To avoid the negative impact of fine-tuning, it would be\nbetter to unlearn solely at inference time by safely guarding the model against\ngenerating responses related to the forget target, without destroying the\nfluency of text generation. In this work, we propose Generation-time Unlearning\nvia Adaptive Restriction and Detection (GUARD), a framework that enables\ndynamic unlearning during LLM generation. Specifically, we first employ a\nprompt classifier to detect unlearning targets and extract the corresponding\nforbidden token. We then dynamically penalize and filter candidate tokens\nduring generation using a combination of token matching and semantic matching,\neffectively preventing the model from leaking the forgotten content.\nExperimental results on copyright content unlearning tasks over the Harry\nPotter dataset and the MUSE benchmark, as well as entity unlearning tasks on\nthe TOFU dataset, demonstrate that GUARD achieves strong forget quality across\nvarious tasks while causing almost no degradation to the LLM's general\ncapabilities, striking an excellent trade-off between forgetting and utility."
                },
                "authors": [
                    {
                        "name": "Zhijie Deng"
                    },
                    {
                        "name": "Chris Yuhao Liu"
                    },
                    {
                        "name": "Zirui Pang"
                    },
                    {
                        "name": "Xinlei He"
                    },
                    {
                        "name": "Lei Feng"
                    },
                    {
                        "name": "Qi Xuan"
                    },
                    {
                        "name": "Zhaowei Zhu"
                    },
                    {
                        "name": "Jiaheng Wei"
                    }
                ],
                "author_detail": {
                    "name": "Jiaheng Wei"
                },
                "author": "Jiaheng Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13308v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13308v1",
                "updated": "2025-05-19T16:26:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    26,
                    2,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T16:26:02Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    26,
                    2,
                    0,
                    139,
                    0
                ],
                "title": "Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient\n  in Latent Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient\n  in Latent Space"
                },
                "summary": "Reasoning ability, a core component of human intelligence, continues to pose\na significant challenge for Large Language Models (LLMs) in the pursuit of AGI.\nAlthough model performance has improved under the training scaling law,\nsignificant challenges remain, particularly with respect to training\nalgorithms, such as catastrophic forgetting, and the limited availability of\nnovel training data. As an alternative, test-time scaling enhances reasoning\nperformance by increasing test-time computation without parameter updating.\nUnlike prior methods in this paradigm focused on token space, we propose\nleveraging latent space for more effective reasoning and better adherence to\nthe test-time scaling law. We introduce LatentSeek, a novel framework that\nenhances LLM reasoning through Test-Time Instance-level Adaptation (TTIA)\nwithin the model's latent space. Specifically, LatentSeek leverages policy\ngradient to iteratively update latent representations, guided by self-generated\nreward signals. LatentSeek is evaluated on a range of reasoning benchmarks,\nincluding GSM8K, MATH-500, and AIME2024, across multiple LLM architectures.\nResults show that LatentSeek consistently outperforms strong baselines, such as\nChain-of-Thought prompting and fine-tuning-based methods. Furthermore, our\nanalysis demonstrates that LatentSeek is highly efficient, typically converging\nwithin a few iterations for problems of average complexity, while also\nbenefiting from additional iterations, thereby highlighting the potential of\ntest-time scaling in the latent space. These findings position LatentSeek as a\nlightweight, scalable, and effective solution for enhancing the reasoning\ncapabilities of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning ability, a core component of human intelligence, continues to pose\na significant challenge for Large Language Models (LLMs) in the pursuit of AGI.\nAlthough model performance has improved under the training scaling law,\nsignificant challenges remain, particularly with respect to training\nalgorithms, such as catastrophic forgetting, and the limited availability of\nnovel training data. As an alternative, test-time scaling enhances reasoning\nperformance by increasing test-time computation without parameter updating.\nUnlike prior methods in this paradigm focused on token space, we propose\nleveraging latent space for more effective reasoning and better adherence to\nthe test-time scaling law. We introduce LatentSeek, a novel framework that\nenhances LLM reasoning through Test-Time Instance-level Adaptation (TTIA)\nwithin the model's latent space. Specifically, LatentSeek leverages policy\ngradient to iteratively update latent representations, guided by self-generated\nreward signals. LatentSeek is evaluated on a range of reasoning benchmarks,\nincluding GSM8K, MATH-500, and AIME2024, across multiple LLM architectures.\nResults show that LatentSeek consistently outperforms strong baselines, such as\nChain-of-Thought prompting and fine-tuning-based methods. Furthermore, our\nanalysis demonstrates that LatentSeek is highly efficient, typically converging\nwithin a few iterations for problems of average complexity, while also\nbenefiting from additional iterations, thereby highlighting the potential of\ntest-time scaling in the latent space. These findings position LatentSeek as a\nlightweight, scalable, and effective solution for enhancing the reasoning\ncapabilities of LLMs."
                },
                "authors": [
                    {
                        "name": "Hengli Li"
                    },
                    {
                        "name": "Chenxi Li"
                    },
                    {
                        "name": "Tong Wu"
                    },
                    {
                        "name": "Xuekai Zhu"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Zhaoxin Yu"
                    },
                    {
                        "name": "Eric Hanchen Jiang"
                    },
                    {
                        "name": "Song-Chun Zhu"
                    },
                    {
                        "name": "Zixia Jia"
                    },
                    {
                        "name": "Ying Nian Wu"
                    },
                    {
                        "name": "Zilong Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zilong Zheng"
                },
                "author": "Zilong Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13308v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13308v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13307v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13307v1",
                "updated": "2025-05-19T16:25:55Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    25,
                    55,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T16:25:55Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    25,
                    55,
                    0,
                    139,
                    0
                ],
                "title": "RBF++: Quantifying and Optimizing Reasoning Boundaries across Measurable\n  and Unmeasurable Capabilities for Chain-of-Thought Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RBF++: Quantifying and Optimizing Reasoning Boundaries across Measurable\n  and Unmeasurable Capabilities for Chain-of-Thought Reasoning"
                },
                "summary": "Chain-of-Thought (CoT) reasoning has proven effective in enhancing large\nlanguage models (LLMs) on complex tasks, spurring research into its underlying\nmechanisms. However, two primary challenges remain for real-world applications:\n(1) the lack of quantitative metrics and actionable guidelines for evaluating\nand optimizing measurable boundaries of CoT capability, and (2) the absence of\nmethods to assess boundaries of unmeasurable CoT capability, such as multimodal\nperception. To address these gaps, we introduce the Reasoning Boundary\nFramework++ (RBF++). To tackle the first challenge, we define the reasoning\nboundary (RB) as the maximum limit of CoT performance. We also propose a\ncombination law for RBs, enabling quantitative analysis and offering actionable\nguidance across various CoT tasks. For the second challenge, particularly in\nmultimodal scenarios, we introduce a constant assumption, which replaces\nunmeasurable RBs with scenario-specific constants. Additionally, we propose the\nreasoning boundary division mechanism, which divides unmeasurable RBs into two\nsub-boundaries, facilitating the quantification and optimization of both\nunmeasurable domain knowledge and multimodal perception capabilities. Extensive\nexperiments involving 38 models across 13 tasks validate the feasibility of our\nframework in cross-modal settings. Additionally, we evaluate 10 CoT strategies,\noffer insights into optimization and decay from two complementary perspectives,\nand expand evaluation benchmarks for measuring RBs in LLM reasoning. We hope\nthis work advances the understanding of RBs and optimization strategies in\nLLMs. Code and data are available at\nhttps://github.com/LightChen233/reasoning-boundary.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) reasoning has proven effective in enhancing large\nlanguage models (LLMs) on complex tasks, spurring research into its underlying\nmechanisms. However, two primary challenges remain for real-world applications:\n(1) the lack of quantitative metrics and actionable guidelines for evaluating\nand optimizing measurable boundaries of CoT capability, and (2) the absence of\nmethods to assess boundaries of unmeasurable CoT capability, such as multimodal\nperception. To address these gaps, we introduce the Reasoning Boundary\nFramework++ (RBF++). To tackle the first challenge, we define the reasoning\nboundary (RB) as the maximum limit of CoT performance. We also propose a\ncombination law for RBs, enabling quantitative analysis and offering actionable\nguidance across various CoT tasks. For the second challenge, particularly in\nmultimodal scenarios, we introduce a constant assumption, which replaces\nunmeasurable RBs with scenario-specific constants. Additionally, we propose the\nreasoning boundary division mechanism, which divides unmeasurable RBs into two\nsub-boundaries, facilitating the quantification and optimization of both\nunmeasurable domain knowledge and multimodal perception capabilities. Extensive\nexperiments involving 38 models across 13 tasks validate the feasibility of our\nframework in cross-modal settings. Additionally, we evaluate 10 CoT strategies,\noffer insights into optimization and decay from two complementary perspectives,\nand expand evaluation benchmarks for measuring RBs in LLM reasoning. We hope\nthis work advances the understanding of RBs and optimization strategies in\nLLMs. Code and data are available at\nhttps://github.com/LightChen233/reasoning-boundary."
                },
                "authors": [
                    {
                        "name": "Qiguang Chen"
                    },
                    {
                        "name": "Libo Qin"
                    },
                    {
                        "name": "Jinhao Liu"
                    },
                    {
                        "name": "Yue Liao"
                    },
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Jingxuan Zhou"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "arxiv_comment": "Manuscript",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13307v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13307v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13292v1",
                "updated": "2025-05-19T16:14:27Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    14,
                    27,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T16:14:27Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    14,
                    27,
                    0,
                    139,
                    0
                ],
                "title": "Cross-Cloud Data Privacy Protection: Optimizing Collaborative Mechanisms\n  of AI Systems by Integrating Federated Learning and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Cloud Data Privacy Protection: Optimizing Collaborative Mechanisms\n  of AI Systems by Integrating Federated Learning and LLMs"
                },
                "summary": "In the age of cloud computing, data privacy protection has become a major\nchallenge, especially when sharing sensitive data across cloud environments.\nHowever, how to optimize collaboration across cloud environments remains an\nunresolved problem. In this paper, we combine federated learning with\nlarge-scale language models to optimize the collaborative mechanism of AI\nsystems. Based on the existing federated learning framework, we introduce a\ncross-cloud architecture in which federated learning works by aggregating model\nupdates from decentralized nodes without exposing the original data. At the\nsame time, combined with large-scale language models, its powerful context and\nsemantic understanding capabilities are used to improve model training\nefficiency and decision-making ability. We've further innovated by introducing\na secure communication layer to ensure the privacy and integrity of model\nupdates and training data. The model enables continuous model adaptation and\nfine-tuning across different cloud environments while protecting sensitive\ndata. Experimental results show that the proposed method is significantly\nbetter than the traditional federated learning model in terms of accuracy,\nconvergence speed and data privacy protection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the age of cloud computing, data privacy protection has become a major\nchallenge, especially when sharing sensitive data across cloud environments.\nHowever, how to optimize collaboration across cloud environments remains an\nunresolved problem. In this paper, we combine federated learning with\nlarge-scale language models to optimize the collaborative mechanism of AI\nsystems. Based on the existing federated learning framework, we introduce a\ncross-cloud architecture in which federated learning works by aggregating model\nupdates from decentralized nodes without exposing the original data. At the\nsame time, combined with large-scale language models, its powerful context and\nsemantic understanding capabilities are used to improve model training\nefficiency and decision-making ability. We've further innovated by introducing\na secure communication layer to ensure the privacy and integrity of model\nupdates and training data. The model enables continuous model adaptation and\nfine-tuning across different cloud environments while protecting sensitive\ndata. Experimental results show that the proposed method is significantly\nbetter than the traditional federated learning model in terms of accuracy,\nconvergence speed and data privacy protection."
                },
                "authors": [
                    {
                        "name": "Huaiying Luo"
                    },
                    {
                        "name": "Cheng Ji"
                    }
                ],
                "author_detail": {
                    "name": "Cheng Ji"
                },
                "author": "Cheng Ji",
                "arxiv_comment": "Accepted by 2025 IEEE 7th International Conference on Communications,\n  Information System and Computer Engineering",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17140v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17140v2",
                "updated": "2025-05-19T16:12:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    12,
                    40,
                    0,
                    139,
                    0
                ],
                "published": "2024-09-25T17:58:08Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    58,
                    8,
                    2,
                    269,
                    0
                ],
                "title": "AXIS: Efficient Human-Agent-Computer Interaction with API-First\n  LLM-Based Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AXIS: Efficient Human-Agent-Computer Interaction with API-First\n  LLM-Based Agents"
                },
                "summary": "Multimodal large language models (MLLMs) have enabled LLM-based agents to\ndirectly interact with application user interfaces (UIs), enhancing agents'\nperformance in complex tasks. However, these agents often suffer from high\nlatency and low reliability due to the extensive sequential UI interactions. To\naddress this issue, we propose AXIS, a novel LLM-based agents framework that\nprioritize actions through application programming interfaces (APIs) over UI\nactions. This framework also facilitates the creation and expansion of APIs\nthrough automated exploration of applications. Our experiments on Microsoft\nWord demonstrate that AXIS reduces task completion time by 65%-70% and\ncognitive workload by 38%-53%, while maintaining accuracy of 97%-98% compared\nto humans. Our work contributes to a new human-agent-computer interaction\n(HACI) framework and explores a fresh UI design principle for application\nproviders to turn applications into agents in the era of LLMs, paving the way\ntowards an agent-centric operating system (Agent OS).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have enabled LLM-based agents to\ndirectly interact with application user interfaces (UIs), enhancing agents'\nperformance in complex tasks. However, these agents often suffer from high\nlatency and low reliability due to the extensive sequential UI interactions. To\naddress this issue, we propose AXIS, a novel LLM-based agents framework that\nprioritize actions through application programming interfaces (APIs) over UI\nactions. This framework also facilitates the creation and expansion of APIs\nthrough automated exploration of applications. Our experiments on Microsoft\nWord demonstrate that AXIS reduces task completion time by 65%-70% and\ncognitive workload by 38%-53%, while maintaining accuracy of 97%-98% compared\nto humans. Our work contributes to a new human-agent-computer interaction\n(HACI) framework and explores a fresh UI design principle for application\nproviders to turn applications into agents in the era of LLMs, paving the way\ntowards an agent-centric operating system (Agent OS)."
                },
                "authors": [
                    {
                        "name": "Junting Lu"
                    },
                    {
                        "name": "Zhiyang Zhang"
                    },
                    {
                        "name": "Fangkai Yang"
                    },
                    {
                        "name": "Jue Zhang"
                    },
                    {
                        "name": "Lu Wang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Qingwei Lin"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    },
                    {
                        "name": "Dongmei Zhang"
                    },
                    {
                        "name": "Qi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qi Zhang"
                },
                "author": "Qi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17140v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17140v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13291v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13291v1",
                "updated": "2025-05-19T16:11:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    11,
                    23,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T16:11:23Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    11,
                    23,
                    0,
                    139,
                    0
                ],
                "title": "TimeSeriesGym: A Scalable Benchmark for (Time Series) Machine Learning\n  Engineering Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TimeSeriesGym: A Scalable Benchmark for (Time Series) Machine Learning\n  Engineering Agents"
                },
                "summary": "We introduce TimeSeriesGym, a scalable benchmarking framework for evaluating\nArtificial Intelligence (AI) agents on time series machine learning engineering\nchallenges. Existing benchmarks lack scalability, focus narrowly on model\nbuilding in well-defined settings, and evaluate only a limited set of research\nartifacts (e.g., CSV submission files). To make AI agent benchmarking more\nrelevant to the practice of machine learning engineering, our framework scales\nalong two critical dimensions. First, recognizing that effective ML engineering\nrequires a range of diverse skills, TimeSeriesGym incorporates challenges from\ndiverse sources spanning multiple domains and tasks. We design challenges to\nevaluate both isolated capabilities (including data handling, understanding\nresearch repositories, and code translation) and their combinations, and rather\nthan addressing each challenge independently, we develop tools that support\ndesigning multiple challenges at scale. Second, we implement evaluation\nmechanisms for multiple research artifacts, including submission files, code,\nand models, using both precise numeric measures and more flexible LLM-based\nevaluation approaches. This dual strategy balances objective assessment with\ncontextual judgment. Although our initial focus is on time series applications,\nour framework can be readily extended to other data modalities, broadly\nenhancing the comprehensiveness and practical utility of agentic AI evaluation.\nWe open-source our benchmarking framework to facilitate future research on the\nML engineering capabilities of AI agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce TimeSeriesGym, a scalable benchmarking framework for evaluating\nArtificial Intelligence (AI) agents on time series machine learning engineering\nchallenges. Existing benchmarks lack scalability, focus narrowly on model\nbuilding in well-defined settings, and evaluate only a limited set of research\nartifacts (e.g., CSV submission files). To make AI agent benchmarking more\nrelevant to the practice of machine learning engineering, our framework scales\nalong two critical dimensions. First, recognizing that effective ML engineering\nrequires a range of diverse skills, TimeSeriesGym incorporates challenges from\ndiverse sources spanning multiple domains and tasks. We design challenges to\nevaluate both isolated capabilities (including data handling, understanding\nresearch repositories, and code translation) and their combinations, and rather\nthan addressing each challenge independently, we develop tools that support\ndesigning multiple challenges at scale. Second, we implement evaluation\nmechanisms for multiple research artifacts, including submission files, code,\nand models, using both precise numeric measures and more flexible LLM-based\nevaluation approaches. This dual strategy balances objective assessment with\ncontextual judgment. Although our initial focus is on time series applications,\nour framework can be readily extended to other data modalities, broadly\nenhancing the comprehensiveness and practical utility of agentic AI evaluation.\nWe open-source our benchmarking framework to facilitate future research on the\nML engineering capabilities of AI agents."
                },
                "authors": [
                    {
                        "name": "Yifu Cai"
                    },
                    {
                        "name": "Xinyu Li"
                    },
                    {
                        "name": "Mononito Goswami"
                    },
                    {
                        "name": "Michał Wiliński"
                    },
                    {
                        "name": "Gus Welter"
                    },
                    {
                        "name": "Artur Dubrawski"
                    }
                ],
                "author_detail": {
                    "name": "Artur Dubrawski"
                },
                "author": "Artur Dubrawski",
                "arxiv_comment": "Open source code available at\n  https://github.com/moment-timeseries-foundation-model/TimeSeriesGym. YC, XL,\n  MG and MW contributed equally, and should be considered joint first authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13291v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09569v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09569v2",
                "updated": "2025-05-19T16:10:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    10,
                    21,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-14T17:11:23Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    17,
                    11,
                    23,
                    2,
                    134,
                    0
                ],
                "title": "MigrationBench: Repository-Level Code Migration Benchmark from Java 8",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MigrationBench: Repository-Level Code Migration Benchmark from Java 8"
                },
                "summary": "With the rapid advancement of powerful large language models (LLMs) in recent\nyears, a wide range of software engineering tasks can now be addressed using\nLLMs, significantly enhancing productivity and scalability. Numerous benchmark\ndatasets have been developed to evaluate the coding capabilities of these\nmodels, while they primarily focus on code generation and issue-resolution\ntasks. In contrast, we introduce a new coding benchmark MigrationBench with a\ndistinct focus: code migration. MigrationBench aims to serve as a comprehensive\nbenchmark for migration from Java $8$ to the latest long-term support (LTS)\nversions (Java $17$, $21$), including a full dataset and its subset selected\nwith $5,102$ and $300$ repositories respectively. Selected is a representative\nsubset curated for complexity and difficulty, offering a versatile resource to\nsupport research in the field of code migration. Additionally, we provide a\ncomprehensive evaluation framework to facilitate rigorous and standardized\nassessment of LLMs on this challenging task. We further propose SD-Feedback and\ndemonstrate that LLMs can effectively tackle repository-level code migration to\nJava $17$. For the selected subset with Claude-3.5-Sonnet-v2, SD-Feedback\nachieves $62.33\\%$ and $27.33\\%$ success rate (pass@1) for minimal and maximal\nmigration respectively. The benchmark dataset and source code are available at:\nhttps://huggingface.co/collections/AmazonScience/migrationbench-68125452fc21a4564b92b6c3\nand https://github.com/amazon-science/MigrationBench respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of powerful large language models (LLMs) in recent\nyears, a wide range of software engineering tasks can now be addressed using\nLLMs, significantly enhancing productivity and scalability. Numerous benchmark\ndatasets have been developed to evaluate the coding capabilities of these\nmodels, while they primarily focus on code generation and issue-resolution\ntasks. In contrast, we introduce a new coding benchmark MigrationBench with a\ndistinct focus: code migration. MigrationBench aims to serve as a comprehensive\nbenchmark for migration from Java $8$ to the latest long-term support (LTS)\nversions (Java $17$, $21$), including a full dataset and its subset selected\nwith $5,102$ and $300$ repositories respectively. Selected is a representative\nsubset curated for complexity and difficulty, offering a versatile resource to\nsupport research in the field of code migration. Additionally, we provide a\ncomprehensive evaluation framework to facilitate rigorous and standardized\nassessment of LLMs on this challenging task. We further propose SD-Feedback and\ndemonstrate that LLMs can effectively tackle repository-level code migration to\nJava $17$. For the selected subset with Claude-3.5-Sonnet-v2, SD-Feedback\nachieves $62.33\\%$ and $27.33\\%$ success rate (pass@1) for minimal and maximal\nmigration respectively. The benchmark dataset and source code are available at:\nhttps://huggingface.co/collections/AmazonScience/migrationbench-68125452fc21a4564b92b6c3\nand https://github.com/amazon-science/MigrationBench respectively."
                },
                "authors": [
                    {
                        "name": "Linbo Liu"
                    },
                    {
                        "name": "Xinle Liu"
                    },
                    {
                        "name": "Qiang Zhou"
                    },
                    {
                        "name": "Lin Chen"
                    },
                    {
                        "name": "Yihan Liu"
                    },
                    {
                        "name": "Hoan Nguyen"
                    },
                    {
                        "name": "Behrooz Omidvar-Tehrani"
                    },
                    {
                        "name": "Xi Shen"
                    },
                    {
                        "name": "Jun Huan"
                    },
                    {
                        "name": "Omer Tripp"
                    },
                    {
                        "name": "Anoop Deoras"
                    }
                ],
                "author_detail": {
                    "name": "Anoop Deoras"
                },
                "author": "Anoop Deoras",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09569v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09569v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13278v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13278v1",
                "updated": "2025-05-19T16:01:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    1,
                    36,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T16:01:36Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    16,
                    1,
                    36,
                    0,
                    139,
                    0
                ],
                "title": "Hybrid Voting-Based Task Assignment in Modular Construction Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Voting-Based Task Assignment in Modular Construction Scenarios"
                },
                "summary": "Modular construction, involving off-site prefabrication and on-site assembly,\noffers significant advantages but presents complex coordination challenges for\nrobotic automation. Effective task allocation is critical for leveraging\nmulti-agent systems (MAS) in these structured environments. This paper\nintroduces the Hybrid Voting-Based Task Assignment (HVBTA) framework, a novel\napproach to optimizing collaboration between heterogeneous multi-agent\nconstruction teams. Inspired by human reasoning in task delegation, HVBTA\nuniquely integrates multiple voting mechanisms with the capabilities of a Large\nLanguage Model (LLM) for nuanced suitability assessment between agent\ncapabilities and task requirements. The framework operates by assigning\nCapability Profiles to agents and detailed requirement lists called Task\nDescriptions to construction tasks, subsequently generating a quantitative\nSuitability Matrix. Six distinct voting methods, augmented by a pre-trained\nLLM, analyze this matrix to robustly identify the optimal agent for each task.\nConflict-Based Search (CBS) is integrated for decentralized, collision-free\npath planning, ensuring efficient and safe spatio-temporal coordination of the\nrobotic team during assembly operations. HVBTA enables efficient, conflict-free\nassignment and coordination, facilitating potentially faster and more accurate\nmodular assembly. Current work is evaluating HVBTA's performance across various\nsimulated construction scenarios involving diverse robotic platforms and task\ncomplexities. While designed as a generalizable framework for any domain with\nclearly definable tasks and capabilities, HVBTA will be particularly effective\nfor addressing the demanding coordination requirements of multi-agent\ncollaborative robotics in modular construction due to the predetermined\nconstruction planning involved.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modular construction, involving off-site prefabrication and on-site assembly,\noffers significant advantages but presents complex coordination challenges for\nrobotic automation. Effective task allocation is critical for leveraging\nmulti-agent systems (MAS) in these structured environments. This paper\nintroduces the Hybrid Voting-Based Task Assignment (HVBTA) framework, a novel\napproach to optimizing collaboration between heterogeneous multi-agent\nconstruction teams. Inspired by human reasoning in task delegation, HVBTA\nuniquely integrates multiple voting mechanisms with the capabilities of a Large\nLanguage Model (LLM) for nuanced suitability assessment between agent\ncapabilities and task requirements. The framework operates by assigning\nCapability Profiles to agents and detailed requirement lists called Task\nDescriptions to construction tasks, subsequently generating a quantitative\nSuitability Matrix. Six distinct voting methods, augmented by a pre-trained\nLLM, analyze this matrix to robustly identify the optimal agent for each task.\nConflict-Based Search (CBS) is integrated for decentralized, collision-free\npath planning, ensuring efficient and safe spatio-temporal coordination of the\nrobotic team during assembly operations. HVBTA enables efficient, conflict-free\nassignment and coordination, facilitating potentially faster and more accurate\nmodular assembly. Current work is evaluating HVBTA's performance across various\nsimulated construction scenarios involving diverse robotic platforms and task\ncomplexities. While designed as a generalizable framework for any domain with\nclearly definable tasks and capabilities, HVBTA will be particularly effective\nfor addressing the demanding coordination requirements of multi-agent\ncollaborative robotics in modular construction due to the predetermined\nconstruction planning involved."
                },
                "authors": [
                    {
                        "name": "Daniel Weiner"
                    },
                    {
                        "name": "Raj Korpan"
                    }
                ],
                "author_detail": {
                    "name": "Raj Korpan"
                },
                "author": "Raj Korpan",
                "arxiv_comment": "Accepted to Block by Block workshop at ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13278v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13278v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09493v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09493v2",
                "updated": "2025-05-19T15:57:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    57,
                    5,
                    0,
                    139,
                    0
                ],
                "published": "2024-09-14T17:40:35Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    17,
                    40,
                    35,
                    5,
                    258,
                    0
                ],
                "title": "Hacking, The Lazy Way: LLM Augmented Pentesting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hacking, The Lazy Way: LLM Augmented Pentesting"
                },
                "summary": "In our research, we introduce a new concept called \"LLM Augmented Pentesting\"\ndemonstrated with a tool named \"Pentest Copilot,\" that revolutionizes the field\nof ethical hacking by integrating Large Language Models (LLMs) into penetration\ntesting workflows, leveraging the advanced GPT-4-turbo model. Our approach\nfocuses on overcoming the traditional resistance to automation in penetration\ntesting by employing LLMs to automate specific sub-tasks while ensuring a\ncomprehensive understanding of the overall testing process.\n  Pentest Copilot showcases remarkable proficiency in tasks such as utilizing\ntesting tools, interpreting outputs, and suggesting follow-up actions,\nefficiently bridging the gap between automated systems and human expertise. By\nintegrating a \"chain of thought\" mechanism, Pentest Copilot optimizes token\nusage and enhances decision-making processes, leading to more accurate and\ncontext-aware outputs. Additionally, our implementation of Retrieval-Augmented\nGeneration (RAG) minimizes hallucinations and ensures the tool remains aligned\nwith the latest cybersecurity techniques and knowledge. We also highlight a\nunique infrastructure system that supports in-browser penetration testing,\nproviding a robust platform for cybersecurity professionals. Our findings\ndemonstrate that LLM Augmented Pentesting can not only significantly enhance\ntask completion rates in penetration testing but also effectively addresses\nreal-world challenges, marking a substantial advancement in the cybersecurity\ndomain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In our research, we introduce a new concept called \"LLM Augmented Pentesting\"\ndemonstrated with a tool named \"Pentest Copilot,\" that revolutionizes the field\nof ethical hacking by integrating Large Language Models (LLMs) into penetration\ntesting workflows, leveraging the advanced GPT-4-turbo model. Our approach\nfocuses on overcoming the traditional resistance to automation in penetration\ntesting by employing LLMs to automate specific sub-tasks while ensuring a\ncomprehensive understanding of the overall testing process.\n  Pentest Copilot showcases remarkable proficiency in tasks such as utilizing\ntesting tools, interpreting outputs, and suggesting follow-up actions,\nefficiently bridging the gap between automated systems and human expertise. By\nintegrating a \"chain of thought\" mechanism, Pentest Copilot optimizes token\nusage and enhances decision-making processes, leading to more accurate and\ncontext-aware outputs. Additionally, our implementation of Retrieval-Augmented\nGeneration (RAG) minimizes hallucinations and ensures the tool remains aligned\nwith the latest cybersecurity techniques and knowledge. We also highlight a\nunique infrastructure system that supports in-browser penetration testing,\nproviding a robust platform for cybersecurity professionals. Our findings\ndemonstrate that LLM Augmented Pentesting can not only significantly enhance\ntask completion rates in penetration testing but also effectively addresses\nreal-world challenges, marking a substantial advancement in the cybersecurity\ndomain."
                },
                "authors": [
                    {
                        "name": "Dhruva Goyal"
                    },
                    {
                        "name": "Sitaraman Subramanian"
                    },
                    {
                        "name": "Aditya Peela"
                    },
                    {
                        "name": "Nisha P. Shetty"
                    }
                ],
                "author_detail": {
                    "name": "Nisha P. Shetty"
                },
                "author": "Nisha P. Shetty",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication.\n  Nisha P. Shetty has been added as an author as the new version includes work\n  under her supervision, enhancing the research. Significant changes have been\n  made in the methodology, survey, and introduction sections",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09493v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09493v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13271v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13271v1",
                "updated": "2025-05-19T15:52:19Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    52,
                    19,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T15:52:19Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    52,
                    19,
                    0,
                    139,
                    0
                ],
                "title": "CSC-SQL: Corrective Self-Consistency in Text-to-SQL via Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSC-SQL: Corrective Self-Consistency in Text-to-SQL via Reinforcement\n  Learning"
                },
                "summary": "Large language models (LLMs) have demonstrated strong capabilities in\ntranslating natural language questions about relational databases into SQL\nqueries. In particular, test-time scaling techniques such as Self-Consistency\nand Self-Correction can enhance SQL generation accuracy by increasing\ncomputational effort during inference. However, these methods have notable\nlimitations: Self-Consistency may select suboptimal outputs despite majority\nvotes, while Self-Correction typically addresses only syntactic errors. To\nleverage the strengths of both approaches, we propose CSC-SQL, a novel method\nthat integrates Self-Consistency and Self-Correction. CSC-SQL selects the two\nmost frequently occurring outputs from parallel sampling and feeds them into a\nmerge revision model for correction. Additionally, we employ the Group Relative\nPolicy Optimization (GRPO) algorithm to fine-tune both the SQL generation and\nrevision models via reinforcement learning, significantly enhancing output\nquality. Experimental results confirm the effectiveness and generalizability of\nCSC-SQL. On the BIRD development set, our 3B model achieves 65.28% execution\naccuracy, while the 7B model achieves 69.19%. The code will be open sourced at\nhttps://github.com/CycloneBoy/csc_sql.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated strong capabilities in\ntranslating natural language questions about relational databases into SQL\nqueries. In particular, test-time scaling techniques such as Self-Consistency\nand Self-Correction can enhance SQL generation accuracy by increasing\ncomputational effort during inference. However, these methods have notable\nlimitations: Self-Consistency may select suboptimal outputs despite majority\nvotes, while Self-Correction typically addresses only syntactic errors. To\nleverage the strengths of both approaches, we propose CSC-SQL, a novel method\nthat integrates Self-Consistency and Self-Correction. CSC-SQL selects the two\nmost frequently occurring outputs from parallel sampling and feeds them into a\nmerge revision model for correction. Additionally, we employ the Group Relative\nPolicy Optimization (GRPO) algorithm to fine-tune both the SQL generation and\nrevision models via reinforcement learning, significantly enhancing output\nquality. Experimental results confirm the effectiveness and generalizability of\nCSC-SQL. On the BIRD development set, our 3B model achieves 65.28% execution\naccuracy, while the 7B model achieves 69.19%. The code will be open sourced at\nhttps://github.com/CycloneBoy/csc_sql."
                },
                "authors": [
                    {
                        "name": "Lei Sheng"
                    },
                    {
                        "name": "Shuai-Shuai Xu"
                    }
                ],
                "author_detail": {
                    "name": "Shuai-Shuai Xu"
                },
                "author": "Shuai-Shuai Xu",
                "arxiv_comment": "11 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13271v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13271v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16707v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16707v3",
                "updated": "2025-05-19T15:51:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    51,
                    40,
                    0,
                    139,
                    0
                ],
                "published": "2024-11-21T19:01:07Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    19,
                    1,
                    7,
                    3,
                    326,
                    0
                ],
                "title": "Enhancing LLMs for Power System Simulations: A Feedback-driven\n  Multi-agent Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLMs for Power System Simulations: A Feedback-driven\n  Multi-agent Framework"
                },
                "summary": "The integration of experimental technologies with large language models\n(LLMs) is transforming scientific research. It positions AI as a versatile\nresearch assistant rather than a mere problem-solving tool. In the field of\npower systems, however, managing simulations -- one of the essential\nexperimental technologies -- remains a challenge for LLMs due to their limited\ndomain-specific knowledge, restricted reasoning capabilities, and imprecise\nhandling of simulation parameters. To address these limitations, this paper\nproposes a feedback-driven, multi-agent framework. It incorporates three\nproposed modules: an enhanced retrieval-augmented generation (RAG) module, an\nimproved reasoning module, and a dynamic environmental acting module with an\nerror-feedback mechanism. Validated on 69 diverse tasks from Daline and\nMATPOWER, this framework achieves success rates of 93.13% and 96.85%,\nrespectively. It significantly outperforms ChatGPT 4o, o1-preview, and the\nfine-tuned GPT-4o, which all achieved a success rate lower than 30% on complex\ntasks. Additionally, the proposed framework also supports rapid, cost-effective\ntask execution, completing each simulation in approximately 30 seconds at an\naverage cost of 0.014 USD for tokens. Overall, this adaptable framework lays a\nfoundation for developing intelligent LLM-based assistants for human\nresearchers, facilitating power system research and beyond.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of experimental technologies with large language models\n(LLMs) is transforming scientific research. It positions AI as a versatile\nresearch assistant rather than a mere problem-solving tool. In the field of\npower systems, however, managing simulations -- one of the essential\nexperimental technologies -- remains a challenge for LLMs due to their limited\ndomain-specific knowledge, restricted reasoning capabilities, and imprecise\nhandling of simulation parameters. To address these limitations, this paper\nproposes a feedback-driven, multi-agent framework. It incorporates three\nproposed modules: an enhanced retrieval-augmented generation (RAG) module, an\nimproved reasoning module, and a dynamic environmental acting module with an\nerror-feedback mechanism. Validated on 69 diverse tasks from Daline and\nMATPOWER, this framework achieves success rates of 93.13% and 96.85%,\nrespectively. It significantly outperforms ChatGPT 4o, o1-preview, and the\nfine-tuned GPT-4o, which all achieved a success rate lower than 30% on complex\ntasks. Additionally, the proposed framework also supports rapid, cost-effective\ntask execution, completing each simulation in approximately 30 seconds at an\naverage cost of 0.014 USD for tokens. Overall, this adaptable framework lays a\nfoundation for developing intelligent LLM-based assistants for human\nresearchers, facilitating power system research and beyond."
                },
                "authors": [
                    {
                        "name": "Mengshuo Jia"
                    },
                    {
                        "name": "Zeyu Cui"
                    },
                    {
                        "name": "Gabriela Hug"
                    }
                ],
                "author_detail": {
                    "name": "Gabriela Hug"
                },
                "author": "Gabriela Hug",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16707v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16707v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16565v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16565v2",
                "updated": "2025-05-19T15:45:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    45,
                    13,
                    0,
                    139,
                    0
                ],
                "published": "2025-02-23T13:12:53Z",
                "published_parsed": [
                    2025,
                    2,
                    23,
                    13,
                    12,
                    53,
                    6,
                    54,
                    0
                ],
                "title": "The Hidden Strength of Disagreement: Unraveling the Consensus-Diversity\n  Tradeoff in Adaptive Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Hidden Strength of Disagreement: Unraveling the Consensus-Diversity\n  Tradeoff in Adaptive Multi-Agent Systems"
                },
                "summary": "Consensus formation is pivotal in multi-agent systems (MAS), balancing\ncollective coherence with individual diversity. Conventional LLM-based MAS\nprimarily rely on explicit coordination, e.g., prompts or voting, risking\npremature homogenization. We argue that implicit consensus, where agents\nexchange information yet independently form decisions via in-context learning,\ncan be more effective in dynamic environments that require long-horizon\nadaptability. By retaining partial diversity, systems can better explore novel\nstrategies and cope with external shocks. We formalize a consensus-diversity\ntradeoff, showing conditions where implicit methods outperform explicit ones.\nExperiments on three scenarios -- Dynamic Disaster Response, Information Spread\nand Manipulation, and Dynamic Public-Goods Provision -- confirm partial\ndeviation from group norms boosts exploration, robustness, and performance. We\nhighlight emergent coordination via in-context learning, underscoring the value\nof preserving diversity for resilient decision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Consensus formation is pivotal in multi-agent systems (MAS), balancing\ncollective coherence with individual diversity. Conventional LLM-based MAS\nprimarily rely on explicit coordination, e.g., prompts or voting, risking\npremature homogenization. We argue that implicit consensus, where agents\nexchange information yet independently form decisions via in-context learning,\ncan be more effective in dynamic environments that require long-horizon\nadaptability. By retaining partial diversity, systems can better explore novel\nstrategies and cope with external shocks. We formalize a consensus-diversity\ntradeoff, showing conditions where implicit methods outperform explicit ones.\nExperiments on three scenarios -- Dynamic Disaster Response, Information Spread\nand Manipulation, and Dynamic Public-Goods Provision -- confirm partial\ndeviation from group norms boosts exploration, robustness, and performance. We\nhighlight emergent coordination via in-context learning, underscoring the value\nof preserving diversity for resilient decision-making."
                },
                "authors": [
                    {
                        "name": "Zengqing Wu"
                    },
                    {
                        "name": "Takayuki Ito"
                    }
                ],
                "author_detail": {
                    "name": "Takayuki Ito"
                },
                "author": "Takayuki Ito",
                "arxiv_comment": "Source codes are available at\n  https://github.com/wuzengqing001225/ConsensusDiversityTradeoffMAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16565v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16565v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13263v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13263v1",
                "updated": "2025-05-19T15:44:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    44,
                    24,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T15:44:24Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    44,
                    24,
                    0,
                    139,
                    0
                ],
                "title": "Are requirements really all you need? A case study of LLM-driven\n  configuration code generation for automotive simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are requirements really all you need? A case study of LLM-driven\n  configuration code generation for automotive simulations"
                },
                "summary": "Large Language Models (LLMs) are taking many industries by storm. They\npossess impressive reasoning capabilities and are capable of handling complex\nproblems, as shown by their steadily improving scores on coding and\nmathematical benchmarks. However, are the models currently available truly\ncapable of addressing real-world challenges, such as those found in the\nautomotive industry? How well can they understand high-level, abstract\ninstructions? Can they translate these instructions directly into functional\ncode, or do they still need help and supervision? In this work, we put one of\nthe current state-of-the-art models to the test. We evaluate its performance in\nthe task of translating abstract requirements, extracted from automotive\nstandards and documents, into configuration code for CARLA simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are taking many industries by storm. They\npossess impressive reasoning capabilities and are capable of handling complex\nproblems, as shown by their steadily improving scores on coding and\nmathematical benchmarks. However, are the models currently available truly\ncapable of addressing real-world challenges, such as those found in the\nautomotive industry? How well can they understand high-level, abstract\ninstructions? Can they translate these instructions directly into functional\ncode, or do they still need help and supervision? In this work, we put one of\nthe current state-of-the-art models to the test. We evaluate its performance in\nthe task of translating abstract requirements, extracted from automotive\nstandards and documents, into configuration code for CARLA simulations."
                },
                "authors": [
                    {
                        "name": "Krzysztof Lebioda"
                    },
                    {
                        "name": "Nenad Petrovic"
                    },
                    {
                        "name": "Fengjunjie Pan"
                    },
                    {
                        "name": "Vahid Zolfaghari"
                    },
                    {
                        "name": "Andre Schamschurko"
                    },
                    {
                        "name": "Alois Knoll"
                    }
                ],
                "author_detail": {
                    "name": "Alois Knoll"
                },
                "author": "Alois Knoll",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13263v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13263v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13259v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13259v1",
                "updated": "2025-05-19T15:41:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    41,
                    32,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T15:41:32Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    41,
                    32,
                    0,
                    139,
                    0
                ],
                "title": "From Automation to Autonomy: A Survey on Large Language Models in\n  Scientific Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Automation to Autonomy: A Survey on Large Language Models in\n  Scientific Discovery"
                },
                "summary": "Large Language Models (LLMs) are catalyzing a paradigm shift in scientific\ndiscovery, evolving from task-specific automation tools into increasingly\nautonomous agents and fundamentally redefining research processes and human-AI\ncollaboration. This survey systematically charts this burgeoning field, placing\na central focus on the changing roles and escalating capabilities of LLMs in\nscience. Through the lens of the scientific method, we introduce a foundational\nthree-level taxonomy-Tool, Analyst, and Scientist-to delineate their escalating\nautonomy and evolving responsibilities within the research lifecycle. We\nfurther identify pivotal challenges and future research trajectories such as\nrobotic automation, self-improvement, and ethical governance. Overall, this\nsurvey provides a conceptual architecture and strategic foresight to navigate\nand shape the future of AI-driven scientific discovery, fostering both rapid\ninnovation and responsible advancement. Github Repository:\nhttps://github.com/HKUST-KnowComp/Awesome-LLM-Scientific-Discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are catalyzing a paradigm shift in scientific\ndiscovery, evolving from task-specific automation tools into increasingly\nautonomous agents and fundamentally redefining research processes and human-AI\ncollaboration. This survey systematically charts this burgeoning field, placing\na central focus on the changing roles and escalating capabilities of LLMs in\nscience. Through the lens of the scientific method, we introduce a foundational\nthree-level taxonomy-Tool, Analyst, and Scientist-to delineate their escalating\nautonomy and evolving responsibilities within the research lifecycle. We\nfurther identify pivotal challenges and future research trajectories such as\nrobotic automation, self-improvement, and ethical governance. Overall, this\nsurvey provides a conceptual architecture and strategic foresight to navigate\nand shape the future of AI-driven scientific discovery, fostering both rapid\ninnovation and responsible advancement. Github Repository:\nhttps://github.com/HKUST-KnowComp/Awesome-LLM-Scientific-Discovery."
                },
                "authors": [
                    {
                        "name": "Tianshi Zheng"
                    },
                    {
                        "name": "Zheye Deng"
                    },
                    {
                        "name": "Hong Ting Tsang"
                    },
                    {
                        "name": "Weiqi Wang"
                    },
                    {
                        "name": "Jiaxin Bai"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Yangqiu Song"
                    }
                ],
                "author_detail": {
                    "name": "Yangqiu Song"
                },
                "author": "Yangqiu Song",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13259v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13259v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13258v1",
                "updated": "2025-05-19T15:40:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    40,
                    29,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T15:40:29Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    40,
                    29,
                    0,
                    139,
                    0
                ],
                "title": "Effective and Transparent RAG: Adaptive-Reward Reinforcement Learning\n  for Decision Traceability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective and Transparent RAG: Adaptive-Reward Reinforcement Learning\n  for Decision Traceability"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has significantly improved the\nperformance of large language models (LLMs) on knowledge-intensive domains.\nHowever, although RAG achieved successes across distinct domains, there are\nstill some unsolved challenges: 1) Effectiveness. Existing research mainly\nfocuses on developing more powerful RAG retrievers, but how to enhance the\ngenerator's (LLM's) ability to utilize the retrieved information for reasoning\nand generation? 2) Transparency. Most RAG methods ignore which retrieved\ncontent actually contributes to the reasoning process, resulting in a lack of\ninterpretability and visibility. To address this, we propose ARENA\n(Adaptive-Rewarded Evidence Navigation Agent), a transparent RAG generator\nframework trained via reinforcement learning (RL) with our proposed rewards.\nBased on the structured generation and adaptive reward calculation, our\nRL-based training enables the model to identify key evidence, perform\nstructured reasoning, and generate answers with interpretable decision traces.\nApplied to Qwen2.5-7B-Instruct and Llama3.1-8B-Instruct, abundant experiments\nwith various RAG baselines demonstrate that our model achieves 10-30%\nimprovements on all multi-hop QA datasets, which is comparable with the SOTA\nCommercially-developed LLMs (e.g., OpenAI-o1, DeepSeek-R1). Further analyses\nshow that ARENA has strong flexibility to be adopted on new datasets without\nextra training. Our models and codes are publicly released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has significantly improved the\nperformance of large language models (LLMs) on knowledge-intensive domains.\nHowever, although RAG achieved successes across distinct domains, there are\nstill some unsolved challenges: 1) Effectiveness. Existing research mainly\nfocuses on developing more powerful RAG retrievers, but how to enhance the\ngenerator's (LLM's) ability to utilize the retrieved information for reasoning\nand generation? 2) Transparency. Most RAG methods ignore which retrieved\ncontent actually contributes to the reasoning process, resulting in a lack of\ninterpretability and visibility. To address this, we propose ARENA\n(Adaptive-Rewarded Evidence Navigation Agent), a transparent RAG generator\nframework trained via reinforcement learning (RL) with our proposed rewards.\nBased on the structured generation and adaptive reward calculation, our\nRL-based training enables the model to identify key evidence, perform\nstructured reasoning, and generate answers with interpretable decision traces.\nApplied to Qwen2.5-7B-Instruct and Llama3.1-8B-Instruct, abundant experiments\nwith various RAG baselines demonstrate that our model achieves 10-30%\nimprovements on all multi-hop QA datasets, which is comparable with the SOTA\nCommercially-developed LLMs (e.g., OpenAI-o1, DeepSeek-R1). Further analyses\nshow that ARENA has strong flexibility to be adopted on new datasets without\nextra training. Our models and codes are publicly released."
                },
                "authors": [
                    {
                        "name": "Jingyi Ren"
                    },
                    {
                        "name": "Yekun Xu"
                    },
                    {
                        "name": "Xiaolong Wang"
                    },
                    {
                        "name": "Weitao Li"
                    },
                    {
                        "name": "Weizhi Ma"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13254v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13254v1",
                "updated": "2025-05-19T15:38:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    38,
                    40,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T15:38:40Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    38,
                    40,
                    0,
                    139,
                    0
                ],
                "title": "HeteroSpec: Leveraging Contextual Heterogeneity for Efficient\n  Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HeteroSpec: Leveraging Contextual Heterogeneity for Efficient\n  Speculative Decoding"
                },
                "summary": "Autoregressive decoding, the standard approach for Large Language Model (LLM)\ninference, remains a significant bottleneck due to its sequential nature. While\nspeculative decoding algorithms mitigate this inefficiency through parallel\nverification, they fail to exploit the inherent heterogeneity in linguistic\ncomplexity, a key factor leading to suboptimal resource allocation. We address\nthis by proposing HeteroSpec, a heterogeneity-adaptive speculative decoding\nframework that dynamically optimizes computational resource allocation based on\nlinguistic context complexity. HeteroSpec introduces two key mechanisms: (1) A\nnovel cumulative meta-path Top-$K$ entropy metric for efficiently identifying\npredictable contexts. (2) A dynamic resource allocation strategy based on\ndata-driven entropy partitioning, enabling adaptive speculative expansion and\npruning tailored to local context difficulty. Evaluated on five public\nbenchmarks and four models, HeteroSpec achieves an average speedup of\n4.26$\\times$. It consistently outperforms state-of-the-art EAGLE-3 across\nspeedup rates, average acceptance length, and verification cost. Notably,\nHeteroSpec requires no draft model retraining, incurs minimal overhead, and is\northogonal to other acceleration techniques. It demonstrates enhanced\nacceleration with stronger draft models, establishing a new paradigm for\ncontext-aware LLM inference acceleration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive decoding, the standard approach for Large Language Model (LLM)\ninference, remains a significant bottleneck due to its sequential nature. While\nspeculative decoding algorithms mitigate this inefficiency through parallel\nverification, they fail to exploit the inherent heterogeneity in linguistic\ncomplexity, a key factor leading to suboptimal resource allocation. We address\nthis by proposing HeteroSpec, a heterogeneity-adaptive speculative decoding\nframework that dynamically optimizes computational resource allocation based on\nlinguistic context complexity. HeteroSpec introduces two key mechanisms: (1) A\nnovel cumulative meta-path Top-$K$ entropy metric for efficiently identifying\npredictable contexts. (2) A dynamic resource allocation strategy based on\ndata-driven entropy partitioning, enabling adaptive speculative expansion and\npruning tailored to local context difficulty. Evaluated on five public\nbenchmarks and four models, HeteroSpec achieves an average speedup of\n4.26$\\times$. It consistently outperforms state-of-the-art EAGLE-3 across\nspeedup rates, average acceptance length, and verification cost. Notably,\nHeteroSpec requires no draft model retraining, incurs minimal overhead, and is\northogonal to other acceleration techniques. It demonstrates enhanced\nacceleration with stronger draft models, establishing a new paradigm for\ncontext-aware LLM inference acceleration."
                },
                "authors": [
                    {
                        "name": "Siran Liu"
                    },
                    {
                        "name": "Yang Ye"
                    },
                    {
                        "name": "Qianchao Zhu"
                    },
                    {
                        "name": "Zheng Cao"
                    },
                    {
                        "name": "Yongchao He"
                    }
                ],
                "author_detail": {
                    "name": "Yongchao He"
                },
                "author": "Yongchao He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13254v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13254v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13252v1",
                "updated": "2025-05-19T15:35:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    35,
                    17,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T15:35:17Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    35,
                    17,
                    0,
                    139,
                    0
                ],
                "title": "Natural Language Planning via Coding and Inference Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language Planning via Coding and Inference Scaling"
                },
                "summary": "Real-life textual planning tasks such as meeting scheduling have posed much\nchallenge to LLMs especially when the complexity is high. While previous work\nprimarily studied auto-regressive generation of plans with closed-source\nmodels, we systematically evaluate both closed- and open-source models,\nincluding those that scales output length with complexity during inference, in\ngenerating programs, which are executed to output the plan. We consider not\nonly standard Python code, but also the code to a constraint satisfaction\nproblem solver. Despite the algorithmic nature of the task, we show that\nprogramming often but not always outperforms planning. Our detailed error\nanalysis also indicates a lack of robustness and efficiency in the generated\ncode that hinders generalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-life textual planning tasks such as meeting scheduling have posed much\nchallenge to LLMs especially when the complexity is high. While previous work\nprimarily studied auto-regressive generation of plans with closed-source\nmodels, we systematically evaluate both closed- and open-source models,\nincluding those that scales output length with complexity during inference, in\ngenerating programs, which are executed to output the plan. We consider not\nonly standard Python code, but also the code to a constraint satisfaction\nproblem solver. Despite the algorithmic nature of the task, we show that\nprogramming often but not always outperforms planning. Our detailed error\nanalysis also indicates a lack of robustness and efficiency in the generated\ncode that hinders generalization."
                },
                "authors": [
                    {
                        "name": "Rikhil Amonkar"
                    },
                    {
                        "name": "Ronan Le Bras"
                    },
                    {
                        "name": "Li Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Li Zhang"
                },
                "author": "Li Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13249v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13249v1",
                "updated": "2025-05-19T15:32:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    32,
                    49,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T15:32:49Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    32,
                    49,
                    0,
                    139,
                    0
                ],
                "title": "RN-F: A Novel Approach for Mitigating Contaminated Data in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RN-F: A Novel Approach for Mitigating Contaminated Data in Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) have become foundational in modern artificial\nintelligence, powering a wide range of applications from code generation and\nvirtual assistants to scientific research and enterprise automation. However,\nconcerns about data contamination--where test data overlaps with training\ndata--have raised serious questions about the reliability of these\napplications. Despite awareness of this issue, existing methods fall short in\neffectively identifying or mitigating contamination. In this paper, we propose\nResidual-Noise Fingerprinting (RN-F), a novel framework for detecting\ncontaminated data in LLMs. RN-F is a single-pass, gradient-free detection\nmethod that leverages residual signal patterns without introducing additional\nfloating-point operations. Our approach is lightweight, model-agnostic, and\nefficient. We evaluate RN-F on multiple LLMs across various contaminated\ndatasets and show that it consistently outperforms existing state-of-the-art\nmethods, achieving performance improvements of up to 10.5% in contamination\ndetection metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become foundational in modern artificial\nintelligence, powering a wide range of applications from code generation and\nvirtual assistants to scientific research and enterprise automation. However,\nconcerns about data contamination--where test data overlaps with training\ndata--have raised serious questions about the reliability of these\napplications. Despite awareness of this issue, existing methods fall short in\neffectively identifying or mitigating contamination. In this paper, we propose\nResidual-Noise Fingerprinting (RN-F), a novel framework for detecting\ncontaminated data in LLMs. RN-F is a single-pass, gradient-free detection\nmethod that leverages residual signal patterns without introducing additional\nfloating-point operations. Our approach is lightweight, model-agnostic, and\nefficient. We evaluate RN-F on multiple LLMs across various contaminated\ndatasets and show that it consistently outperforms existing state-of-the-art\nmethods, achieving performance improvements of up to 10.5% in contamination\ndetection metrics."
                },
                "authors": [
                    {
                        "name": "Le Vu Anh"
                    },
                    {
                        "name": "Dinh Duc Nha Nguyen"
                    },
                    {
                        "name": "Phi Long Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Phi Long Nguyen"
                },
                "author": "Phi Long Nguyen",
                "arxiv_comment": "12 pages, 4 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13249v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13249v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00975v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00975v3",
                "updated": "2025-05-19T15:28:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    28,
                    43,
                    0,
                    139,
                    0
                ],
                "published": "2025-04-01T17:14:01Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    14,
                    1,
                    1,
                    91,
                    0
                ],
                "title": "Resource Allocation for RIS-Assisted CoMP-NOMA Networks using\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource Allocation for RIS-Assisted CoMP-NOMA Networks using\n  Reinforcement Learning"
                },
                "summary": "This thesis delves into the forefront of wireless communication by exploring\nthe synergistic integration of three transformative technologies: STAR-RIS,\nCoMP, and NOMA. Driven by the ever-increasing demand for higher data rates,\nimproved spectral efficiency, and expanded coverage in the evolving landscape\nof 6G development, this research investigates the potential of these\ntechnologies to revolutionize future wireless networks.\n  The thesis analyzes the performance gains achievable through strategic\ndeployment of STAR-RIS, focusing on mitigating inter-cell interference,\nenhancing signal strength, and extending coverage to cell-edge users. Resource\nsharing strategies for STAR-RIS elements are explored, optimizing both\ntransmission and reflection functionalities. Analytical frameworks are\ndeveloped to quantify the benefits of STAR-RIS assisted CoMP-NOMA networks\nunder realistic channel conditions, deriving key performance metrics such as\nergodic rates and outage probabilities. Additionally, the research delves into\nenergy-efficient design approaches for CoMP-NOMA networks incorporating RIS,\nproposing novel RIS configurations and optimization algorithms to achieve a\nbalance between performance and energy consumption. Furthermore, the\napplication of Deep Reinforcement Learning (DRL) techniques for intelligent and\nadaptive optimization in aerial RIS-assisted CoMP-NOMA networks is explored,\naiming to maximize network sum rate while meeting user quality of service\nrequirements. Through a comprehensive investigation of these technologies and\ntheir synergistic potential, this thesis contributes valuable insights into the\nfuture of wireless communication, paving the way for the development of more\nefficient, reliable, and sustainable networks capable of meeting the demands of\nour increasingly connected world.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This thesis delves into the forefront of wireless communication by exploring\nthe synergistic integration of three transformative technologies: STAR-RIS,\nCoMP, and NOMA. Driven by the ever-increasing demand for higher data rates,\nimproved spectral efficiency, and expanded coverage in the evolving landscape\nof 6G development, this research investigates the potential of these\ntechnologies to revolutionize future wireless networks.\n  The thesis analyzes the performance gains achievable through strategic\ndeployment of STAR-RIS, focusing on mitigating inter-cell interference,\nenhancing signal strength, and extending coverage to cell-edge users. Resource\nsharing strategies for STAR-RIS elements are explored, optimizing both\ntransmission and reflection functionalities. Analytical frameworks are\ndeveloped to quantify the benefits of STAR-RIS assisted CoMP-NOMA networks\nunder realistic channel conditions, deriving key performance metrics such as\nergodic rates and outage probabilities. Additionally, the research delves into\nenergy-efficient design approaches for CoMP-NOMA networks incorporating RIS,\nproposing novel RIS configurations and optimization algorithms to achieve a\nbalance between performance and energy consumption. Furthermore, the\napplication of Deep Reinforcement Learning (DRL) techniques for intelligent and\nadaptive optimization in aerial RIS-assisted CoMP-NOMA networks is explored,\naiming to maximize network sum rate while meeting user quality of service\nrequirements. Through a comprehensive investigation of these technologies and\ntheir synergistic potential, this thesis contributes valuable insights into the\nfuture of wireless communication, paving the way for the development of more\nefficient, reliable, and sustainable networks capable of meeting the demands of\nour increasingly connected world."
                },
                "authors": [
                    {
                        "name": "Muhammad Umer"
                    },
                    {
                        "name": "Muhammad Ahmed Mohsin"
                    },
                    {
                        "name": "Huma Ghafoor"
                    },
                    {
                        "name": "Syed Ali Hassan"
                    }
                ],
                "author_detail": {
                    "name": "Syed Ali Hassan"
                },
                "author": "Syed Ali Hassan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00975v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00975v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13246v1",
                "updated": "2025-05-19T15:28:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    28,
                    10,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T15:28:10Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    28,
                    10,
                    0,
                    139,
                    0
                ],
                "title": "Agentic Publications: An LLM-Driven Framework for Interactive Scientific\n  Publishing, Supplementing Traditional Papers with AI-Powered Knowledge\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic Publications: An LLM-Driven Framework for Interactive Scientific\n  Publishing, Supplementing Traditional Papers with AI-Powered Knowledge\n  Systems"
                },
                "summary": "The exponential growth of scientific literature presents significant\nchallenges for researchers navigating the complex knowledge landscape. We\npropose \"Agentic Publications\", a novel LLM-driven framework complementing\ntraditional publishing by transforming papers into interactive knowledge\nsystems. Our architecture integrates structured data with unstructured content\nthrough retrieval-augmented generation and multi-agent verification. The\nframework offers interfaces for both humans and machines, combining narrative\nexplanations with machine-readable outputs while addressing ethical\nconsiderations through automated validation and transparent governance. Key\nfeatures include continuous knowledge updates, automatic integration of new\nfindings, and customizable detail levels. Our proof-of-concept demonstrates\nmultilingual interaction, API accessibility, and structured knowledge\nrepresentation through vector databases, knowledge graphs, and verification\nagents. This approach enhances scientific communication across disciplines,\nimproving efficiency and collaboration while preserving traditional publishing\npathways, particularly valuable for interdisciplinary fields where knowledge\nintegration remains challenging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential growth of scientific literature presents significant\nchallenges for researchers navigating the complex knowledge landscape. We\npropose \"Agentic Publications\", a novel LLM-driven framework complementing\ntraditional publishing by transforming papers into interactive knowledge\nsystems. Our architecture integrates structured data with unstructured content\nthrough retrieval-augmented generation and multi-agent verification. The\nframework offers interfaces for both humans and machines, combining narrative\nexplanations with machine-readable outputs while addressing ethical\nconsiderations through automated validation and transparent governance. Key\nfeatures include continuous knowledge updates, automatic integration of new\nfindings, and customizable detail levels. Our proof-of-concept demonstrates\nmultilingual interaction, API accessibility, and structured knowledge\nrepresentation through vector databases, knowledge graphs, and verification\nagents. This approach enhances scientific communication across disciplines,\nimproving efficiency and collaboration while preserving traditional publishing\npathways, particularly valuable for interdisciplinary fields where knowledge\nintegration remains challenging."
                },
                "authors": [
                    {
                        "name": "Roberto Pugliese"
                    },
                    {
                        "name": "George Kourousias"
                    },
                    {
                        "name": "Francesco Venier"
                    },
                    {
                        "name": "Grazia Garlatti Costa"
                    }
                ],
                "author_detail": {
                    "name": "Grazia Garlatti Costa"
                },
                "author": "Grazia Garlatti Costa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13244v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13244v1",
                "updated": "2025-05-19T15:24:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    24,
                    53,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T15:24:53Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    24,
                    53,
                    0,
                    139,
                    0
                ],
                "title": "JNLP at SemEval-2025 Task 11: Cross-Lingual Multi-Label Emotion\n  Detection Using Generative Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JNLP at SemEval-2025 Task 11: Cross-Lingual Multi-Label Emotion\n  Detection Using Generative Models"
                },
                "summary": "With the rapid advancement of global digitalization, users from different\ncountries increasingly rely on social media for information exchange. In this\ncontext, multilingual multi-label emotion detection has emerged as a critical\nresearch area. This study addresses SemEval-2025 Task 11: Bridging the Gap in\nText-Based Emotion Detection. Our paper focuses on two sub-tracks of this task:\n(1) Track A: Multi-label emotion detection, and (2) Track B: Emotion intensity.\nTo tackle multilingual challenges, we leverage pre-trained multilingual models\nand focus on two architectures: (1) a fine-tuned BERT-based classification\nmodel and (2) an instruction-tuned generative LLM. Additionally, we propose two\nmethods for handling multi-label classification: the base method, which maps an\ninput directly to all its corresponding emotion labels, and the pairwise\nmethod, which models the relationship between the input text and each emotion\ncategory individually. Experimental results demonstrate the strong\ngeneralization ability of our approach in multilingual emotion recognition. In\nTrack A, our method achieved Top 4 performance across 10 languages, ranking 1st\nin Hindi. In Track B, our approach also secured Top 5 performance in 7\nlanguages, highlighting its simplicity and effectiveness\\footnote{Our code is\navailable at https://github.com/yingjie7/mlingual_multilabel_emo_detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of global digitalization, users from different\ncountries increasingly rely on social media for information exchange. In this\ncontext, multilingual multi-label emotion detection has emerged as a critical\nresearch area. This study addresses SemEval-2025 Task 11: Bridging the Gap in\nText-Based Emotion Detection. Our paper focuses on two sub-tracks of this task:\n(1) Track A: Multi-label emotion detection, and (2) Track B: Emotion intensity.\nTo tackle multilingual challenges, we leverage pre-trained multilingual models\nand focus on two architectures: (1) a fine-tuned BERT-based classification\nmodel and (2) an instruction-tuned generative LLM. Additionally, we propose two\nmethods for handling multi-label classification: the base method, which maps an\ninput directly to all its corresponding emotion labels, and the pairwise\nmethod, which models the relationship between the input text and each emotion\ncategory individually. Experimental results demonstrate the strong\ngeneralization ability of our approach in multilingual emotion recognition. In\nTrack A, our method achieved Top 4 performance across 10 languages, ranking 1st\nin Hindi. In Track B, our approach also secured Top 5 performance in 7\nlanguages, highlighting its simplicity and effectiveness\\footnote{Our code is\navailable at https://github.com/yingjie7/mlingual_multilabel_emo_detection."
                },
                "authors": [
                    {
                        "name": "Jieying Xue"
                    },
                    {
                        "name": "Phuong Minh Nguyen"
                    },
                    {
                        "name": "Minh Le Nguyen"
                    },
                    {
                        "name": "Xin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xin Liu"
                },
                "author": "Xin Liu",
                "arxiv_comment": "Published in The 19th International Workshop on Semantic Evaluation\n  (SemEval-2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13244v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13244v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08638v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08638v2",
                "updated": "2025-05-19T15:15:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    15,
                    46,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-13T14:55:31Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    14,
                    55,
                    31,
                    1,
                    133,
                    0
                ],
                "title": "TRAIL: Trace Reasoning and Agentic Issue Localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRAIL: Trace Reasoning and Agentic Issue Localization"
                },
                "summary": "The increasing adoption of agentic workflows across diverse domains brings a\ncritical need to scalably and systematically evaluate the complex traces these\nsystems generate. Current evaluation methods depend on manual, domain-specific\nhuman analysis of lengthy workflow traces - an approach that does not scale\nwith the growing complexity and volume of agentic outputs. Error analysis in\nthese settings is further complicated by the interplay of external tool outputs\nand language model reasoning, making it more challenging than traditional\nsoftware debugging. In this work, we (1) articulate the need for robust and\ndynamic evaluation methods for agentic workflow traces, (2) introduce a formal\ntaxonomy of error types encountered in agentic systems, and (3) present a set\nof 148 large human-annotated traces (TRAIL) constructed using this taxonomy and\ngrounded in established agentic benchmarks. To ensure ecological validity, we\ncurate traces from both single and multi-agent systems, focusing on real-world\napplications such as software engineering and open-world information retrieval.\nOur evaluations reveal that modern long context LLMs perform poorly at trace\ndebugging, with the best Gemini-2.5-pro model scoring a mere 11% on TRAIL. Our\ndataset and code are made publicly available to support and accelerate future\nresearch in scalable evaluation for agentic workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing adoption of agentic workflows across diverse domains brings a\ncritical need to scalably and systematically evaluate the complex traces these\nsystems generate. Current evaluation methods depend on manual, domain-specific\nhuman analysis of lengthy workflow traces - an approach that does not scale\nwith the growing complexity and volume of agentic outputs. Error analysis in\nthese settings is further complicated by the interplay of external tool outputs\nand language model reasoning, making it more challenging than traditional\nsoftware debugging. In this work, we (1) articulate the need for robust and\ndynamic evaluation methods for agentic workflow traces, (2) introduce a formal\ntaxonomy of error types encountered in agentic systems, and (3) present a set\nof 148 large human-annotated traces (TRAIL) constructed using this taxonomy and\ngrounded in established agentic benchmarks. To ensure ecological validity, we\ncurate traces from both single and multi-agent systems, focusing on real-world\napplications such as software engineering and open-world information retrieval.\nOur evaluations reveal that modern long context LLMs perform poorly at trace\ndebugging, with the best Gemini-2.5-pro model scoring a mere 11% on TRAIL. Our\ndataset and code are made publicly available to support and accelerate future\nresearch in scalable evaluation for agentic workflows."
                },
                "authors": [
                    {
                        "name": "Darshan Deshpande"
                    },
                    {
                        "name": "Varun Gangal"
                    },
                    {
                        "name": "Hersh Mehta"
                    },
                    {
                        "name": "Jitin Krishnan"
                    },
                    {
                        "name": "Anand Kannappan"
                    },
                    {
                        "name": "Rebecca Qian"
                    }
                ],
                "author_detail": {
                    "name": "Rebecca Qian"
                },
                "author": "Rebecca Qian",
                "arxiv_comment": "Dataset: https://huggingface.co/datasets/PatronusAI/TRAIL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08638v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08638v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13233v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13233v1",
                "updated": "2025-05-19T15:15:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    15,
                    37,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T15:15:37Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    15,
                    37,
                    0,
                    139,
                    0
                ],
                "title": "From Local Details to Global Context: Advancing Vision-Language Models\n  with Attention-Based Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Local Details to Global Context: Advancing Vision-Language Models\n  with Attention-Based Selection"
                },
                "summary": "Pretrained vision-language models (VLMs), e.g., CLIP, demonstrate impressive\nzero-shot capabilities on downstream tasks. Prior research highlights the\ncrucial role of visual augmentation techniques, like random cropping, in\nalignment with fine-grained class descriptions generated by large language\nmodels (LLMs), significantly enhancing zero-shot performance by incorporating\nmulti-view information. However, the inherent randomness of these augmentations\ncan inevitably introduce background artifacts and cause models to overly focus\non local details, compromising global semantic understanding. To address these\nissues, we propose an \\textbf{A}ttention-\\textbf{B}ased \\textbf{S}election\n(\\textbf{ABS}) method from local details to global context, which applies\nattention-guided cropping in both raw images and feature space, supplement\nglobal semantic information through strategic feature selection. Additionally,\nwe introduce a soft matching technique to effectively filter LLM descriptions\nfor better alignment. \\textbf{ABS} achieves state-of-the-art performance on\nout-of-distribution generalization and zero-shot classification tasks. Notably,\n\\textbf{ABS} is training-free and even rivals few-shot and test-time adaptation\nmethods. Our code is available at\n\\href{https://github.com/BIT-DA/ABS}{\\textcolor{darkgreen}{https://github.com/BIT-DA/ABS}}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pretrained vision-language models (VLMs), e.g., CLIP, demonstrate impressive\nzero-shot capabilities on downstream tasks. Prior research highlights the\ncrucial role of visual augmentation techniques, like random cropping, in\nalignment with fine-grained class descriptions generated by large language\nmodels (LLMs), significantly enhancing zero-shot performance by incorporating\nmulti-view information. However, the inherent randomness of these augmentations\ncan inevitably introduce background artifacts and cause models to overly focus\non local details, compromising global semantic understanding. To address these\nissues, we propose an \\textbf{A}ttention-\\textbf{B}ased \\textbf{S}election\n(\\textbf{ABS}) method from local details to global context, which applies\nattention-guided cropping in both raw images and feature space, supplement\nglobal semantic information through strategic feature selection. Additionally,\nwe introduce a soft matching technique to effectively filter LLM descriptions\nfor better alignment. \\textbf{ABS} achieves state-of-the-art performance on\nout-of-distribution generalization and zero-shot classification tasks. Notably,\n\\textbf{ABS} is training-free and even rivals few-shot and test-time adaptation\nmethods. Our code is available at\n\\href{https://github.com/BIT-DA/ABS}{\\textcolor{darkgreen}{https://github.com/BIT-DA/ABS}}."
                },
                "authors": [
                    {
                        "name": "Lincan Cai"
                    },
                    {
                        "name": "Jingxuan Kang"
                    },
                    {
                        "name": "Shuang Li"
                    },
                    {
                        "name": "Wenxuan Ma"
                    },
                    {
                        "name": "Binhui Xie"
                    },
                    {
                        "name": "Zhida Qin"
                    },
                    {
                        "name": "Jian Liang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Liang"
                },
                "author": "Jian Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13233v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13233v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05265v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05265v3",
                "updated": "2025-05-19T15:12:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    12,
                    39,
                    0,
                    139,
                    0
                ],
                "published": "2024-12-06T18:53:49Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    18,
                    53,
                    49,
                    4,
                    341,
                    0
                ],
                "title": "Reinforcement Learning: An Overview",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning: An Overview"
                },
                "summary": "This manuscript gives a big-picture, up-to-date overview of the field of\n(deep) reinforcement learning and sequential decision making, covering\nvalue-based methods, policy-based methods, model-based methods, multi-agent RL,\nLLMs and RL, and various other topics (e.g., offline RL, hierarchical RL,\nintrinsic reward).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This manuscript gives a big-picture, up-to-date overview of the field of\n(deep) reinforcement learning and sequential decision making, covering\nvalue-based methods, policy-based methods, model-based methods, multi-agent RL,\nLLMs and RL, and various other topics (e.g., offline RL, hierarchical RL,\nintrinsic reward)."
                },
                "authors": [
                    {
                        "name": "Kevin Murphy"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Murphy"
                },
                "author": "Kevin Murphy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05265v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05265v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13225v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13225v1",
                "updated": "2025-05-19T15:08:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    8,
                    23,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T15:08:23Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    8,
                    23,
                    0,
                    139,
                    0
                ],
                "title": "Automatic Complementary Separation Pruning Toward Lightweight CNNs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Complementary Separation Pruning Toward Lightweight CNNs"
                },
                "summary": "In this paper, we present Automatic Complementary Separation Pruning (ACSP),\na novel and fully automated pruning method for convolutional neural networks.\nACSP integrates the strengths of both structured pruning and activation-based\npruning, enabling the efficient removal of entire components such as neurons\nand channels while leveraging activations to identify and retain the most\nrelevant components. Our approach is designed specifically for supervised\nlearning tasks, where we construct a graph space that encodes the separation\ncapabilities of each component with respect to all class pairs. By employing\ncomplementary selection principles and utilizing a clustering algorithm, ACSP\nensures that the selected components maintain diverse and complementary\nseparation capabilities, reducing redundancy and maintaining high network\nperformance. The method automatically determines the optimal subset of\ncomponents in each layer, utilizing a knee-finding algorithm to select the\nminimal subset that preserves performance without requiring user-defined\npruning volumes. Extensive experiments on multiple architectures, including\nVGG-16, ResNet-50, and MobileNet-V2, across datasets like CIFAR-10, CIFAR-100,\nand ImageNet-1K, demonstrate that ACSP achieves competitive accuracy compared\nto other methods while significantly reducing computational costs. This fully\nautomated approach not only enhances scalability but also makes ACSP especially\npractical for real-world deployment by eliminating the need for manually\ndefining the pruning volume.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present Automatic Complementary Separation Pruning (ACSP),\na novel and fully automated pruning method for convolutional neural networks.\nACSP integrates the strengths of both structured pruning and activation-based\npruning, enabling the efficient removal of entire components such as neurons\nand channels while leveraging activations to identify and retain the most\nrelevant components. Our approach is designed specifically for supervised\nlearning tasks, where we construct a graph space that encodes the separation\ncapabilities of each component with respect to all class pairs. By employing\ncomplementary selection principles and utilizing a clustering algorithm, ACSP\nensures that the selected components maintain diverse and complementary\nseparation capabilities, reducing redundancy and maintaining high network\nperformance. The method automatically determines the optimal subset of\ncomponents in each layer, utilizing a knee-finding algorithm to select the\nminimal subset that preserves performance without requiring user-defined\npruning volumes. Extensive experiments on multiple architectures, including\nVGG-16, ResNet-50, and MobileNet-V2, across datasets like CIFAR-10, CIFAR-100,\nand ImageNet-1K, demonstrate that ACSP achieves competitive accuracy compared\nto other methods while significantly reducing computational costs. This fully\nautomated approach not only enhances scalability but also makes ACSP especially\npractical for real-world deployment by eliminating the need for manually\ndefining the pruning volume."
                },
                "authors": [
                    {
                        "name": "David Levin"
                    },
                    {
                        "name": "Gonen Singer"
                    }
                ],
                "author_detail": {
                    "name": "Gonen Singer"
                },
                "author": "Gonen Singer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13225v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13225v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13220v1",
                "updated": "2025-05-19T15:02:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    2,
                    59,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T15:02:59Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    2,
                    59,
                    0,
                    139,
                    0
                ],
                "title": "SeedBench: A Multi-task Benchmark for Evaluating Large Language Models\n  in Seed Science",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SeedBench: A Multi-task Benchmark for Evaluating Large Language Models\n  in Seed Science"
                },
                "summary": "Seed science is essential for modern agriculture, directly influencing crop\nyields and global food security. However, challenges such as interdisciplinary\ncomplexity and high costs with limited returns hinder progress, leading to a\nshortage of experts and insufficient technological support. While large\nlanguage models (LLMs) have shown promise across various fields, their\napplication in seed science remains limited due to the scarcity of digital\nresources, complex gene-trait relationships, and the lack of standardized\nbenchmarks. To address this gap, we introduce SeedBench -- the first multi-task\nbenchmark specifically designed for seed science. Developed in collaboration\nwith domain experts, SeedBench focuses on seed breeding and simulates key\naspects of modern breeding processes. We conduct a comprehensive evaluation of\n26 leading LLMs, encompassing proprietary, open-source, and domain-specific\nfine-tuned models. Our findings not only highlight the substantial gaps between\nthe power of LLMs and the real-world seed science problems, but also make a\nfoundational step for research on LLMs for seed design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seed science is essential for modern agriculture, directly influencing crop\nyields and global food security. However, challenges such as interdisciplinary\ncomplexity and high costs with limited returns hinder progress, leading to a\nshortage of experts and insufficient technological support. While large\nlanguage models (LLMs) have shown promise across various fields, their\napplication in seed science remains limited due to the scarcity of digital\nresources, complex gene-trait relationships, and the lack of standardized\nbenchmarks. To address this gap, we introduce SeedBench -- the first multi-task\nbenchmark specifically designed for seed science. Developed in collaboration\nwith domain experts, SeedBench focuses on seed breeding and simulates key\naspects of modern breeding processes. We conduct a comprehensive evaluation of\n26 leading LLMs, encompassing proprietary, open-source, and domain-specific\nfine-tuned models. Our findings not only highlight the substantial gaps between\nthe power of LLMs and the real-world seed science problems, but also make a\nfoundational step for research on LLMs for seed design."
                },
                "authors": [
                    {
                        "name": "Jie Ying"
                    },
                    {
                        "name": "Zihong Chen"
                    },
                    {
                        "name": "Zhefan Wang"
                    },
                    {
                        "name": "Wanli Jiang"
                    },
                    {
                        "name": "Chenyang Wang"
                    },
                    {
                        "name": "Zhonghang Yuan"
                    },
                    {
                        "name": "Haoyang Su"
                    },
                    {
                        "name": "Huanjun Kong"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Nanqing Dong"
                    }
                ],
                "author_detail": {
                    "name": "Nanqing Dong"
                },
                "author": "Nanqing Dong",
                "arxiv_comment": "Accepted by ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13211v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13211v1",
                "updated": "2025-05-19T14:58:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    58,
                    50,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T14:58:50Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    58,
                    50,
                    0,
                    139,
                    0
                ],
                "title": "MAGI-1: Autoregressive Video Generation at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAGI-1: Autoregressive Video Generation at Scale"
                },
                "summary": "We present MAGI-1, a world model that generates videos by autoregressively\npredicting a sequence of video chunks, defined as fixed-length segments of\nconsecutive frames. Trained to denoise per-chunk noise that increases\nmonotonically over time, MAGI-1 enables causal temporal modeling and naturally\nsupports streaming generation. It achieves strong performance on image-to-video\n(I2V) tasks conditioned on text instructions, providing high temporal\nconsistency and scalability, which are made possible by several algorithmic\ninnovations and a dedicated infrastructure stack. MAGI-1 facilitates\ncontrollable generation via chunk-wise prompting and supports real-time,\nmemory-efficient deployment by maintaining constant peak inference cost,\nregardless of video length. The largest variant of MAGI-1 comprises 24 billion\nparameters and supports context lengths of up to 4 million tokens,\ndemonstrating the scalability and robustness of our approach. The code and\nmodels are available at https://github.com/SandAI-org/MAGI-1 and\nhttps://github.com/SandAI-org/MagiAttention. The product can be accessed at\nhttps://sand.ai.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present MAGI-1, a world model that generates videos by autoregressively\npredicting a sequence of video chunks, defined as fixed-length segments of\nconsecutive frames. Trained to denoise per-chunk noise that increases\nmonotonically over time, MAGI-1 enables causal temporal modeling and naturally\nsupports streaming generation. It achieves strong performance on image-to-video\n(I2V) tasks conditioned on text instructions, providing high temporal\nconsistency and scalability, which are made possible by several algorithmic\ninnovations and a dedicated infrastructure stack. MAGI-1 facilitates\ncontrollable generation via chunk-wise prompting and supports real-time,\nmemory-efficient deployment by maintaining constant peak inference cost,\nregardless of video length. The largest variant of MAGI-1 comprises 24 billion\nparameters and supports context lengths of up to 4 million tokens,\ndemonstrating the scalability and robustness of our approach. The code and\nmodels are available at https://github.com/SandAI-org/MAGI-1 and\nhttps://github.com/SandAI-org/MagiAttention. The product can be accessed at\nhttps://sand.ai."
                },
                "authors": [
                    {
                        "name": "Sand. ai"
                    },
                    {
                        "name": "Hansi Teng"
                    },
                    {
                        "name": "Hongyu Jia"
                    },
                    {
                        "name": "Lei Sun"
                    },
                    {
                        "name": "Lingzhi Li"
                    },
                    {
                        "name": "Maolin Li"
                    },
                    {
                        "name": "Mingqiu Tang"
                    },
                    {
                        "name": "Shuai Han"
                    },
                    {
                        "name": "Tianning Zhang"
                    },
                    {
                        "name": "W. Q. Zhang"
                    },
                    {
                        "name": "Weifeng Luo"
                    },
                    {
                        "name": "Xiaoyang Kang"
                    },
                    {
                        "name": "Yuchen Sun"
                    },
                    {
                        "name": "Yue Cao"
                    },
                    {
                        "name": "Yunpeng Huang"
                    },
                    {
                        "name": "Yutong Lin"
                    },
                    {
                        "name": "Yuxin Fang"
                    },
                    {
                        "name": "Zewei Tao"
                    },
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Zhongshu Wang"
                    },
                    {
                        "name": "Zixun Liu"
                    },
                    {
                        "name": "Dai Shi"
                    },
                    {
                        "name": "Guoli Su"
                    },
                    {
                        "name": "Hanwen Sun"
                    },
                    {
                        "name": "Hong Pan"
                    },
                    {
                        "name": "Jie Wang"
                    },
                    {
                        "name": "Jiexin Sheng"
                    },
                    {
                        "name": "Min Cui"
                    },
                    {
                        "name": "Min Hu"
                    },
                    {
                        "name": "Ming Yan"
                    },
                    {
                        "name": "Shucheng Yin"
                    },
                    {
                        "name": "Siran Zhang"
                    },
                    {
                        "name": "Tingting Liu"
                    },
                    {
                        "name": "Xianping Yin"
                    },
                    {
                        "name": "Xiaoyu Yang"
                    },
                    {
                        "name": "Xin Song"
                    },
                    {
                        "name": "Xuan Hu"
                    },
                    {
                        "name": "Yankai Zhang"
                    },
                    {
                        "name": "Yuqiao Li"
                    }
                ],
                "author_detail": {
                    "name": "Yuqiao Li"
                },
                "author": "Yuqiao Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13211v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13211v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13210v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13210v1",
                "updated": "2025-05-19T14:58:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    58,
                    44,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T14:58:44Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    58,
                    44,
                    0,
                    139,
                    0
                ],
                "title": "Picturized and Recited with Dialects: A Multimodal Chinese\n  Representation Framework for Sentiment Analysis of Classical Chinese Poetry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Picturized and Recited with Dialects: A Multimodal Chinese\n  Representation Framework for Sentiment Analysis of Classical Chinese Poetry"
                },
                "summary": "Classical Chinese poetry is a vital and enduring part of Chinese literature,\nconveying profound emotional resonance. Existing studies analyze sentiment\nbased on textual meanings, overlooking the unique rhythmic and visual features\ninherent in poetry,especially since it is often recited and accompanied by\nChinese paintings. In this work, we propose a dialect-enhanced multimodal\nframework for classical Chinese poetry sentiment analysis. We extract\nsentence-level audio features from the poetry and incorporate audio from\nmultiple dialects,which may retain regional ancient Chinese phonetic features,\nenriching the phonetic representation. Additionally, we generate sentence-level\nvisual features, and the multimodal features are fused with textual features\nenhanced by LLM translation through multimodal contrastive representation\nlearning. Our framework outperforms state-of-the-art methods on two public\ndatasets, achieving at least 2.51% improvement in accuracy and 1.63% in macro\nF1. We open-source the code to facilitate research in this area and provide\ninsights for general multimodal Chinese representation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Classical Chinese poetry is a vital and enduring part of Chinese literature,\nconveying profound emotional resonance. Existing studies analyze sentiment\nbased on textual meanings, overlooking the unique rhythmic and visual features\ninherent in poetry,especially since it is often recited and accompanied by\nChinese paintings. In this work, we propose a dialect-enhanced multimodal\nframework for classical Chinese poetry sentiment analysis. We extract\nsentence-level audio features from the poetry and incorporate audio from\nmultiple dialects,which may retain regional ancient Chinese phonetic features,\nenriching the phonetic representation. Additionally, we generate sentence-level\nvisual features, and the multimodal features are fused with textual features\nenhanced by LLM translation through multimodal contrastive representation\nlearning. Our framework outperforms state-of-the-art methods on two public\ndatasets, achieving at least 2.51% improvement in accuracy and 1.63% in macro\nF1. We open-source the code to facilitate research in this area and provide\ninsights for general multimodal Chinese representation."
                },
                "authors": [
                    {
                        "name": "Xiaocong Du"
                    },
                    {
                        "name": "Haoyu Pei"
                    },
                    {
                        "name": "Haipeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Haipeng Zhang"
                },
                "author": "Haipeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13210v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13210v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13205v1",
                "updated": "2025-05-19T14:56:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    56,
                    24,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T14:56:24Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    56,
                    24,
                    0,
                    139,
                    0
                ],
                "title": "Quantum Knowledge Distillation for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Knowledge Distillation for Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are integral to advancing natural language\nprocessing, used extensively from machine translation to content creation.\nHowever, as these models scale to billions of parameters, their resource\ndemands increase dramatically. Meanwhile, quantum computing is recognized for\nefficiently solving complex problems with quantum characteristics like\nsuperposition and entanglement, providing a novel approach to these challenges.\nThis paper attempts to combine quantum computing with LLMs and proposes a\nQuantum knowledge Distillation algorithm for LLMs (QD-LLM), aimed at reducing\nthe computational and memory overhead required for model loading and inference.\nSpecifically, during the distillation stage, data is fed simultaneously into\nboth the LLMs and the designed quantum student model to initially quantify the\ndifference between their outputs; subsequently, with the help of the true\nlabel, the optimization of the quantum student model is executed to minimize\nthe difference with the LLM's output. Throughout this process, only the\nparameters of the quantum student network are updated to make its output closer\nto that of the LLMs, thereby achieving the purpose of distillation. Finally,\nthe optimized student model obtained by QD-LLM can efficiently solve\ndomain-specific tasks during inference without the usage of the original LLMs.\nExperimental results show that, compared to mainstream compression methods,\nQD-LLM significantly reduces the number of training parameters, memory\nconsumption, training time, and inference time while maintaining performance.\nMoreover, the optimized student model obtained by QD-LLM surpasses specific\nmodels designed for these tasks. We believe that QD-LLM can lay the groundwork\nfor exploring the utilization of quantum computing in model compression and its\npotential extension to other natural language processing challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are integral to advancing natural language\nprocessing, used extensively from machine translation to content creation.\nHowever, as these models scale to billions of parameters, their resource\ndemands increase dramatically. Meanwhile, quantum computing is recognized for\nefficiently solving complex problems with quantum characteristics like\nsuperposition and entanglement, providing a novel approach to these challenges.\nThis paper attempts to combine quantum computing with LLMs and proposes a\nQuantum knowledge Distillation algorithm for LLMs (QD-LLM), aimed at reducing\nthe computational and memory overhead required for model loading and inference.\nSpecifically, during the distillation stage, data is fed simultaneously into\nboth the LLMs and the designed quantum student model to initially quantify the\ndifference between their outputs; subsequently, with the help of the true\nlabel, the optimization of the quantum student model is executed to minimize\nthe difference with the LLM's output. Throughout this process, only the\nparameters of the quantum student network are updated to make its output closer\nto that of the LLMs, thereby achieving the purpose of distillation. Finally,\nthe optimized student model obtained by QD-LLM can efficiently solve\ndomain-specific tasks during inference without the usage of the original LLMs.\nExperimental results show that, compared to mainstream compression methods,\nQD-LLM significantly reduces the number of training parameters, memory\nconsumption, training time, and inference time while maintaining performance.\nMoreover, the optimized student model obtained by QD-LLM surpasses specific\nmodels designed for these tasks. We believe that QD-LLM can lay the groundwork\nfor exploring the utilization of quantum computing in model compression and its\npotential extension to other natural language processing challenges."
                },
                "authors": [
                    {
                        "name": "Lingxiao Li"
                    },
                    {
                        "name": "Yihao Wang"
                    },
                    {
                        "name": "Jiacheng Fan"
                    },
                    {
                        "name": "Jing Li"
                    },
                    {
                        "name": "Sujuan Qin"
                    },
                    {
                        "name": "Qiaoyan Wen"
                    },
                    {
                        "name": "Fei Gao"
                    }
                ],
                "author_detail": {
                    "name": "Fei Gao"
                },
                "author": "Fei Gao",
                "arxiv_comment": "Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11266v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11266v5",
                "updated": "2025-05-19T14:51:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    51,
                    44,
                    0,
                    139,
                    0
                ],
                "published": "2024-11-18T03:45:34Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    3,
                    45,
                    34,
                    0,
                    323,
                    0
                ],
                "title": "VersaTune: An Efficient Data Composition Framework for Training\n  Multi-Capability LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VersaTune: An Efficient Data Composition Framework for Training\n  Multi-Capability LLMs"
                },
                "summary": "As demonstrated by the proprietary Large Language Models (LLMs) such as GPT\nand Claude series, LLMs have the potential to achieve remarkable proficiency\nacross a wide range of domains, including law, medicine, finance, science,\ncode, etc., all within a single model. These capabilities are further augmented\nduring the Supervised Fine-Tuning (SFT) phase. Despite their potential,\nexisting work mainly focuses on domain-specific enhancements during\nfine-tuning, the challenge of which lies in catastrophic forgetting of\nknowledge across other domains. In this study, we introduce **VersaTune**, a\nnovel data composition framework designed for enhancing LLMs' overall\nmulti-domain capabilities during training. We begin with detecting the\ndistribution of domain-specific knowledge within the base model, followed by\nthe training data composition that aligns with the model's existing knowledge\ndistribution. During the subsequent training process, domain weights are\ndynamically adjusted based on their learnable potential and forgetting degree.\nExperimental results indicate that VersaTune is effective in multi-domain\nfostering, with an improvement of 35.21\\% in the overall multi-ability\nperformances compared to uniform domain weights. Furthermore, we find that\nQwen-2.5-32B + VersaTune even surpasses frontier models, including GPT-4o,\nClaude3.5-Sonnet and DeepSeek-V3 by 0.86\\%, 4.76\\% and 4.60\\%. Additionally, in\nscenarios where flexible expansion of a specific domain is required, VersaTune\nreduces the performance degradation in other domains by 38.77\\%, while\npreserving the training efficacy of the target domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As demonstrated by the proprietary Large Language Models (LLMs) such as GPT\nand Claude series, LLMs have the potential to achieve remarkable proficiency\nacross a wide range of domains, including law, medicine, finance, science,\ncode, etc., all within a single model. These capabilities are further augmented\nduring the Supervised Fine-Tuning (SFT) phase. Despite their potential,\nexisting work mainly focuses on domain-specific enhancements during\nfine-tuning, the challenge of which lies in catastrophic forgetting of\nknowledge across other domains. In this study, we introduce **VersaTune**, a\nnovel data composition framework designed for enhancing LLMs' overall\nmulti-domain capabilities during training. We begin with detecting the\ndistribution of domain-specific knowledge within the base model, followed by\nthe training data composition that aligns with the model's existing knowledge\ndistribution. During the subsequent training process, domain weights are\ndynamically adjusted based on their learnable potential and forgetting degree.\nExperimental results indicate that VersaTune is effective in multi-domain\nfostering, with an improvement of 35.21\\% in the overall multi-ability\nperformances compared to uniform domain weights. Furthermore, we find that\nQwen-2.5-32B + VersaTune even surpasses frontier models, including GPT-4o,\nClaude3.5-Sonnet and DeepSeek-V3 by 0.86\\%, 4.76\\% and 4.60\\%. Additionally, in\nscenarios where flexible expansion of a specific domain is required, VersaTune\nreduces the performance degradation in other domains by 38.77\\%, while\npreserving the training efficacy of the target domain."
                },
                "authors": [
                    {
                        "name": "Keer Lu"
                    },
                    {
                        "name": "Keshi Zhao"
                    },
                    {
                        "name": "Zhuoran Zhang"
                    },
                    {
                        "name": "Zheng Liang"
                    },
                    {
                        "name": "Da Pan"
                    },
                    {
                        "name": "Shusen Zhang"
                    },
                    {
                        "name": "Xin Wu"
                    },
                    {
                        "name": "Guosheng Dong"
                    },
                    {
                        "name": "Bin Cui"
                    },
                    {
                        "name": "Tengjiao Wang"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11266v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11266v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13195v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13195v1",
                "updated": "2025-05-19T14:50:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    50,
                    44,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T14:50:44Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    50,
                    44,
                    0,
                    139,
                    0
                ],
                "title": "Adversarial Testing in LLMs: Insights into Decision-Making\n  Vulnerabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial Testing in LLMs: Insights into Decision-Making\n  Vulnerabilities"
                },
                "summary": "As Large Language Models (LLMs) become increasingly integrated into\nreal-world decision-making systems, understanding their behavioural\nvulnerabilities remains a critical challenge for AI safety and alignment. While\nexisting evaluation metrics focus primarily on reasoning accuracy or factual\ncorrectness, they often overlook whether LLMs are robust to adversarial\nmanipulation or capable of using adaptive strategy in dynamic environments.\nThis paper introduces an adversarial evaluation framework designed to\nsystematically stress-test the decision-making processes of LLMs under\ninteractive and adversarial conditions. Drawing on methodologies from cognitive\npsychology and game theory, our framework probes how models respond in two\ncanonical tasks: the two-armed bandit task and the Multi-Round Trust Task.\nThese tasks capture key aspects of exploration-exploitation trade-offs, social\ncooperation, and strategic flexibility. We apply this framework to several\nstate-of-the-art LLMs, including GPT-3.5, GPT-4, Gemini-1.5, and DeepSeek-V3,\nrevealing model-specific susceptibilities to manipulation and rigidity in\nstrategy adaptation. Our findings highlight distinct behavioral patterns across\nmodels and emphasize the importance of adaptability and fairness recognition\nfor trustworthy AI deployment. Rather than offering a performance benchmark,\nthis work proposes a methodology for diagnosing decision-making weaknesses in\nLLM-based agents, providing actionable insights for alignment and safety\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) become increasingly integrated into\nreal-world decision-making systems, understanding their behavioural\nvulnerabilities remains a critical challenge for AI safety and alignment. While\nexisting evaluation metrics focus primarily on reasoning accuracy or factual\ncorrectness, they often overlook whether LLMs are robust to adversarial\nmanipulation or capable of using adaptive strategy in dynamic environments.\nThis paper introduces an adversarial evaluation framework designed to\nsystematically stress-test the decision-making processes of LLMs under\ninteractive and adversarial conditions. Drawing on methodologies from cognitive\npsychology and game theory, our framework probes how models respond in two\ncanonical tasks: the two-armed bandit task and the Multi-Round Trust Task.\nThese tasks capture key aspects of exploration-exploitation trade-offs, social\ncooperation, and strategic flexibility. We apply this framework to several\nstate-of-the-art LLMs, including GPT-3.5, GPT-4, Gemini-1.5, and DeepSeek-V3,\nrevealing model-specific susceptibilities to manipulation and rigidity in\nstrategy adaptation. Our findings highlight distinct behavioral patterns across\nmodels and emphasize the importance of adaptability and fairness recognition\nfor trustworthy AI deployment. Rather than offering a performance benchmark,\nthis work proposes a methodology for diagnosing decision-making weaknesses in\nLLM-based agents, providing actionable insights for alignment and safety\nresearch."
                },
                "authors": [
                    {
                        "name": "Lili Zhang"
                    },
                    {
                        "name": "Haomiaomiao Wang"
                    },
                    {
                        "name": "Long Cheng"
                    },
                    {
                        "name": "Libao Deng"
                    },
                    {
                        "name": "Tomas Ward"
                    }
                ],
                "author_detail": {
                    "name": "Tomas Ward"
                },
                "author": "Tomas Ward",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13195v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13195v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13192v1",
                "updated": "2025-05-19T14:49:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    49,
                    10,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T14:49:10Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    49,
                    10,
                    0,
                    139,
                    0
                ],
                "title": "True Zero-Shot Inference of Dynamical Systems Preserving Long-Term\n  Statistics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "True Zero-Shot Inference of Dynamical Systems Preserving Long-Term\n  Statistics"
                },
                "summary": "Complex, temporally evolving phenomena, from climate to brain activity, are\ngoverned by dynamical systems (DS). DS reconstruction (DSR) seeks to infer\ngenerative surrogate models of these from observed data, reproducing their\nlong-term behavior. Existing DSR approaches require purpose-training for any\nnew system observed, lacking the zero-shot and in-context inference\ncapabilities known from LLMs. Here we introduce DynaMix, a novel multivariate\nALRNN-based mixture-of-experts architecture pre-trained for DSR, the first DSR\nmodel able to generalize zero-shot to out-of-domain DS. Just from a provided\ncontext signal, without any re-training, DynaMix faithfully forecasts the\nlong-term evolution of novel DS where existing time series (TS) foundation\nmodels, like Chronos, fail -- at a fraction of the number of parameters and\norders of magnitude faster inference times. DynaMix outperforms TS foundation\nmodels in terms of long-term statistics, and often also short-term forecasts,\neven on real-world time series, like traffic or weather data, typically used\nfor training and evaluating TS models, but not at all part of DynaMix' training\ncorpus. We illustrate some of the failure modes of TS models for DSR problems,\nand conclude that models built on DS principles may bear a huge potential also\nfor advancing the TS prediction field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Complex, temporally evolving phenomena, from climate to brain activity, are\ngoverned by dynamical systems (DS). DS reconstruction (DSR) seeks to infer\ngenerative surrogate models of these from observed data, reproducing their\nlong-term behavior. Existing DSR approaches require purpose-training for any\nnew system observed, lacking the zero-shot and in-context inference\ncapabilities known from LLMs. Here we introduce DynaMix, a novel multivariate\nALRNN-based mixture-of-experts architecture pre-trained for DSR, the first DSR\nmodel able to generalize zero-shot to out-of-domain DS. Just from a provided\ncontext signal, without any re-training, DynaMix faithfully forecasts the\nlong-term evolution of novel DS where existing time series (TS) foundation\nmodels, like Chronos, fail -- at a fraction of the number of parameters and\norders of magnitude faster inference times. DynaMix outperforms TS foundation\nmodels in terms of long-term statistics, and often also short-term forecasts,\neven on real-world time series, like traffic or weather data, typically used\nfor training and evaluating TS models, but not at all part of DynaMix' training\ncorpus. We illustrate some of the failure modes of TS models for DSR problems,\nand conclude that models built on DS principles may bear a huge potential also\nfor advancing the TS prediction field."
                },
                "authors": [
                    {
                        "name": "Christoph Jürgen Hemmer"
                    },
                    {
                        "name": "Daniel Durstewitz"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Durstewitz"
                },
                "author": "Daniel Durstewitz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nlin.CD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15654v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15654v5",
                "updated": "2025-05-19T14:34:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    34,
                    42,
                    0,
                    139,
                    0
                ],
                "published": "2025-02-21T18:22:36Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    18,
                    22,
                    36,
                    4,
                    52,
                    0
                ],
                "title": "Machine-generated text detection prevents language model collapse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine-generated text detection prevents language model collapse"
                },
                "summary": "As Large Language Models (LLMs) become increasingly prevalent, their\ngenerated outputs are proliferating across the web, risking a future where\nmachine-generated content dilutes human-authored text. Since online data is the\nprimary resource for LLM pre-training, subsequent models could be trained on an\nunknown portion of synthetic samples. This will lead to model collapse, a\ndegenerative process whereby LLMs reinforce their own errors, converge to a low\nvariance output distribution, and ultimately yield a declining performance. In\nthis study, we investigate the impact of decoding strategy on model collapse,\nanalysing the text characteristics at each model generation, the similarity to\nhuman references, and the resulting model performance. Using the decoding\nstrategies that lead to the most significant degradation, we evaluate model\ncollapse in more realistic scenarios where the origin of the data (human or\nsynthetic) is unknown. We train a machine-generated text detector and propose\nan importance sampling approach to alleviate model collapse. Our method is\nvalidated on two LLM variants (GPT-2 and SmolLM2), across a range of model\nsizes (124M to 1.7B), on the open-ended text generation task. We demonstrate\nthat it can not only prevent model collapse but also improve performance when\nsufficient human-authored samples are present. Source code:\ngithub.com/GeorgeDrayson/model_collapse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) become increasingly prevalent, their\ngenerated outputs are proliferating across the web, risking a future where\nmachine-generated content dilutes human-authored text. Since online data is the\nprimary resource for LLM pre-training, subsequent models could be trained on an\nunknown portion of synthetic samples. This will lead to model collapse, a\ndegenerative process whereby LLMs reinforce their own errors, converge to a low\nvariance output distribution, and ultimately yield a declining performance. In\nthis study, we investigate the impact of decoding strategy on model collapse,\nanalysing the text characteristics at each model generation, the similarity to\nhuman references, and the resulting model performance. Using the decoding\nstrategies that lead to the most significant degradation, we evaluate model\ncollapse in more realistic scenarios where the origin of the data (human or\nsynthetic) is unknown. We train a machine-generated text detector and propose\nan importance sampling approach to alleviate model collapse. Our method is\nvalidated on two LLM variants (GPT-2 and SmolLM2), across a range of model\nsizes (124M to 1.7B), on the open-ended text generation task. We demonstrate\nthat it can not only prevent model collapse but also improve performance when\nsufficient human-authored samples are present. Source code:\ngithub.com/GeorgeDrayson/model_collapse."
                },
                "authors": [
                    {
                        "name": "George Drayson"
                    },
                    {
                        "name": "Emine Yilmaz"
                    },
                    {
                        "name": "Vasileios Lampos"
                    }
                ],
                "author_detail": {
                    "name": "Vasileios Lampos"
                },
                "author": "Vasileios Lampos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15654v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15654v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13176v1",
                "updated": "2025-05-19T14:30:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    30,
                    46,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T14:30:46Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    30,
                    46,
                    0,
                    139,
                    0
                ],
                "title": "ToolSpectrum : Towards Personalized Tool Utilization for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ToolSpectrum : Towards Personalized Tool Utilization for Large Language\n  Models"
                },
                "summary": "While integrating external tools into large language models (LLMs) enhances\ntheir ability to access real-time information and domain-specific services,\nexisting approaches focus narrowly on functional tool selection following user\ninstructions, overlooking the context-aware personalization in tool selection.\nThis oversight leads to suboptimal user satisfaction and inefficient tool\nutilization, particularly when overlapping toolsets require nuanced selection\nbased on contextual factors. To bridge this gap, we introduce ToolSpectrum, a\nbenchmark designed to evaluate LLMs' capabilities in personalized tool\nutilization. Specifically, we formalize two key dimensions of personalization,\nuser profile and environmental factors, and analyze their individual and\nsynergistic impacts on tool utilization. Through extensive experiments on\nToolSpectrum, we demonstrate that personalized tool utilization significantly\nimproves user experience across diverse scenarios. However, even\nstate-of-the-art LLMs exhibit the limited ability to reason jointly about user\nprofiles and environmental factors, often prioritizing one dimension at the\nexpense of the other. Our findings underscore the necessity of context-aware\npersonalization in tool-augmented LLMs and reveal critical limitations for\ncurrent models. Our data and code are available at\nhttps://github.com/Chengziha0/ToolSpectrum.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While integrating external tools into large language models (LLMs) enhances\ntheir ability to access real-time information and domain-specific services,\nexisting approaches focus narrowly on functional tool selection following user\ninstructions, overlooking the context-aware personalization in tool selection.\nThis oversight leads to suboptimal user satisfaction and inefficient tool\nutilization, particularly when overlapping toolsets require nuanced selection\nbased on contextual factors. To bridge this gap, we introduce ToolSpectrum, a\nbenchmark designed to evaluate LLMs' capabilities in personalized tool\nutilization. Specifically, we formalize two key dimensions of personalization,\nuser profile and environmental factors, and analyze their individual and\nsynergistic impacts on tool utilization. Through extensive experiments on\nToolSpectrum, we demonstrate that personalized tool utilization significantly\nimproves user experience across diverse scenarios. However, even\nstate-of-the-art LLMs exhibit the limited ability to reason jointly about user\nprofiles and environmental factors, often prioritizing one dimension at the\nexpense of the other. Our findings underscore the necessity of context-aware\npersonalization in tool-augmented LLMs and reveal critical limitations for\ncurrent models. Our data and code are available at\nhttps://github.com/Chengziha0/ToolSpectrum."
                },
                "authors": [
                    {
                        "name": "Zihao Cheng"
                    },
                    {
                        "name": "Hongru Wang"
                    },
                    {
                        "name": "Zeming Liu"
                    },
                    {
                        "name": "Yuhang Guo"
                    },
                    {
                        "name": "Yuanfang Guo"
                    },
                    {
                        "name": "Yunhong Wang"
                    },
                    {
                        "name": "Haifeng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haifeng Wang"
                },
                "author": "Haifeng Wang",
                "arxiv_comment": "Accepted by ACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13175v1",
                "updated": "2025-05-19T14:30:41Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    30,
                    41,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T14:30:41Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    30,
                    41,
                    0,
                    139,
                    0
                ],
                "title": "Enhancing LLMs for Time Series Forecasting via Structure-Guided\n  Cross-Modal Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLMs for Time Series Forecasting via Structure-Guided\n  Cross-Modal Alignment"
                },
                "summary": "The emerging paradigm of leveraging pretrained large language models (LLMs)\nfor time series forecasting has predominantly employed linguistic-temporal\nmodality alignment strategies through token-level or layer-wise feature\nmapping. However, these approaches fundamentally neglect a critical insight:\nthe core competency of LLMs resides not merely in processing localized token\nfeatures but in their inherent capacity to model holistic sequence structures.\nThis paper posits that effective cross-modal alignment necessitates structural\nconsistency at the sequence level. We propose the Structure-Guided Cross-Modal\nAlignment (SGCMA), a framework that fully exploits and aligns the\nstate-transition graph structures shared by time-series and linguistic data as\nsequential modalities, thereby endowing time series with language-like\nproperties and delivering stronger generalization after modality alignment.\nSGCMA consists of two key components, namely Structure Alignment and Semantic\nAlignment. In Structure Alignment, a state transition matrix is learned from\ntext data through Hidden Markov Models (HMMs), and a shallow transformer-based\nMaximum Entropy Markov Model (MEMM) receives the hot-start transition matrix\nand annotates each temporal patch into state probability, ensuring that the\ntemporal representation sequence inherits language-like sequential dynamics. In\nSemantic Alignment, cross-attention is applied between temporal patches and the\ntop-k tokens within each state, and the ultimate temporal embeddings are\nderived by the expected value of these embeddings using a weighted average\nbased on state probabilities. Experiments on multiple benchmarks demonstrate\nthat SGCMA achieves state-of-the-art performance, offering a novel approach to\ncross-modal alignment in time series forecasting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emerging paradigm of leveraging pretrained large language models (LLMs)\nfor time series forecasting has predominantly employed linguistic-temporal\nmodality alignment strategies through token-level or layer-wise feature\nmapping. However, these approaches fundamentally neglect a critical insight:\nthe core competency of LLMs resides not merely in processing localized token\nfeatures but in their inherent capacity to model holistic sequence structures.\nThis paper posits that effective cross-modal alignment necessitates structural\nconsistency at the sequence level. We propose the Structure-Guided Cross-Modal\nAlignment (SGCMA), a framework that fully exploits and aligns the\nstate-transition graph structures shared by time-series and linguistic data as\nsequential modalities, thereby endowing time series with language-like\nproperties and delivering stronger generalization after modality alignment.\nSGCMA consists of two key components, namely Structure Alignment and Semantic\nAlignment. In Structure Alignment, a state transition matrix is learned from\ntext data through Hidden Markov Models (HMMs), and a shallow transformer-based\nMaximum Entropy Markov Model (MEMM) receives the hot-start transition matrix\nand annotates each temporal patch into state probability, ensuring that the\ntemporal representation sequence inherits language-like sequential dynamics. In\nSemantic Alignment, cross-attention is applied between temporal patches and the\ntop-k tokens within each state, and the ultimate temporal embeddings are\nderived by the expected value of these embeddings using a weighted average\nbased on state probabilities. Experiments on multiple benchmarks demonstrate\nthat SGCMA achieves state-of-the-art performance, offering a novel approach to\ncross-modal alignment in time series forecasting."
                },
                "authors": [
                    {
                        "name": "Siming Sun"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Xuejun Jiang"
                    },
                    {
                        "name": "Wenchao Meng"
                    },
                    {
                        "name": "Qinmin Yang"
                    }
                ],
                "author_detail": {
                    "name": "Qinmin Yang"
                },
                "author": "Qinmin Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13173v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13173v1",
                "updated": "2025-05-19T14:30:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    30,
                    10,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T14:30:10Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    30,
                    10,
                    0,
                    139,
                    0
                ],
                "title": "A Case Study of Cross-Lingual Zero-Shot Generalization for Classical\n  Languages in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Case Study of Cross-Lingual Zero-Shot Generalization for Classical\n  Languages in LLMs"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable generalization\ncapabilities across diverse tasks and languages. In this study, we focus on\nnatural language understanding in three classical languages -- Sanskrit,\nAncient Greek and Latin -- to investigate the factors affecting cross-lingual\nzero-shot generalization. First, we explore named entity recognition and\nmachine translation into English. While LLMs perform equal to or better than\nfine-tuned baselines on out-of-domain data, smaller models often struggle,\nespecially with niche or abstract entity types. In addition, we concentrate on\nSanskrit by presenting a factoid question-answering (QA) dataset and show that\nincorporating context via retrieval-augmented generation approach significantly\nboosts performance. In contrast, we observe pronounced performance drops for\nsmaller LLMs across these QA tasks. These results suggest model scale as an\nimportant factor influencing cross-lingual generalization. Assuming that models\nused such as GPT-4o and Llama-3.1 are not instruction fine-tuned on classical\nlanguages, our findings provide insights into how LLMs may generalize on these\nlanguages and their consequent utility in classical studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable generalization\ncapabilities across diverse tasks and languages. In this study, we focus on\nnatural language understanding in three classical languages -- Sanskrit,\nAncient Greek and Latin -- to investigate the factors affecting cross-lingual\nzero-shot generalization. First, we explore named entity recognition and\nmachine translation into English. While LLMs perform equal to or better than\nfine-tuned baselines on out-of-domain data, smaller models often struggle,\nespecially with niche or abstract entity types. In addition, we concentrate on\nSanskrit by presenting a factoid question-answering (QA) dataset and show that\nincorporating context via retrieval-augmented generation approach significantly\nboosts performance. In contrast, we observe pronounced performance drops for\nsmaller LLMs across these QA tasks. These results suggest model scale as an\nimportant factor influencing cross-lingual generalization. Assuming that models\nused such as GPT-4o and Llama-3.1 are not instruction fine-tuned on classical\nlanguages, our findings provide insights into how LLMs may generalize on these\nlanguages and their consequent utility in classical studies."
                },
                "authors": [
                    {
                        "name": "V. S. D. S. Mahesh Akavarapu"
                    },
                    {
                        "name": "Hrishikesh Terdalkar"
                    },
                    {
                        "name": "Pramit Bhattacharyya"
                    },
                    {
                        "name": "Shubhangi Agarwal"
                    },
                    {
                        "name": "Vishakha Deulgaonkar"
                    },
                    {
                        "name": "Pralay Manna"
                    },
                    {
                        "name": "Chaitali Dangarikar"
                    },
                    {
                        "name": "Arnab Bhattacharya"
                    }
                ],
                "author_detail": {
                    "name": "Arnab Bhattacharya"
                },
                "author": "Arnab Bhattacharya",
                "arxiv_comment": "Accepted to ACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13173v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13173v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02327v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02327v2",
                "updated": "2025-05-19T14:30:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    30,
                    0,
                    0,
                    139,
                    0
                ],
                "published": "2024-06-04T13:57:34Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    13,
                    57,
                    34,
                    1,
                    156,
                    0
                ],
                "title": "Iterative Deployment Exposure for Unsupervised Out-of-Distribution\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iterative Deployment Exposure for Unsupervised Out-of-Distribution\n  Detection"
                },
                "summary": "Deep learning models are vulnerable to performance degradation when\nencountering out-of-distribution (OOD) images, potentially leading to\nmisdiagnoses and compromised patient care. These shortcomings have led to great\ninterest in the field of OOD detection. Existing unsupervised OOD (U-OOD)\ndetection methods typically assume that OOD samples originate from an\nunconcentrated distribution complementary to the training distribution,\nneglecting the reality that deployed models passively accumulate task-specific\nOOD samples over time. To better reflect this real-world scenario, we introduce\nIterative Deployment Exposure (IDE), a novel and more realistic setting for\nU-OOD detection. We propose CSO, a method for IDE that starts from a U-OOD\ndetector that is agnostic to the OOD distribution and slowly refines it during\ndeployment using observed unlabeled data. CSO uses a new U-OOD scoring function\nthat combines the Mahalanobis distance with a nearest-neighbor approach, along\nwith a novel confidence-scaled few-shot OOD detector to effectively learn from\nlimited OOD examples. We validate our approach on a dedicated benchmark,\nshowing that our method greatly improves upon strong baselines on three medical\nimaging modalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning models are vulnerable to performance degradation when\nencountering out-of-distribution (OOD) images, potentially leading to\nmisdiagnoses and compromised patient care. These shortcomings have led to great\ninterest in the field of OOD detection. Existing unsupervised OOD (U-OOD)\ndetection methods typically assume that OOD samples originate from an\nunconcentrated distribution complementary to the training distribution,\nneglecting the reality that deployed models passively accumulate task-specific\nOOD samples over time. To better reflect this real-world scenario, we introduce\nIterative Deployment Exposure (IDE), a novel and more realistic setting for\nU-OOD detection. We propose CSO, a method for IDE that starts from a U-OOD\ndetector that is agnostic to the OOD distribution and slowly refines it during\ndeployment using observed unlabeled data. CSO uses a new U-OOD scoring function\nthat combines the Mahalanobis distance with a nearest-neighbor approach, along\nwith a novel confidence-scaled few-shot OOD detector to effectively learn from\nlimited OOD examples. We validate our approach on a dedicated benchmark,\nshowing that our method greatly improves upon strong baselines on three medical\nimaging modalities."
                },
                "authors": [
                    {
                        "name": "Lars Doorenbos"
                    },
                    {
                        "name": "Raphael Sznitman"
                    },
                    {
                        "name": "Pablo Márquez-Neila"
                    }
                ],
                "author_detail": {
                    "name": "Pablo Márquez-Neila"
                },
                "author": "Pablo Márquez-Neila",
                "arxiv_comment": "Accepted at MICCAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02327v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02327v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13171v1",
                "updated": "2025-05-19T14:28:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    28,
                    35,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T14:28:35Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    28,
                    35,
                    0,
                    139,
                    0
                ],
                "title": "Positional Fragility in LLMs: How Offset Effects Reshape Our\n  Understanding of Memorization Risks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Positional Fragility in LLMs: How Offset Effects Reshape Our\n  Understanding of Memorization Risks"
                },
                "summary": "Large language models are known to memorize parts of their training data,\nposing risk of copyright violations. To systematically examine this risk, we\npretrain language models (1B/3B/8B) from scratch on 83B tokens, mixing\nweb-scale data with public domain books used to simulate copyrighted content at\ncontrolled frequencies at lengths at least ten times longer than prior work. We\nthereby identified the offset effect, a phenomenon characterized by two key\nfindings: (1) verbatim memorization is most strongly triggered by short\nprefixes drawn from the beginning of the context window, with memorization\ndecreasing counterintuitively as prefix length increases; and (2) a sharp\ndecline in verbatim recall when prefix begins offset from the initial tokens of\nthe context window. We attribute this to positional fragility: models rely\ndisproportionately on the earliest tokens in their context window as retrieval\nanchors, making them sensitive to even slight shifts. We further observe that\nwhen the model fails to retrieve memorized content, it often produces\ndegenerated text. Leveraging these findings, we show that shifting sensitive\ndata deeper into the context window suppresses both extractable memorization\nand degeneration. Our results suggest that positional offset is a critical and\npreviously overlooked axis for evaluating memorization risks, since prior work\nimplicitly assumed uniformity by probing only from the beginning of training\nsequences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are known to memorize parts of their training data,\nposing risk of copyright violations. To systematically examine this risk, we\npretrain language models (1B/3B/8B) from scratch on 83B tokens, mixing\nweb-scale data with public domain books used to simulate copyrighted content at\ncontrolled frequencies at lengths at least ten times longer than prior work. We\nthereby identified the offset effect, a phenomenon characterized by two key\nfindings: (1) verbatim memorization is most strongly triggered by short\nprefixes drawn from the beginning of the context window, with memorization\ndecreasing counterintuitively as prefix length increases; and (2) a sharp\ndecline in verbatim recall when prefix begins offset from the initial tokens of\nthe context window. We attribute this to positional fragility: models rely\ndisproportionately on the earliest tokens in their context window as retrieval\nanchors, making them sensitive to even slight shifts. We further observe that\nwhen the model fails to retrieve memorized content, it often produces\ndegenerated text. Leveraging these findings, we show that shifting sensitive\ndata deeper into the context window suppresses both extractable memorization\nand degeneration. Our results suggest that positional offset is a critical and\npreviously overlooked axis for evaluating memorization risks, since prior work\nimplicitly assumed uniformity by probing only from the beginning of training\nsequences."
                },
                "authors": [
                    {
                        "name": "Yixuan Xu"
                    },
                    {
                        "name": "Antoine Bosselut"
                    },
                    {
                        "name": "Imanol Schlag"
                    }
                ],
                "author_detail": {
                    "name": "Imanol Schlag"
                },
                "author": "Imanol Schlag",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02506v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02506v3",
                "updated": "2025-05-19T14:26:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    26,
                    34,
                    0,
                    139,
                    0
                ],
                "published": "2025-01-05T11:06:55Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    11,
                    6,
                    55,
                    6,
                    5,
                    0
                ],
                "title": "ToolHop: A Query-Driven Benchmark for Evaluating Large Language Models\n  in Multi-Hop Tool Use",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ToolHop: A Query-Driven Benchmark for Evaluating Large Language Models\n  in Multi-Hop Tool Use"
                },
                "summary": "Effective evaluation of multi-hop tool use is critical for analyzing the\nunderstanding, reasoning, and function-calling capabilities of large language\nmodels (LLMs). However, progress has been hindered by a lack of reliable\nevaluation datasets. To address this, we present ToolHop, a dataset comprising\n995 user queries and 3,912 associated tools, specifically designed for rigorous\nevaluation of multi-hop tool use. ToolHop ensures diverse queries, meaningful\ninterdependencies, locally executable tools, detailed feedback, and verifiable\nanswers through a novel query-driven data construction approach that includes\ntool creation, document refinement, and code generation. We evaluate 14 LLMs\nacross five model families (i.e., LLaMA3.1, Qwen2.5, Gemini1.5, Claude3.5, and\nGPT), uncovering significant challenges in handling multi-hop tool-use\nscenarios. The leading model, GPT-4o, achieves an accuracy of 49.04%,\nunderscoring substantial room for improvement. Further analysis reveals\nvariations in tool-use strategies for various families, offering actionable\ninsights to guide the development of more effective approaches. Code and data\ncan be found in https://huggingface.co/datasets/bytedance-research/ToolHop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective evaluation of multi-hop tool use is critical for analyzing the\nunderstanding, reasoning, and function-calling capabilities of large language\nmodels (LLMs). However, progress has been hindered by a lack of reliable\nevaluation datasets. To address this, we present ToolHop, a dataset comprising\n995 user queries and 3,912 associated tools, specifically designed for rigorous\nevaluation of multi-hop tool use. ToolHop ensures diverse queries, meaningful\ninterdependencies, locally executable tools, detailed feedback, and verifiable\nanswers through a novel query-driven data construction approach that includes\ntool creation, document refinement, and code generation. We evaluate 14 LLMs\nacross five model families (i.e., LLaMA3.1, Qwen2.5, Gemini1.5, Claude3.5, and\nGPT), uncovering significant challenges in handling multi-hop tool-use\nscenarios. The leading model, GPT-4o, achieves an accuracy of 49.04%,\nunderscoring substantial room for improvement. Further analysis reveals\nvariations in tool-use strategies for various families, offering actionable\ninsights to guide the development of more effective approaches. Code and data\ncan be found in https://huggingface.co/datasets/bytedance-research/ToolHop."
                },
                "authors": [
                    {
                        "name": "Junjie Ye"
                    },
                    {
                        "name": "Zhengyin Du"
                    },
                    {
                        "name": "Xuesong Yao"
                    },
                    {
                        "name": "Weijian Lin"
                    },
                    {
                        "name": "Yufei Xu"
                    },
                    {
                        "name": "Zehui Chen"
                    },
                    {
                        "name": "Zaiyuan Wang"
                    },
                    {
                        "name": "Sining Zhu"
                    },
                    {
                        "name": "Zhiheng Xi"
                    },
                    {
                        "name": "Siyu Yuan"
                    },
                    {
                        "name": "Tao Gui"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Jiecao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jiecao Chen"
                },
                "author": "Jiecao Chen",
                "arxiv_comment": "Accepted by ACL 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02506v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02506v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10849v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10849v2",
                "updated": "2025-05-19T14:26:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    26,
                    7,
                    0,
                    139,
                    0
                ],
                "published": "2024-12-14T14:46:18Z",
                "published_parsed": [
                    2024,
                    12,
                    14,
                    14,
                    46,
                    18,
                    5,
                    349,
                    0
                ],
                "title": "Superhuman performance of a large language model on the reasoning tasks\n  of a physician",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Superhuman performance of a large language model on the reasoning tasks\n  of a physician"
                },
                "summary": "A seminal paper published by Ledley and Lusted in 1959 introduced complex\nclinical diagnostic reasoning cases as the gold standard for the evaluation of\nexpert medical computing systems, a standard that has held ever since. Here, we\nreport the results of a physician evaluation of a large language model (LLM) on\nchallenging clinical cases against a baseline of hundreds of physicians. We\nconduct five experiments to measure clinical reasoning across differential\ndiagnosis generation, display of diagnostic reasoning, triage differential\ndiagnosis, probabilistic reasoning, and management reasoning, all adjudicated\nby physician experts with validated psychometrics. We then report a real-world\nstudy comparing human expert and AI second opinions in randomly-selected\npatients in the emergency room of a major tertiary academic medical center in\nBoston, MA. We compared LLMs and board-certified physicians at three predefined\ndiagnostic touchpoints: triage in the emergency room, initial evaluation by a\nphysician, and admission to the hospital or intensive care unit. In all\nexperiments--both vignettes and emergency room second opinions--the LLM\ndisplayed superhuman diagnostic and reasoning abilities, as well as continued\nimprovement from prior generations of AI clinical decision support. Our study\nsuggests that LLMs have achieved superhuman performance on general medical\ndiagnostic and management reasoning, fulfilling the vision put forth by Ledley\nand Lusted, and motivating the urgent need for prospective trials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A seminal paper published by Ledley and Lusted in 1959 introduced complex\nclinical diagnostic reasoning cases as the gold standard for the evaluation of\nexpert medical computing systems, a standard that has held ever since. Here, we\nreport the results of a physician evaluation of a large language model (LLM) on\nchallenging clinical cases against a baseline of hundreds of physicians. We\nconduct five experiments to measure clinical reasoning across differential\ndiagnosis generation, display of diagnostic reasoning, triage differential\ndiagnosis, probabilistic reasoning, and management reasoning, all adjudicated\nby physician experts with validated psychometrics. We then report a real-world\nstudy comparing human expert and AI second opinions in randomly-selected\npatients in the emergency room of a major tertiary academic medical center in\nBoston, MA. We compared LLMs and board-certified physicians at three predefined\ndiagnostic touchpoints: triage in the emergency room, initial evaluation by a\nphysician, and admission to the hospital or intensive care unit. In all\nexperiments--both vignettes and emergency room second opinions--the LLM\ndisplayed superhuman diagnostic and reasoning abilities, as well as continued\nimprovement from prior generations of AI clinical decision support. Our study\nsuggests that LLMs have achieved superhuman performance on general medical\ndiagnostic and management reasoning, fulfilling the vision put forth by Ledley\nand Lusted, and motivating the urgent need for prospective trials."
                },
                "authors": [
                    {
                        "name": "Peter G. Brodeur"
                    },
                    {
                        "name": "Thomas A. Buckley"
                    },
                    {
                        "name": "Zahir Kanjee"
                    },
                    {
                        "name": "Ethan Goh"
                    },
                    {
                        "name": "Evelyn Bin Ling"
                    },
                    {
                        "name": "Priyank Jain"
                    },
                    {
                        "name": "Stephanie Cabral"
                    },
                    {
                        "name": "Raja-Elie Abdulnour"
                    },
                    {
                        "name": "Adrian D. Haimovich"
                    },
                    {
                        "name": "Jason A. Freed"
                    },
                    {
                        "name": "Andrew Olson"
                    },
                    {
                        "name": "Daniel J. Morgan"
                    },
                    {
                        "name": "Jason Hom"
                    },
                    {
                        "name": "Robert Gallo"
                    },
                    {
                        "name": "Liam G. McCoy"
                    },
                    {
                        "name": "Haadi Mombini"
                    },
                    {
                        "name": "Christopher Lucas"
                    },
                    {
                        "name": "Misha Fotoohi"
                    },
                    {
                        "name": "Matthew Gwiazdon"
                    },
                    {
                        "name": "Daniele Restifo"
                    },
                    {
                        "name": "Daniel Restrepo"
                    },
                    {
                        "name": "Eric Horvitz"
                    },
                    {
                        "name": "Jonathan Chen"
                    },
                    {
                        "name": "Arjun K. Manrai"
                    },
                    {
                        "name": "Adam Rodman"
                    }
                ],
                "author_detail": {
                    "name": "Adam Rodman"
                },
                "author": "Adam Rodman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10849v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10849v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13157v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13157v1",
                "updated": "2025-05-19T14:18:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    18,
                    16,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T14:18:16Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    18,
                    16,
                    0,
                    139,
                    0
                ],
                "title": "Role-Playing Evaluation for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Role-Playing Evaluation for Large Language Models"
                },
                "summary": "Large Language Models (LLMs) demonstrate a notable capacity for adopting\npersonas and engaging in role-playing. However, evaluating this ability\npresents significant challenges, as human assessments are resource-intensive\nand automated evaluations can be biased. To address this, we introduce\nRole-Playing Eval (RPEval), a novel benchmark designed to assess LLM\nrole-playing capabilities across four key dimensions: emotional understanding,\ndecision-making, moral alignment, and in-character consistency. This article\ndetails the construction of RPEval and presents baseline evaluations. Our code\nand dataset are available at https://github.com/yelboudouri/RPEval",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate a notable capacity for adopting\npersonas and engaging in role-playing. However, evaluating this ability\npresents significant challenges, as human assessments are resource-intensive\nand automated evaluations can be biased. To address this, we introduce\nRole-Playing Eval (RPEval), a novel benchmark designed to assess LLM\nrole-playing capabilities across four key dimensions: emotional understanding,\ndecision-making, moral alignment, and in-character consistency. This article\ndetails the construction of RPEval and presents baseline evaluations. Our code\nand dataset are available at https://github.com/yelboudouri/RPEval"
                },
                "authors": [
                    {
                        "name": "Yassine El Boudouri"
                    },
                    {
                        "name": "Walter Nuninger"
                    },
                    {
                        "name": "Julian Alvarez"
                    },
                    {
                        "name": "Yvan Peter"
                    }
                ],
                "author_detail": {
                    "name": "Yvan Peter"
                },
                "author": "Yvan Peter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13157v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13157v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13156v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13156v1",
                "updated": "2025-05-19T14:17:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    17,
                    37,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T14:17:37Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    17,
                    37,
                    0,
                    139,
                    0
                ],
                "title": "Tianyi: A Traditional Chinese Medicine all-rounder language model and\n  its Real-World Clinical Practice",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tianyi: A Traditional Chinese Medicine all-rounder language model and\n  its Real-World Clinical Practice"
                },
                "summary": "Natural medicines, particularly Traditional Chinese Medicine (TCM), are\ngaining global recognition for their therapeutic potential in addressing human\nsymptoms and diseases. TCM, with its systematic theories and extensive\npractical experience, provides abundant resources for healthcare. However, the\neffective application of TCM requires precise syndrome diagnosis, determination\nof treatment principles, and prescription formulation, which demand decades of\nclinical expertise. Despite advancements in TCM-based decision systems, machine\nlearning, and deep learning research, limitations in data and single-objective\nconstraints hinder their practical application. In recent years, large language\nmodels (LLMs) have demonstrated potential in complex tasks, but lack\nspecialization in TCM and face significant challenges, such as too big model\nscale to deploy and issues with hallucination. To address these challenges, we\nintroduce Tianyi with 7.6-billion-parameter LLM, a model scale proper and\nspecifically designed for TCM, pre-trained and fine-tuned on diverse TCM\ncorpora, including classical texts, expert treatises, clinical records, and\nknowledge graphs. Tianyi is designed to assimilate interconnected and\nsystematic TCM knowledge through a progressive learning manner. Additionally,\nwe establish TCMEval, a comprehensive evaluation benchmark, to assess LLMs in\nTCM examinations, clinical tasks, domain-specific question-answering, and\nreal-world trials. The extensive evaluations demonstrate the significant\npotential of Tianyi as an AI assistant in TCM clinical practice and research,\nbridging the gap between TCM knowledge and practical application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural medicines, particularly Traditional Chinese Medicine (TCM), are\ngaining global recognition for their therapeutic potential in addressing human\nsymptoms and diseases. TCM, with its systematic theories and extensive\npractical experience, provides abundant resources for healthcare. However, the\neffective application of TCM requires precise syndrome diagnosis, determination\nof treatment principles, and prescription formulation, which demand decades of\nclinical expertise. Despite advancements in TCM-based decision systems, machine\nlearning, and deep learning research, limitations in data and single-objective\nconstraints hinder their practical application. In recent years, large language\nmodels (LLMs) have demonstrated potential in complex tasks, but lack\nspecialization in TCM and face significant challenges, such as too big model\nscale to deploy and issues with hallucination. To address these challenges, we\nintroduce Tianyi with 7.6-billion-parameter LLM, a model scale proper and\nspecifically designed for TCM, pre-trained and fine-tuned on diverse TCM\ncorpora, including classical texts, expert treatises, clinical records, and\nknowledge graphs. Tianyi is designed to assimilate interconnected and\nsystematic TCM knowledge through a progressive learning manner. Additionally,\nwe establish TCMEval, a comprehensive evaluation benchmark, to assess LLMs in\nTCM examinations, clinical tasks, domain-specific question-answering, and\nreal-world trials. The extensive evaluations demonstrate the significant\npotential of Tianyi as an AI assistant in TCM clinical practice and research,\nbridging the gap between TCM knowledge and practical application."
                },
                "authors": [
                    {
                        "name": "Zhi Liu"
                    },
                    {
                        "name": "Tao Yang"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Yexin Chen"
                    },
                    {
                        "name": "Zhan Gao"
                    },
                    {
                        "name": "Jiaxi Yang"
                    },
                    {
                        "name": "Kui Chen"
                    },
                    {
                        "name": "Bingji Lu"
                    },
                    {
                        "name": "Xiaochen Li"
                    },
                    {
                        "name": "Changyong Luo"
                    },
                    {
                        "name": "Yan Li"
                    },
                    {
                        "name": "Xiaohong Gu"
                    },
                    {
                        "name": "Peng Cao"
                    }
                ],
                "author_detail": {
                    "name": "Peng Cao"
                },
                "author": "Peng Cao",
                "arxiv_comment": "23 pages, 4 figures, and 1 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13156v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13156v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11799v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11799v2",
                "updated": "2025-05-19T14:10:55Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    10,
                    55,
                    0,
                    139,
                    0
                ],
                "published": "2025-02-17T13:42:12Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    13,
                    42,
                    12,
                    0,
                    48,
                    0
                ],
                "title": "Table-Critic: A Multi-Agent Framework for Collaborative Criticism and\n  Refinement in Table Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Table-Critic: A Multi-Agent Framework for Collaborative Criticism and\n  Refinement in Table Reasoning"
                },
                "summary": "Despite the remarkable capabilities of large language models (LLMs) in\nvarious reasoning tasks, they still struggle with table reasoning tasks,\nparticularly in maintaining consistency throughout multi-step reasoning\nprocesses. While existing approaches have explored various decomposition\nstrategies, they often lack effective mechanisms to identify and correct errors\nin intermediate reasoning steps, leading to cascading error propagation. To\naddress these issues, we propose Table-Critic, a novel multi-agent framework\nthat facilitates collaborative criticism and iterative refinement of the\nreasoning process until convergence to correct solutions. Our framework\nconsists of four specialized agents: a Judge for error identification, a Critic\nfor comprehensive critiques, a Refiner for process improvement, and a Curator\nfor pattern distillation. To effectively deal with diverse and unpredictable\nerror types, we introduce a self-evolving template tree that systematically\naccumulates critique knowledge through experience-driven learning and guides\nfuture reflections. Extensive experiments have demonstrated that Table-Critic\nachieves substantial improvements over existing methods, achieving superior\naccuracy and error correction rates while maintaining computational efficiency\nand lower solution degradation rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable capabilities of large language models (LLMs) in\nvarious reasoning tasks, they still struggle with table reasoning tasks,\nparticularly in maintaining consistency throughout multi-step reasoning\nprocesses. While existing approaches have explored various decomposition\nstrategies, they often lack effective mechanisms to identify and correct errors\nin intermediate reasoning steps, leading to cascading error propagation. To\naddress these issues, we propose Table-Critic, a novel multi-agent framework\nthat facilitates collaborative criticism and iterative refinement of the\nreasoning process until convergence to correct solutions. Our framework\nconsists of four specialized agents: a Judge for error identification, a Critic\nfor comprehensive critiques, a Refiner for process improvement, and a Curator\nfor pattern distillation. To effectively deal with diverse and unpredictable\nerror types, we introduce a self-evolving template tree that systematically\naccumulates critique knowledge through experience-driven learning and guides\nfuture reflections. Extensive experiments have demonstrated that Table-Critic\nachieves substantial improvements over existing methods, achieving superior\naccuracy and error correction rates while maintaining computational efficiency\nand lower solution degradation rate."
                },
                "authors": [
                    {
                        "name": "Peiying Yu"
                    },
                    {
                        "name": "Guoxin Chen"
                    },
                    {
                        "name": "Jingjing Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jingjing Wang"
                },
                "author": "Jingjing Wang",
                "arxiv_comment": "ACL 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11799v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11799v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13141v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13141v1",
                "updated": "2025-05-19T14:10:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    10,
                    15,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T14:10:15Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    10,
                    15,
                    0,
                    139,
                    0
                ],
                "title": "Understanding Cross-Lingual Inconsistency in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Cross-Lingual Inconsistency in Large Language Models"
                },
                "summary": "Large language models (LLMs) are demonstrably capable of cross-lingual\ntransfer, but can produce inconsistent output when prompted with the same\nqueries written in different languages. To understand how language models are\nable to generalize knowledge from one language to the others, we apply the\nlogit lens to interpret the implicit steps taken by LLMs to solve multilingual\nmulti-choice reasoning questions. We find LLMs predict inconsistently and are\nless accurate because they rely on subspaces of individual languages, rather\nthan working in a shared semantic space. While larger models are more\nmultilingual, we show their hidden states are more likely to dissociate from\nthe shared representation compared to smaller models, but are nevertheless more\ncapable of retrieving knowledge embedded across different languages. Finally,\nwe demonstrate that knowledge sharing can be modulated by steering the models'\nlatent processing towards the shared semantic space. We find reinforcing\nutilization of the shared space improves the models' multilingual reasoning\nperformance, as a result of more knowledge transfer from, and better output\nconsistency with English.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are demonstrably capable of cross-lingual\ntransfer, but can produce inconsistent output when prompted with the same\nqueries written in different languages. To understand how language models are\nable to generalize knowledge from one language to the others, we apply the\nlogit lens to interpret the implicit steps taken by LLMs to solve multilingual\nmulti-choice reasoning questions. We find LLMs predict inconsistently and are\nless accurate because they rely on subspaces of individual languages, rather\nthan working in a shared semantic space. While larger models are more\nmultilingual, we show their hidden states are more likely to dissociate from\nthe shared representation compared to smaller models, but are nevertheless more\ncapable of retrieving knowledge embedded across different languages. Finally,\nwe demonstrate that knowledge sharing can be modulated by steering the models'\nlatent processing towards the shared semantic space. We find reinforcing\nutilization of the shared space improves the models' multilingual reasoning\nperformance, as a result of more knowledge transfer from, and better output\nconsistency with English."
                },
                "authors": [
                    {
                        "name": "Zheng Wei Lim"
                    },
                    {
                        "name": "Alham Fikri Aji"
                    },
                    {
                        "name": "Trevor Cohn"
                    }
                ],
                "author_detail": {
                    "name": "Trevor Cohn"
                },
                "author": "Trevor Cohn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13141v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13141v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14094v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14094v2",
                "updated": "2025-05-19T14:09:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    9,
                    26,
                    0,
                    139,
                    0
                ],
                "published": "2025-04-18T22:21:06Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    22,
                    21,
                    6,
                    4,
                    108,
                    0
                ],
                "title": "Leakage and Interpretability in Concept-Based Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leakage and Interpretability in Concept-Based Models"
                },
                "summary": "Concept Bottleneck Models aim to improve interpretability by predicting\nhigh-level intermediate concepts, representing a promising approach for\ndeployment in high-risk scenarios. However, they are known to suffer from\ninformation leakage, whereby models exploit unintended information encoded\nwithin the learned concepts. We introduce an information-theoretic framework to\nrigorously characterise and quantify leakage, and define two complementary\nmeasures: the concepts-task leakage (CTL) and interconcept leakage (ICL)\nscores. We show that these measures are strongly predictive of model behaviour\nunder interventions and outperform existing alternatives in robustness and\nreliability. Using this framework, we identify the primary causes of leakage\nand provide strong evidence that Concept Embedding Models exhibit substantial\nleakage regardless of the hyperparameters choice. Finally, we propose practical\nguidelines for designing concept-based models to reduce leakage and ensure\ninterpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concept Bottleneck Models aim to improve interpretability by predicting\nhigh-level intermediate concepts, representing a promising approach for\ndeployment in high-risk scenarios. However, they are known to suffer from\ninformation leakage, whereby models exploit unintended information encoded\nwithin the learned concepts. We introduce an information-theoretic framework to\nrigorously characterise and quantify leakage, and define two complementary\nmeasures: the concepts-task leakage (CTL) and interconcept leakage (ICL)\nscores. We show that these measures are strongly predictive of model behaviour\nunder interventions and outperform existing alternatives in robustness and\nreliability. Using this framework, we identify the primary causes of leakage\nand provide strong evidence that Concept Embedding Models exhibit substantial\nleakage regardless of the hyperparameters choice. Finally, we propose practical\nguidelines for designing concept-based models to reduce leakage and ensure\ninterpretability."
                },
                "authors": [
                    {
                        "name": "Enrico Parisini"
                    },
                    {
                        "name": "Tapabrata Chakraborti"
                    },
                    {
                        "name": "Chris Harbron"
                    },
                    {
                        "name": "Ben D. MacArthur"
                    },
                    {
                        "name": "Christopher R. S. Banerji"
                    }
                ],
                "author_detail": {
                    "name": "Christopher R. S. Banerji"
                },
                "author": "Christopher R. S. Banerji",
                "arxiv_comment": "35 pages, 24 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14094v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14094v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02613v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02613v2",
                "updated": "2025-05-19T14:02:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    2,
                    1,
                    0,
                    139,
                    0
                ],
                "published": "2024-06-03T08:23:45Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    8,
                    23,
                    45,
                    0,
                    155,
                    0
                ],
                "title": "ACCO: Accumulate While You Communicate for Communication-Overlapped\n  Sharded LLM Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACCO: Accumulate While You Communicate for Communication-Overlapped\n  Sharded LLM Training"
                },
                "summary": "Training LLMs relies on distributed implementations using multiple GPUs to\ncompute gradients in parallel with sharded optimizers. However, synchronizing\ngradients in data parallel setups introduces communication overhead that grows\nwith the number of workers, limiting parallelization efficiency. Local\noptimization algorithms reduce communications but incur high memory costs as\nthey prevent optimizer state sharding, hindering scalability. To address this,\nwe propose \\textbf{AC}cumulate while \\textbf{CO}mmunicate (\\acco), a\nmemory-efficient optimization algorithm for distributed LLM training. By\nsynchronizing delayed gradients while computing new ones, \\acco~reduces GPU\nidle time and supports heterogeneous hardware. To mitigate the convergence\nissues caused by delayed updates, we introduce a novel technique ensuring\ntraining dynamics align with standard distributed optimization. Compared to\nZeRO-1, our approach is significantly faster and scales effectively across\nheterogeneous hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training LLMs relies on distributed implementations using multiple GPUs to\ncompute gradients in parallel with sharded optimizers. However, synchronizing\ngradients in data parallel setups introduces communication overhead that grows\nwith the number of workers, limiting parallelization efficiency. Local\noptimization algorithms reduce communications but incur high memory costs as\nthey prevent optimizer state sharding, hindering scalability. To address this,\nwe propose \\textbf{AC}cumulate while \\textbf{CO}mmunicate (\\acco), a\nmemory-efficient optimization algorithm for distributed LLM training. By\nsynchronizing delayed gradients while computing new ones, \\acco~reduces GPU\nidle time and supports heterogeneous hardware. To mitigate the convergence\nissues caused by delayed updates, we introduce a novel technique ensuring\ntraining dynamics align with standard distributed optimization. Compared to\nZeRO-1, our approach is significantly faster and scales effectively across\nheterogeneous hardware."
                },
                "authors": [
                    {
                        "name": "Adel Nabli"
                    },
                    {
                        "name": "Louis Fournier"
                    },
                    {
                        "name": "Pierre Erbacher"
                    },
                    {
                        "name": "Louis Serrano"
                    },
                    {
                        "name": "Eugene Belilovsky"
                    },
                    {
                        "name": "Edouard Oyallon"
                    }
                ],
                "author_detail": {
                    "name": "Edouard Oyallon"
                },
                "arxiv_affiliation": "MLIA",
                "author": "Edouard Oyallon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02613v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02613v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13691v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13691v2",
                "updated": "2025-05-19T14:01:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    1,
                    5,
                    0,
                    139,
                    0
                ],
                "published": "2025-02-19T13:03:06Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    13,
                    3,
                    6,
                    2,
                    50,
                    0
                ],
                "title": "Is This Collection Worth My LLM's Time? Automatically Measuring\n  Information Potential in Text Corpora",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is This Collection Worth My LLM's Time? Automatically Measuring\n  Information Potential in Text Corpora"
                },
                "summary": "As large language models (LLMs) converge towards similar capabilities, the\nkey to advancing their performance lies in identifying and incorporating\nvaluable new information sources. However, evaluating which text collections\nare worth the substantial investment required for digitization, preprocessing,\nand integration into LLM systems remains a significant challenge. We present a\nnovel approach to this challenge: an automated pipeline that evaluates the\npotential information gain from text collections without requiring model\ntraining or fine-tuning. Our method generates multiple choice questions (MCQs)\nfrom texts and measures an LLM's performance both with and without access to\nthe source material. The performance gap between these conditions serves as a\nproxy for the collection's information potential. We validate our approach\nusing five strategically selected datasets: EPFL PhD manuscripts, a private\ncollection of Venetian historical records, two sets of Wikipedia articles on\nrelated topics, and a synthetic baseline dataset. Our results demonstrate that\nthis method effectively identifies collections containing valuable novel\ninformation, providing a practical tool for prioritizing data acquisition and\nintegration efforts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) converge towards similar capabilities, the\nkey to advancing their performance lies in identifying and incorporating\nvaluable new information sources. However, evaluating which text collections\nare worth the substantial investment required for digitization, preprocessing,\nand integration into LLM systems remains a significant challenge. We present a\nnovel approach to this challenge: an automated pipeline that evaluates the\npotential information gain from text collections without requiring model\ntraining or fine-tuning. Our method generates multiple choice questions (MCQs)\nfrom texts and measures an LLM's performance both with and without access to\nthe source material. The performance gap between these conditions serves as a\nproxy for the collection's information potential. We validate our approach\nusing five strategically selected datasets: EPFL PhD manuscripts, a private\ncollection of Venetian historical records, two sets of Wikipedia articles on\nrelated topics, and a synthetic baseline dataset. Our results demonstrate that\nthis method effectively identifies collections containing valuable novel\ninformation, providing a practical tool for prioritizing data acquisition and\nintegration efforts."
                },
                "authors": [
                    {
                        "name": "Tristan Karch"
                    },
                    {
                        "name": "Luca Engel"
                    },
                    {
                        "name": "Philippe Schwaller"
                    },
                    {
                        "name": "Frédéric Kaplan"
                    }
                ],
                "author_detail": {
                    "name": "Frédéric Kaplan"
                },
                "author": "Frédéric Kaplan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13691v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13691v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07610v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07610v2",
                "updated": "2025-05-19T14:00:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    0,
                    52,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-12T14:31:51Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    14,
                    31,
                    51,
                    0,
                    132,
                    0
                ],
                "title": "Concept-Level Explainability for Auditing & Steering LLM Responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concept-Level Explainability for Auditing & Steering LLM Responses"
                },
                "summary": "As large language models (LLMs) become widely deployed, concerns about their\nsafety and alignment grow. An approach to steer LLM behavior, such as\nmitigating biases or defending against jailbreaks, is to identify which parts\nof a prompt influence specific aspects of the model's output. Token-level\nattribution methods offer a promising solution, but still struggle in text\ngeneration, explaining the presence of each token in the output separately,\nrather than the underlying semantics of the entire LLM response. We introduce\nConceptX, a model-agnostic, concept-level explainability method that identifies\nthe concepts, i.e., semantically rich tokens in the prompt, and assigns them\nimportance based on the outputs' semantic similarity. Unlike current\ntoken-level methods, ConceptX also offers to preserve context integrity through\nin-place token replacements and supports flexible explanation goals, e.g.,\ngender bias. ConceptX enables both auditing, by uncovering sources of bias, and\nsteering, by modifying prompts to shift the sentiment or reduce the harmfulness\nof LLM responses, without requiring retraining. Across three LLMs, ConceptX\noutperforms token-level methods like TokenSHAP in both faithfulness and human\nalignment. Steering tasks boost sentiment shift by 0.252 versus 0.131 for\nrandom edits and lower attack success rates from 0.463 to 0.242, outperforming\nattribution and paraphrasing baselines. While prompt engineering and\nself-explaining methods sometimes yield safer responses, ConceptX offers a\ntransparent and faithful alternative for improving LLM safety and alignment,\ndemonstrating the practical value of attribution-based explainability in\nguiding LLM behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become widely deployed, concerns about their\nsafety and alignment grow. An approach to steer LLM behavior, such as\nmitigating biases or defending against jailbreaks, is to identify which parts\nof a prompt influence specific aspects of the model's output. Token-level\nattribution methods offer a promising solution, but still struggle in text\ngeneration, explaining the presence of each token in the output separately,\nrather than the underlying semantics of the entire LLM response. We introduce\nConceptX, a model-agnostic, concept-level explainability method that identifies\nthe concepts, i.e., semantically rich tokens in the prompt, and assigns them\nimportance based on the outputs' semantic similarity. Unlike current\ntoken-level methods, ConceptX also offers to preserve context integrity through\nin-place token replacements and supports flexible explanation goals, e.g.,\ngender bias. ConceptX enables both auditing, by uncovering sources of bias, and\nsteering, by modifying prompts to shift the sentiment or reduce the harmfulness\nof LLM responses, without requiring retraining. Across three LLMs, ConceptX\noutperforms token-level methods like TokenSHAP in both faithfulness and human\nalignment. Steering tasks boost sentiment shift by 0.252 versus 0.131 for\nrandom edits and lower attack success rates from 0.463 to 0.242, outperforming\nattribution and paraphrasing baselines. While prompt engineering and\nself-explaining methods sometimes yield safer responses, ConceptX offers a\ntransparent and faithful alternative for improving LLM safety and alignment,\ndemonstrating the practical value of attribution-based explainability in\nguiding LLM behavior."
                },
                "authors": [
                    {
                        "name": "Kenza Amara"
                    },
                    {
                        "name": "Rita Sevastjanova"
                    },
                    {
                        "name": "Mennatallah El-Assady"
                    }
                ],
                "author_detail": {
                    "name": "Mennatallah El-Assady"
                },
                "author": "Mennatallah El-Assady",
                "arxiv_comment": "9 pages, 7 figures, Submission to Neurips 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07610v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07610v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13131v1",
                "updated": "2025-05-19T14:00:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    0,
                    17,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T14:00:17Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    0,
                    17,
                    0,
                    139,
                    0
                ],
                "title": "Constraint-Aware Diffusion Guidance for Robotics: Real-Time Obstacle\n  Avoidance for Autonomous Racing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constraint-Aware Diffusion Guidance for Robotics: Real-Time Obstacle\n  Avoidance for Autonomous Racing"
                },
                "summary": "Diffusion models hold great potential in robotics due to their ability to\ncapture complex, high-dimensional data distributions. However, their lack of\nconstraint-awareness limits their deployment in safety-critical applications.\nWe propose Constraint-Aware Diffusion Guidance (CoDiG), a data-efficient and\ngeneral-purpose framework that integrates barrier functions into the denoising\nprocess, guiding diffusion sampling toward constraint-satisfying outputs. CoDiG\nenables constraint satisfaction even with limited training data and generalizes\nacross tasks. We evaluate our framework in the challenging setting of miniature\nautonomous racing, where real-time obstacle avoidance is essential. Real-world\nexperiments show that CoDiG generates safe outputs efficiently under dynamic\nconditions, highlighting its potential for broader robotic applications. A\ndemonstration video is available at https://youtu.be/KNYsTdtdxOU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models hold great potential in robotics due to their ability to\ncapture complex, high-dimensional data distributions. However, their lack of\nconstraint-awareness limits their deployment in safety-critical applications.\nWe propose Constraint-Aware Diffusion Guidance (CoDiG), a data-efficient and\ngeneral-purpose framework that integrates barrier functions into the denoising\nprocess, guiding diffusion sampling toward constraint-satisfying outputs. CoDiG\nenables constraint satisfaction even with limited training data and generalizes\nacross tasks. We evaluate our framework in the challenging setting of miniature\nautonomous racing, where real-time obstacle avoidance is essential. Real-world\nexperiments show that CoDiG generates safe outputs efficiently under dynamic\nconditions, highlighting its potential for broader robotic applications. A\ndemonstration video is available at https://youtu.be/KNYsTdtdxOU."
                },
                "authors": [
                    {
                        "name": "Hao Ma"
                    },
                    {
                        "name": "Sabrina Bodmer"
                    },
                    {
                        "name": "Andrea Carron"
                    },
                    {
                        "name": "Melanie Zeilinger"
                    },
                    {
                        "name": "Michael Muehlebach"
                    }
                ],
                "author_detail": {
                    "name": "Michael Muehlebach"
                },
                "author": "Michael Muehlebach",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13126v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13126v1",
                "updated": "2025-05-19T13:58:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    58,
                    15,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T13:58:15Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    58,
                    15,
                    0,
                    139,
                    0
                ],
                "title": "Zero-Shot Iterative Formalization and Planning in Partially Observable\n  Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot Iterative Formalization and Planning in Partially Observable\n  Environments"
                },
                "summary": "In planning, using LLMs not to predict plans but to formalize an environment\ninto the Planning Domain Definition Language (PDDL) has been shown to greatly\nimprove performance and control. While most work focused on fully observable\nenvironments, we tackle the more realistic and challenging partially observable\nenvironments where existing methods are incapacitated by the lack of complete\ninformation. We propose PDDLego+, a framework to iteratively formalize, plan,\ngrow, and refine PDDL representations in a zero-shot manner, without needing\naccess to any existing trajectories. On two textual simulated environments, we\nshow that PDDLego+ not only achieves superior performance, but also shows\nrobustness against problem complexity. We also show that the domain knowledge\ncaptured after a successful trial is interpretable and benefits future tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In planning, using LLMs not to predict plans but to formalize an environment\ninto the Planning Domain Definition Language (PDDL) has been shown to greatly\nimprove performance and control. While most work focused on fully observable\nenvironments, we tackle the more realistic and challenging partially observable\nenvironments where existing methods are incapacitated by the lack of complete\ninformation. We propose PDDLego+, a framework to iteratively formalize, plan,\ngrow, and refine PDDL representations in a zero-shot manner, without needing\naccess to any existing trajectories. On two textual simulated environments, we\nshow that PDDLego+ not only achieves superior performance, but also shows\nrobustness against problem complexity. We also show that the domain knowledge\ncaptured after a successful trial is interpretable and benefits future tasks."
                },
                "authors": [
                    {
                        "name": "Liancheng Gong"
                    },
                    {
                        "name": "Wang Zhu"
                    },
                    {
                        "name": "Jesse Thomason"
                    },
                    {
                        "name": "Li Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Li Zhang"
                },
                "author": "Li Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13126v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13126v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15796v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15796v6",
                "updated": "2025-05-19T13:54:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    54,
                    52,
                    0,
                    139,
                    0
                ],
                "published": "2024-06-22T09:40:07Z",
                "published_parsed": [
                    2024,
                    6,
                    22,
                    9,
                    40,
                    7,
                    5,
                    174,
                    0
                ],
                "title": "Unveiling Entity-Level Unlearning for Large Language Models: A\n  Comprehensive Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling Entity-Level Unlearning for Large Language Models: A\n  Comprehensive Analysis"
                },
                "summary": "Large language model unlearning has garnered increasing attention due to its\npotential to address security and privacy concerns, leading to extensive\nresearch in the field. However, much of this research has concentrated on\ninstance-level unlearning, specifically targeting the removal of predefined\ninstances containing sensitive content. This focus has left a significant gap\nin the exploration of full entity-level unlearning, which is critical in\nreal-world scenarios such as copyright protection. To this end, we propose a\nnovel task of Entity-level unlearning, which aims to erase entity-related\nknowledge from the target model completely. To thoroughly investigate this\ntask, we systematically evaluate trending unlearning algorithms, revealing that\ncurrent methods struggle to achieve effective entity-level unlearning. Then, we\nfurther explore the factors that influence the performance of the unlearning\nalgorithms, identifying that knowledge coverage and the size of the forget set\nplay pivotal roles. Notably, our analysis also uncovers that entities\nintroduced through fine-tuning are more vulnerable to unlearning than\npre-trained entities. These findings collectively offer valuable insights for\nadvancing entity-level unlearning for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model unlearning has garnered increasing attention due to its\npotential to address security and privacy concerns, leading to extensive\nresearch in the field. However, much of this research has concentrated on\ninstance-level unlearning, specifically targeting the removal of predefined\ninstances containing sensitive content. This focus has left a significant gap\nin the exploration of full entity-level unlearning, which is critical in\nreal-world scenarios such as copyright protection. To this end, we propose a\nnovel task of Entity-level unlearning, which aims to erase entity-related\nknowledge from the target model completely. To thoroughly investigate this\ntask, we systematically evaluate trending unlearning algorithms, revealing that\ncurrent methods struggle to achieve effective entity-level unlearning. Then, we\nfurther explore the factors that influence the performance of the unlearning\nalgorithms, identifying that knowledge coverage and the size of the forget set\nplay pivotal roles. Notably, our analysis also uncovers that entities\nintroduced through fine-tuning are more vulnerable to unlearning than\npre-trained entities. These findings collectively offer valuable insights for\nadvancing entity-level unlearning for LLMs."
                },
                "authors": [
                    {
                        "name": "Weitao Ma"
                    },
                    {
                        "name": "Xiaocheng Feng"
                    },
                    {
                        "name": "Weihong Zhong"
                    },
                    {
                        "name": "Lei Huang"
                    },
                    {
                        "name": "Yangfan Ye"
                    },
                    {
                        "name": "Xiachong Feng"
                    },
                    {
                        "name": "Bing Qin"
                    }
                ],
                "author_detail": {
                    "name": "Bing Qin"
                },
                "author": "Bing Qin",
                "arxiv_comment": "Accepted by COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15796v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15796v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11525v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11525v2",
                "updated": "2025-05-19T13:48:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    48,
                    45,
                    0,
                    139,
                    0
                ],
                "published": "2025-02-17T07:54:50Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    7,
                    54,
                    50,
                    0,
                    48,
                    0
                ],
                "title": "Beyond Single-Task: Robust Multi-Task Length Generalization for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Single-Task: Robust Multi-Task Length Generalization for LLMs"
                },
                "summary": "Length generalization, the ability to solve problems longer than those seen\nduring training, remains a critical challenge for large language models (LLMs).\nPrevious work modifies positional encodings (PEs) and data formats to improve\nlength generalization on specific symbolic tasks such as addition and sorting.\nHowever, these approaches are fundamentally limited to special tasks, often\ndegrading general language performance. Furthermore, they are typically\nevaluated on small transformers trained from scratch on single tasks and can\ncause performance drop when applied during post-training stage of practical\nLLMs with general capabilities. Hu et al., (2024) proposed Rule-Following\nFine-Tuning (RFFT) to improve length generalization in the post-training stage\nof LLMs. Despite its compatibility with practical models and strong\nperformance, RFFT is proposed for single tasks too, requiring re-training for\neach individual task with extensive examples. In this paper, we study length\ngeneralization in multi-task settings and propose Meta Rule-Following\nFine-Tuning (Meta-RFFT), the first framework enabling robust cross-task length\ngeneralization. As our first contribution, we construct a large length\ngeneralization dataset containing 86 tasks spanning code execution, number\nprocessing, symbolic and logical reasoning tasks, beyond the common addition or\nmultiplication tasks. Secondly, we show that cross-task length generalization\nis possible with Meta-RFFT. After training on a large number of tasks and\ninstances, the models achieve remarkable length generalization ability on\nunseen tasks with minimal fine-tuning or one-shot prompting. For example, after\nfine-tuning on 1 to 5 digit addition, our 32B model achieves 95% accuracy on 30\ndigit addition, significantly outperforming the state-of-the-art reasoning\nmodels (DeepSeek-R1-671B: 72%), despite never seeing this task during\nRF-pretraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Length generalization, the ability to solve problems longer than those seen\nduring training, remains a critical challenge for large language models (LLMs).\nPrevious work modifies positional encodings (PEs) and data formats to improve\nlength generalization on specific symbolic tasks such as addition and sorting.\nHowever, these approaches are fundamentally limited to special tasks, often\ndegrading general language performance. Furthermore, they are typically\nevaluated on small transformers trained from scratch on single tasks and can\ncause performance drop when applied during post-training stage of practical\nLLMs with general capabilities. Hu et al., (2024) proposed Rule-Following\nFine-Tuning (RFFT) to improve length generalization in the post-training stage\nof LLMs. Despite its compatibility with practical models and strong\nperformance, RFFT is proposed for single tasks too, requiring re-training for\neach individual task with extensive examples. In this paper, we study length\ngeneralization in multi-task settings and propose Meta Rule-Following\nFine-Tuning (Meta-RFFT), the first framework enabling robust cross-task length\ngeneralization. As our first contribution, we construct a large length\ngeneralization dataset containing 86 tasks spanning code execution, number\nprocessing, symbolic and logical reasoning tasks, beyond the common addition or\nmultiplication tasks. Secondly, we show that cross-task length generalization\nis possible with Meta-RFFT. After training on a large number of tasks and\ninstances, the models achieve remarkable length generalization ability on\nunseen tasks with minimal fine-tuning or one-shot prompting. For example, after\nfine-tuning on 1 to 5 digit addition, our 32B model achieves 95% accuracy on 30\ndigit addition, significantly outperforming the state-of-the-art reasoning\nmodels (DeepSeek-R1-671B: 72%), despite never seeing this task during\nRF-pretraining."
                },
                "authors": [
                    {
                        "name": "Yi Hu"
                    },
                    {
                        "name": "Shijia Kang"
                    },
                    {
                        "name": "Haotong Yang"
                    },
                    {
                        "name": "Haotian Xu"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11525v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11525v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13115v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13115v1",
                "updated": "2025-05-19T13:46:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    46,
                    35,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T13:46:35Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    46,
                    35,
                    0,
                    139,
                    0
                ],
                "title": "Benchmarking and Confidence Evaluation of LALMs For Temporal Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking and Confidence Evaluation of LALMs For Temporal Reasoning"
                },
                "summary": "The popular success of text-based large language models (LLM) has streamlined\nthe attention of the multimodal community to combine other modalities like\nvision and audio along with text to achieve similar multimodal capabilities. In\nthis quest, large audio language models (LALMs) have to be evaluated on\nreasoning related tasks which are different from traditional classification or\ngeneration tasks. Towards this goal, we propose a novel dataset called temporal\nreasoning evaluation of audio (TREA).\n  We benchmark open-source LALMs and observe that they are consistently behind\nhuman capabilities on the tasks in the TREA dataset. While evaluating LALMs, we\nalso propose an uncertainty metric, which computes the invariance of the model\nto semantically identical perturbations of the input. Our analysis shows that\nthe accuracy and uncertainty metrics are not necessarily correlated and thus,\npoints to a need for wholesome evaluation of LALMs for high-stakes\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The popular success of text-based large language models (LLM) has streamlined\nthe attention of the multimodal community to combine other modalities like\nvision and audio along with text to achieve similar multimodal capabilities. In\nthis quest, large audio language models (LALMs) have to be evaluated on\nreasoning related tasks which are different from traditional classification or\ngeneration tasks. Towards this goal, we propose a novel dataset called temporal\nreasoning evaluation of audio (TREA).\n  We benchmark open-source LALMs and observe that they are consistently behind\nhuman capabilities on the tasks in the TREA dataset. While evaluating LALMs, we\nalso propose an uncertainty metric, which computes the invariance of the model\nto semantically identical perturbations of the input. Our analysis shows that\nthe accuracy and uncertainty metrics are not necessarily correlated and thus,\npoints to a need for wholesome evaluation of LALMs for high-stakes\napplications."
                },
                "authors": [
                    {
                        "name": "Debarpan Bhattacharya"
                    },
                    {
                        "name": "Apoorva Kulkarni"
                    },
                    {
                        "name": "Sriram Ganapathy"
                    }
                ],
                "author_detail": {
                    "name": "Sriram Ganapathy"
                },
                "author": "Sriram Ganapathy",
                "arxiv_comment": "Accepted in INTERSPEECH, 2025, Rotterdam, The Netherlands",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13115v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13115v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03008v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03008v2",
                "updated": "2025-05-19T13:39:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    39,
                    47,
                    0,
                    139,
                    0
                ],
                "published": "2025-03-04T21:08:17Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    21,
                    8,
                    17,
                    1,
                    63,
                    0
                ],
                "title": "MoSE: Hierarchical Self-Distillation Enhances Early Layer Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoSE: Hierarchical Self-Distillation Enhances Early Layer Embeddings"
                },
                "summary": "Deploying language models often requires navigating accuracy vs. performance\ntrade-offs to meet latency constraints while preserving utility. Traditional\nmodel distillation reduces size but incurs substantial costs through training\nseparate models. We introduce ModularStarEncoder (MoSE), a 1-billion-parameter\nmulti-exit encoder for code retrieval and classification that employs a novel\nSelf-Distillation mechanism. This approach significantly enhances lower-layer\nrepresentations, enabling flexible deployment of different model portions with\nfavorable performance trade-offs. Our architecture improves text-to-code and\ncode-to-code search by targeting specific encoder layers as exit heads, where\nhigher layers guide earlier ones during training-improving intermediate\nrepresentations at minimal additional cost. We further enhance MoSE with a\nrepository-level contextual loss that maximizes training context window\nutilization. Additionally, we release a new dataset created through code\ntranslation that extends text-to-code benchmarks with cross-language\ncode-to-code pairs. Evaluations demonstrate the effectiveness of\nSelf-Distillation as a principled approach to trading inference cost for\naccuracy across various code understanding tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying language models often requires navigating accuracy vs. performance\ntrade-offs to meet latency constraints while preserving utility. Traditional\nmodel distillation reduces size but incurs substantial costs through training\nseparate models. We introduce ModularStarEncoder (MoSE), a 1-billion-parameter\nmulti-exit encoder for code retrieval and classification that employs a novel\nSelf-Distillation mechanism. This approach significantly enhances lower-layer\nrepresentations, enabling flexible deployment of different model portions with\nfavorable performance trade-offs. Our architecture improves text-to-code and\ncode-to-code search by targeting specific encoder layers as exit heads, where\nhigher layers guide earlier ones during training-improving intermediate\nrepresentations at minimal additional cost. We further enhance MoSE with a\nrepository-level contextual loss that maximizes training context window\nutilization. Additionally, we release a new dataset created through code\ntranslation that extends text-to-code benchmarks with cross-language\ncode-to-code pairs. Evaluations demonstrate the effectiveness of\nSelf-Distillation as a principled approach to trading inference cost for\naccuracy across various code understanding tasks."
                },
                "authors": [
                    {
                        "name": "Andrea Gurioli"
                    },
                    {
                        "name": "Federico Pennino"
                    },
                    {
                        "name": "João Monteiro"
                    },
                    {
                        "name": "Maurizio Gabbrielli"
                    }
                ],
                "author_detail": {
                    "name": "Maurizio Gabbrielli"
                },
                "author": "Maurizio Gabbrielli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03008v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03008v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13111v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13111v1",
                "updated": "2025-05-19T13:39:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    39,
                    47,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T13:39:47Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    39,
                    47,
                    0,
                    139,
                    0
                ],
                "title": "Why Knowledge Distillation Works in Generative Models: A Minimal Working\n  Explanation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why Knowledge Distillation Works in Generative Models: A Minimal Working\n  Explanation"
                },
                "summary": "Knowledge distillation (KD) is a core component in the training and\ndeployment of modern generative models, particularly large language models\n(LLMs). While its empirical benefits are well documented--enabling smaller\nstudent models to emulate the performance of much larger teachers--the\nunderlying mechanisms by which KD improves generative quality remain poorly\nunderstood. In this work, we present a minimal working explanation of KD in\ngenerative modeling. Using a controlled simulation with mixtures of Gaussians,\nwe demonstrate that distillation induces a trade-off between precision and\nrecall in the student model. As the teacher distribution becomes more\nselective, the student concentrates more probability mass on high-likelihood\nregions at the expense of coverage--a behavior modulated by a single\nentropy-controlling parameter. We then validate this effect in a large-scale\nlanguage modeling setup using the SmolLM2 family of models. Empirical results\nreveal the same precision-recall dynamics observed in simulation, where\nprecision corresponds to sample quality and recall to distributional coverage.\nThis precision-recall trade-off proves especially beneficial in scenarios where\nsample quality outweighs diversity, such as instruction tuning or downstream\ngeneration. Our analysis provides a simple and general explanation for the\neffectiveness of KD in generative modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge distillation (KD) is a core component in the training and\ndeployment of modern generative models, particularly large language models\n(LLMs). While its empirical benefits are well documented--enabling smaller\nstudent models to emulate the performance of much larger teachers--the\nunderlying mechanisms by which KD improves generative quality remain poorly\nunderstood. In this work, we present a minimal working explanation of KD in\ngenerative modeling. Using a controlled simulation with mixtures of Gaussians,\nwe demonstrate that distillation induces a trade-off between precision and\nrecall in the student model. As the teacher distribution becomes more\nselective, the student concentrates more probability mass on high-likelihood\nregions at the expense of coverage--a behavior modulated by a single\nentropy-controlling parameter. We then validate this effect in a large-scale\nlanguage modeling setup using the SmolLM2 family of models. Empirical results\nreveal the same precision-recall dynamics observed in simulation, where\nprecision corresponds to sample quality and recall to distributional coverage.\nThis precision-recall trade-off proves especially beneficial in scenarios where\nsample quality outweighs diversity, such as instruction tuning or downstream\ngeneration. Our analysis provides a simple and general explanation for the\neffectiveness of KD in generative modeling."
                },
                "authors": [
                    {
                        "name": "Sungmin Cha"
                    },
                    {
                        "name": "Kyunghyun Cho"
                    }
                ],
                "author_detail": {
                    "name": "Kyunghyun Cho"
                },
                "author": "Kyunghyun Cho",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13111v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13111v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13109v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13109v1",
                "updated": "2025-05-19T13:36:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    36,
                    45,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T13:36:45Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    36,
                    45,
                    0,
                    139,
                    0
                ],
                "title": "FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference"
                },
                "summary": "Large language models (LLMs) have been widely deployed with rapidly expanding\ncontext windows to support increasingly demanding applications. However, long\ncontexts pose significant deployment challenges, primarily due to the KV cache\nwhose size grows proportionally with context length. While KV cache compression\nmethods are proposed to address this issue, KV dropping methods incur\nconsiderable accuracy loss, and KV retrieval methods suffer from significant\nefficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization\nframework to enhance KV retrieval efficiency while preserving accuracy. On the\nalgorithm side, FreeKV introduces speculative retrieval to shift the KV\nselection and recall processes out of the critical path, combined with\nfine-grained correction to ensure accuracy. On the system side, FreeKV employs\nhybrid KV layouts across CPU and GPU memory to eliminate fragmented data\ntransfers, and leverages double-buffered streamed recall to further improve\nefficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy\nacross various scenarios and models, delivering up to 13$\\times$ speedup\ncompared to SOTA KV retrieval methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been widely deployed with rapidly expanding\ncontext windows to support increasingly demanding applications. However, long\ncontexts pose significant deployment challenges, primarily due to the KV cache\nwhose size grows proportionally with context length. While KV cache compression\nmethods are proposed to address this issue, KV dropping methods incur\nconsiderable accuracy loss, and KV retrieval methods suffer from significant\nefficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization\nframework to enhance KV retrieval efficiency while preserving accuracy. On the\nalgorithm side, FreeKV introduces speculative retrieval to shift the KV\nselection and recall processes out of the critical path, combined with\nfine-grained correction to ensure accuracy. On the system side, FreeKV employs\nhybrid KV layouts across CPU and GPU memory to eliminate fragmented data\ntransfers, and leverages double-buffered streamed recall to further improve\nefficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy\nacross various scenarios and models, delivering up to 13$\\times$ speedup\ncompared to SOTA KV retrieval methods."
                },
                "authors": [
                    {
                        "name": "Guangda Liu"
                    },
                    {
                        "name": "Chengwei Li"
                    },
                    {
                        "name": "Zhenyu Ning"
                    },
                    {
                        "name": "Jing Lin"
                    },
                    {
                        "name": "Yiwu Yao"
                    },
                    {
                        "name": "Danning Ke"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Jieru Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jieru Zhao"
                },
                "author": "Jieru Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13109v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13109v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13103v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13103v1",
                "updated": "2025-05-19T13:32:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    32,
                    51,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T13:32:51Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    32,
                    51,
                    0,
                    139,
                    0
                ],
                "title": "Fixing 7,400 Bugs for 1$: Cheap Crash-Site Program Repair",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fixing 7,400 Bugs for 1$: Cheap Crash-Site Program Repair"
                },
                "summary": "The rapid advancement of bug-finding techniques has led to the discovery of\nmore vulnerabilities than developers can reasonably fix, creating an urgent\nneed for effective Automated Program Repair (APR) methods. However, the\ncomplexity of modern bugs often makes precise root cause analysis difficult and\nunreliable. To address this challenge, we propose crash-site repair to simplify\nthe repair task while still mitigating the risk of exploitation. In addition,\nwe introduce a template-guided patch generation approach that significantly\nreduces the token cost of Large Language Models (LLMs) while maintaining both\nefficiency and effectiveness.\n  We implement our prototype system, WILLIAMT, and evaluate it against\nstate-of-the-art APR tools. Our results show that, when combined with the\ntop-performing agent CodeRover-S, WILLIAMT reduces token cost by 45.9% and\nincreases the bug-fixing rate to 73.5% (+29.6%) on ARVO, a ground-truth open\nsource software vulnerabilities benchmark. Furthermore, we demonstrate that\nWILLIAMT can function effectively even without access to frontier LLMs: even a\nlocal model running on a Mac M4 Mini achieves a reasonable repair rate. These\nfindings highlight the broad applicability and scalability of WILLIAMT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of bug-finding techniques has led to the discovery of\nmore vulnerabilities than developers can reasonably fix, creating an urgent\nneed for effective Automated Program Repair (APR) methods. However, the\ncomplexity of modern bugs often makes precise root cause analysis difficult and\nunreliable. To address this challenge, we propose crash-site repair to simplify\nthe repair task while still mitigating the risk of exploitation. In addition,\nwe introduce a template-guided patch generation approach that significantly\nreduces the token cost of Large Language Models (LLMs) while maintaining both\nefficiency and effectiveness.\n  We implement our prototype system, WILLIAMT, and evaluate it against\nstate-of-the-art APR tools. Our results show that, when combined with the\ntop-performing agent CodeRover-S, WILLIAMT reduces token cost by 45.9% and\nincreases the bug-fixing rate to 73.5% (+29.6%) on ARVO, a ground-truth open\nsource software vulnerabilities benchmark. Furthermore, we demonstrate that\nWILLIAMT can function effectively even without access to frontier LLMs: even a\nlocal model running on a Mac M4 Mini achieves a reasonable repair rate. These\nfindings highlight the broad applicability and scalability of WILLIAMT."
                },
                "authors": [
                    {
                        "name": "Han Zheng"
                    },
                    {
                        "name": "Ilia Shumailov"
                    },
                    {
                        "name": "Tianqi Fan"
                    },
                    {
                        "name": "Aiden Hall"
                    },
                    {
                        "name": "Mathias Payer"
                    }
                ],
                "author_detail": {
                    "name": "Mathias Payer"
                },
                "author": "Mathias Payer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13103v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13103v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13098v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13098v1",
                "updated": "2025-05-19T13:29:27Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    29,
                    27,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T13:29:27Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    29,
                    27,
                    0,
                    139,
                    0
                ],
                "title": "LLM-KG-Bench 3.0: A Compass for SemanticTechnology Capabilities in the\n  Ocean of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-KG-Bench 3.0: A Compass for SemanticTechnology Capabilities in the\n  Ocean of LLMs"
                },
                "summary": "Current Large Language Models (LLMs) can assist developing program code\nbeside many other things, but can they support working with Knowledge Graphs\n(KGs) as well? Which LLM is offering the best capabilities in the field of\nSemantic Web and Knowledge Graph Engineering (KGE)? Is this possible to\ndetermine without checking many answers manually? The LLM-KG-Bench framework in\nVersion 3.0 is designed to answer these questions. It consists of an extensible\nset of tasks for automated evaluation of LLM answers and covers different\naspects of working with semantic technologies. In this paper the LLM-KG-Bench\nframework is presented in Version 3 along with a dataset of prompts, answers\nand evaluations generated with it and several state-of-the-art LLMs.\nSignificant enhancements have been made to the framework since its initial\nrelease, including an updated task API that offers greater flexibility in\nhandling evaluation tasks, revised tasks, and extended support for various open\nmodels through the vllm library, among other improvements. A comprehensive\ndataset has been generated using more than 30 contemporary open and proprietary\nLLMs, enabling the creation of exemplary model cards that demonstrate the\nmodels' capabilities in working with RDF and SPARQL, as well as comparing their\nperformance on Turtle and JSON-LD RDF serialization tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current Large Language Models (LLMs) can assist developing program code\nbeside many other things, but can they support working with Knowledge Graphs\n(KGs) as well? Which LLM is offering the best capabilities in the field of\nSemantic Web and Knowledge Graph Engineering (KGE)? Is this possible to\ndetermine without checking many answers manually? The LLM-KG-Bench framework in\nVersion 3.0 is designed to answer these questions. It consists of an extensible\nset of tasks for automated evaluation of LLM answers and covers different\naspects of working with semantic technologies. In this paper the LLM-KG-Bench\nframework is presented in Version 3 along with a dataset of prompts, answers\nand evaluations generated with it and several state-of-the-art LLMs.\nSignificant enhancements have been made to the framework since its initial\nrelease, including an updated task API that offers greater flexibility in\nhandling evaluation tasks, revised tasks, and extended support for various open\nmodels through the vllm library, among other improvements. A comprehensive\ndataset has been generated using more than 30 contemporary open and proprietary\nLLMs, enabling the creation of exemplary model cards that demonstrate the\nmodels' capabilities in working with RDF and SPARQL, as well as comparing their\nperformance on Turtle and JSON-LD RDF serialization tasks."
                },
                "authors": [
                    {
                        "name": "Lars-Peter Meyer"
                    },
                    {
                        "name": "Johannes Frey"
                    },
                    {
                        "name": "Desiree Heim"
                    },
                    {
                        "name": "Felix Brei"
                    },
                    {
                        "name": "Claus Stadler"
                    },
                    {
                        "name": "Kurt Junghanns"
                    },
                    {
                        "name": "Michael Martin"
                    }
                ],
                "author_detail": {
                    "name": "Michael Martin"
                },
                "author": "Michael Martin",
                "arxiv_comment": "Peer reviewed publication at ESWC 2025 Resources Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13098v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13098v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13090v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13090v1",
                "updated": "2025-05-19T13:24:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    24,
                    1,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T13:24:01Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    24,
                    1,
                    0,
                    139,
                    0
                ],
                "title": "The Effect of Language Diversity When Fine-Tuning Large Language Models\n  for Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Effect of Language Diversity When Fine-Tuning Large Language Models\n  for Translation"
                },
                "summary": "Prior research diverges on language diversity in LLM fine-tuning: Some\nstudies report benefits while others find no advantages. Through controlled\nfine-tuning experiments across 132 translation directions, we systematically\nresolve these disparities. We find that expanding language diversity during\nfine-tuning improves translation quality for both unsupervised and --\nsurprisingly -- supervised pairs, despite less diverse models being fine-tuned\nexclusively on these supervised pairs. However, benefits plateau or decrease\nbeyond a certain diversity threshold. We show that increased language diversity\ncreates more language-agnostic representations. These representational\nadaptations help explain the improved performance in models fine-tuned with\ngreater diversity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prior research diverges on language diversity in LLM fine-tuning: Some\nstudies report benefits while others find no advantages. Through controlled\nfine-tuning experiments across 132 translation directions, we systematically\nresolve these disparities. We find that expanding language diversity during\nfine-tuning improves translation quality for both unsupervised and --\nsurprisingly -- supervised pairs, despite less diverse models being fine-tuned\nexclusively on these supervised pairs. However, benefits plateau or decrease\nbeyond a certain diversity threshold. We show that increased language diversity\ncreates more language-agnostic representations. These representational\nadaptations help explain the improved performance in models fine-tuned with\ngreater diversity."
                },
                "authors": [
                    {
                        "name": "David Stap"
                    },
                    {
                        "name": "Christof Monz"
                    }
                ],
                "author_detail": {
                    "name": "Christof Monz"
                },
                "author": "Christof Monz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13090v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13085v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13085v2",
                "updated": "2025-05-20T10:22:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    10,
                    22,
                    17,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-19T13:19:49Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    19,
                    49,
                    0,
                    139,
                    0
                ],
                "title": "Universal Semantic Disentangled Privacy-preserving Speech Representation\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Universal Semantic Disentangled Privacy-preserving Speech Representation\n  Learning"
                },
                "summary": "The use of audio recordings of human speech to train LLMs poses privacy\nconcerns due to these models' potential to generate outputs that closely\nresemble artifacts in the training data. In this study, we propose a speaker\nprivacy-preserving representation learning method through the Universal Speech\nCodec (USC), a computationally efficient encoder-decoder model that\ndisentangles speech into: (i) privacy-preserving semantically rich\nrepresentations, capturing content and speech paralinguistics, and (ii)\nresidual acoustic and speaker representations that enables high-fidelity\nreconstruction. Extensive evaluations presented show that USC's semantic\nrepresentation preserves content, prosody, and sentiment, while removing\npotentially identifiable speaker attributes. Combining both representations,\nUSC achieves state-of-the-art speech reconstruction. Additionally, we introduce\nan evaluation methodology for measuring privacy-preserving properties, aligning\nwith perceptual tests. We compare USC against other codecs in the literature\nand demonstrate its effectiveness on privacy-preserving representation\nlearning, illustrating the trade-offs of speaker anonymization, paralinguistics\nretention and content preservation in the learned semantic representations.\nAudio samples are shared in https://www.amazon.science/usc-samples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of audio recordings of human speech to train LLMs poses privacy\nconcerns due to these models' potential to generate outputs that closely\nresemble artifacts in the training data. In this study, we propose a speaker\nprivacy-preserving representation learning method through the Universal Speech\nCodec (USC), a computationally efficient encoder-decoder model that\ndisentangles speech into: (i) privacy-preserving semantically rich\nrepresentations, capturing content and speech paralinguistics, and (ii)\nresidual acoustic and speaker representations that enables high-fidelity\nreconstruction. Extensive evaluations presented show that USC's semantic\nrepresentation preserves content, prosody, and sentiment, while removing\npotentially identifiable speaker attributes. Combining both representations,\nUSC achieves state-of-the-art speech reconstruction. Additionally, we introduce\nan evaluation methodology for measuring privacy-preserving properties, aligning\nwith perceptual tests. We compare USC against other codecs in the literature\nand demonstrate its effectiveness on privacy-preserving representation\nlearning, illustrating the trade-offs of speaker anonymization, paralinguistics\nretention and content preservation in the learned semantic representations.\nAudio samples are shared in https://www.amazon.science/usc-samples."
                },
                "authors": [
                    {
                        "name": "Biel Tura Vecino"
                    },
                    {
                        "name": "Subhadeep Maji"
                    },
                    {
                        "name": "Aravind Varier"
                    },
                    {
                        "name": "Antonio Bonafonte"
                    },
                    {
                        "name": "Ivan Valles"
                    },
                    {
                        "name": "Michael Owen"
                    },
                    {
                        "name": "Leif Rädel"
                    },
                    {
                        "name": "Grant Strimel"
                    },
                    {
                        "name": "Seyi Feyisetan"
                    },
                    {
                        "name": "Roberto Barra Chicote"
                    },
                    {
                        "name": "Ariya Rastrow"
                    },
                    {
                        "name": "Constantinos Papayiannis"
                    },
                    {
                        "name": "Volker Leutnant"
                    },
                    {
                        "name": "Trevor Wood"
                    }
                ],
                "author_detail": {
                    "name": "Trevor Wood"
                },
                "author": "Trevor Wood",
                "arxiv_comment": "Extended report of the article accepted at Interspeech 2025 (v1)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13085v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13085v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13082v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13082v1",
                "updated": "2025-05-19T13:13:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    13,
                    46,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T13:13:46Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    13,
                    46,
                    0,
                    139,
                    0
                ],
                "title": "MultiActor-Audiobook: Zero-Shot Audiobook Generation with Faces and\n  Voices of Multiple Speakers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MultiActor-Audiobook: Zero-Shot Audiobook Generation with Faces and\n  Voices of Multiple Speakers"
                },
                "summary": "We introduce MultiActor-Audiobook, a zero-shot approach for generating\naudiobooks that automatically produces consistent, expressive, and\nspeaker-appropriate prosody, including intonation and emotion. Previous\naudiobook systems have several limitations: they require users to manually\nconfigure the speaker's prosody, read each sentence with a monotonic tone\ncompared to voice actors, or rely on costly training. However, our\nMultiActor-Audiobook addresses these issues by introducing two novel processes:\n(1) MSP (**Multimodal Speaker Persona Generation**) and (2) LSI (**LLM-based\nScript Instruction Generation**). With these two processes,\nMultiActor-Audiobook can generate more emotionally expressive audiobooks with a\nconsistent speaker prosody without additional training. We compare our system\nwith commercial products, through human and MLLM evaluations, achieving\ncompetitive results. Furthermore, we demonstrate the effectiveness of MSP and\nLSI through ablation studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce MultiActor-Audiobook, a zero-shot approach for generating\naudiobooks that automatically produces consistent, expressive, and\nspeaker-appropriate prosody, including intonation and emotion. Previous\naudiobook systems have several limitations: they require users to manually\nconfigure the speaker's prosody, read each sentence with a monotonic tone\ncompared to voice actors, or rely on costly training. However, our\nMultiActor-Audiobook addresses these issues by introducing two novel processes:\n(1) MSP (**Multimodal Speaker Persona Generation**) and (2) LSI (**LLM-based\nScript Instruction Generation**). With these two processes,\nMultiActor-Audiobook can generate more emotionally expressive audiobooks with a\nconsistent speaker prosody without additional training. We compare our system\nwith commercial products, through human and MLLM evaluations, achieving\ncompetitive results. Furthermore, we demonstrate the effectiveness of MSP and\nLSI through ablation studies."
                },
                "authors": [
                    {
                        "name": "Kyeongman Park"
                    },
                    {
                        "name": "Seongho Joo"
                    },
                    {
                        "name": "Kyomin Jung"
                    }
                ],
                "author_detail": {
                    "name": "Kyomin Jung"
                },
                "author": "Kyomin Jung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13082v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13082v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13081v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13081v1",
                "updated": "2025-05-19T13:13:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    13,
                    38,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T13:13:38Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    13,
                    38,
                    0,
                    139,
                    0
                ],
                "title": "Walking the Tightrope: Disentangling Beneficial and Detrimental Drifts\n  in Non-Stationary Custom-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Walking the Tightrope: Disentangling Beneficial and Detrimental Drifts\n  in Non-Stationary Custom-Tuning"
                },
                "summary": "This paper uncovers a critical yet overlooked phenomenon in multi-modal large\nlanguage models (MLLMs): detrimental concept drift within chain-of-thought\n(CoT) reasoning during non-stationary reinforcement fine-tuning (RFT), where\nreasoning token distributions evolve unpredictably, thereby introducing\nsignificant biases in final predictions. To address this, we are pioneers in\nestablishing the theoretical bridge between concept drift theory and RFT\nprocesses by formalizing CoT's autoregressive token streams as non-stationary\ndistributions undergoing arbitrary temporal shifts. Leveraging this framework,\nwe propose a novel counterfact-aware RFT that systematically decouples\nbeneficial distribution adaptation from harmful concept drift through concept\ngraph-empowered LLM experts generating counterfactual reasoning trajectories.\nOur solution, Counterfactual Preference Optimization (CPO), enables stable RFT\nin non-stationary environments, particularly within the medical domain, through\ncustom-tuning of counterfactual-aware preference alignment. Extensive\nexperiments demonstrate our superior performance of robustness, generalization\nand coordination within RFT. Besides, we also contributed a large-scale dataset\nCXR-CounterFact (CCF), comprising 320,416 meticulously curated counterfactual\nreasoning trajectories derived from MIMIC-CXR. Our code and data are public.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper uncovers a critical yet overlooked phenomenon in multi-modal large\nlanguage models (MLLMs): detrimental concept drift within chain-of-thought\n(CoT) reasoning during non-stationary reinforcement fine-tuning (RFT), where\nreasoning token distributions evolve unpredictably, thereby introducing\nsignificant biases in final predictions. To address this, we are pioneers in\nestablishing the theoretical bridge between concept drift theory and RFT\nprocesses by formalizing CoT's autoregressive token streams as non-stationary\ndistributions undergoing arbitrary temporal shifts. Leveraging this framework,\nwe propose a novel counterfact-aware RFT that systematically decouples\nbeneficial distribution adaptation from harmful concept drift through concept\ngraph-empowered LLM experts generating counterfactual reasoning trajectories.\nOur solution, Counterfactual Preference Optimization (CPO), enables stable RFT\nin non-stationary environments, particularly within the medical domain, through\ncustom-tuning of counterfactual-aware preference alignment. Extensive\nexperiments demonstrate our superior performance of robustness, generalization\nand coordination within RFT. Besides, we also contributed a large-scale dataset\nCXR-CounterFact (CCF), comprising 320,416 meticulously curated counterfactual\nreasoning trajectories derived from MIMIC-CXR. Our code and data are public."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Yang"
                    },
                    {
                        "name": "Jie Lu"
                    },
                    {
                        "name": "En Yu"
                    }
                ],
                "author_detail": {
                    "name": "En Yu"
                },
                "author": "En Yu",
                "arxiv_comment": "17 pages, 5figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13081v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13081v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21582v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21582v2",
                "updated": "2025-05-19T13:12:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    12,
                    36,
                    0,
                    139,
                    0
                ],
                "published": "2025-04-30T12:41:51Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    12,
                    41,
                    51,
                    2,
                    120,
                    0
                ],
                "title": "MF-LLM: Simulating Population Decision Dynamics via a Mean-Field Large\n  Language Model Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MF-LLM: Simulating Population Decision Dynamics via a Mean-Field Large\n  Language Model Framework"
                },
                "summary": "Simulating collective decision-making involves more than aggregating\nindividual behaviors; it emerges from dynamic interactions among individuals.\nWhile large language models (LLMs) offer strong potential for social\nsimulation, achieving quantitative alignment with real-world data remains a key\nchallenge. To bridge this gap, we propose the Mean-Field LLM (MF-LLM)\nframework, the first to incorporate mean field theory into LLM-based social\nsimulation. MF-LLM models bidirectional interactions between individuals and\nthe population through an iterative process, generating population signals to\nguide individual decisions, which in turn update the signals. This interplay\nproduces coherent trajectories of collective behavior. To improve alignment\nwith real-world data, we introduce IB-Tune, a novel fine-tuning method inspired\nby the Information Bottleneck principle, which retains population signals most\npredictive of future actions while filtering redundant history. Evaluated on a\nreal-world social dataset, MF-LLM reduces KL divergence to human population\ndistributions by 47\\% compared to non-mean-field baselines, enabling accurate\ntrend forecasting and effective intervention planning. Generalizing across 7\ndomains and 4 LLM backbones, MF-LLM provides a scalable, high-fidelity\nfoundation for social simulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating collective decision-making involves more than aggregating\nindividual behaviors; it emerges from dynamic interactions among individuals.\nWhile large language models (LLMs) offer strong potential for social\nsimulation, achieving quantitative alignment with real-world data remains a key\nchallenge. To bridge this gap, we propose the Mean-Field LLM (MF-LLM)\nframework, the first to incorporate mean field theory into LLM-based social\nsimulation. MF-LLM models bidirectional interactions between individuals and\nthe population through an iterative process, generating population signals to\nguide individual decisions, which in turn update the signals. This interplay\nproduces coherent trajectories of collective behavior. To improve alignment\nwith real-world data, we introduce IB-Tune, a novel fine-tuning method inspired\nby the Information Bottleneck principle, which retains population signals most\npredictive of future actions while filtering redundant history. Evaluated on a\nreal-world social dataset, MF-LLM reduces KL divergence to human population\ndistributions by 47\\% compared to non-mean-field baselines, enabling accurate\ntrend forecasting and effective intervention planning. Generalizing across 7\ndomains and 4 LLM backbones, MF-LLM provides a scalable, high-fidelity\nfoundation for social simulation."
                },
                "authors": [
                    {
                        "name": "Qirui Mi"
                    },
                    {
                        "name": "Mengyue Yang"
                    },
                    {
                        "name": "Xiangning Yu"
                    },
                    {
                        "name": "Zhiyu Zhao"
                    },
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Bo An"
                    },
                    {
                        "name": "Haifeng Zhang"
                    },
                    {
                        "name": "Xu Chen"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "arxiv_comment": "29 pages, 8 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21582v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21582v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13077v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13077v1",
                "updated": "2025-05-19T13:11:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    11,
                    28,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T13:11:28Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    11,
                    28,
                    0,
                    139,
                    0
                ],
                "title": "Advancing Sequential Numerical Prediction in Autoregressive Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Sequential Numerical Prediction in Autoregressive Models"
                },
                "summary": "Autoregressive models have become the de facto choice for sequence generation\ntasks, but standard approaches treat digits as independent tokens and apply\ncross-entropy loss, overlooking the coherent structure of numerical sequences.\nThis paper introduces Numerical Token Integrity Loss (NTIL) to address this\ngap. NTIL operates at two levels: (1) token-level, where it extends the Earth\nMover's Distance (EMD) to preserve ordinal relationships between numerical\nvalues, and (2) sequence-level, where it penalizes the overall discrepancy\nbetween the predicted and actual sequences. This dual approach improves\nnumerical prediction and integrates effectively with LLMs/MLLMs. Extensive\nexperiments show significant performance improvements with NTIL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive models have become the de facto choice for sequence generation\ntasks, but standard approaches treat digits as independent tokens and apply\ncross-entropy loss, overlooking the coherent structure of numerical sequences.\nThis paper introduces Numerical Token Integrity Loss (NTIL) to address this\ngap. NTIL operates at two levels: (1) token-level, where it extends the Earth\nMover's Distance (EMD) to preserve ordinal relationships between numerical\nvalues, and (2) sequence-level, where it penalizes the overall discrepancy\nbetween the predicted and actual sequences. This dual approach improves\nnumerical prediction and integrates effectively with LLMs/MLLMs. Extensive\nexperiments show significant performance improvements with NTIL."
                },
                "authors": [
                    {
                        "name": "Xiang Fei"
                    },
                    {
                        "name": "Jinghui Lu"
                    },
                    {
                        "name": "Qi Sun"
                    },
                    {
                        "name": "Hao Feng"
                    },
                    {
                        "name": "Yanjie Wang"
                    },
                    {
                        "name": "Wei Shi"
                    },
                    {
                        "name": "An-Lan Wang"
                    },
                    {
                        "name": "Jingqun Tang"
                    },
                    {
                        "name": "Can Huang"
                    }
                ],
                "author_detail": {
                    "name": "Can Huang"
                },
                "author": "Can Huang",
                "arxiv_comment": "Accepted to ACL 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13077v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13077v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13076v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13076v1",
                "updated": "2025-05-19T13:10:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    10,
                    29,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T13:10:29Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    10,
                    29,
                    0,
                    139,
                    0
                ],
                "title": "The Hidden Dangers of Browsing AI Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Hidden Dangers of Browsing AI Agents"
                },
                "summary": "Autonomous browsing agents powered by large language models (LLMs) are\nincreasingly used to automate web-based tasks. However, their reliance on\ndynamic content, tool execution, and user-provided data exposes them to a broad\nattack surface. This paper presents a comprehensive security evaluation of such\nagents, focusing on systemic vulnerabilities across multiple architectural\nlayers. Our work outlines the first end-to-end threat model for browsing agents\nand provides actionable guidance for securing their deployment in real-world\nenvironments. To address discovered threats, we propose a defense in depth\nstrategy incorporating input sanitization, planner executor isolation, formal\nanalyzers, and session safeguards. These measures protect against both initial\naccess and post exploitation attack vectors. Through a white box analysis of a\npopular open source project, Browser Use, we demonstrate how untrusted web\ncontent can hijack agent behavior and lead to critical security breaches. Our\nfindings include prompt injection, domain validation bypass, and credential\nexfiltration, evidenced by a disclosed CVE and a working proof of concept\nexploit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous browsing agents powered by large language models (LLMs) are\nincreasingly used to automate web-based tasks. However, their reliance on\ndynamic content, tool execution, and user-provided data exposes them to a broad\nattack surface. This paper presents a comprehensive security evaluation of such\nagents, focusing on systemic vulnerabilities across multiple architectural\nlayers. Our work outlines the first end-to-end threat model for browsing agents\nand provides actionable guidance for securing their deployment in real-world\nenvironments. To address discovered threats, we propose a defense in depth\nstrategy incorporating input sanitization, planner executor isolation, formal\nanalyzers, and session safeguards. These measures protect against both initial\naccess and post exploitation attack vectors. Through a white box analysis of a\npopular open source project, Browser Use, we demonstrate how untrusted web\ncontent can hijack agent behavior and lead to critical security breaches. Our\nfindings include prompt injection, domain validation bypass, and credential\nexfiltration, evidenced by a disclosed CVE and a working proof of concept\nexploit."
                },
                "authors": [
                    {
                        "name": "Mykyta Mudryi"
                    },
                    {
                        "name": "Markiyan Chaklosh"
                    },
                    {
                        "name": "Grzegorz Wójcik"
                    }
                ],
                "author_detail": {
                    "name": "Grzegorz Wójcik"
                },
                "author": "Grzegorz Wójcik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13076v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13076v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13073v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13073v1",
                "updated": "2025-05-19T13:09:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    9,
                    32,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T13:09:32Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    9,
                    32,
                    0,
                    139,
                    0
                ],
                "title": "Structure-Aware Corpus Construction and User-Perception-Aligned Metrics\n  for Large-Language-Model Code Completion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structure-Aware Corpus Construction and User-Perception-Aligned Metrics\n  for Large-Language-Model Code Completion"
                },
                "summary": "Code completion technology based on large language model has significantly\nimproved the development efficiency of programmers. However, in practical\napplications, there remains a gap between current commonly used code completion\nevaluation metrics and users' actual perception. To address this issue, we\npropose two evaluation metrics for code completion tasks--LCP and ROUGE-LCP,\nfrom the perspective of probabilistic modeling. Furthermore, to tackle the lack\nof effective structural semantic modeling and cross-module dependency\ninformation in LLMs for repository-level code completion scenarios, we propose\na data processing method based on a Structure-Preserving and\nSemantically-Reordered Code Graph (SPSR-Graph). Through theoretical analysis\nand experimental validation, we demonstrate the superiority of the proposed\nevaluation metrics in terms of user perception consistency, as well as the\neffectiveness of the data processing method in enhancing model performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code completion technology based on large language model has significantly\nimproved the development efficiency of programmers. However, in practical\napplications, there remains a gap between current commonly used code completion\nevaluation metrics and users' actual perception. To address this issue, we\npropose two evaluation metrics for code completion tasks--LCP and ROUGE-LCP,\nfrom the perspective of probabilistic modeling. Furthermore, to tackle the lack\nof effective structural semantic modeling and cross-module dependency\ninformation in LLMs for repository-level code completion scenarios, we propose\na data processing method based on a Structure-Preserving and\nSemantically-Reordered Code Graph (SPSR-Graph). Through theoretical analysis\nand experimental validation, we demonstrate the superiority of the proposed\nevaluation metrics in terms of user perception consistency, as well as the\neffectiveness of the data processing method in enhancing model performance."
                },
                "authors": [
                    {
                        "name": "Dengfeng Liu"
                    },
                    {
                        "name": "Jucai Zhai"
                    },
                    {
                        "name": "Xiaoguang Jiang"
                    },
                    {
                        "name": "Ziqun Li"
                    },
                    {
                        "name": "Qianjin Yu"
                    },
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Rui Ye"
                    },
                    {
                        "name": "Huang Liu"
                    },
                    {
                        "name": "Zhiguo Yang"
                    },
                    {
                        "name": "Yongsheng Du"
                    },
                    {
                        "name": "Fang Tan"
                    }
                ],
                "author_detail": {
                    "name": "Fang Tan"
                },
                "author": "Fang Tan",
                "arxiv_comment": "14 pages,8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13073v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13073v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.12132v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.12132v2",
                "updated": "2025-05-19T13:05:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    5,
                    53,
                    0,
                    139,
                    0
                ],
                "published": "2023-09-21T14:53:36Z",
                "published_parsed": [
                    2023,
                    9,
                    21,
                    14,
                    53,
                    36,
                    3,
                    264,
                    0
                ],
                "title": "Automating construction contract review using knowledge graph-enhanced\n  large language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating construction contract review using knowledge graph-enhanced\n  large language models"
                },
                "summary": "An effective and efficient review of construction contracts is essential for\nminimizing construction projects losses, but current methods are time-consuming\nand error-prone. Studies using methods based on Natural Language Processing\n(NLP) exist, but their scope is often limited to text classification or\nsegmented label prediction. This paper investigates whether integrating Large\nLanguage Models (LLMs) and Knowledge Graphs (KGs) can enhance the accuracy and\ninterpretability of automated contract risk identification. A tuning-free\napproach is proposed that integrates LLMs with a Nested Contract Knowledge\nGraph (NCKG) using a Graph Retrieval-Augmented Generation (GraphRAG) framework\nfor contract knowledge retrieval and reasoning. Tested on international EPC\ncontracts, the method achieves more accurate risk evaluation and interpretable\nrisk summaries than baseline models. These findings demonstrate the potential\nof combining LLMs and KGs for reliable reasoning in tasks that are\nknowledge-intensive and specialized, such as contract review.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An effective and efficient review of construction contracts is essential for\nminimizing construction projects losses, but current methods are time-consuming\nand error-prone. Studies using methods based on Natural Language Processing\n(NLP) exist, but their scope is often limited to text classification or\nsegmented label prediction. This paper investigates whether integrating Large\nLanguage Models (LLMs) and Knowledge Graphs (KGs) can enhance the accuracy and\ninterpretability of automated contract risk identification. A tuning-free\napproach is proposed that integrates LLMs with a Nested Contract Knowledge\nGraph (NCKG) using a Graph Retrieval-Augmented Generation (GraphRAG) framework\nfor contract knowledge retrieval and reasoning. Tested on international EPC\ncontracts, the method achieves more accurate risk evaluation and interpretable\nrisk summaries than baseline models. These findings demonstrate the potential\nof combining LLMs and KGs for reliable reasoning in tasks that are\nknowledge-intensive and specialized, such as contract review."
                },
                "authors": [
                    {
                        "name": "Chunmo Zheng"
                    },
                    {
                        "name": "Saika Wong"
                    },
                    {
                        "name": "Xing Su"
                    },
                    {
                        "name": "Yinqiu Tang"
                    },
                    {
                        "name": "Ahsan Nawaz"
                    },
                    {
                        "name": "Mohamad Kassem"
                    }
                ],
                "author_detail": {
                    "name": "Mohamad Kassem"
                },
                "author": "Mohamad Kassem",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.12132v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.12132v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13070v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13070v1",
                "updated": "2025-05-19T13:04:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    4,
                    51,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T13:04:51Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    4,
                    51,
                    0,
                    139,
                    0
                ],
                "title": "RSS-Based Localization: Ensuring Consistency and Asymptotic Efficiency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RSS-Based Localization: Ensuring Consistency and Asymptotic Efficiency"
                },
                "summary": "We study the problem of signal source localization using received signal\nstrength measurements. We begin by presenting verifiable geometric conditions\nfor sensor deployment that ensure the model's asymptotic localizability. Then\nwe establish the consistency and asymptotic efficiency of the maximum\nlikelihood (ML) estimator. However, computing the ML estimator is challenging\ndue to its reliance on solving a non-convex optimization problem. To overcome\nthis, we propose a two-step estimator that retains the same asymptotic\nproperties as the ML estimator while offering low computational complexity,\nlinear in the number of measurements. The main challenge lies in obtaining a\nconsistent estimator in the first step. To address this, we construct two\nlinear least-squares estimation problems by applying algebraic transformations\nto the nonlinear measurement model, leading to closed-form solutions. In the\nsecond step, we perform a single Gauss-Newton iteration using the consistent\nestimator from the first step as the initialization, achieving the same\nasymptotic efficiency as the ML estimator. Finally, simulation results validate\nthe theoretical property and practical effectiveness of the proposed two-step\nestimator.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the problem of signal source localization using received signal\nstrength measurements. We begin by presenting verifiable geometric conditions\nfor sensor deployment that ensure the model's asymptotic localizability. Then\nwe establish the consistency and asymptotic efficiency of the maximum\nlikelihood (ML) estimator. However, computing the ML estimator is challenging\ndue to its reliance on solving a non-convex optimization problem. To overcome\nthis, we propose a two-step estimator that retains the same asymptotic\nproperties as the ML estimator while offering low computational complexity,\nlinear in the number of measurements. The main challenge lies in obtaining a\nconsistent estimator in the first step. To address this, we construct two\nlinear least-squares estimation problems by applying algebraic transformations\nto the nonlinear measurement model, leading to closed-form solutions. In the\nsecond step, we perform a single Gauss-Newton iteration using the consistent\nestimator from the first step as the initialization, achieving the same\nasymptotic efficiency as the ML estimator. Finally, simulation results validate\nthe theoretical property and practical effectiveness of the proposed two-step\nestimator."
                },
                "authors": [
                    {
                        "name": "Shenghua Hu"
                    },
                    {
                        "name": "Guangyang Zeng"
                    },
                    {
                        "name": "Wenchao Xue"
                    },
                    {
                        "name": "Haitao Fang"
                    },
                    {
                        "name": "Junfeng Wu"
                    },
                    {
                        "name": "Biqiang Mu"
                    }
                ],
                "author_detail": {
                    "name": "Biqiang Mu"
                },
                "author": "Biqiang Mu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13070v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13070v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]