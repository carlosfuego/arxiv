[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2409.07331v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07331v1",
                "updated": "2024-09-11T15:11:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    11,
                    39,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T15:11:39Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    11,
                    39,
                    2,
                    255,
                    0
                ],
                "title": "Learning to Compress Contexts for Efficient Knowledge-based Visual\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Compress Contexts for Efficient Knowledge-based Visual\n  Question Answering"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have demonstrated great zero-shot\nperformance on visual question answering (VQA). However, when it comes to\nknowledge-based VQA (KB-VQA), MLLMs may lack human commonsense or specialized\ndomain knowledge to answer such questions and require obtaining necessary\ninformation from external knowledge sources. Previous works like\nRetrival-Augmented VQA-v2 (RAVQA-v2) focus on utilizing as much input\ninformation, such as image-based textual descriptions and retrieved knowledge,\nas possible to improve performance, but they all overlook the issue that with\nthe number of input tokens increasing, inference efficiency significantly\ndecreases, which contradicts the demands of practical applications. To address\nthis issue, we propose Retrieval-Augmented MLLM with Compressed Contexts\n(RACC). RACC learns to compress and aggregate retrieved contexts, from which it\ngenerates a compact modulation in the form of Key-Value (KV) cache. This\nmodulation is then used to adapt the downstream frozen MLLM, thereby achieving\neffective and efficient inference. RACC achieves a state-of-the-art (SOTA)\nperformance of 62.9% on OK-VQA. Moreover, it significantly reduces inference\nlatency by 22.0%-59.7% compared to the prominent RAVQA-v2. Abundant experiments\nshow RACC's broad applicability. It is compatible with various off-the-shelf\nMLLMs and can also handle different knowledge sources including textual and\nmultimodal documents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have demonstrated great zero-shot\nperformance on visual question answering (VQA). However, when it comes to\nknowledge-based VQA (KB-VQA), MLLMs may lack human commonsense or specialized\ndomain knowledge to answer such questions and require obtaining necessary\ninformation from external knowledge sources. Previous works like\nRetrival-Augmented VQA-v2 (RAVQA-v2) focus on utilizing as much input\ninformation, such as image-based textual descriptions and retrieved knowledge,\nas possible to improve performance, but they all overlook the issue that with\nthe number of input tokens increasing, inference efficiency significantly\ndecreases, which contradicts the demands of practical applications. To address\nthis issue, we propose Retrieval-Augmented MLLM with Compressed Contexts\n(RACC). RACC learns to compress and aggregate retrieved contexts, from which it\ngenerates a compact modulation in the form of Key-Value (KV) cache. This\nmodulation is then used to adapt the downstream frozen MLLM, thereby achieving\neffective and efficient inference. RACC achieves a state-of-the-art (SOTA)\nperformance of 62.9% on OK-VQA. Moreover, it significantly reduces inference\nlatency by 22.0%-59.7% compared to the prominent RAVQA-v2. Abundant experiments\nshow RACC's broad applicability. It is compatible with various off-the-shelf\nMLLMs and can also handle different knowledge sources including textual and\nmultimodal documents."
                },
                "authors": [
                    {
                        "name": "Weixi Weng"
                    },
                    {
                        "name": "Jieming Zhu"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Xiaojun Meng"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Chun Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Chun Yuan"
                },
                "author": "Chun Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07331v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07331v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07196v1",
                "updated": "2024-09-11T11:40:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    11,
                    40,
                    23,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T11:40:23Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    11,
                    40,
                    23,
                    2,
                    255,
                    0
                ],
                "title": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses"
                },
                "summary": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed."
                },
                "authors": [
                    {
                        "name": "Benjamin Colmey"
                    },
                    {
                        "name": "Rodrigo T. Paulino"
                    },
                    {
                        "name": "David G. Cooke"
                    }
                ],
                "author_detail": {
                    "name": "David G. Cooke"
                },
                "author": "David G. Cooke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10926v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10926v2",
                "updated": "2024-09-11T08:12:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    8,
                    12,
                    55,
                    2,
                    255,
                    0
                ],
                "published": "2024-07-15T17:25:42Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    17,
                    25,
                    42,
                    0,
                    197,
                    0
                ],
                "title": "In-Loop Filtering via Trained Look-Up Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Loop Filtering via Trained Look-Up Tables"
                },
                "summary": "In-loop filtering (ILF) is a key technology for removing the artifacts in\nimage/video coding standards. Recently, neural network-based in-loop filtering\nmethods achieve remarkable coding gains beyond the capability of advanced video\ncoding standards, which becomes a powerful coding tool candidate for future\nvideo coding standards. However, the utilization of deep neural networks brings\nheavy time and computational complexity, and high demands of high-performance\nhardware, which is challenging to apply to the general uses of coding scene. To\naddress this limitation, inspired by explorations in image restoration, we\npropose an efficient and practical in-loop filtering scheme by adopting the\nLook-up Table (LUT). We train the DNN of in-loop filtering within a fixed\nfiltering reference range, and cache the output values of the DNN into a LUT\nvia traversing all possible inputs. At testing time in the coding process, the\nfiltered pixel is generated by locating input pixels (to-be-filtered pixel with\nreference pixels) and interpolating cached filtered pixel values. To further\nenable the large filtering reference range with the limited storage cost of\nLUT, we introduce the enhanced indexing mechanism in the filtering process, and\nclipping/finetuning mechanism in the training. The proposed method is\nimplemented into the Versatile Video Coding (VVC) reference software, VTM-11.0.\nExperimental results show that the ultrafast, very fast, and fast mode of the\nproposed method achieves on average 0.13%/0.34%/0.51%, and 0.10%/0.27%/0.39%\nBD-rate reduction, under the all intra (AI) and random access (RA)\nconfigurations. Especially, our method has friendly time and computational\ncomplexity, only 101%/102%-104%/108% time increase with 0.13-0.93 kMACs/pixel,\nand only 164-1148 KB storage cost for a single model. Our solution may shed\nlight on the journey of practical neural network-based coding tool evolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-loop filtering (ILF) is a key technology for removing the artifacts in\nimage/video coding standards. Recently, neural network-based in-loop filtering\nmethods achieve remarkable coding gains beyond the capability of advanced video\ncoding standards, which becomes a powerful coding tool candidate for future\nvideo coding standards. However, the utilization of deep neural networks brings\nheavy time and computational complexity, and high demands of high-performance\nhardware, which is challenging to apply to the general uses of coding scene. To\naddress this limitation, inspired by explorations in image restoration, we\npropose an efficient and practical in-loop filtering scheme by adopting the\nLook-up Table (LUT). We train the DNN of in-loop filtering within a fixed\nfiltering reference range, and cache the output values of the DNN into a LUT\nvia traversing all possible inputs. At testing time in the coding process, the\nfiltered pixel is generated by locating input pixels (to-be-filtered pixel with\nreference pixels) and interpolating cached filtered pixel values. To further\nenable the large filtering reference range with the limited storage cost of\nLUT, we introduce the enhanced indexing mechanism in the filtering process, and\nclipping/finetuning mechanism in the training. The proposed method is\nimplemented into the Versatile Video Coding (VVC) reference software, VTM-11.0.\nExperimental results show that the ultrafast, very fast, and fast mode of the\nproposed method achieves on average 0.13%/0.34%/0.51%, and 0.10%/0.27%/0.39%\nBD-rate reduction, under the all intra (AI) and random access (RA)\nconfigurations. Especially, our method has friendly time and computational\ncomplexity, only 101%/102%-104%/108% time increase with 0.13-0.93 kMACs/pixel,\nand only 164-1148 KB storage cost for a single model. Our solution may shed\nlight on the journey of practical neural network-based coding tool evolution."
                },
                "authors": [
                    {
                        "name": "Zhuoyuan Li"
                    },
                    {
                        "name": "Jiacheng Li"
                    },
                    {
                        "name": "Yao Li"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Feng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Feng Wu"
                },
                "author": "Feng Wu",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10926v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10926v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2208.12453v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2208.12453v2",
                "updated": "2024-09-11T02:33:06Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    2,
                    33,
                    6,
                    2,
                    255,
                    0
                ],
                "published": "2022-08-26T06:28:08Z",
                "published_parsed": [
                    2022,
                    8,
                    26,
                    6,
                    28,
                    8,
                    4,
                    238,
                    0
                ],
                "title": "Exploiting Deep Reinforcement Learning for Edge Caching in Cell-Free\n  Massive MIMO Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploiting Deep Reinforcement Learning for Edge Caching in Cell-Free\n  Massive MIMO Systems"
                },
                "summary": "Cell-free massive multiple-input-multiple-output is promising to meet the\nstringent quality-of-experience (QoE) requirements of railway wireless\ncommunications by coordinating many successional access points (APs) to serve\nthe onboard users coherently. A key challenge is how to deliver the desired\ncontents timely due to the radical changing propagation environment caused by\nthe growing train speed. In this paper, we propose to proactively cache the\nlikely-requesting contents at the upcoming APs which perform the coherent\ntransmission to reduce end-to-end delay. A long-term QoE-maximization problem\nis formulated and two cache placement algorithms are proposed. One is based on\nheuristic convex optimization (HCO) and the other exploits deep reinforcement\nlearning (DRL) with soft actor-critic (SAC). Compared to the conventional\nbenchmark, numerical results show the advantage of our proposed algorithms on\nQoE and hit probability. With the advanced DRL model, SAC outperforms HCO on\nQoE by predicting the user requests accurately.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cell-free massive multiple-input-multiple-output is promising to meet the\nstringent quality-of-experience (QoE) requirements of railway wireless\ncommunications by coordinating many successional access points (APs) to serve\nthe onboard users coherently. A key challenge is how to deliver the desired\ncontents timely due to the radical changing propagation environment caused by\nthe growing train speed. In this paper, we propose to proactively cache the\nlikely-requesting contents at the upcoming APs which perform the coherent\ntransmission to reduce end-to-end delay. A long-term QoE-maximization problem\nis formulated and two cache placement algorithms are proposed. One is based on\nheuristic convex optimization (HCO) and the other exploits deep reinforcement\nlearning (DRL) with soft actor-critic (SAC). Compared to the conventional\nbenchmark, numerical results show the advantage of our proposed algorithms on\nQoE and hit probability. With the advanced DRL model, SAC outperforms HCO on\nQoE by predicting the user requests accurately."
                },
                "authors": [
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Shuaifei Chen"
                    },
                    {
                        "name": "Jiayi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiayi Zhang"
                },
                "author": "Jiayi Zhang",
                "arxiv_comment": "The focus of the research has shifted, and the current submission is\n  no longer aligned with our objectives",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2208.12453v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2208.12453v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.11504v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.11504v3",
                "updated": "2024-09-11T02:22:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    2,
                    22,
                    58,
                    2,
                    255,
                    0
                ],
                "published": "2024-01-21T14:28:41Z",
                "published_parsed": [
                    2024,
                    1,
                    21,
                    14,
                    28,
                    41,
                    6,
                    21,
                    0
                ],
                "title": "With Greater Text Comes Greater Necessity: Inference-Time Training Helps\n  Long Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With Greater Text Comes Greater Necessity: Inference-Time Training Helps\n  Long Text Generation"
                },
                "summary": "Long text generation, such as novel writing and discourse-level translation\nwith extremely long contexts, presents significant challenges to current\nlanguage models. Existing methods mainly focus on extending the model's context\nwindow through strategies like length extrapolation. However, these approaches\ndemand substantial hardware resources during the training and/or inference\nphases. Our proposed method, Temp-Lora, introduces an alternative concept.\nInstead of relying on the KV cache to store all context information, we embeds\nthis information directly into a temporary Lora module. In the process of long\ntext generation, this module is progressively trained with text generated\npreviously. This approach not only efficiently preserves contextual knowledge\nbut also prevents any permanent alteration to the model's parameters given that\nthe module is discarded post-generation. Extensive experiments on the PG19\nlanguage modeling benchmark and the GuoFeng discourse-level translation\nbenchmark validate the effectiveness of Temp-Lora. Our results show that: 1)\nTemp-Lora substantially enhances generation quality for long text, as indicated\nby a 13.2% decrease in perplexity (PPL) on a subset of PG19, and a 29.3%\ndecrease in PPL along with a 113.2% increase in BLEU score on a subset of\nGuoFeng, 2) Temp-Lora is compatible with and enhances most existing long text\ngeneration methods, and 3) Temp-Lora can greatly reduce computational costs by\nshortening the context window. For example, we can ensure a moderate\nimprovement in generation quality (a decrease of 3.8% in PPL) while enabling a\n51.5% memory usage reduction and a 60.0% decrease in latency for inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long text generation, such as novel writing and discourse-level translation\nwith extremely long contexts, presents significant challenges to current\nlanguage models. Existing methods mainly focus on extending the model's context\nwindow through strategies like length extrapolation. However, these approaches\ndemand substantial hardware resources during the training and/or inference\nphases. Our proposed method, Temp-Lora, introduces an alternative concept.\nInstead of relying on the KV cache to store all context information, we embeds\nthis information directly into a temporary Lora module. In the process of long\ntext generation, this module is progressively trained with text generated\npreviously. This approach not only efficiently preserves contextual knowledge\nbut also prevents any permanent alteration to the model's parameters given that\nthe module is discarded post-generation. Extensive experiments on the PG19\nlanguage modeling benchmark and the GuoFeng discourse-level translation\nbenchmark validate the effectiveness of Temp-Lora. Our results show that: 1)\nTemp-Lora substantially enhances generation quality for long text, as indicated\nby a 13.2% decrease in perplexity (PPL) on a subset of PG19, and a 29.3%\ndecrease in PPL along with a 113.2% increase in BLEU score on a subset of\nGuoFeng, 2) Temp-Lora is compatible with and enhances most existing long text\ngeneration methods, and 3) Temp-Lora can greatly reduce computational costs by\nshortening the context window. For example, we can ensure a moderate\nimprovement in generation quality (a decrease of 3.8% in PPL) while enabling a\n51.5% memory usage reduction and a 60.0% decrease in latency for inference."
                },
                "authors": [
                    {
                        "name": "Y. Wang"
                    },
                    {
                        "name": "D. Ma"
                    },
                    {
                        "name": "D. Cai"
                    }
                ],
                "author_detail": {
                    "name": "D. Cai"
                },
                "author": "D. Cai",
                "arxiv_comment": "COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.11504v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.11504v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06217v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06217v1",
                "updated": "2024-09-10T04:58:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    58,
                    48,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T04:58:48Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    58,
                    48,
                    1,
                    254,
                    0
                ],
                "title": "DACAT: Dual-stream Adaptive Clip-aware Time Modeling for Robust Online\n  Surgical Phase Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DACAT: Dual-stream Adaptive Clip-aware Time Modeling for Robust Online\n  Surgical Phase Recognition"
                },
                "summary": "Surgical phase recognition has become a crucial requirement in laparoscopic\nsurgery, enabling various clinical applications like surgical risk forecasting.\nCurrent methods typically identify the surgical phase using individual\nframe-wise embeddings as the fundamental unit for time modeling. However, this\napproach is overly sensitive to current observations, often resulting in\ndiscontinuous and erroneous predictions within a complete surgical phase. In\nthis paper, we propose DACAT, a novel dual-stream model that adaptively learns\nclip-aware context information to enhance the temporal relationship. In one\nstream, DACAT pretrains a frame encoder, caching all historical frame-wise\nfeatures. In the other stream, DACAT fine-tunes a new frame encoder to extract\nthe frame-wise feature at the current moment. Additionally, a max clip-response\nread-out (Max-R) module is introduced to bridge the two streams by using the\ncurrent frame-wise feature to adaptively fetch the most relevant past clip from\nthe feature cache. The clip-aware context feature is then encoded via\ncross-attention between the current frame and its fetched adaptive clip, and\nfurther utilized to enhance the time modeling for accurate online surgical\nphase recognition. The benchmark results on three public datasets, i.e.,\nCholec80, M2CAI16, and AutoLaparo, demonstrate the superiority of our proposed\nDACAT over existing state-of-the-art methods, with improvements in Jaccard\nscores of at least 4.5%, 4.6%, and 2.7%, respectively. Our code and models have\nbeen released at https://github.com/kk42yy/DACAT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surgical phase recognition has become a crucial requirement in laparoscopic\nsurgery, enabling various clinical applications like surgical risk forecasting.\nCurrent methods typically identify the surgical phase using individual\nframe-wise embeddings as the fundamental unit for time modeling. However, this\napproach is overly sensitive to current observations, often resulting in\ndiscontinuous and erroneous predictions within a complete surgical phase. In\nthis paper, we propose DACAT, a novel dual-stream model that adaptively learns\nclip-aware context information to enhance the temporal relationship. In one\nstream, DACAT pretrains a frame encoder, caching all historical frame-wise\nfeatures. In the other stream, DACAT fine-tunes a new frame encoder to extract\nthe frame-wise feature at the current moment. Additionally, a max clip-response\nread-out (Max-R) module is introduced to bridge the two streams by using the\ncurrent frame-wise feature to adaptively fetch the most relevant past clip from\nthe feature cache. The clip-aware context feature is then encoded via\ncross-attention between the current frame and its fetched adaptive clip, and\nfurther utilized to enhance the time modeling for accurate online surgical\nphase recognition. The benchmark results on three public datasets, i.e.,\nCholec80, M2CAI16, and AutoLaparo, demonstrate the superiority of our proposed\nDACAT over existing state-of-the-art methods, with improvements in Jaccard\nscores of at least 4.5%, 4.6%, and 2.7%, respectively. Our code and models have\nbeen released at https://github.com/kk42yy/DACAT."
                },
                "authors": [
                    {
                        "name": "Kaixiang Yang"
                    },
                    {
                        "name": "Qiang Li"
                    },
                    {
                        "name": "Zhiwei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhiwei Wang"
                },
                "author": "Zhiwei Wang",
                "arxiv_comment": "5 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06217v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06217v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06207v1",
                "updated": "2024-09-10T04:24:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    24,
                    22,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T04:24:22Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    24,
                    22,
                    1,
                    254,
                    0
                ],
                "title": "Design and Implementation of Online Live Streaming System Using A 3D\n  Engine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design and Implementation of Online Live Streaming System Using A 3D\n  Engine"
                },
                "summary": "With the growing demand for live video streaming, there is an increasing need\nfor low-latency and high-quality transmission, especially with the advent of 5G\nnetworks. While 5G offers hardware-level improvements, effective software\nsolutions for minimizing latency remain essential. Current methods, such as\nmulti-channel streaming, fail to address latency issues fundamentally, often\nonly adding new channels without optimizing overall performance. This thesis\nproposes a novel approach using a 3D engine (e.g., Unity 3D) to stream\nmulti-input video data through a single channel with reduced latency. By\nleveraging 3D engine capabilities, such as World/Screen Space Cameras, 3D\nCanvases, and Webcam Textures, the proposed system consolidates video streams\nfrom multiple external cameras into a unified, low-latency output. The\naffiliated project of this thesis demonstrates the implementation of a\nlow-latency multi-channel live video streaming system. It employs the RTSP\nprotocol and examines video encoding techniques, alongside a client-side\napplication based on Unity 3D. The system architecture includes a WebSocket\nserver for persistent connections, an HTTP server for communication, a MySQL\ndatabase for storage, Redis for caching, and Nginx for load balancing. Each\nmodule operates independently, ensuring flexibility and scalability in the\nsystem's design. A key innovation of this system is its use of a 3D scene to\nmap multiple video inputs onto a virtual canvas, recorded by an in-engine\ncamera for transmission. This design minimizes redundant data, enabling an\nefficient and director-guided live streaming network. The thesis concludes by\ndiscussing challenges encountered during the project and provides solutions for\nfuture improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing demand for live video streaming, there is an increasing need\nfor low-latency and high-quality transmission, especially with the advent of 5G\nnetworks. While 5G offers hardware-level improvements, effective software\nsolutions for minimizing latency remain essential. Current methods, such as\nmulti-channel streaming, fail to address latency issues fundamentally, often\nonly adding new channels without optimizing overall performance. This thesis\nproposes a novel approach using a 3D engine (e.g., Unity 3D) to stream\nmulti-input video data through a single channel with reduced latency. By\nleveraging 3D engine capabilities, such as World/Screen Space Cameras, 3D\nCanvases, and Webcam Textures, the proposed system consolidates video streams\nfrom multiple external cameras into a unified, low-latency output. The\naffiliated project of this thesis demonstrates the implementation of a\nlow-latency multi-channel live video streaming system. It employs the RTSP\nprotocol and examines video encoding techniques, alongside a client-side\napplication based on Unity 3D. The system architecture includes a WebSocket\nserver for persistent connections, an HTTP server for communication, a MySQL\ndatabase for storage, Redis for caching, and Nginx for load balancing. Each\nmodule operates independently, ensuring flexibility and scalability in the\nsystem's design. A key innovation of this system is its use of a 3D scene to\nmap multiple video inputs onto a virtual canvas, recorded by an in-engine\ncamera for transmission. This design minimizes redundant data, enabling an\nefficient and director-guided live streaming network. The thesis concludes by\ndiscussing challenges encountered during the project and provides solutions for\nfuture improvement."
                },
                "authors": [
                    {
                        "name": "Aizierjiang Aiersilan"
                    }
                ],
                "author_detail": {
                    "name": "Aizierjiang Aiersilan"
                },
                "author": "Aizierjiang Aiersilan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05867v1",
                "updated": "2024-09-09T17:59:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    59,
                    57,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T17:59:57Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    59,
                    57,
                    0,
                    253,
                    0
                ],
                "title": "Flash Cache: Reducing Bias in Radiance Cache Based Inverse Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flash Cache: Reducing Bias in Radiance Cache Based Inverse Rendering"
                },
                "summary": "State-of-the-art techniques for 3D reconstruction are largely based on\nvolumetric scene representations, which require sampling multiple points to\ncompute the color arriving along a ray. Using these representations for more\ngeneral inverse rendering -- reconstructing geometry, materials, and lighting\nfrom observed images -- is challenging because recursively path-tracing such\nvolumetric representations is expensive. Recent works alleviate this issue\nthrough the use of radiance caches: data structures that store the\nsteady-state, infinite-bounce radiance arriving at any point from any\ndirection. However, these solutions rely on approximations that introduce bias\ninto the renderings and, more importantly, into the gradients used for\noptimization. We present a method that avoids these approximations while\nremaining computationally efficient. In particular, we leverage two techniques\nto reduce variance for unbiased estimators of the rendering equation: (1) an\nocclusion-aware importance sampler for incoming illumination and (2) a fast\ncache architecture that can be used as a control variate for the radiance from\na high-quality, but more expensive, volumetric cache. We show that by removing\nthese biases our approach improves the generality of radiance cache based\ninverse rendering, as well as increasing quality in the presence of challenging\nlight transport effects such as specular reflections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State-of-the-art techniques for 3D reconstruction are largely based on\nvolumetric scene representations, which require sampling multiple points to\ncompute the color arriving along a ray. Using these representations for more\ngeneral inverse rendering -- reconstructing geometry, materials, and lighting\nfrom observed images -- is challenging because recursively path-tracing such\nvolumetric representations is expensive. Recent works alleviate this issue\nthrough the use of radiance caches: data structures that store the\nsteady-state, infinite-bounce radiance arriving at any point from any\ndirection. However, these solutions rely on approximations that introduce bias\ninto the renderings and, more importantly, into the gradients used for\noptimization. We present a method that avoids these approximations while\nremaining computationally efficient. In particular, we leverage two techniques\nto reduce variance for unbiased estimators of the rendering equation: (1) an\nocclusion-aware importance sampler for incoming illumination and (2) a fast\ncache architecture that can be used as a control variate for the radiance from\na high-quality, but more expensive, volumetric cache. We show that by removing\nthese biases our approach improves the generality of radiance cache based\ninverse rendering, as well as increasing quality in the presence of challenging\nlight transport effects such as specular reflections."
                },
                "authors": [
                    {
                        "name": "Benjamin Attal"
                    },
                    {
                        "name": "Dor Verbin"
                    },
                    {
                        "name": "Ben Mildenhall"
                    },
                    {
                        "name": "Peter Hedman"
                    },
                    {
                        "name": "Jonathan T. Barron"
                    },
                    {
                        "name": "Matthew O'Toole"
                    },
                    {
                        "name": "Pratul P. Srinivasan"
                    }
                ],
                "author_detail": {
                    "name": "Pratul P. Srinivasan"
                },
                "author": "Pratul P. Srinivasan",
                "arxiv_comment": "Website: https://benattal.github.io/flash-cache/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03753v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03753v2",
                "updated": "2024-09-09T10:04:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    10,
                    4,
                    0,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-05T17:59:15Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    59,
                    15,
                    3,
                    249,
                    0
                ],
                "title": "WildVis: Open Source Visualizer for Million-Scale Chat Logs in the Wild",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WildVis: Open Source Visualizer for Million-Scale Chat Logs in the Wild"
                },
                "summary": "The increasing availability of real-world conversation data offers exciting\nopportunities for researchers to study user-chatbot interactions. However, the\nsheer volume of this data makes manually examining individual conversations\nimpractical. To overcome this challenge, we introduce WildVis, an interactive\ntool that enables fast, versatile, and large-scale conversation analysis.\nWildVis provides search and visualization capabilities in the text and\nembedding spaces based on a list of criteria. To manage million-scale datasets,\nwe implemented optimizations including search index construction, embedding\nprecomputation and compression, and caching to ensure responsive user\ninteractions within seconds. We demonstrate WildVis' utility through three case\nstudies: facilitating chatbot misuse research, visualizing and comparing topic\ndistributions across datasets, and characterizing user-specific conversation\npatterns. WildVis is open-source and designed to be extendable, supporting\nadditional datasets and customized search and visualization functionalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing availability of real-world conversation data offers exciting\nopportunities for researchers to study user-chatbot interactions. However, the\nsheer volume of this data makes manually examining individual conversations\nimpractical. To overcome this challenge, we introduce WildVis, an interactive\ntool that enables fast, versatile, and large-scale conversation analysis.\nWildVis provides search and visualization capabilities in the text and\nembedding spaces based on a list of criteria. To manage million-scale datasets,\nwe implemented optimizations including search index construction, embedding\nprecomputation and compression, and caching to ensure responsive user\ninteractions within seconds. We demonstrate WildVis' utility through three case\nstudies: facilitating chatbot misuse research, visualizing and comparing topic\ndistributions across datasets, and characterizing user-specific conversation\npatterns. WildVis is open-source and designed to be extendable, supporting\nadditional datasets and customized search and visualization functionalities."
                },
                "authors": [
                    {
                        "name": "Yuntian Deng"
                    },
                    {
                        "name": "Wenting Zhao"
                    },
                    {
                        "name": "Jack Hessel"
                    },
                    {
                        "name": "Xiang Ren"
                    },
                    {
                        "name": "Claire Cardie"
                    },
                    {
                        "name": "Yejin Choi"
                    }
                ],
                "author_detail": {
                    "name": "Yejin Choi"
                },
                "author": "Yejin Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03753v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03753v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05025v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05025v1",
                "updated": "2024-09-08T08:39:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    8,
                    39,
                    50,
                    6,
                    252,
                    0
                ],
                "published": "2024-09-08T08:39:50Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    8,
                    39,
                    50,
                    6,
                    252,
                    0
                ],
                "title": "Cooperative Learning-Based Framework for VNF Caching and Placement\n  Optimization over Low Earth Orbit Satellite Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cooperative Learning-Based Framework for VNF Caching and Placement\n  Optimization over Low Earth Orbit Satellite Networks"
                },
                "summary": "Low Earth Orbit Satellite Networks (LSNs) are integral to supporting a broad\nrange of modern applications, which are typically modeled as Service Function\nChains (SFCs). Each SFC is composed of Virtual Network Functions (VNFs), where\neach VNF performs a specific task. In this work, we tackle two key challenges\nin deploying SFCs across an LSN. Firstly, we aim to optimize the long-term\nsystem performance by minimizing the average end-to-end SFC execution delay,\ngiven that each satellite comes with a pre-installed/cached subset of VNFs. To\nachieve optimal SFC placement, we formulate an offline Dynamic Programming (DP)\nequation. To overcome the challenges associated with DP, such as its\ncomplexity, the need for probability knowledge, and centralized\ndecision-making, we put forth an online Multi-Agent Q-Learning (MAQL) solution.\nOur MAQL approach addresses convergence issues in the non-stationary LSN\nenvironment by enabling satellites to share learning parameters and update\ntheir Q-tables based on distinct rules for their selected actions. Secondly, to\ndetermine the optimal VNF subsets for satellite caching, we develop a Bayesian\nOptimization (BO)-based learning mechanism that operates both offline and\ncontinuously in the background during runtime. Extensive experiments\ndemonstrate that our MAQL approach achieves near-optimal performance comparable\nto the DP model and significantly outperforms existing baselines. Moreover, the\nBO-based approach effectively enhances the request serving rate over time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low Earth Orbit Satellite Networks (LSNs) are integral to supporting a broad\nrange of modern applications, which are typically modeled as Service Function\nChains (SFCs). Each SFC is composed of Virtual Network Functions (VNFs), where\neach VNF performs a specific task. In this work, we tackle two key challenges\nin deploying SFCs across an LSN. Firstly, we aim to optimize the long-term\nsystem performance by minimizing the average end-to-end SFC execution delay,\ngiven that each satellite comes with a pre-installed/cached subset of VNFs. To\nachieve optimal SFC placement, we formulate an offline Dynamic Programming (DP)\nequation. To overcome the challenges associated with DP, such as its\ncomplexity, the need for probability knowledge, and centralized\ndecision-making, we put forth an online Multi-Agent Q-Learning (MAQL) solution.\nOur MAQL approach addresses convergence issues in the non-stationary LSN\nenvironment by enabling satellites to share learning parameters and update\ntheir Q-tables based on distinct rules for their selected actions. Secondly, to\ndetermine the optimal VNF subsets for satellite caching, we develop a Bayesian\nOptimization (BO)-based learning mechanism that operates both offline and\ncontinuously in the background during runtime. Extensive experiments\ndemonstrate that our MAQL approach achieves near-optimal performance comparable\nto the DP model and significantly outperforms existing baselines. Moreover, the\nBO-based approach effectively enhances the request serving rate over time."
                },
                "authors": [
                    {
                        "name": "Khai Doan"
                    },
                    {
                        "name": "Marios Avgeris"
                    },
                    {
                        "name": "Aris Leivadeas"
                    },
                    {
                        "name": "Ioannis Lambadaris"
                    },
                    {
                        "name": "Wonjae Shin"
                    }
                ],
                "author_detail": {
                    "name": "Wonjae Shin"
                },
                "author": "Wonjae Shin",
                "arxiv_comment": "40 pages, 11 figure, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05025v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05025v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04992v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04992v1",
                "updated": "2024-09-08T06:06:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    6,
                    6,
                    44,
                    6,
                    252,
                    0
                ],
                "published": "2024-09-08T06:06:44Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    6,
                    6,
                    44,
                    6,
                    252,
                    0
                ],
                "title": "InstInfer: In-Storage Attention Offloading for Cost-Effective\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstInfer: In-Storage Attention Offloading for Cost-Effective\n  Long-Context LLM Inference"
                },
                "summary": "The widespread of Large Language Models (LLMs) marks a significant milestone\nin generative AI. Nevertheless, the increasing context length and batch size in\noffline LLM inference escalate the memory requirement of the key-value (KV)\ncache, which imposes a huge burden on the GPU VRAM, especially for\nresource-constraint scenarios (e.g., edge computing and personal devices).\nSeveral cost-effective solutions leverage host memory or SSDs to reduce storage\ncosts for offline inference scenarios and improve the throughput. Nevertheless,\nthey suffer from significant performance penalties imposed by intensive KV\ncache accesses due to limited PCIe bandwidth. To address these issues, we\npropose InstInfer, a novel LLM inference system that offloads the most\nperformance-critical computation (i.e., attention in decoding phase) and data\n(i.e., KV cache) parts to Computational Storage Drives (CSDs), which minimize\nthe enormous KV transfer overheads. InstInfer designs a dedicated flash-aware\nin-storage attention engine with KV cache management mechanisms to exploit the\nhigh internal bandwidths of CSDs instead of being limited by the PCIe\nbandwidth. The optimized P2P transmission between GPU and CSDs further reduces\ndata migration overheads. Experimental results demonstrate that for a 13B model\nusing an NVIDIA A6000 GPU, InstInfer improves throughput for long-sequence\ninference by up to 11.1$\\times$, compared to existing SSD-based solutions such\nas FlexGen.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread of Large Language Models (LLMs) marks a significant milestone\nin generative AI. Nevertheless, the increasing context length and batch size in\noffline LLM inference escalate the memory requirement of the key-value (KV)\ncache, which imposes a huge burden on the GPU VRAM, especially for\nresource-constraint scenarios (e.g., edge computing and personal devices).\nSeveral cost-effective solutions leverage host memory or SSDs to reduce storage\ncosts for offline inference scenarios and improve the throughput. Nevertheless,\nthey suffer from significant performance penalties imposed by intensive KV\ncache accesses due to limited PCIe bandwidth. To address these issues, we\npropose InstInfer, a novel LLM inference system that offloads the most\nperformance-critical computation (i.e., attention in decoding phase) and data\n(i.e., KV cache) parts to Computational Storage Drives (CSDs), which minimize\nthe enormous KV transfer overheads. InstInfer designs a dedicated flash-aware\nin-storage attention engine with KV cache management mechanisms to exploit the\nhigh internal bandwidths of CSDs instead of being limited by the PCIe\nbandwidth. The optimized P2P transmission between GPU and CSDs further reduces\ndata migration overheads. Experimental results demonstrate that for a 13B model\nusing an NVIDIA A6000 GPU, InstInfer improves throughput for long-sequence\ninference by up to 11.1$\\times$, compared to existing SSD-based solutions such\nas FlexGen."
                },
                "authors": [
                    {
                        "name": "Xiurui Pan"
                    },
                    {
                        "name": "Endian Li"
                    },
                    {
                        "name": "Qiao Li"
                    },
                    {
                        "name": "Shengwen Liang"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Ke Zhou"
                    },
                    {
                        "name": "Yingwei Luo"
                    },
                    {
                        "name": "Xiaolin Wang"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "author": "Jie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04992v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04992v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04750v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04750v1",
                "updated": "2024-09-07T07:50:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    7,
                    7,
                    50,
                    13,
                    5,
                    251,
                    0
                ],
                "published": "2024-09-07T07:50:13Z",
                "published_parsed": [
                    2024,
                    9,
                    7,
                    7,
                    50,
                    13,
                    5,
                    251,
                    0
                ],
                "title": "Training-Free Style Consistent Image Synthesis with Condition and Mask\n  Guidance in E-Commerce",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Style Consistent Image Synthesis with Condition and Mask\n  Guidance in E-Commerce"
                },
                "summary": "Generating style-consistent images is a common task in the e-commerce field,\nand current methods are largely based on diffusion models, which have achieved\nexcellent results. This paper introduces the concept of the QKV\n(query/key/value) level, referring to modifications in the attention maps\n(self-attention and cross-attention) when integrating UNet with image\nconditions. Without disrupting the product's main composition in e-commerce\nimages, we aim to use a train-free method guided by pre-set conditions. This\ninvolves using shared KV to enhance similarity in cross-attention and\ngenerating mask guidance from the attention map to cleverly direct the\ngeneration of style-consistent images. Our method has shown promising results\nin practical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating style-consistent images is a common task in the e-commerce field,\nand current methods are largely based on diffusion models, which have achieved\nexcellent results. This paper introduces the concept of the QKV\n(query/key/value) level, referring to modifications in the attention maps\n(self-attention and cross-attention) when integrating UNet with image\nconditions. Without disrupting the product's main composition in e-commerce\nimages, we aim to use a train-free method guided by pre-set conditions. This\ninvolves using shared KV to enhance similarity in cross-attention and\ngenerating mask guidance from the attention map to cleverly direct the\ngeneration of style-consistent images. Our method has shown promising results\nin practical applications."
                },
                "authors": [
                    {
                        "name": "Guandong Li"
                    }
                ],
                "author_detail": {
                    "name": "Guandong Li"
                },
                "author": "Guandong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04750v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14366v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14366v2",
                "updated": "2024-09-07T02:52:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    7,
                    2,
                    52,
                    29,
                    5,
                    251,
                    0
                ],
                "published": "2024-05-23T09:43:52Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    9,
                    43,
                    52,
                    3,
                    144,
                    0
                ],
                "title": "MiniCache: KV Cache Compression in Depth Dimension for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiniCache: KV Cache Compression in Depth Dimension for Large Language\n  Models"
                },
                "summary": "A critical approach for efficiently deploying computationally demanding large\nlanguage models (LLMs) is Key-Value (KV) caching. The KV cache stores key-value\nstates of previously generated tokens, significantly reducing the need for\nrepetitive computations and thereby lowering latency in autoregressive\ngeneration. However, the size of the KV cache grows linearly with sequence\nlength, posing challenges for applications requiring long context input and\nextensive sequence generation. In this paper, we present a simple yet effective\napproach, called MiniCache, to compress the KV cache across layers from a novel\ndepth perspective, significantly reducing the memory footprint for LLM\ninference. Our approach is based on the observation that KV cache states\nexhibit high similarity between the adjacent layers in the middle-to-deep\nportion of LLMs. To facilitate merging, we propose disentangling the states\ninto the magnitude and direction components, interpolating the directions of\nthe state vectors while preserving their lengths unchanged. Furthermore, we\nintroduce a token retention strategy to keep highly distinct state pairs\nunmerged, thus preserving the information with minimal additional storage\noverhead. Our MiniCache is training-free and general, complementing existing KV\ncache compression strategies, such as quantization and sparsity. We conduct a\ncomprehensive evaluation of MiniCache utilizing various models including\nLLaMA-2, LLaMA-3, Phi-3, Mistral, and Mixtral across multiple benchmarks,\ndemonstrating its exceptional performance in achieving superior compression\nratios and high throughput. On the ShareGPT dataset, LLaMA-2-7B with 4-bit\nMiniCache achieves a remarkable compression ratio of up to 5.02x, enhances\ninference throughput by approximately 5x, and reduces the memory footprint by\n41% compared to the FP16 full cache baseline, all while maintaining\nnear-lossless performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A critical approach for efficiently deploying computationally demanding large\nlanguage models (LLMs) is Key-Value (KV) caching. The KV cache stores key-value\nstates of previously generated tokens, significantly reducing the need for\nrepetitive computations and thereby lowering latency in autoregressive\ngeneration. However, the size of the KV cache grows linearly with sequence\nlength, posing challenges for applications requiring long context input and\nextensive sequence generation. In this paper, we present a simple yet effective\napproach, called MiniCache, to compress the KV cache across layers from a novel\ndepth perspective, significantly reducing the memory footprint for LLM\ninference. Our approach is based on the observation that KV cache states\nexhibit high similarity between the adjacent layers in the middle-to-deep\nportion of LLMs. To facilitate merging, we propose disentangling the states\ninto the magnitude and direction components, interpolating the directions of\nthe state vectors while preserving their lengths unchanged. Furthermore, we\nintroduce a token retention strategy to keep highly distinct state pairs\nunmerged, thus preserving the information with minimal additional storage\noverhead. Our MiniCache is training-free and general, complementing existing KV\ncache compression strategies, such as quantization and sparsity. We conduct a\ncomprehensive evaluation of MiniCache utilizing various models including\nLLaMA-2, LLaMA-3, Phi-3, Mistral, and Mixtral across multiple benchmarks,\ndemonstrating its exceptional performance in achieving superior compression\nratios and high throughput. On the ShareGPT dataset, LLaMA-2-7B with 4-bit\nMiniCache achieves a remarkable compression ratio of up to 5.02x, enhances\ninference throughput by approximately 5x, and reduces the memory footprint by\n41% compared to the FP16 full cache baseline, all while maintaining\nnear-lossless performance."
                },
                "authors": [
                    {
                        "name": "Akide Liu"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Zizheng Pan"
                    },
                    {
                        "name": "Yefei He"
                    },
                    {
                        "name": "Gholamreza Haffari"
                    },
                    {
                        "name": "Bohan Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Bohan Zhuang"
                },
                "author": "Bohan Zhuang",
                "arxiv_comment": "Project is available at https://minicache.vmv.re",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14366v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14366v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03637v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03637v4",
                "updated": "2024-09-06T08:28:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    8,
                    28,
                    1,
                    4,
                    250,
                    0
                ],
                "published": "2024-07-04T05:13:58Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    5,
                    13,
                    58,
                    3,
                    186,
                    0
                ],
                "title": "QET: Enhancing Quantized LLM Parameters and KV cache Compression through\n  Element Substitution and Residual Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QET: Enhancing Quantized LLM Parameters and KV cache Compression through\n  Element Substitution and Residual Clustering"
                },
                "summary": "The matrix quantization entails representing matrix elements in a more\nspace-efficient form to reduce storage usage, with dequantization restoring the\noriginal matrix for use. We formulate the Quantization Error Minimization (QEM)\nproblem as minimizing the distance between a matrix before and after\nquantization, under the condition that the quantized matrix occupies the same\nmemory space. Matrix quantization is crucial in various applications, including\nLarge Language Models (LLMs) weight quantization, vector databases, KV cache\nquantization, graph compression, and image compression. Recent advancements in\nLLMs, such as GPT-4 and BERT, have highlighted the importance of matrix\ncompression due to the large size of parameters and KV cache, which are stored\nas matrices.\n  We propose Quantum Entanglement Trees (QET) to address the QEM problem by\nleveraging the local orderliness of matrix elements, involving iterative\nelement swapping to form a locally ordered matrix. This matrix is then grouped\nand quantized by columns. To enhance QET, we introduce two optimizations:\nfurther quantizing residuals to reduce MSE, and using masking and batch\nprocessing to accelerate the algorithm.\n  Experimental results demonstrate that QET can effectively reduce MSE to\n5.05%, 13.33%, and 11.89% of the current best method on the LLM dataset, K\ncache, and V cache, respectively. Our contributions include the abstraction of\nthe QEM problem, the design of the QET algorithm, and the proposal of two\noptimizations to improve accuracy and speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The matrix quantization entails representing matrix elements in a more\nspace-efficient form to reduce storage usage, with dequantization restoring the\noriginal matrix for use. We formulate the Quantization Error Minimization (QEM)\nproblem as minimizing the distance between a matrix before and after\nquantization, under the condition that the quantized matrix occupies the same\nmemory space. Matrix quantization is crucial in various applications, including\nLarge Language Models (LLMs) weight quantization, vector databases, KV cache\nquantization, graph compression, and image compression. Recent advancements in\nLLMs, such as GPT-4 and BERT, have highlighted the importance of matrix\ncompression due to the large size of parameters and KV cache, which are stored\nas matrices.\n  We propose Quantum Entanglement Trees (QET) to address the QEM problem by\nleveraging the local orderliness of matrix elements, involving iterative\nelement swapping to form a locally ordered matrix. This matrix is then grouped\nand quantized by columns. To enhance QET, we introduce two optimizations:\nfurther quantizing residuals to reduce MSE, and using masking and batch\nprocessing to accelerate the algorithm.\n  Experimental results demonstrate that QET can effectively reduce MSE to\n5.05%, 13.33%, and 11.89% of the current best method on the LLM dataset, K\ncache, and V cache, respectively. Our contributions include the abstraction of\nthe QEM problem, the design of the QET algorithm, and the proposal of two\noptimizations to improve accuracy and speed."
                },
                "authors": [
                    {
                        "name": "Yanshu Wang"
                    },
                    {
                        "name": "Wang Li"
                    },
                    {
                        "name": "Zhaoqian Yao"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03637v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03637v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04040v1",
                "updated": "2024-09-06T06:16:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    6,
                    16,
                    55,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T06:16:55Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    6,
                    16,
                    55,
                    4,
                    250,
                    0
                ],
                "title": "A First Look At Efficient And Secure On-Device LLM Inference Against KV\n  Leakage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A First Look At Efficient And Secure On-Device LLM Inference Against KV\n  Leakage"
                },
                "summary": "Running LLMs on end devices has garnered significant attention recently due\nto their advantages in privacy preservation. With the advent of lightweight LLM\nmodels and specially designed GPUs, on-device LLM inference has achieved the\nnecessary accuracy and performance metrics. However, we have identified that\nLLM inference on GPUs can leak privacy-sensitive intermediate information,\nspecifically the KV pairs. An attacker could exploit these KV pairs to\nreconstruct the entire user conversation, leading to significant\nvulnerabilities. Existing solutions, such as Fully Homomorphic Encryption (FHE)\nand Trusted Execution Environments (TEE), are either too computation-intensive\nor resource-limited. To address these issues, we designed KV-Shield, which\noperates in two phases. In the initialization phase, it permutes the weight\nmatrices so that all KV pairs are correspondingly permuted. During the runtime\nphase, the attention vector is inversely permuted to ensure the correctness of\nthe layer output. All permutation-related operations are executed within the\nTEE, ensuring that insecure GPUs cannot access the original KV pairs, thus\npreventing conversation reconstruction. Finally, we theoretically analyze the\ncorrectness of KV-Shield, along with its advantages and overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Running LLMs on end devices has garnered significant attention recently due\nto their advantages in privacy preservation. With the advent of lightweight LLM\nmodels and specially designed GPUs, on-device LLM inference has achieved the\nnecessary accuracy and performance metrics. However, we have identified that\nLLM inference on GPUs can leak privacy-sensitive intermediate information,\nspecifically the KV pairs. An attacker could exploit these KV pairs to\nreconstruct the entire user conversation, leading to significant\nvulnerabilities. Existing solutions, such as Fully Homomorphic Encryption (FHE)\nand Trusted Execution Environments (TEE), are either too computation-intensive\nor resource-limited. To address these issues, we designed KV-Shield, which\noperates in two phases. In the initialization phase, it permutes the weight\nmatrices so that all KV pairs are correspondingly permuted. During the runtime\nphase, the attention vector is inversely permuted to ensure the correctness of\nthe layer output. All permutation-related operations are executed within the\nTEE, ensuring that insecure GPUs cannot access the original KV pairs, thus\npreventing conversation reconstruction. Finally, we theoretically analyze the\ncorrectness of KV-Shield, along with its advantages and overhead."
                },
                "authors": [
                    {
                        "name": "Huan Yang"
                    },
                    {
                        "name": "Deyu Zhang"
                    },
                    {
                        "name": "Yudong Zhao"
                    },
                    {
                        "name": "Yuanchun Li"
                    },
                    {
                        "name": "Yunxin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yunxin Liu"
                },
                "author": "Yunxin Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03308v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03308v2",
                "updated": "2024-09-05T20:21:54Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    20,
                    21,
                    54,
                    3,
                    249,
                    0
                ],
                "published": "2024-08-06T17:16:19Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    17,
                    16,
                    19,
                    1,
                    219,
                    0
                ],
                "title": "Potential and Limitation of High-Frequency Cores and Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Potential and Limitation of High-Frequency Cores and Caches"
                },
                "summary": "This paper explores the potential of cryogenic semiconductor computing and\nsuperconductor electronics as promising alternatives to traditional\nsemiconductor devices. As semiconductor devices face challenges such as\nincreased leakage currents and reduced performance at higher temperatures,\nthese novel technologies offer high performance and low power computation.\nConventional semiconductor electronics operating at cryogenic temperatures\n(below -150{\\deg}C or 123.15 K) can benefit from reduced leakage currents and\nimproved electron mobility. On the other hand, superconductor electronics,\noperating below 10 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconductor electronics and cryogenic semiconductor\ncomputing in gem5. We evaluate the performance of these components using\nworkloads representative of real-world applications like NPB, SPEC CPU2006, and\nGAPBS. Our results show the potential speedups achievable by these components\nand the limitations posed by cache bandwidth. This work provides valuable\ninsights into the performance implications and design trade-offs associated\nwith cryogenic and superconductor technologies, laying the foundation for\nfuture research in this field using gem5.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the potential of cryogenic semiconductor computing and\nsuperconductor electronics as promising alternatives to traditional\nsemiconductor devices. As semiconductor devices face challenges such as\nincreased leakage currents and reduced performance at higher temperatures,\nthese novel technologies offer high performance and low power computation.\nConventional semiconductor electronics operating at cryogenic temperatures\n(below -150{\\deg}C or 123.15 K) can benefit from reduced leakage currents and\nimproved electron mobility. On the other hand, superconductor electronics,\noperating below 10 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconductor electronics and cryogenic semiconductor\ncomputing in gem5. We evaluate the performance of these components using\nworkloads representative of real-world applications like NPB, SPEC CPU2006, and\nGAPBS. Our results show the potential speedups achievable by these components\nand the limitations posed by cache bandwidth. This work provides valuable\ninsights into the performance implications and design trade-offs associated\nwith cryogenic and superconductor technologies, laying the foundation for\nfuture research in this field using gem5."
                },
                "authors": [
                    {
                        "name": "Kunal Pai"
                    },
                    {
                        "name": "Anusheel Nand"
                    },
                    {
                        "name": "Jason Lowe-Power"
                    }
                ],
                "author_detail": {
                    "name": "Jason Lowe-Power"
                },
                "author": "Jason Lowe-Power",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03308v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03308v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03743v1",
                "updated": "2024-09-05T17:56:19Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    56,
                    19,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T17:56:19Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    56,
                    19,
                    3,
                    249,
                    0
                ],
                "title": "Libra: Architectural Support For Principled, Secure And Efficient\n  Balanced Execution On High-End Processors (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Libra: Architectural Support For Principled, Secure And Efficient\n  Balanced Execution On High-End Processors (Extended Version)"
                },
                "summary": "Control-flow leakage (CFL) attacks enable an attacker to expose control-flow\ndecisions of a victim program via side-channel observations. Linearization\n(i.e., elimination) of secret-dependent control flow is the main countermeasure\nagainst these attacks, yet it comes at a non-negligible cost. Conversely,\nbalancing secret-dependent branches often incurs a smaller overhead, but is\nnotoriously insecure on high-end processors. Hence, linearization has been\nwidely believed to be the only effective countermeasure against CFL attacks. In\nthis paper, we challenge this belief and investigate an unexplored alternative:\nhow to securely balance secret-dependent branches on higher-end processors?\n  We propose Libra, a generic and principled hardware-software codesign to\nefficiently address CFL on high-end processors. We perform a systematic\nclassification of hardware primitives leaking control flow from the literature,\nand provide guidelines to handle them with our design. Importantly, Libra\nenables secure control-flow balancing without the need to disable\nperformance-critical hardware such as the instruction cache and the prefetcher.\nWe formalize the semantics of Libra and propose a code transformation algorithm\nfor securing programs, which we prove correct and secure. Finally, we implement\nand evaluate Libra on an out-of-order RISC-V processor, showing performance\noverhead on par with insecure balanced code, and outperforming state-of-the-art\nlinearized code by 19.3%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Control-flow leakage (CFL) attacks enable an attacker to expose control-flow\ndecisions of a victim program via side-channel observations. Linearization\n(i.e., elimination) of secret-dependent control flow is the main countermeasure\nagainst these attacks, yet it comes at a non-negligible cost. Conversely,\nbalancing secret-dependent branches often incurs a smaller overhead, but is\nnotoriously insecure on high-end processors. Hence, linearization has been\nwidely believed to be the only effective countermeasure against CFL attacks. In\nthis paper, we challenge this belief and investigate an unexplored alternative:\nhow to securely balance secret-dependent branches on higher-end processors?\n  We propose Libra, a generic and principled hardware-software codesign to\nefficiently address CFL on high-end processors. We perform a systematic\nclassification of hardware primitives leaking control flow from the literature,\nand provide guidelines to handle them with our design. Importantly, Libra\nenables secure control-flow balancing without the need to disable\nperformance-critical hardware such as the instruction cache and the prefetcher.\nWe formalize the semantics of Libra and propose a code transformation algorithm\nfor securing programs, which we prove correct and secure. Finally, we implement\nand evaluate Libra on an out-of-order RISC-V processor, showing performance\noverhead on par with insecure balanced code, and outperforming state-of-the-art\nlinearized code by 19.3%."
                },
                "authors": [
                    {
                        "name": "Hans Winderix"
                    },
                    {
                        "name": "Marton Bognar"
                    },
                    {
                        "name": "Lesly-Ann Daniel"
                    },
                    {
                        "name": "Frank Piessens"
                    }
                ],
                "author_detail": {
                    "name": "Frank Piessens"
                },
                "author": "Frank Piessens",
                "arxiv_doi": "10.1145/3658644.3690319",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3658644.3690319",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.03743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03568v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03568v1",
                "updated": "2024-09-05T14:22:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    22,
                    2,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T14:22:02Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    22,
                    2,
                    3,
                    249,
                    0
                ],
                "title": "Enabling Practical and Privacy-Preserving Image Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Practical and Privacy-Preserving Image Processing"
                },
                "summary": "Fully Homomorphic Encryption (FHE) enables computations on encrypted data,\npreserving confidentiality without the need for decryption. However, FHE is\noften hindered by significant performance overhead, particularly for\nhigh-precision and complex data like images. Due to serious efficiency issues,\ntraditional FHE methods often encrypt images by monolithic data blocks (such as\npixel rows), instead of pixels. However, this strategy compromises the\nadvantages of homomorphic operations and disables pixel-level image processing.\nIn this study, we address these challenges by proposing and implementing a\npixel-level homomorphic encryption approach, iCHEETAH, based on the CKKS\nscheme. To enhance computational efficiency, we introduce three novel caching\nmechanisms to pre-encrypt radix values or frequently occurring pixel values,\nsubstantially reducing redundant encryption operations. Extensive experiments\ndemonstrate that our approach achieves up to a 19-fold improvement in\nencryption speed compared to the original CKKS, while maintaining high image\nquality. Additionally, real-world image applications such as mean filtering,\nbrightness enhancement, image matching and watermarking are tested based on\nFHE, showcasing up to a 91.53% speed improvement. We also proved that our\nmethod is IND-CPA (Indistinguishability under Chosen Plaintext Attack) secure,\nproviding strong encryption security. These results underscore the practicality\nand efficiency of iCHEETAH, marking a significant advancement in\nprivacy-preserving image processing at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fully Homomorphic Encryption (FHE) enables computations on encrypted data,\npreserving confidentiality without the need for decryption. However, FHE is\noften hindered by significant performance overhead, particularly for\nhigh-precision and complex data like images. Due to serious efficiency issues,\ntraditional FHE methods often encrypt images by monolithic data blocks (such as\npixel rows), instead of pixels. However, this strategy compromises the\nadvantages of homomorphic operations and disables pixel-level image processing.\nIn this study, we address these challenges by proposing and implementing a\npixel-level homomorphic encryption approach, iCHEETAH, based on the CKKS\nscheme. To enhance computational efficiency, we introduce three novel caching\nmechanisms to pre-encrypt radix values or frequently occurring pixel values,\nsubstantially reducing redundant encryption operations. Extensive experiments\ndemonstrate that our approach achieves up to a 19-fold improvement in\nencryption speed compared to the original CKKS, while maintaining high image\nquality. Additionally, real-world image applications such as mean filtering,\nbrightness enhancement, image matching and watermarking are tested based on\nFHE, showcasing up to a 91.53% speed improvement. We also proved that our\nmethod is IND-CPA (Indistinguishability under Chosen Plaintext Attack) secure,\nproviding strong encryption security. These results underscore the practicality\nand efficiency of iCHEETAH, marking a significant advancement in\nprivacy-preserving image processing at scale."
                },
                "authors": [
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Shubing Yang"
                    },
                    {
                        "name": "Xiaoyan Sun"
                    },
                    {
                        "name": "Jun Dai"
                    },
                    {
                        "name": "Dongfang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Dongfang Zhao"
                },
                "author": "Dongfang Zhao",
                "arxiv_comment": "16 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03568v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03568v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.0; K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02088v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02088v2",
                "updated": "2024-09-05T01:12:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    1,
                    12,
                    4,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-03T17:40:24Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    17,
                    40,
                    24,
                    1,
                    247,
                    0
                ],
                "title": "SELCC: Coherent Caching over Compute-Limited Disaggregated Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SELCC: Coherent Caching over Compute-Limited Disaggregated Memory"
                },
                "summary": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in data centers. It is important to cache data in the compute\nnodes and maintain cache coherence across multiple compute nodes to save on\nround-trip communication cost between the disaggregated memory and the compute\nnodes. However, the limited computing power on the disaggregated memory servers\nmakes it challenging to maintain cache coherence among multiple compute-side\ncaches over disaggregated shared memory. This paper introduces SELCC; a\nShared-Exclusive Latch Cache Coherence protocol that maintains cache coherence\nwithout imposing any computational burden on the remote memory side. SELCC\nbuilds on a one-sided shared-exclusive latch protocol by introducing lazy latch\nrelease and invalidation messages among the compute nodes so that it can\nguarantee both data access atomicity and cache coherence. SELCC minimizes\ncommunication round-trips by embedding the current cache copy holder IDs into\nRDMA latch words and prioritizes local concurrency control over global\nconcurrency control. We instantiate the SELCC protocol onto compute-sided\ncache, forming an abstraction layer over disaggregated memory. This abstraction\nlayer provides main-memory-like APIs to upper-level applications, and thus\nenabling existing data structures and algorithms to function over disaggregated\nmemory with minimal code change. To demonstrate the usability of SELCC, we\nimplement a B-tree and three transaction concurrency control algorithms over\nSELCC's APIs. Micro-benchmark results show that the SELCC protocol achieves\nbetter performance compared to RPC-based cache-coherence protocols.\nAdditionally, YCSB and TPC-C benchmarks indicate that applications over SELCC\ncan achieve comparable or superior performance against competitors over\ndisaggregated memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in data centers. It is important to cache data in the compute\nnodes and maintain cache coherence across multiple compute nodes to save on\nround-trip communication cost between the disaggregated memory and the compute\nnodes. However, the limited computing power on the disaggregated memory servers\nmakes it challenging to maintain cache coherence among multiple compute-side\ncaches over disaggregated shared memory. This paper introduces SELCC; a\nShared-Exclusive Latch Cache Coherence protocol that maintains cache coherence\nwithout imposing any computational burden on the remote memory side. SELCC\nbuilds on a one-sided shared-exclusive latch protocol by introducing lazy latch\nrelease and invalidation messages among the compute nodes so that it can\nguarantee both data access atomicity and cache coherence. SELCC minimizes\ncommunication round-trips by embedding the current cache copy holder IDs into\nRDMA latch words and prioritizes local concurrency control over global\nconcurrency control. We instantiate the SELCC protocol onto compute-sided\ncache, forming an abstraction layer over disaggregated memory. This abstraction\nlayer provides main-memory-like APIs to upper-level applications, and thus\nenabling existing data structures and algorithms to function over disaggregated\nmemory with minimal code change. To demonstrate the usability of SELCC, we\nimplement a B-tree and three transaction concurrency control algorithms over\nSELCC's APIs. Micro-benchmark results show that the SELCC protocol achieves\nbetter performance compared to RPC-based cache-coherence protocols.\nAdditionally, YCSB and TPC-C benchmarks indicate that applications over SELCC\ncan achieve comparable or superior performance against competitors over\ndisaggregated memory."
                },
                "authors": [
                    {
                        "name": "Ruihong Wang"
                    },
                    {
                        "name": "Jianguo Wang"
                    },
                    {
                        "name": "Walid G. Aref"
                    }
                ],
                "author_detail": {
                    "name": "Walid G. Aref"
                },
                "author": "Walid G. Aref",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02088v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02088v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10443v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10443v3",
                "updated": "2024-09-05T01:06:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    1,
                    6,
                    40,
                    3,
                    249,
                    0
                ],
                "published": "2024-05-16T21:07:42Z",
                "published_parsed": [
                    2024,
                    5,
                    16,
                    21,
                    7,
                    42,
                    3,
                    137,
                    0
                ],
                "title": "Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in\n  Fine-tuning LLMs for Simultaneous Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in\n  Fine-tuning LLMs for Simultaneous Translation"
                },
                "summary": "Large language models (LLMs) have achieved state-of-the-art performance in\nvarious language processing tasks, motivating their adoption in simultaneous\ntranslation. Current fine-tuning methods to adapt LLMs for simultaneous\ntranslation focus on prompting optimization strategies using either data\naugmentation or prompt structure modifications. However, these methods suffer\nfrom several issues, such as unnecessarily expanded training sets,\ncomputational inefficiency from dumping the key and value cache, increased\nprompt sizes, or restriction to a single decision policy. To eliminate these\nissues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs\nfor simultaneous translation. It utilizes a novel attention mask approach that\nmodels simultaneous translation during fine-tuning by masking attention for a\ndesired decision policy. Applying the proposed SimulMask on a Falcon LLM for\nthe IWSLT 2017 dataset, we have observed a significant translation quality\nimprovement compared to state-of-the-art prompting optimization strategies on\nfive language pairs while reducing the computational cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved state-of-the-art performance in\nvarious language processing tasks, motivating their adoption in simultaneous\ntranslation. Current fine-tuning methods to adapt LLMs for simultaneous\ntranslation focus on prompting optimization strategies using either data\naugmentation or prompt structure modifications. However, these methods suffer\nfrom several issues, such as unnecessarily expanded training sets,\ncomputational inefficiency from dumping the key and value cache, increased\nprompt sizes, or restriction to a single decision policy. To eliminate these\nissues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs\nfor simultaneous translation. It utilizes a novel attention mask approach that\nmodels simultaneous translation during fine-tuning by masking attention for a\ndesired decision policy. Applying the proposed SimulMask on a Falcon LLM for\nthe IWSLT 2017 dataset, we have observed a significant translation quality\nimprovement compared to state-of-the-art prompting optimization strategies on\nfive language pairs while reducing the computational cost."
                },
                "authors": [
                    {
                        "name": "Matthew Raffel"
                    },
                    {
                        "name": "Victor Agostinelli"
                    },
                    {
                        "name": "Lizhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lizhong Chen"
                },
                "author": "Lizhong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10443v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10443v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.04985v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.04985v6",
                "updated": "2024-09-04T10:04:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    10,
                    4,
                    52,
                    2,
                    248,
                    0
                ],
                "published": "2023-12-08T11:47:35Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    11,
                    47,
                    35,
                    4,
                    342,
                    0
                ],
                "title": "SparQ Attention: Bandwidth-Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparQ Attention: Bandwidth-Efficient LLM Inference"
                },
                "summary": "The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data transfers without substantial drops in accuracy, by evaluating\nLlama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data transfers without substantial drops in accuracy, by evaluating\nLlama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream\ntasks."
                },
                "authors": [
                    {
                        "name": "Luka Ribar"
                    },
                    {
                        "name": "Ivan Chelombiev"
                    },
                    {
                        "name": "Luke Hudlass-Galley"
                    },
                    {
                        "name": "Charlie Blake"
                    },
                    {
                        "name": "Carlo Luschi"
                    },
                    {
                        "name": "Douglas Orr"
                    }
                ],
                "author_detail": {
                    "name": "Douglas Orr"
                },
                "author": "Douglas Orr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.04985v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.04985v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02480v1",
                "updated": "2024-09-04T07:13:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    7,
                    13,
                    1,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T07:13:01Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    7,
                    13,
                    1,
                    2,
                    248,
                    0
                ],
                "title": "A brown dwarf orbiting around the planetary-nebula central binary KV Vel",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A brown dwarf orbiting around the planetary-nebula central binary KV Vel"
                },
                "summary": "KV Vel is a non-eclipsing short-period (P = 0.3571 days) close binary\ncontaining a very hot subdwarf primary (77000 K) and a cool low-mass secondary\nstar (3400 K) that is located at the center of the planetary nebula DS 1. The\nchanges in the orbital period of the close binary were analyzed based on 262\nnew times of light maximum together with those compiled from the literature. It\nis discovered that the O-C curve shows a small-amplitude (0.0034 days) cyclic\nperiod variation with a period of 29.55 years. The explanation by the\nsolar-type magnetic activity cycles of the cool component is ruled out because\nthe required energies are much larger than the total radiant energy of this\ncomponent in a whole cycle. Therefore, the cyclic variation was plausibly\nexplained as the light-travel time effect via the presence of a tertiary\ncomponent, which is supported by the periodic changes of the O-C curve and the\nrather symmetric and stable light curves obtained by TESS. The mass of the\ntertiary companion is determined to be M_3sini' = 0.060(7) M_sun. If the third\nbody is coplanar with the central binary (i.e., i' = 62.5{\\deg}), the mass of\nthe tertiary component is computed as M_3 ~ 0.068 M\\sun, and thus it would be\nbelow the stable hydrogen-burning limit and is a brown dwarf. The orbital\nseparation is shorter than 9.35 astronomical units (AU). KV Vel together with\nits surrounding planetary nebula and the brown-dwarf companion may be formed\nthrough the common-envelope evolution after the primary filled its Roche lobe\nduring the early asymptotic giant branch stage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Vel is a non-eclipsing short-period (P = 0.3571 days) close binary\ncontaining a very hot subdwarf primary (77000 K) and a cool low-mass secondary\nstar (3400 K) that is located at the center of the planetary nebula DS 1. The\nchanges in the orbital period of the close binary were analyzed based on 262\nnew times of light maximum together with those compiled from the literature. It\nis discovered that the O-C curve shows a small-amplitude (0.0034 days) cyclic\nperiod variation with a period of 29.55 years. The explanation by the\nsolar-type magnetic activity cycles of the cool component is ruled out because\nthe required energies are much larger than the total radiant energy of this\ncomponent in a whole cycle. Therefore, the cyclic variation was plausibly\nexplained as the light-travel time effect via the presence of a tertiary\ncomponent, which is supported by the periodic changes of the O-C curve and the\nrather symmetric and stable light curves obtained by TESS. The mass of the\ntertiary companion is determined to be M_3sini' = 0.060(7) M_sun. If the third\nbody is coplanar with the central binary (i.e., i' = 62.5{\\deg}), the mass of\nthe tertiary component is computed as M_3 ~ 0.068 M\\sun, and thus it would be\nbelow the stable hydrogen-burning limit and is a brown dwarf. The orbital\nseparation is shorter than 9.35 astronomical units (AU). KV Vel together with\nits surrounding planetary nebula and the brown-dwarf companion may be formed\nthrough the common-envelope evolution after the primary filled its Roche lobe\nduring the early asymptotic giant branch stage."
                },
                "authors": [
                    {
                        "name": "S. -B. Qian"
                    },
                    {
                        "name": "L. -Y. Zhu"
                    },
                    {
                        "name": "F. -X. Li"
                    },
                    {
                        "name": "L. -J. Li"
                    },
                    {
                        "name": "Z. -T. Han"
                    },
                    {
                        "name": "J. -J. He"
                    },
                    {
                        "name": "L. Zang"
                    },
                    {
                        "name": "L. -F. Chang"
                    },
                    {
                        "name": "Q. -B. Sun"
                    },
                    {
                        "name": "M. -Y. Li"
                    },
                    {
                        "name": "H. -T. Zhang"
                    },
                    {
                        "name": "F. -Z. Yan"
                    }
                ],
                "author_detail": {
                    "name": "F. -Z. Yan"
                },
                "author": "F. -Z. Yan",
                "arxiv_doi": "10.3847/1538-4357/ad631a",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4357/ad631a",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.02480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01990v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01990v1",
                "updated": "2024-09-03T15:35:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    15,
                    35,
                    1,
                    1,
                    247,
                    0
                ],
                "published": "2024-09-03T15:35:01Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    15,
                    35,
                    1,
                    1,
                    247,
                    0
                ],
                "title": "Contemporary Model Compression on Large Language Models Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary Model Compression on Large Language Models Inference"
                },
                "summary": "Large Language Models (LLMs) have revolutionized natural language processing\nby achieving state-of-the-art results across a variety of tasks. However, the\ncomputational demands of LLM inference, including high memory consumption and\nslow processing speeds, pose significant challenges for real-world\napplications, particularly on resource-constrained devices. Efficient inference\nis crucial for scaling the deployment of LLMs to a broader range of platforms,\nincluding mobile and edge devices.\n  This survey explores contemporary techniques in model compression that\naddress these challenges by reducing the size and computational requirements of\nLLMs while maintaining their performance. We focus on model-level compression\nmethods, including quantization, knowledge distillation, and pruning, as well\nas system-level optimizations like KV cache efficient design. Each of these\nmethodologies offers a unique approach to optimizing LLMs, from reducing\nnumerical precision to transferring knowledge between models and structurally\nsimplifying neural networks. Additionally, we discuss emerging trends in\nsystem-level design that further enhance the efficiency of LLM inference. This\nsurvey aims to provide a comprehensive overview of current advancements in\nmodel compression and their potential to make LLMs more accessible and\npractical for diverse applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized natural language processing\nby achieving state-of-the-art results across a variety of tasks. However, the\ncomputational demands of LLM inference, including high memory consumption and\nslow processing speeds, pose significant challenges for real-world\napplications, particularly on resource-constrained devices. Efficient inference\nis crucial for scaling the deployment of LLMs to a broader range of platforms,\nincluding mobile and edge devices.\n  This survey explores contemporary techniques in model compression that\naddress these challenges by reducing the size and computational requirements of\nLLMs while maintaining their performance. We focus on model-level compression\nmethods, including quantization, knowledge distillation, and pruning, as well\nas system-level optimizations like KV cache efficient design. Each of these\nmethodologies offers a unique approach to optimizing LLMs, from reducing\nnumerical precision to transferring knowledge between models and structurally\nsimplifying neural networks. Additionally, we discuss emerging trends in\nsystem-level design that further enhance the efficiency of LLM inference. This\nsurvey aims to provide a comprehensive overview of current advancements in\nmodel compression and their potential to make LLMs more accessible and\npractical for diverse applications."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Liu"
                },
                "author": "Dong Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01990v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01990v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01890v1",
                "updated": "2024-09-03T13:29:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    13,
                    29,
                    13,
                    1,
                    247,
                    0
                ],
                "published": "2024-09-03T13:29:13Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    13,
                    29,
                    13,
                    1,
                    247,
                    0
                ],
                "title": "A Fresh Take on Stale Embeddings: Improving Dense Retriever Training\n  with Corrector Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Fresh Take on Stale Embeddings: Improving Dense Retriever Training\n  with Corrector Networks"
                },
                "summary": "In dense retrieval, deep encoders provide embeddings for both inputs and\ntargets, and the softmax function is used to parameterize a distribution over a\nlarge number of candidate targets (e.g., textual passages for information\nretrieval). Significant challenges arise in training such encoders in the\nincreasingly prevalent scenario of (1) a large number of targets, (2) a\ncomputationally expensive target encoder model, (3) cached target embeddings\nthat are out-of-date due to ongoing training of target encoder parameters. This\npaper presents a simple and highly scalable response to these challenges by\ntraining a small parametric corrector network that adjusts stale cached target\nembeddings, enabling an accurate softmax approximation and thereby sampling of\nup-to-date high scoring \"hard negatives.\" We theoretically investigate the\ngeneralization properties of our proposed target corrector, relating the\ncomplexity of the network, staleness of cached representations, and the amount\nof training data. We present experimental results on large benchmark dense\nretrieval datasets as well as on QA with retrieval augmented language models.\nOur approach matches state-of-the-art results even when no target embedding\nupdates are made during training beyond an initial cache from the unsupervised\npre-trained model, providing a 4-80x reduction in re-embedding computational\ncost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In dense retrieval, deep encoders provide embeddings for both inputs and\ntargets, and the softmax function is used to parameterize a distribution over a\nlarge number of candidate targets (e.g., textual passages for information\nretrieval). Significant challenges arise in training such encoders in the\nincreasingly prevalent scenario of (1) a large number of targets, (2) a\ncomputationally expensive target encoder model, (3) cached target embeddings\nthat are out-of-date due to ongoing training of target encoder parameters. This\npaper presents a simple and highly scalable response to these challenges by\ntraining a small parametric corrector network that adjusts stale cached target\nembeddings, enabling an accurate softmax approximation and thereby sampling of\nup-to-date high scoring \"hard negatives.\" We theoretically investigate the\ngeneralization properties of our proposed target corrector, relating the\ncomplexity of the network, staleness of cached representations, and the amount\nof training data. We present experimental results on large benchmark dense\nretrieval datasets as well as on QA with retrieval augmented language models.\nOur approach matches state-of-the-art results even when no target embedding\nupdates are made during training beyond an initial cache from the unsupervised\npre-trained model, providing a 4-80x reduction in re-embedding computational\ncost."
                },
                "authors": [
                    {
                        "name": "Nicholas Monath"
                    },
                    {
                        "name": "Will Grathwohl"
                    },
                    {
                        "name": "Michael Boratko"
                    },
                    {
                        "name": "Rob Fergus"
                    },
                    {
                        "name": "Andrew McCallum"
                    },
                    {
                        "name": "Manzil Zaheer"
                    }
                ],
                "author_detail": {
                    "name": "Manzil Zaheer"
                },
                "author": "Manzil Zaheer",
                "arxiv_comment": "ICML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02137v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02137v1",
                "updated": "2024-09-02T15:07:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    15,
                    7,
                    5,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T15:07:05Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    15,
                    7,
                    5,
                    0,
                    246,
                    0
                ],
                "title": "Reward Augmentation in Reinforcement Learning for Testing Distributed\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward Augmentation in Reinforcement Learning for Testing Distributed\n  Systems"
                },
                "summary": "Bugs in popular distributed protocol implementations have been the source of\nmany downtimes in popular internet services. We describe a randomized testing\napproach for distributed protocol implementations based on reinforcement\nlearning. Since the natural reward structure is very sparse, the key to\nsuccessful exploration in reinforcement learning is reward augmentation. We\nshow two different techniques that build on one another. First, we provide a\ndecaying exploration bonus based on the discovery of new states -- the reward\ndecays as the same state is visited multiple times. The exploration bonus\ncaptures the intuition from coverage-guided fuzzing of prioritizing new\ncoverage points; in contrast to other schemes, we show that taking the maximum\nof the bonus and the Q-value leads to more effective exploration. Second, we\nprovide waypoints to the algorithm as a sequence of predicates that capture\ninteresting semantic scenarios. Waypoints exploit designer insight about the\nprotocol and guide the exploration to ``interesting'' parts of the state space.\nOur reward structure ensures that new episodes can reliably get to deep\ninteresting states even without execution caching. We have implemented our\nalgorithm in Go. Our evaluation on three large benchmarks (RedisRaft, Etcd, and\nRSL) shows that our algorithm can significantly outperform baseline approaches\nin terms of coverage and bug finding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bugs in popular distributed protocol implementations have been the source of\nmany downtimes in popular internet services. We describe a randomized testing\napproach for distributed protocol implementations based on reinforcement\nlearning. Since the natural reward structure is very sparse, the key to\nsuccessful exploration in reinforcement learning is reward augmentation. We\nshow two different techniques that build on one another. First, we provide a\ndecaying exploration bonus based on the discovery of new states -- the reward\ndecays as the same state is visited multiple times. The exploration bonus\ncaptures the intuition from coverage-guided fuzzing of prioritizing new\ncoverage points; in contrast to other schemes, we show that taking the maximum\nof the bonus and the Q-value leads to more effective exploration. Second, we\nprovide waypoints to the algorithm as a sequence of predicates that capture\ninteresting semantic scenarios. Waypoints exploit designer insight about the\nprotocol and guide the exploration to ``interesting'' parts of the state space.\nOur reward structure ensures that new episodes can reliably get to deep\ninteresting states even without execution caching. We have implemented our\nalgorithm in Go. Our evaluation on three large benchmarks (RedisRaft, Etcd, and\nRSL) shows that our algorithm can significantly outperform baseline approaches\nin terms of coverage and bug finding."
                },
                "authors": [
                    {
                        "name": "Andrea Borgarelli"
                    },
                    {
                        "name": "Constantin Enea"
                    },
                    {
                        "name": "Rupak Majumdar"
                    },
                    {
                        "name": "Srinidhi Nagendra"
                    }
                ],
                "author_detail": {
                    "name": "Srinidhi Nagendra"
                },
                "author": "Srinidhi Nagendra",
                "arxiv_doi": "10.1145/3689779",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3689779",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.02137v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02137v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01066v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01066v1",
                "updated": "2024-09-02T08:41:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    8,
                    41,
                    45,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T08:41:45Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    8,
                    41,
                    45,
                    0,
                    246,
                    0
                ],
                "title": "Learning in Hybrid Active Inference Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning in Hybrid Active Inference Models"
                },
                "summary": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work in computational neuroscience has considered this\nfunctional integration of discrete and continuous variables during\ndecision-making under the formalism of active inference (Parr, Friston & de\nVries, 2017; Parr & Friston, 2018). However, their focus is on the expressive\nphysical implementation of categorical decisions and the hierarchical mixed\ngenerative model is assumed to be known. As a consequence, it is unclear how\nthis framework might be extended to learning. We therefore present a novel\nhierarchical hybrid active inference agent in which a high-level discrete\nactive inference planner sits above a low-level continuous active inference\ncontroller. We make use of recent work in recurrent switching linear dynamical\nsystems (rSLDS) which implement end-to-end learning of meaningful discrete\nrepresentations via the piecewise linear decomposition of complex continuous\ndynamics (Linderman et al., 2016). The representations learned by the rSLDS\ninform the structure of the hybrid decision-making agent and allow us to (1)\nspecify temporally-abstracted sub-goals in a method reminiscent of the options\nframework, (2) lift the exploration into discrete space allowing us to exploit\ninformation-theoretic exploration bonuses and (3) `cache' the approximate\nsolutions to low-level problems in the discrete planner. We apply our model to\nthe sparse Continuous Mountain Car task, demonstrating fast system\nidentification via enhanced exploration and successful planning through the\ndelineation of abstract sub-goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work in computational neuroscience has considered this\nfunctional integration of discrete and continuous variables during\ndecision-making under the formalism of active inference (Parr, Friston & de\nVries, 2017; Parr & Friston, 2018). However, their focus is on the expressive\nphysical implementation of categorical decisions and the hierarchical mixed\ngenerative model is assumed to be known. As a consequence, it is unclear how\nthis framework might be extended to learning. We therefore present a novel\nhierarchical hybrid active inference agent in which a high-level discrete\nactive inference planner sits above a low-level continuous active inference\ncontroller. We make use of recent work in recurrent switching linear dynamical\nsystems (rSLDS) which implement end-to-end learning of meaningful discrete\nrepresentations via the piecewise linear decomposition of complex continuous\ndynamics (Linderman et al., 2016). The representations learned by the rSLDS\ninform the structure of the hybrid decision-making agent and allow us to (1)\nspecify temporally-abstracted sub-goals in a method reminiscent of the options\nframework, (2) lift the exploration into discrete space allowing us to exploit\ninformation-theoretic exploration bonuses and (3) `cache' the approximate\nsolutions to low-level problems in the discrete planner. We apply our model to\nthe sparse Continuous Mountain Car task, demonstrating fast system\nidentification via enhanced exploration and successful planning through the\ndelineation of abstract sub-goals."
                },
                "authors": [
                    {
                        "name": "Poppy Collis"
                    },
                    {
                        "name": "Ryan Singh"
                    },
                    {
                        "name": "Paul F Kinghorn"
                    },
                    {
                        "name": "Christopher L Buckley"
                    }
                ],
                "author_detail": {
                    "name": "Christopher L Buckley"
                },
                "author": "Christopher L Buckley",
                "arxiv_comment": "11 pages (+ appendix). Accepted to the International Workshop on\n  Active Inference 2024. arXiv admin note: substantial text overlap with\n  arXiv:2408.10970",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01066v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01066v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00905v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00905v1",
                "updated": "2024-09-02T02:36:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    2,
                    36,
                    22,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T02:36:22Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    2,
                    36,
                    22,
                    0,
                    246,
                    0
                ],
                "title": "Throughput Optimization in Cache-aided Networks: An Opportunistic\n  Probing and Scheduling Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Throughput Optimization in Cache-aided Networks: An Opportunistic\n  Probing and Scheduling Approach"
                },
                "summary": "This paper addresses the challenges of throughput optimization in wireless\ncache-aided cooperative networks. We propose an opportunistic cooperative\nprobing and scheduling strategy for efficient content delivery. The strategy\ninvolves the base station probing the relaying channels and cache states of\nmultiple cooperative nodes, thereby enabling opportunistic user scheduling for\ncontent delivery. Leveraging the theory of Sequentially Planned Decision (SPD)\noptimization, we dynamically formulate decisions on cooperative probing and\nstopping time. Our proposed Reward Expected Thresholds (RET)-based strategy\noptimizes opportunistic probing and scheduling. This approach significantly\nenhances system throughput by exploiting gains from local caching, cooperative\ntransmission and time diversity. Simulations confirm the effectiveness and\npracticality of the proposed Media Access Control (MAC) strategy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the challenges of throughput optimization in wireless\ncache-aided cooperative networks. We propose an opportunistic cooperative\nprobing and scheduling strategy for efficient content delivery. The strategy\ninvolves the base station probing the relaying channels and cache states of\nmultiple cooperative nodes, thereby enabling opportunistic user scheduling for\ncontent delivery. Leveraging the theory of Sequentially Planned Decision (SPD)\noptimization, we dynamically formulate decisions on cooperative probing and\nstopping time. Our proposed Reward Expected Thresholds (RET)-based strategy\noptimizes opportunistic probing and scheduling. This approach significantly\nenhances system throughput by exploiting gains from local caching, cooperative\ntransmission and time diversity. Simulations confirm the effectiveness and\npracticality of the proposed Media Access Control (MAC) strategy."
                },
                "authors": [
                    {
                        "name": "Zhou Zhang"
                    },
                    {
                        "name": "Saman Atapattu"
                    },
                    {
                        "name": "Yizhu Wang"
                    },
                    {
                        "name": "Marco Di Renzo"
                    }
                ],
                "author_detail": {
                    "name": "Marco Di Renzo"
                },
                "author": "Marco Di Renzo",
                "arxiv_comment": "2024 IEEE GLOBECOM, Cape Town, South Africa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00905v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00905v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00876v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00876v1",
                "updated": "2024-09-02T00:05:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    0,
                    5,
                    20,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T00:05:20Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    0,
                    5,
                    20,
                    0,
                    246,
                    0
                ],
                "title": "Rapid GPU-Based Pangenome Graph Layout",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid GPU-Based Pangenome Graph Layout"
                },
                "summary": "Computational Pangenomics is an emerging field that studies genetic variation\nusing a graph structure encompassing multiple genomes. Visualizing pangenome\ngraphs is vital for understanding genome diversity. Yet, handling large graphs\ncan be challenging due to the high computational demands of the graph layout\nprocess.\n  In this work, we conduct a thorough performance characterization of a\nstate-of-the-art pangenome graph layout algorithm, revealing significant\ndata-level parallelism, which makes GPUs a promising option for compute\nacceleration. However, irregular data access and the algorithm's memory-bound\nnature present significant hurdles. To overcome these challenges, we develop a\nsolution implementing three key optimizations: a cache-friendly data layout,\ncoalesced random states, and warp merging. Additionally, we propose a\nquantitative metric for scalable evaluation of pangenome layout quality.\n  Evaluated on 24 human whole-chromosome pangenomes, our GPU-based solution\nachieves a 57.3x speedup over the state-of-the-art multithreaded CPU baseline\nwithout layout quality loss, reducing execution time from hours to minutes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational Pangenomics is an emerging field that studies genetic variation\nusing a graph structure encompassing multiple genomes. Visualizing pangenome\ngraphs is vital for understanding genome diversity. Yet, handling large graphs\ncan be challenging due to the high computational demands of the graph layout\nprocess.\n  In this work, we conduct a thorough performance characterization of a\nstate-of-the-art pangenome graph layout algorithm, revealing significant\ndata-level parallelism, which makes GPUs a promising option for compute\nacceleration. However, irregular data access and the algorithm's memory-bound\nnature present significant hurdles. To overcome these challenges, we develop a\nsolution implementing three key optimizations: a cache-friendly data layout,\ncoalesced random states, and warp merging. Additionally, we propose a\nquantitative metric for scalable evaluation of pangenome layout quality.\n  Evaluated on 24 human whole-chromosome pangenomes, our GPU-based solution\nachieves a 57.3x speedup over the state-of-the-art multithreaded CPU baseline\nwithout layout quality loss, reducing execution time from hours to minutes."
                },
                "authors": [
                    {
                        "name": "Jiajie Li"
                    },
                    {
                        "name": "Jan-Niklas Schmelzle"
                    },
                    {
                        "name": "Yixiao Du"
                    },
                    {
                        "name": "Simon Heumos"
                    },
                    {
                        "name": "Andrea Guarracino"
                    },
                    {
                        "name": "Giulia Guidi"
                    },
                    {
                        "name": "Pjotr Prins"
                    },
                    {
                        "name": "Erik Garrison"
                    },
                    {
                        "name": "Zhiru Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhiru Zhang"
                },
                "author": "Zhiru Zhang",
                "arxiv_comment": "SC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00876v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00876v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00364v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00364v1",
                "updated": "2024-08-31T06:33:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    31,
                    6,
                    33,
                    50,
                    5,
                    244,
                    0
                ],
                "published": "2024-08-31T06:33:50Z",
                "published_parsed": [
                    2024,
                    8,
                    31,
                    6,
                    33,
                    50,
                    5,
                    244,
                    0
                ],
                "title": "Resource Management for IRS-Assisted Full-Duplex Integrated Sensing,\n  Communication and Computing Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource Management for IRS-Assisted Full-Duplex Integrated Sensing,\n  Communication and Computing Systems"
                },
                "summary": "In this paper, we investigate an intelligent reflecting surface (IRS)\nassisted full-duplex (FD) integrated sensing, communication and computing\nsystem. Specifically, an FD base station (BS) provides service for uplink and\ndownlink transmission, and a local cache is connected to the BS through a\nbackhaul link to store data. Meanwhile, active sensing elements are deployed on\nthe IRS to receive target echo signals. On this basis, in order to evaluate the\noverall performance of the system under consideration, we propose a system\nutility maximization problem while ensuring the sensing quality, expressed as\nthe difference between the sum of communication throughput, total computation\nbits (offloading bits and local computation bits) and the total backhaul cost\nfor content delivery. This makes the problem difficult to solve due to the\nhighly non-convex coupling of the optimization variables. To effectively solve\nthis problem, we first design the most effective caching strategy. Then, we\ndevelop an algorithm based on weighted minimum mean square error, alternative\ndirection method of multipliers, majorization-minimization framework,\nsemi-definite relaxation techniques, and several complex transformations to\njointly solve the optimization variables. Finally, simulation results are\nprovided to verify the utility performance of the proposed algorithm and\ndemonstrate the advantages of the proposed scheme compared with the baseline\nscheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate an intelligent reflecting surface (IRS)\nassisted full-duplex (FD) integrated sensing, communication and computing\nsystem. Specifically, an FD base station (BS) provides service for uplink and\ndownlink transmission, and a local cache is connected to the BS through a\nbackhaul link to store data. Meanwhile, active sensing elements are deployed on\nthe IRS to receive target echo signals. On this basis, in order to evaluate the\noverall performance of the system under consideration, we propose a system\nutility maximization problem while ensuring the sensing quality, expressed as\nthe difference between the sum of communication throughput, total computation\nbits (offloading bits and local computation bits) and the total backhaul cost\nfor content delivery. This makes the problem difficult to solve due to the\nhighly non-convex coupling of the optimization variables. To effectively solve\nthis problem, we first design the most effective caching strategy. Then, we\ndevelop an algorithm based on weighted minimum mean square error, alternative\ndirection method of multipliers, majorization-minimization framework,\nsemi-definite relaxation techniques, and several complex transformations to\njointly solve the optimization variables. Finally, simulation results are\nprovided to verify the utility performance of the proposed algorithm and\ndemonstrate the advantages of the proposed scheme compared with the baseline\nscheme."
                },
                "authors": [
                    {
                        "name": "Wanming Hao"
                    },
                    {
                        "name": "Xue Wu"
                    },
                    {
                        "name": "Xingwang Li"
                    },
                    {
                        "name": "Gangcan Sun"
                    },
                    {
                        "name": "Qingqing Wu"
                    },
                    {
                        "name": "Liang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Liang Yang"
                },
                "author": "Liang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00364v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00364v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00344v1",
                "updated": "2024-08-31T04:20:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    31,
                    4,
                    20,
                    58,
                    5,
                    244,
                    0
                ],
                "published": "2024-08-31T04:20:58Z",
                "published_parsed": [
                    2024,
                    8,
                    31,
                    4,
                    20,
                    58,
                    5,
                    244,
                    0
                ],
                "title": ">3kV NiO/Ga2O3 Heterojunction Diodes with Space-Modulated Junction\n  Termination Extension and Sub-1V Turn-on",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": ">3kV NiO/Ga2O3 Heterojunction Diodes with Space-Modulated Junction\n  Termination Extension and Sub-1V Turn-on"
                },
                "summary": "This work demonstrates high-performance vertical NiO/Ga2O3 heterojunction\ndiodes (HJDs) with a 2-step space-modulated junction termination extension.\nDistinct from the current state-of-the-art Ga2O3 HJDs, we achieve breakdown\nvoltage exceeding 3 kV with a low turn on voltage (VON) of 0.8V, estimated at a\nforward current density (IF) of 1 A-cm-2. The measured devices exhibit\nexcellent turn-on characteristics achieving 100 A-cm-2 current density at a\nforward bias of 1.5V along with a low differential specific on-resistance\n(Ron,sp) of 4.4 m{\\Omega}-cm2. The SM-JTE was realized using concentric NiO\nrings with varying widths and spacing that approximates a gradual reduction in\nJTE charge. The unipolar figure of merit (FOM) calculated exceeds 2 GW-cm2 and\nis among the best reported for devices with a sub-1V turn-on. The fabricated\ndevices also displayed minimal change in forward I-V characteristics post\nreverse bias stress of 3 kV applied during breakdown voltage testing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work demonstrates high-performance vertical NiO/Ga2O3 heterojunction\ndiodes (HJDs) with a 2-step space-modulated junction termination extension.\nDistinct from the current state-of-the-art Ga2O3 HJDs, we achieve breakdown\nvoltage exceeding 3 kV with a low turn on voltage (VON) of 0.8V, estimated at a\nforward current density (IF) of 1 A-cm-2. The measured devices exhibit\nexcellent turn-on characteristics achieving 100 A-cm-2 current density at a\nforward bias of 1.5V along with a low differential specific on-resistance\n(Ron,sp) of 4.4 m{\\Omega}-cm2. The SM-JTE was realized using concentric NiO\nrings with varying widths and spacing that approximates a gradual reduction in\nJTE charge. The unipolar figure of merit (FOM) calculated exceeds 2 GW-cm2 and\nis among the best reported for devices with a sub-1V turn-on. The fabricated\ndevices also displayed minimal change in forward I-V characteristics post\nreverse bias stress of 3 kV applied during breakdown voltage testing."
                },
                "authors": [
                    {
                        "name": "Advait Gilankar"
                    },
                    {
                        "name": "Abishek Katta"
                    },
                    {
                        "name": "Nabasindhu Das"
                    },
                    {
                        "name": "Nidhin Kurian Kalarickal"
                    }
                ],
                "author_detail": {
                    "name": "Nidhin Kurian Kalarickal"
                },
                "author": "Nidhin Kurian Kalarickal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00184v1",
                "updated": "2024-08-30T18:04:53Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    18,
                    4,
                    53,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T18:04:53Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    18,
                    4,
                    53,
                    4,
                    243,
                    0
                ],
                "title": "Adaptive Multi-Resolution Encoding for Interactive Large-Scale Volume\n  Visualization through Functional Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Multi-Resolution Encoding for Interactive Large-Scale Volume\n  Visualization through Functional Approximation"
                },
                "summary": "Functional approximation as a high-order continuous representation provides a\nmore accurate value and gradient query compared to the traditional discrete\nvolume representation. Volume visualization directly rendered from functional\napproximation generates high-quality rendering results without high-order\nartifacts caused by trilinear interpolations. However, querying an encoded\nfunctional approximation is computationally expensive, especially when the\ninput dataset is large, making functional approximation impractical for\ninteractive visualization. In this paper, we proposed a novel functional\napproximation multi-resolution representation, Adaptive-FAM, which is\nlightweight and fast to query. We also design a GPU-accelerated out-of-core\nmulti-resolution volume visualization framework that directly utilizes the\nAdaptive-FAM representation to generate high-quality rendering with interactive\nresponsiveness. Our method can not only dramatically decrease the caching time,\none of the main contributors to input latency, but also effectively improve the\ncache hit rate through prefetching. Our approach significantly outperforms the\ntraditional function approximation method in terms of input latency while\nmaintaining comparable rendering quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Functional approximation as a high-order continuous representation provides a\nmore accurate value and gradient query compared to the traditional discrete\nvolume representation. Volume visualization directly rendered from functional\napproximation generates high-quality rendering results without high-order\nartifacts caused by trilinear interpolations. However, querying an encoded\nfunctional approximation is computationally expensive, especially when the\ninput dataset is large, making functional approximation impractical for\ninteractive visualization. In this paper, we proposed a novel functional\napproximation multi-resolution representation, Adaptive-FAM, which is\nlightweight and fast to query. We also design a GPU-accelerated out-of-core\nmulti-resolution volume visualization framework that directly utilizes the\nAdaptive-FAM representation to generate high-quality rendering with interactive\nresponsiveness. Our method can not only dramatically decrease the caching time,\none of the main contributors to input latency, but also effectively improve the\ncache hit rate through prefetching. Our approach significantly outperforms the\ntraditional function approximation method in terms of input latency while\nmaintaining comparable rendering quality."
                },
                "authors": [
                    {
                        "name": "Jianxin Sun"
                    },
                    {
                        "name": "David Lenz"
                    },
                    {
                        "name": "Hongfeng Yu"
                    },
                    {
                        "name": "Tom Peterka"
                    }
                ],
                "author_detail": {
                    "name": "Tom Peterka"
                },
                "author": "Tom Peterka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17178v1",
                "updated": "2024-08-30T10:26:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    10,
                    26,
                    50,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T10:26:50Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    10,
                    26,
                    50,
                    4,
                    243,
                    0
                ],
                "title": "Modelling the High-Voltage Grid Using Open Data for Europe and Beyond",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modelling the High-Voltage Grid Using Open Data for Europe and Beyond"
                },
                "summary": "This paper provides the background, methodology and validation for\nconstructing a representation of the European high-voltage grid, including and\nabove 200 kV, based on public data provided by OpenStreetMap. The\nmodel-independent grid dataset is published under the Open Data Commons Open\nDatabase (ODbL 1.0) licence and can be used for large-scale electricity as well\nas energy system modelling. The dataset and workflow are provided as part of\nPyPSA-Eur -- an open-source, sector-coupled optimisation model of the European\nenergy system. By integrating with the codebase for initiatives such as\nPyPSA-Earth, the value of open and maintainable high-voltage grid data extends\nto the global context. By accessing the latest data through the the Overpass\nturbo API, the dataset can be easily reconstructed and updated within minutes.\nTo assess the data quality, this paper further compares the dataset with\nofficial statistics and representative model runs using PyPSA-Eur based on\ndifferent electricity grid representations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides the background, methodology and validation for\nconstructing a representation of the European high-voltage grid, including and\nabove 200 kV, based on public data provided by OpenStreetMap. The\nmodel-independent grid dataset is published under the Open Data Commons Open\nDatabase (ODbL 1.0) licence and can be used for large-scale electricity as well\nas energy system modelling. The dataset and workflow are provided as part of\nPyPSA-Eur -- an open-source, sector-coupled optimisation model of the European\nenergy system. By integrating with the codebase for initiatives such as\nPyPSA-Earth, the value of open and maintainable high-voltage grid data extends\nto the global context. By accessing the latest data through the the Overpass\nturbo API, the dataset can be easily reconstructed and updated within minutes.\nTo assess the data quality, this paper further compares the dataset with\nofficial statistics and representative model runs using PyPSA-Eur based on\ndifferent electricity grid representations."
                },
                "authors": [
                    {
                        "name": "Bobby Xiong"
                    },
                    {
                        "name": "Davide Fioriti"
                    },
                    {
                        "name": "Fabian Neumann"
                    },
                    {
                        "name": "Iegor Riepin"
                    },
                    {
                        "name": "Tom Brown"
                    }
                ],
                "author_detail": {
                    "name": "Tom Brown"
                },
                "author": "Tom Brown",
                "arxiv_comment": "20 pages, 15 figures, 8 tables. For associated prebuilt electricity\n  network, see https://doi.org/10.5281/zenodo.13358976",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16967v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16967v1",
                "updated": "2024-08-30T02:01:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    2,
                    1,
                    56,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T02:01:56Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    2,
                    1,
                    56,
                    4,
                    243,
                    0
                ],
                "title": "MemLong: Memory-Augmented Retrieval for Long Text Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemLong: Memory-Augmented Retrieval for Long Text Modeling"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have yielded remarkable\nsuccess across diverse fields. However, handling long contexts remains a\nsignificant challenge for LLMs due to the quadratic time and space complexity\nof attention mechanisms and the growing memory consumption of the key-value\ncache during generation. This work introduces MemLong: Memory-Augmented\nRetrieval for Long Text Generation, a method designed to enhance the\ncapabilities of long-context language modeling by utilizing an external\nretriever for historical information retrieval. MemLong combines a\nnon-differentiable ``ret-mem'' module with a partially trainable decoder-only\nlanguage model and introduces a fine-grained, controllable retrieval attention\nmechanism that leverages semantic-level relevant chunks. Comprehensive\nevaluations on multiple long-context language modeling benchmarks demonstrate\nthat MemLong consistently outperforms other state-of-the-art LLMs. More\nimportantly, MemLong can extend the context length on a single 3090 GPU from 4k\nup to 80k. Our code is available at https://github.com/Bui1dMySea/MemLong",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have yielded remarkable\nsuccess across diverse fields. However, handling long contexts remains a\nsignificant challenge for LLMs due to the quadratic time and space complexity\nof attention mechanisms and the growing memory consumption of the key-value\ncache during generation. This work introduces MemLong: Memory-Augmented\nRetrieval for Long Text Generation, a method designed to enhance the\ncapabilities of long-context language modeling by utilizing an external\nretriever for historical information retrieval. MemLong combines a\nnon-differentiable ``ret-mem'' module with a partially trainable decoder-only\nlanguage model and introduces a fine-grained, controllable retrieval attention\nmechanism that leverages semantic-level relevant chunks. Comprehensive\nevaluations on multiple long-context language modeling benchmarks demonstrate\nthat MemLong consistently outperforms other state-of-the-art LLMs. More\nimportantly, MemLong can extend the context length on a single 3090 GPU from 4k\nup to 80k. Our code is available at https://github.com/Bui1dMySea/MemLong"
                },
                "authors": [
                    {
                        "name": "Weijie Liu"
                    },
                    {
                        "name": "Zecheng Tang"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16967v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.07975v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.07975v2",
                "updated": "2024-08-29T17:43:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    43,
                    26,
                    3,
                    242,
                    0
                ],
                "published": "2023-09-14T18:18:10Z",
                "published_parsed": [
                    2023,
                    9,
                    14,
                    18,
                    18,
                    10,
                    3,
                    257,
                    0
                ],
                "title": "Smart Helper-Aided F-RANs: Improving Delay and Reducing Fronthaul Load",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart Helper-Aided F-RANs: Improving Delay and Reducing Fronthaul Load"
                },
                "summary": "In traditional Fog-Radio Access Networks (F-RANs), enhanced remote radio\nheads (eRRHs) are connected to a macro base station (MBS) through fronthaul\nlinks. Deploying a massive number of eRRHs is not always feasible due to site\nconstraints and the cost of fronthaul links. This paper introduces an\ninnovative concept of using smart helpers (SHs) in F-RANs. These SHs do not\nrequire fronthaul links and listen to the nearby eRRHs' communications. Then,\nthey smartly select and cache popular content. This capability enables SHs to\nserve users with frequent on-demand service requests potentially. As such,\nnetwork operators have the flexibility to easily deploy SHs in various\nscenarios, such as dense urban areas and temporary public events, to expand\ntheir F-RANs and improve the quality of service (QoS). To study the performance\nof the proposed SH-aided F-RAN, we formulate an optimization problem of\nminimizing the average transmission delay that jointly optimizes cache\nresources and user scheduling. To tackle the formulated problem, we develop an\ninnovative multi-stage algorithm that uses a reinforcement learning (RL)\nframework. Various performance measures, e.g., the average transmission delay,\nfronthaul load, and cache hit rate of the proposed SH-aided F-RAN are evaluated\nnumerically and compared with those of traditional F-RANs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In traditional Fog-Radio Access Networks (F-RANs), enhanced remote radio\nheads (eRRHs) are connected to a macro base station (MBS) through fronthaul\nlinks. Deploying a massive number of eRRHs is not always feasible due to site\nconstraints and the cost of fronthaul links. This paper introduces an\ninnovative concept of using smart helpers (SHs) in F-RANs. These SHs do not\nrequire fronthaul links and listen to the nearby eRRHs' communications. Then,\nthey smartly select and cache popular content. This capability enables SHs to\nserve users with frequent on-demand service requests potentially. As such,\nnetwork operators have the flexibility to easily deploy SHs in various\nscenarios, such as dense urban areas and temporary public events, to expand\ntheir F-RANs and improve the quality of service (QoS). To study the performance\nof the proposed SH-aided F-RAN, we formulate an optimization problem of\nminimizing the average transmission delay that jointly optimizes cache\nresources and user scheduling. To tackle the formulated problem, we develop an\ninnovative multi-stage algorithm that uses a reinforcement learning (RL)\nframework. Various performance measures, e.g., the average transmission delay,\nfronthaul load, and cache hit rate of the proposed SH-aided F-RAN are evaluated\nnumerically and compared with those of traditional F-RANs."
                },
                "authors": [
                    {
                        "name": "Hesameddin Mokhtarzadeh"
                    },
                    {
                        "name": "Mohammed S. Al-Abiad"
                    },
                    {
                        "name": "Md Jahangir Hossain"
                    },
                    {
                        "name": "Julian Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Julian Cheng"
                },
                "author": "Julian Cheng",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.07975v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.07975v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16730v1",
                "updated": "2024-08-29T17:21:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    21,
                    58,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T17:21:58Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    21,
                    58,
                    3,
                    242,
                    0
                ],
                "title": "VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths\n  Vision Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths\n  Vision Computation"
                },
                "summary": "A well-known dilemma in large vision-language models (e.g., GPT-4, LLaVA) is\nthat while increasing the number of vision tokens generally enhances visual\nunderstanding, it also significantly raises memory and computational costs,\nespecially in long-term, dense video frame streaming scenarios. Although\nlearnable approaches like Q-Former and Perceiver Resampler have been developed\nto reduce the vision token burden, they overlook the context causally modeled\nby LLMs (i.e., key-value cache), potentially leading to missed visual cues when\naddressing user queries. In this paper, we introduce a novel approach to reduce\nvision compute by leveraging redundant vision tokens \"skipping layers\" rather\nthan decreasing the number of vision tokens. Our method, VideoLLM-MoD, is\ninspired by mixture-of-depths LLMs and addresses the challenge of numerous\nvision tokens in long-term or streaming video. Specifically, for each\ntransformer layer, we learn to skip the computation for a high proportion\n(e.g., 80\\%) of vision tokens, passing them directly to the next layer. This\napproach significantly enhances model efficiency, achieving approximately\n\\textasciitilde42\\% time and \\textasciitilde30\\% memory savings for the entire\ntraining. Moreover, our method reduces the computation in the context and avoid\ndecreasing the vision tokens, thus preserving or even improving performance\ncompared to the vanilla model. We conduct extensive experiments to demonstrate\nthe effectiveness of VideoLLM-MoD, showing its state-of-the-art results on\nmultiple benchmarks, including narration, forecasting, and summarization tasks\nin COIN, Ego4D, and Ego-Exo4D datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A well-known dilemma in large vision-language models (e.g., GPT-4, LLaVA) is\nthat while increasing the number of vision tokens generally enhances visual\nunderstanding, it also significantly raises memory and computational costs,\nespecially in long-term, dense video frame streaming scenarios. Although\nlearnable approaches like Q-Former and Perceiver Resampler have been developed\nto reduce the vision token burden, they overlook the context causally modeled\nby LLMs (i.e., key-value cache), potentially leading to missed visual cues when\naddressing user queries. In this paper, we introduce a novel approach to reduce\nvision compute by leveraging redundant vision tokens \"skipping layers\" rather\nthan decreasing the number of vision tokens. Our method, VideoLLM-MoD, is\ninspired by mixture-of-depths LLMs and addresses the challenge of numerous\nvision tokens in long-term or streaming video. Specifically, for each\ntransformer layer, we learn to skip the computation for a high proportion\n(e.g., 80\\%) of vision tokens, passing them directly to the next layer. This\napproach significantly enhances model efficiency, achieving approximately\n\\textasciitilde42\\% time and \\textasciitilde30\\% memory savings for the entire\ntraining. Moreover, our method reduces the computation in the context and avoid\ndecreasing the vision tokens, thus preserving or even improving performance\ncompared to the vanilla model. We conduct extensive experiments to demonstrate\nthe effectiveness of VideoLLM-MoD, showing its state-of-the-art results on\nmultiple benchmarks, including narration, forecasting, and summarization tasks\nin COIN, Ego4D, and Ego-Exo4D datasets."
                },
                "authors": [
                    {
                        "name": "Shiwei Wu"
                    },
                    {
                        "name": "Joya Chen"
                    },
                    {
                        "name": "Kevin Qinghong Lin"
                    },
                    {
                        "name": "Qimeng Wang"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Qianli Xu"
                    },
                    {
                        "name": "Tong Xu"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Enhong Chen"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    }
                ],
                "author_detail": {
                    "name": "Mike Zheng Shou"
                },
                "author": "Mike Zheng Shou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05527v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05527v3",
                "updated": "2024-08-29T16:48:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    16,
                    48,
                    58,
                    3,
                    242,
                    0
                ],
                "published": "2024-03-08T18:48:30Z",
                "published_parsed": [
                    2024,
                    3,
                    8,
                    18,
                    48,
                    30,
                    4,
                    68,
                    0
                ],
                "title": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM"
                },
                "summary": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR."
                },
                "authors": [
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Qingru Zhang"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Geonhwa Jeong"
                    },
                    {
                        "name": "Zaoxing Liu"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Tuo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Tuo Zhao"
                },
                "author": "Tuo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05527v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05527v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16220v1",
                "updated": "2024-08-29T02:31:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    31,
                    28,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T02:31:28Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    31,
                    28,
                    3,
                    242,
                    0
                ],
                "title": "LightSLH: Provable and Low-Overhead Spectre v1 Mitigation through\n  Targeted Instruction Hardening",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightSLH: Provable and Low-Overhead Spectre v1 Mitigation through\n  Targeted Instruction Hardening"
                },
                "summary": "Several software mitigations have been proposed to defend against Spectre\nvulnerabilities. However, these countermeasures often suffer from high\nperformance overhead, largely due to unnecessary protections. We propose\nLightSLH, designed to mitigate this overhead by hardening instructions only\nwhen they are under threat from Spectre vulnerabilities. LightSLH leverages\nprogram analysis techniques based on abstract interpretation to identify all\ninstructions that could potentially lead to Spectre vulnerabilities and\nprovides provable protection. To enhance analysis efficiency and precision,\nLightSLH employs novel taint and value domains. The taint domain enables\nbit-level taint tracking, while the value domain allows LightSLH to analyze\ncomplex program structures such as pointers and structures. Furthermore,\nLightSLH uses a two-stage abstract interpretation approach to circumvent\npotential analysis paralysis issues.\n  We demonstrate the security guarantees of LightSLH and evaluate its\nperformance on cryptographic algorithm implementations from OpenSSL. LightSLH\nsignificantly reduces the overhead associated with speculative-load-hardening\ntechniques. Our results show that LightSLH introduces no protection and thus no\noverhead on 4 out of the 7 studied algorithms, which contrasts with existing\ncountermeasures that introduce additional overhead due to unnecessary\nhardening. Additionally, LightSLH performs, for the first time, a rigorous\nanalysis of the security guarantees of RSA against Spectre v1, highlighting\nthat the memory access patterns generated by the scatter-gather algorithm\ndepend on secrets, even for observers at the cache line granularity,\nnecessitating protection for such accesses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several software mitigations have been proposed to defend against Spectre\nvulnerabilities. However, these countermeasures often suffer from high\nperformance overhead, largely due to unnecessary protections. We propose\nLightSLH, designed to mitigate this overhead by hardening instructions only\nwhen they are under threat from Spectre vulnerabilities. LightSLH leverages\nprogram analysis techniques based on abstract interpretation to identify all\ninstructions that could potentially lead to Spectre vulnerabilities and\nprovides provable protection. To enhance analysis efficiency and precision,\nLightSLH employs novel taint and value domains. The taint domain enables\nbit-level taint tracking, while the value domain allows LightSLH to analyze\ncomplex program structures such as pointers and structures. Furthermore,\nLightSLH uses a two-stage abstract interpretation approach to circumvent\npotential analysis paralysis issues.\n  We demonstrate the security guarantees of LightSLH and evaluate its\nperformance on cryptographic algorithm implementations from OpenSSL. LightSLH\nsignificantly reduces the overhead associated with speculative-load-hardening\ntechniques. Our results show that LightSLH introduces no protection and thus no\noverhead on 4 out of the 7 studied algorithms, which contrasts with existing\ncountermeasures that introduce additional overhead due to unnecessary\nhardening. Additionally, LightSLH performs, for the first time, a rigorous\nanalysis of the security guarantees of RSA against Spectre v1, highlighting\nthat the memory access patterns generated by the scatter-gather algorithm\ndepend on secrets, even for observers at the cache line granularity,\nnecessitating protection for such accesses."
                },
                "authors": [
                    {
                        "name": "Yiming Zhu"
                    },
                    {
                        "name": "Wenchao Huang"
                    },
                    {
                        "name": "Yan Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Yan Xiong"
                },
                "author": "Yan Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.06942v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.06942v3",
                "updated": "2024-08-28T08:41:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    8,
                    41,
                    45,
                    2,
                    241,
                    0
                ],
                "published": "2023-06-12T08:24:14Z",
                "published_parsed": [
                    2023,
                    6,
                    12,
                    8,
                    24,
                    14,
                    0,
                    163,
                    0
                ],
                "title": "RIP Linked List",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RIP Linked List"
                },
                "summary": "Linked lists have long served as a valuable teaching tool in programming.\nHowever, the question arises: Are they truly practical for everyday program\nuse? In most cases, it appears that array-based data structures offer distinct\nadvantages, particularly in terms of memory efficiency and,more importantly,\nexecution speed. While it's relatively straightforward to calculate the\ncomplexity of operations, gauging actual execution efficiency remains a\nchallenge. This paper addresses this question by introducing a new benchmark.\nOur study compares various linked list implementations with several array-based\nalternatives. We also demonstrate the ease of incorporating memory caching for\nlinked lists, enhancing their performance. Additionally, we introduce a new\narray-based data structure designed to excel in a wide range of operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linked lists have long served as a valuable teaching tool in programming.\nHowever, the question arises: Are they truly practical for everyday program\nuse? In most cases, it appears that array-based data structures offer distinct\nadvantages, particularly in terms of memory efficiency and,more importantly,\nexecution speed. While it's relatively straightforward to calculate the\ncomplexity of operations, gauging actual execution efficiency remains a\nchallenge. This paper addresses this question by introducing a new benchmark.\nOur study compares various linked list implementations with several array-based\nalternatives. We also demonstrate the ease of incorporating memory caching for\nlinked lists, enhancing their performance. Additionally, we introduce a new\narray-based data structure designed to excel in a wide range of operations."
                },
                "authors": [
                    {
                        "name": "Benoît Sonntag"
                    },
                    {
                        "name": "Dominique Colnet"
                    }
                ],
                "author_detail": {
                    "name": "Dominique Colnet"
                },
                "arxiv_affiliation": "LORIA",
                "author": "Dominique Colnet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.06942v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.06942v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17678v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17678v2",
                "updated": "2024-08-27T22:06:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    22,
                    6,
                    20,
                    1,
                    240,
                    0
                ],
                "published": "2024-07-25T00:27:07Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    0,
                    27,
                    7,
                    3,
                    207,
                    0
                ],
                "title": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads"
                },
                "summary": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM."
                },
                "authors": [
                    {
                        "name": "Xihui Lin"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Barun Patra"
                    },
                    {
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Xia Song"
                    }
                ],
                "author_detail": {
                    "name": "Xia Song"
                },
                "author": "Xia Song",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17678v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17678v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.06893v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.06893v3",
                "updated": "2024-08-27T17:30:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    30,
                    41,
                    1,
                    240,
                    0
                ],
                "published": "2023-12-11T23:34:23Z",
                "published_parsed": [
                    2023,
                    12,
                    11,
                    23,
                    34,
                    23,
                    0,
                    345,
                    0
                ],
                "title": "Styx: Transactional Stateful Functions on Streaming Dataflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Styx: Transactional Stateful Functions on Streaming Dataflows"
                },
                "summary": "Developing stateful cloud applications, such as low-latency workflows and\nmicroservices with strict consistency requirements, remains arduous for\nprogrammers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve\nthese use cases. However, existing approaches either provide serializable\ntransactional guarantees at the level of individual functions, or separate\napplication logic from the state and use inefficient transactional protocols.\nThese design choices increase the execution latency, limiting the adoption of\nSFaaS systems.\n  In this paper, we present Styx, a novel SFaaS runtime that executes\nserializable transactions across functions with exactly-once guarantees. Styx\nextends a deterministic transactional protocol to support an arbitrary call\ngraph of stateful functions. It introduces a transaction-execution\nacknowledgment scheme that allows tracking a transactional workflow's SFaaS\ncalls, guaranteeing atomicity and exactly-once processing. Finally, Styx\nfeatures a function-execution caching mechanism and early transactional commit\nreplies for optimized performance. Experiments with the YCSB-T, TPC-C, and\nDeathstar benchmarks show that Styx outperforms state-of-the-art approaches by\nachieving at least one order of magnitude higher throughput while exhibiting\nnear-linear scalability and low latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing stateful cloud applications, such as low-latency workflows and\nmicroservices with strict consistency requirements, remains arduous for\nprogrammers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve\nthese use cases. However, existing approaches either provide serializable\ntransactional guarantees at the level of individual functions, or separate\napplication logic from the state and use inefficient transactional protocols.\nThese design choices increase the execution latency, limiting the adoption of\nSFaaS systems.\n  In this paper, we present Styx, a novel SFaaS runtime that executes\nserializable transactions across functions with exactly-once guarantees. Styx\nextends a deterministic transactional protocol to support an arbitrary call\ngraph of stateful functions. It introduces a transaction-execution\nacknowledgment scheme that allows tracking a transactional workflow's SFaaS\ncalls, guaranteeing atomicity and exactly-once processing. Finally, Styx\nfeatures a function-execution caching mechanism and early transactional commit\nreplies for optimized performance. Experiments with the YCSB-T, TPC-C, and\nDeathstar benchmarks show that Styx outperforms state-of-the-art approaches by\nachieving at least one order of magnitude higher throughput while exhibiting\nnear-linear scalability and low latency."
                },
                "authors": [
                    {
                        "name": "Kyriakos Psarakis"
                    },
                    {
                        "name": "George Siachamis"
                    },
                    {
                        "name": "George Christodoulou"
                    },
                    {
                        "name": "Marios Fragkoulis"
                    },
                    {
                        "name": "Asterios Katsifodimos"
                    }
                ],
                "author_detail": {
                    "name": "Asterios Katsifodimos"
                },
                "author": "Asterios Katsifodimos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.06893v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.06893v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14906v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14906v1",
                "updated": "2024-08-27T09:34:38Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    9,
                    34,
                    38,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T09:34:38Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    9,
                    34,
                    38,
                    1,
                    240,
                    0
                ],
                "title": "Writing in the Margins: Better Inference Pattern for Long Context\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Writing in the Margins: Better Inference Pattern for Long Context\n  Retrieval"
                },
                "summary": "In this paper, we introduce Writing in the Margins (WiM), a new inference\npattern for Large Language Models designed to optimize the handling of long\ninput sequences in retrieval-oriented tasks. This approach leverages the\nchunked prefill of the key-value cache to perform segment-wise inference, which\nenables efficient processing of extensive contexts along with the generation\nand classification of intermediate information (\"margins\") that guide the model\ntowards specific tasks. This method increases computational overhead marginally\nwhile significantly enhancing the performance of off-the-shelf models without\nthe need for fine-tuning. Specifically, we observe that WiM provides an average\nenhancement of 7.5% in accuracy for reasoning skills (HotpotQA, MultiHop-RAG)\nand more than a 30.0% increase in the F1-score for aggregation tasks (CWE).\nAdditionally, we show how the proposed pattern fits into an interactive\nretrieval design that provides end-users with ongoing updates about the\nprogress of context processing, and pinpoints the integration of relevant\ninformation into the final response. We release our implementation of WiM using\nHugging Face Transformers library at\nhttps://github.com/writer/writing-in-the-margins.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Writing in the Margins (WiM), a new inference\npattern for Large Language Models designed to optimize the handling of long\ninput sequences in retrieval-oriented tasks. This approach leverages the\nchunked prefill of the key-value cache to perform segment-wise inference, which\nenables efficient processing of extensive contexts along with the generation\nand classification of intermediate information (\"margins\") that guide the model\ntowards specific tasks. This method increases computational overhead marginally\nwhile significantly enhancing the performance of off-the-shelf models without\nthe need for fine-tuning. Specifically, we observe that WiM provides an average\nenhancement of 7.5% in accuracy for reasoning skills (HotpotQA, MultiHop-RAG)\nand more than a 30.0% increase in the F1-score for aggregation tasks (CWE).\nAdditionally, we show how the proposed pattern fits into an interactive\nretrieval design that provides end-users with ongoing updates about the\nprogress of context processing, and pinpoints the integration of relevant\ninformation into the final response. We release our implementation of WiM using\nHugging Face Transformers library at\nhttps://github.com/writer/writing-in-the-margins."
                },
                "authors": [
                    {
                        "name": "Melisa Russak"
                    },
                    {
                        "name": "Umar Jamil"
                    },
                    {
                        "name": "Christopher Bryant"
                    },
                    {
                        "name": "Kiran Kamble"
                    },
                    {
                        "name": "Axel Magnuson"
                    },
                    {
                        "name": "Mateusz Russak"
                    },
                    {
                        "name": "Waseem AlShikh"
                    }
                ],
                "author_detail": {
                    "name": "Waseem AlShikh"
                },
                "author": "Waseem AlShikh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14906v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14906v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14735v1",
                "updated": "2024-08-27T02:03:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    2,
                    3,
                    36,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T02:03:36Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    2,
                    3,
                    36,
                    1,
                    240,
                    0
                ],
                "title": "PPVF: An Efficient Privacy-Preserving Online Video Fetching Framework\n  with Correlated Differential Privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PPVF: An Efficient Privacy-Preserving Online Video Fetching Framework\n  with Correlated Differential Privacy"
                },
                "summary": "Online video streaming has evolved into an integral component of the\ncontemporary Internet landscape. Yet, the disclosure of user requests presents\nformidable privacy challenges. As users stream their preferred online videos,\ntheir requests are automatically seized by video content providers, potentially\nleaking users' privacy.\n  Unfortunately, current protection methods are not well-suited to preserving\nuser request privacy from content providers while maintaining high-quality\nonline video services. To tackle this challenge, we introduce a novel\nPrivacy-Preserving Video Fetching (PPVF) framework, which utilizes trusted edge\ndevices to pre-fetch and cache videos, ensuring the privacy of users' requests\nwhile optimizing the efficiency of edge caching. More specifically, we design\nPPVF with three core components: (1) \\textit{Online privacy budget scheduler},\nwhich employs a theoretically guaranteed online algorithm to select\nnon-requested videos as candidates with assigned privacy budgets. Alternative\nvideos are chosen by an online algorithm that is theoretically guaranteed to\nconsider both video utilities and available privacy budgets. (2) \\textit{Noisy\nvideo request generator}, which generates redundant video requests (in addition\nto original ones) utilizing correlated differential privacy to obfuscate\nrequest privacy. (3) \\textit{Online video utility predictor}, which leverages\nfederated learning to collaboratively evaluate video utility in an online\nfashion, aiding in video selection in (1) and noise generation in (2). Finally,\nwe conduct extensive experiments using real-world video request traces from\nTencent Video. The results demonstrate that PPVF effectively safeguards user\nrequest privacy while upholding high video caching performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online video streaming has evolved into an integral component of the\ncontemporary Internet landscape. Yet, the disclosure of user requests presents\nformidable privacy challenges. As users stream their preferred online videos,\ntheir requests are automatically seized by video content providers, potentially\nleaking users' privacy.\n  Unfortunately, current protection methods are not well-suited to preserving\nuser request privacy from content providers while maintaining high-quality\nonline video services. To tackle this challenge, we introduce a novel\nPrivacy-Preserving Video Fetching (PPVF) framework, which utilizes trusted edge\ndevices to pre-fetch and cache videos, ensuring the privacy of users' requests\nwhile optimizing the efficiency of edge caching. More specifically, we design\nPPVF with three core components: (1) \\textit{Online privacy budget scheduler},\nwhich employs a theoretically guaranteed online algorithm to select\nnon-requested videos as candidates with assigned privacy budgets. Alternative\nvideos are chosen by an online algorithm that is theoretically guaranteed to\nconsider both video utilities and available privacy budgets. (2) \\textit{Noisy\nvideo request generator}, which generates redundant video requests (in addition\nto original ones) utilizing correlated differential privacy to obfuscate\nrequest privacy. (3) \\textit{Online video utility predictor}, which leverages\nfederated learning to collaboratively evaluate video utility in an online\nfashion, aiding in video selection in (1) and noise generation in (2). Finally,\nwe conduct extensive experiments using real-world video request traces from\nTencent Video. The results demonstrate that PPVF effectively safeguards user\nrequest privacy while upholding high video caching performance."
                },
                "authors": [
                    {
                        "name": "Xianzhi Zhang"
                    },
                    {
                        "name": "Yipeng Zhou"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Quan Z. Sheng"
                    },
                    {
                        "name": "Miao Hu"
                    },
                    {
                        "name": "Linchang Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Linchang Xiao"
                },
                "author": "Linchang Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10774v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10774v2",
                "updated": "2024-08-26T21:01:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    21,
                    1,
                    2,
                    0,
                    239,
                    0
                ],
                "published": "2024-06-16T01:33:02Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    1,
                    33,
                    2,
                    6,
                    168,
                    0
                ],
                "title": "Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference"
                },
                "summary": "As the demand for long-context large language models (LLMs) increases, models\nwith context windows of up to 128K or 1M tokens are becoming increasingly\nprevalent. However, long-context LLM inference is challenging since the\ninference speed decreases significantly as the sequence length grows. This\nslowdown is primarily caused by loading a large KV cache during self-attention.\nPrevious works have shown that a small portion of critical tokens will dominate\nthe attention outcomes. However, we observe the criticality of a token highly\ndepends on the query. To this end, we propose Quest, a query-aware KV cache\nselection algorithm. Quest keeps track of the minimal and maximal Key values in\nKV cache pages and estimates the criticality of a given page using Query\nvectors. By only loading the Top-K critical KV cache pages for attention, Quest\nsignificantly speeds up self-attention without sacrificing accuracy. We show\nthat Quest can achieve up to 2.23x self-attention speedup, which reduces\ninference latency by 7.03x while performing well on tasks with long\ndependencies with negligible accuracy loss. Code is available at\nhttp://github.com/mit-han-lab/Quest .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the demand for long-context large language models (LLMs) increases, models\nwith context windows of up to 128K or 1M tokens are becoming increasingly\nprevalent. However, long-context LLM inference is challenging since the\ninference speed decreases significantly as the sequence length grows. This\nslowdown is primarily caused by loading a large KV cache during self-attention.\nPrevious works have shown that a small portion of critical tokens will dominate\nthe attention outcomes. However, we observe the criticality of a token highly\ndepends on the query. To this end, we propose Quest, a query-aware KV cache\nselection algorithm. Quest keeps track of the minimal and maximal Key values in\nKV cache pages and estimates the criticality of a given page using Query\nvectors. By only loading the Top-K critical KV cache pages for attention, Quest\nsignificantly speeds up self-attention without sacrificing accuracy. We show\nthat Quest can achieve up to 2.23x self-attention speedup, which reduces\ninference latency by 7.03x while performing well on tasks with long\ndependencies with negligible accuracy loss. Code is available at\nhttp://github.com/mit-han-lab/Quest ."
                },
                "authors": [
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Yilong Zhao"
                    },
                    {
                        "name": "Kan Zhu"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Baris Kasikci"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "ICML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10774v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10774v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14434v1",
                "updated": "2024-08-26T17:21:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    21,
                    19,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T17:21:19Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    21,
                    19,
                    0,
                    239,
                    0
                ],
                "title": "Employing Artificial Intelligence to Steer Exascale Workflows with\n  Colmena",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Employing Artificial Intelligence to Steer Exascale Workflows with\n  Colmena"
                },
                "summary": "Computational workflows are a common class of application on supercomputers,\nyet the loosely coupled and heterogeneous nature of workflows often fails to\ntake full advantage of their capabilities. We created Colmena to leverage the\nmassive parallelism of a supercomputer by using Artificial Intelligence (AI) to\nlearn from and adapt a workflow as it executes. Colmena allows scientists to\ndefine how their application should respond to events (e.g., task completion)\nas a series of cooperative agents. In this paper, we describe the design of\nColmena, the challenges we overcame while deploying applications on exascale\nsystems, and the science workflows we have enhanced through interweaving AI.\nThe scaling challenges we discuss include developing steering strategies that\nmaximize node utilization, introducing data fabrics that reduce communication\noverhead of data-intensive tasks, and implementing workflow tasks that cache\ncostly operations between invocations. These innovations coupled with a variety\nof application patterns accessible through our agent-based steering model have\nenabled science advances in chemistry, biophysics, and materials science using\ndifferent types of AI. Our vision is that Colmena will spur creative solutions\nthat harness AI across many domains of scientific computing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational workflows are a common class of application on supercomputers,\nyet the loosely coupled and heterogeneous nature of workflows often fails to\ntake full advantage of their capabilities. We created Colmena to leverage the\nmassive parallelism of a supercomputer by using Artificial Intelligence (AI) to\nlearn from and adapt a workflow as it executes. Colmena allows scientists to\ndefine how their application should respond to events (e.g., task completion)\nas a series of cooperative agents. In this paper, we describe the design of\nColmena, the challenges we overcame while deploying applications on exascale\nsystems, and the science workflows we have enhanced through interweaving AI.\nThe scaling challenges we discuss include developing steering strategies that\nmaximize node utilization, introducing data fabrics that reduce communication\noverhead of data-intensive tasks, and implementing workflow tasks that cache\ncostly operations between invocations. These innovations coupled with a variety\nof application patterns accessible through our agent-based steering model have\nenabled science advances in chemistry, biophysics, and materials science using\ndifferent types of AI. Our vision is that Colmena will spur creative solutions\nthat harness AI across many domains of scientific computing."
                },
                "authors": [
                    {
                        "name": "Logan Ward"
                    },
                    {
                        "name": "J. Gregory Pauloski"
                    },
                    {
                        "name": "Valerie Hayot-Sasson"
                    },
                    {
                        "name": "Yadu Babuji"
                    },
                    {
                        "name": "Alexander Brace"
                    },
                    {
                        "name": "Ryan Chard"
                    },
                    {
                        "name": "Kyle Chard"
                    },
                    {
                        "name": "Rajeev Thakur"
                    },
                    {
                        "name": "Ian Foster"
                    }
                ],
                "author_detail": {
                    "name": "Ian Foster"
                },
                "author": "Ian Foster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06876v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06876v2",
                "updated": "2024-08-26T11:29:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    11,
                    29,
                    7,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-13T13:14:54Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    14,
                    54,
                    1,
                    226,
                    0
                ],
                "title": "Decision-Focused Learning to Predict Action Costs for Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decision-Focused Learning to Predict Action Costs for Planning"
                },
                "summary": "In many automated planning applications, action costs can be hard to specify.\nAn example is the time needed to travel through a certain road segment, which\ndepends on many factors, such as the current weather conditions. A natural way\nto address this issue is to learn to predict these parameters based on input\nfeatures (e.g., weather forecasts) and use the predicted action costs in\nautomated planning afterward. Decision-Focused Learning (DFL) has been\nsuccessful in learning to predict the parameters of combinatorial optimization\nproblems in a way that optimizes solution quality rather than prediction\nquality. This approach yields better results than treating prediction and\noptimization as separate tasks. In this paper, we investigate for the first\ntime the challenges of implementing DFL for automated planning in order to\nlearn to predict the action costs. There are two main challenges to overcome:\n(1) planning systems are called during gradient descent learning, to solve\nplanning problems with negative action costs, which are not supported in\nplanning. We propose novel methods for gradient computation to avoid this\nissue. (2) DFL requires repeated planner calls during training, which can limit\nthe scalability of the method. We experiment with different methods\napproximating the optimal plan as well as an easy-to-implement caching\nmechanism to speed up the learning process. As the first work that addresses\nDFL for automated planning, we demonstrate that the proposed gradient\ncomputation consistently yields significantly better plans than predictions\naimed at minimizing prediction error; and that caching can temper the\ncomputation requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In many automated planning applications, action costs can be hard to specify.\nAn example is the time needed to travel through a certain road segment, which\ndepends on many factors, such as the current weather conditions. A natural way\nto address this issue is to learn to predict these parameters based on input\nfeatures (e.g., weather forecasts) and use the predicted action costs in\nautomated planning afterward. Decision-Focused Learning (DFL) has been\nsuccessful in learning to predict the parameters of combinatorial optimization\nproblems in a way that optimizes solution quality rather than prediction\nquality. This approach yields better results than treating prediction and\noptimization as separate tasks. In this paper, we investigate for the first\ntime the challenges of implementing DFL for automated planning in order to\nlearn to predict the action costs. There are two main challenges to overcome:\n(1) planning systems are called during gradient descent learning, to solve\nplanning problems with negative action costs, which are not supported in\nplanning. We propose novel methods for gradient computation to avoid this\nissue. (2) DFL requires repeated planner calls during training, which can limit\nthe scalability of the method. We experiment with different methods\napproximating the optimal plan as well as an easy-to-implement caching\nmechanism to speed up the learning process. As the first work that addresses\nDFL for automated planning, we demonstrate that the proposed gradient\ncomputation consistently yields significantly better plans than predictions\naimed at minimizing prediction error; and that caching can temper the\ncomputation requirements."
                },
                "authors": [
                    {
                        "name": "Jayanta Mandi"
                    },
                    {
                        "name": "Marco Foschini"
                    },
                    {
                        "name": "Daniel Holler"
                    },
                    {
                        "name": "Sylvie Thiebaux"
                    },
                    {
                        "name": "Jorg Hoffmann"
                    },
                    {
                        "name": "Tias Guns"
                    }
                ],
                "author_detail": {
                    "name": "Tias Guns"
                },
                "author": "Tias Guns",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06876v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.16343v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.16343v2",
                "updated": "2024-08-26T07:26:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    7,
                    26,
                    27,
                    0,
                    239,
                    0
                ],
                "published": "2024-02-26T06:55:36Z",
                "published_parsed": [
                    2024,
                    2,
                    26,
                    6,
                    55,
                    36,
                    0,
                    57,
                    0
                ],
                "title": "Trimma: Trimming Metadata Storage and Latency for Hybrid Memory Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trimma: Trimming Metadata Storage and Latency for Hybrid Memory Systems"
                },
                "summary": "Hybrid main memory systems combine both performance and capacity advantages\nfrom heterogeneous memory technologies. With larger capacities, higher\nassociativities, and finer granularities, hybrid memory systems currently\nexhibit significant metadata storage and lookup overheads for flexibly\nremapping data blocks between the two memory tiers. To alleviate the\ninefficiencies of existing designs, we propose Trimma, the combination of a\nmulti-level metadata structure and an efficient metadata cache design. Trimma\nuses a multi-level metadata table to only track truly necessary address remap\nentries. The saved memory space is effectively utilized as extra DRAM cache\ncapacity to improve performance. Trimma also uses separate formats to store the\nentries with non-identity and identity address mappings. This improves the\noverall remap cache hit rate, further boosting the performance. Trimma is\ntransparent to software and compatible with various types of hybrid memory\nsystems. When evaluated on a representative hybrid memory system with HBM3 and\nDDR5, Trimma achieves up to 1.68$\\times$ and on average 1.33$\\times$ speedup\nbenefits, compared to state-of-the-art hybrid memory designs. These results\nshow that Trimma effectively addresses metadata management overheads,\nespecially for future scalable large-scale hybrid memory architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid main memory systems combine both performance and capacity advantages\nfrom heterogeneous memory technologies. With larger capacities, higher\nassociativities, and finer granularities, hybrid memory systems currently\nexhibit significant metadata storage and lookup overheads for flexibly\nremapping data blocks between the two memory tiers. To alleviate the\ninefficiencies of existing designs, we propose Trimma, the combination of a\nmulti-level metadata structure and an efficient metadata cache design. Trimma\nuses a multi-level metadata table to only track truly necessary address remap\nentries. The saved memory space is effectively utilized as extra DRAM cache\ncapacity to improve performance. Trimma also uses separate formats to store the\nentries with non-identity and identity address mappings. This improves the\noverall remap cache hit rate, further boosting the performance. Trimma is\ntransparent to software and compatible with various types of hybrid memory\nsystems. When evaluated on a representative hybrid memory system with HBM3 and\nDDR5, Trimma achieves up to 1.68$\\times$ and on average 1.33$\\times$ speedup\nbenefits, compared to state-of-the-art hybrid memory designs. These results\nshow that Trimma effectively addresses metadata management overheads,\nespecially for future scalable large-scale hybrid memory architectures."
                },
                "authors": [
                    {
                        "name": "Yiwei Li"
                    },
                    {
                        "name": "Boyu Tian"
                    },
                    {
                        "name": "Mingyu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Mingyu Gao"
                },
                "author": "Mingyu Gao",
                "arxiv_comment": "Accepted by PACT 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.16343v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.16343v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08795v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08795v2",
                "updated": "2024-08-26T04:32:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    4,
                    32,
                    56,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-16T15:11:12Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    15,
                    11,
                    12,
                    4,
                    229,
                    0
                ],
                "title": "RollingCache: Using Runtime Behavior to Defend Against Cache Side\n  Channel Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RollingCache: Using Runtime Behavior to Defend Against Cache Side\n  Channel Attacks"
                },
                "summary": "Shared caches are vulnerable to side channel attacks through contention in\ncache sets. Besides being a simple source of information leak, these side\nchannels form useful gadgets for more sophisticated attacks that compromise the\nsecurity of shared systems.\n  The fundamental design aspect that contention attacks exploit is the\ndeterministic nature of the set of addresses contending for a cache set. In\nthis paper, we present RollingCache, a cache design that defends against\ncontention attacks by dynamically changing the set of addresses contending for\ncache sets. Unlike prior defenses, RollingCache does not rely on address\nencryption/decryption, data relocation, or cache partitioning. We use one level\nof indirection to implement dynamic mapping controlled by the whole-cache\nruntime behavior. Our solution does not depend on having defined security\ndomains, and can defend against an attacker running on the same or another\ncore.\n  We evaluate RollingCache on ChampSim using the SPEC-2017 benchmark suite. Our\nsecurity evaluation shows that our dynamic mapping removes the deterministic\nability to identify the source of contention. The performance evaluation shows\nan impact of 1.67\\% over a mix of workloads, with a corresponding",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared caches are vulnerable to side channel attacks through contention in\ncache sets. Besides being a simple source of information leak, these side\nchannels form useful gadgets for more sophisticated attacks that compromise the\nsecurity of shared systems.\n  The fundamental design aspect that contention attacks exploit is the\ndeterministic nature of the set of addresses contending for a cache set. In\nthis paper, we present RollingCache, a cache design that defends against\ncontention attacks by dynamically changing the set of addresses contending for\ncache sets. Unlike prior defenses, RollingCache does not rely on address\nencryption/decryption, data relocation, or cache partitioning. We use one level\nof indirection to implement dynamic mapping controlled by the whole-cache\nruntime behavior. Our solution does not depend on having defined security\ndomains, and can defend against an attacker running on the same or another\ncore.\n  We evaluate RollingCache on ChampSim using the SPEC-2017 benchmark suite. Our\nsecurity evaluation shows that our dynamic mapping removes the deterministic\nability to identify the source of contention. The performance evaluation shows\nan impact of 1.67\\% over a mix of workloads, with a corresponding"
                },
                "authors": [
                    {
                        "name": "Divya Ojha"
                    },
                    {
                        "name": "Sandhya Dwarkadas"
                    }
                ],
                "author_detail": {
                    "name": "Sandhya Dwarkadas"
                },
                "author": "Sandhya Dwarkadas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08795v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08795v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14001v1",
                "updated": "2024-08-26T03:58:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    3,
                    58,
                    20,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T03:58:20Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    3,
                    58,
                    20,
                    0,
                    239,
                    0
                ],
                "title": "Decentralized Federated Learning with Model Caching on Mobile Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized Federated Learning with Model Caching on Mobile Agents"
                },
                "summary": "Federated Learning (FL) aims to train a shared model using data and\ncomputation power on distributed agents coordinated by a central server.\nDecentralized FL (DFL) utilizes local model exchange and aggregation between\nagents to reduce the communication and computation overheads on the central\nserver. However, when agents are mobile, the communication opportunity between\nagents can be sporadic, largely hindering the convergence and accuracy of DFL.\nIn this paper, we study delay-tolerant model spreading and aggregation enabled\nby model caching on mobile agents. Each agent stores not only its own model,\nbut also models of agents encountered in the recent past. When two agents meet,\nthey exchange their own models as well as the cached models. Local model\naggregation works on all models in the cache. We theoretically analyze the\nconvergence of DFL with cached models, explicitly taking into account the model\nstaleness introduced by caching. We design and compare different model caching\nalgorithms for different DFL and mobility scenarios. We conduct detailed case\nstudies in a vehicular network to systematically investigate the interplay\nbetween agent mobility, cache staleness, and model convergence. In our\nexperiments, cached DFL converges quickly, and significantly outperforms DFL\nwithout caching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) aims to train a shared model using data and\ncomputation power on distributed agents coordinated by a central server.\nDecentralized FL (DFL) utilizes local model exchange and aggregation between\nagents to reduce the communication and computation overheads on the central\nserver. However, when agents are mobile, the communication opportunity between\nagents can be sporadic, largely hindering the convergence and accuracy of DFL.\nIn this paper, we study delay-tolerant model spreading and aggregation enabled\nby model caching on mobile agents. Each agent stores not only its own model,\nbut also models of agents encountered in the recent past. When two agents meet,\nthey exchange their own models as well as the cached models. Local model\naggregation works on all models in the cache. We theoretically analyze the\nconvergence of DFL with cached models, explicitly taking into account the model\nstaleness introduced by caching. We design and compare different model caching\nalgorithms for different DFL and mobility scenarios. We conduct detailed case\nstudies in a vehicular network to systematically investigate the interplay\nbetween agent mobility, cache staleness, and model convergence. In our\nexperiments, cached DFL converges quickly, and significantly outperforms DFL\nwithout caching."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Wang"
                    },
                    {
                        "name": "Guojun Xiong"
                    },
                    {
                        "name": "Houwei Cao"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "27 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13605v1",
                "updated": "2024-08-24T15:23:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    24,
                    15,
                    23,
                    32,
                    5,
                    237,
                    0
                ],
                "published": "2024-08-24T15:23:32Z",
                "published_parsed": [
                    2024,
                    8,
                    24,
                    15,
                    23,
                    32,
                    5,
                    237,
                    0
                ],
                "title": "Mobile Edge Computing Networks: Online Low-Latency and Fresh Service\n  Provisioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile Edge Computing Networks: Online Low-Latency and Fresh Service\n  Provisioning"
                },
                "summary": "Edge service caching can significantly mitigate latency and reduce\ncommunication and computing overhead by fetching and initializing services\n(applications) from clouds. The freshness of cached service data is critical\nwhen providing satisfactory services to users, but has been overlooked in\nexisting research efforts. In this paper, we study the online low-latency and\nfresh service provisioning in mobile edge computing (MEC) networks.\nSpecifically, we jointly optimize the service caching, task offloading, and\nresource allocation without knowledge of future system information, which is\nformulated as a joint online long-term optimization problem. This problem is\nNP-hard. To solve the problem, we design a Lyapunov-based online framework that\ndecouples the problem at temporal level into a series of per-time-slot\nsubproblems. For each subproblem, we propose an online integrated\noptimization-deep reinforcement learning (OIODRL) method, which contains an\noptimization stage including a quadratically constrained quadratic program\n(QCQP) transformation and a semidefinite relaxation (SDR) method, and a\nlearning stage including a deep reinforcement learning (DRL) algorithm.\nExtensive simulations show that the proposed OIODRL method achieves a\nnear-optimal solution and outperforms other benchmark methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge service caching can significantly mitigate latency and reduce\ncommunication and computing overhead by fetching and initializing services\n(applications) from clouds. The freshness of cached service data is critical\nwhen providing satisfactory services to users, but has been overlooked in\nexisting research efforts. In this paper, we study the online low-latency and\nfresh service provisioning in mobile edge computing (MEC) networks.\nSpecifically, we jointly optimize the service caching, task offloading, and\nresource allocation without knowledge of future system information, which is\nformulated as a joint online long-term optimization problem. This problem is\nNP-hard. To solve the problem, we design a Lyapunov-based online framework that\ndecouples the problem at temporal level into a series of per-time-slot\nsubproblems. For each subproblem, we propose an online integrated\noptimization-deep reinforcement learning (OIODRL) method, which contains an\noptimization stage including a quadratically constrained quadratic program\n(QCQP) transformation and a semidefinite relaxation (SDR) method, and a\nlearning stage including a deep reinforcement learning (DRL) algorithm.\nExtensive simulations show that the proposed OIODRL method achieves a\nnear-optimal solution and outperforms other benchmark methods."
                },
                "authors": [
                    {
                        "name": "Yuhan Yi"
                    },
                    {
                        "name": "Guanglin Zhang"
                    },
                    {
                        "name": "Hai Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Hai Jiang"
                },
                "author": "Hai Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11049v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11049v3",
                "updated": "2024-08-23T17:54:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    17,
                    54,
                    34,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-20T17:57:31Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    57,
                    31,
                    1,
                    233,
                    0
                ],
                "title": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding"
                },
                "summary": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency without sacrificing performance but the\nconventional wisdom suggests that its efficacy is limited to small batch sizes.\nIn MagicDec, we show that surprisingly SD can achieve speedup even for a high\nthroughput inference regime for moderate to long sequences. More interestingly,\nan intelligent drafting strategy can achieve better speedup with increasing\nbatch size based on our rigorous analysis. MagicDec first identifies the\nbottleneck shifts with increasing batch size and sequence length, and uses\nthese insights to deploy speculative decoding more effectively for high\nthroughput inference. Then, it leverages draft models with sparse KV cache to\naddress the KV bottleneck that scales with both sequence length and batch size.\nThis finding underscores the broad applicability of speculative decoding in\nlong-context serving, as it can enhance throughput and reduce latency without\ncompromising accuracy. For moderate to long sequences, we demonstrate up to 2x\nspeedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving\nbatch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is available\nat https://github.com/Infini-AI-Lab/MagicDec/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency without sacrificing performance but the\nconventional wisdom suggests that its efficacy is limited to small batch sizes.\nIn MagicDec, we show that surprisingly SD can achieve speedup even for a high\nthroughput inference regime for moderate to long sequences. More interestingly,\nan intelligent drafting strategy can achieve better speedup with increasing\nbatch size based on our rigorous analysis. MagicDec first identifies the\nbottleneck shifts with increasing batch size and sequence length, and uses\nthese insights to deploy speculative decoding more effectively for high\nthroughput inference. Then, it leverages draft models with sparse KV cache to\naddress the KV bottleneck that scales with both sequence length and batch size.\nThis finding underscores the broad applicability of speculative decoding in\nlong-context serving, as it can enhance throughput and reduce latency without\ncompromising accuracy. For moderate to long sequences, we demonstrate up to 2x\nspeedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving\nbatch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is available\nat https://github.com/Infini-AI-Lab/MagicDec/."
                },
                "authors": [
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Vashisth Tiwari"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Jinyuan Shi"
                    },
                    {
                        "name": "Ian En-Hsu Yen"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11049v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11049v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13165v1",
                "updated": "2024-08-23T15:39:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    15,
                    39,
                    20,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T15:39:20Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    15,
                    39,
                    20,
                    4,
                    236,
                    0
                ],
                "title": "Cyclic Wrap-Around Multi-Access Coded Caching with Private Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cyclic Wrap-Around Multi-Access Coded Caching with Private Caches"
                },
                "summary": "We consider a variant of the coded caching problem where users connect to two\ntypes of caches, called private caches and access caches. The problem setting\nconsists of a server having a library of files and a set of access caches.\nEvery user, equipped with a private cache, connects to $L$ neighboring access\ncaches in a cyclic wrap-around fashion. The server populates the private and\naccess caches with file contents in either coded or uncoded format. For this\nsetting, we derive a lower bound on the optimal worst-case transmission rate\nusing cut-set arguments. This lower bound applies to both coded and uncoded\nplacements. We then provide an achievable scheme with uncoded placement and\nshow that our scheme specializes to the well-known Maddah-Ali-Niesen scheme for\nthe dedicated cache network in the absence of access caches. Finally, we show\nthat the proposed scheme achieves optimality in large memory regimes and\nprovide numerical plots comparing the rate of the proposed scheme with the\nderived lower bound, demonstrating the optimality of our scheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a variant of the coded caching problem where users connect to two\ntypes of caches, called private caches and access caches. The problem setting\nconsists of a server having a library of files and a set of access caches.\nEvery user, equipped with a private cache, connects to $L$ neighboring access\ncaches in a cyclic wrap-around fashion. The server populates the private and\naccess caches with file contents in either coded or uncoded format. For this\nsetting, we derive a lower bound on the optimal worst-case transmission rate\nusing cut-set arguments. This lower bound applies to both coded and uncoded\nplacements. We then provide an achievable scheme with uncoded placement and\nshow that our scheme specializes to the well-known Maddah-Ali-Niesen scheme for\nthe dedicated cache network in the absence of access caches. Finally, we show\nthat the proposed scheme achieves optimality in large memory regimes and\nprovide numerical plots comparing the rate of the proposed scheme with the\nderived lower bound, demonstrating the optimality of our scheme."
                },
                "authors": [
                    {
                        "name": "Dhruv Pratap Singh"
                    },
                    {
                        "name": "Anjana A. Mahesh"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "15 pages, 5 figures and one table. Some overlap of introductory and\n  background materials with our earlier submission arXiv:2407.00677v1 dated 30\n  June 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.05332v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.05332v5",
                "updated": "2024-08-23T13:25:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    13,
                    25,
                    7,
                    4,
                    236,
                    0
                ],
                "published": "2023-05-09T10:41:36Z",
                "published_parsed": [
                    2023,
                    5,
                    9,
                    10,
                    41,
                    36,
                    1,
                    129,
                    0
                ],
                "title": "Fundamental Limits of Multi-Message Private Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fundamental Limits of Multi-Message Private Computation"
                },
                "summary": "In a typical formulation of the private information retrieval (PIR) problem,\na single user wishes to retrieve one out of $ K$ files from $N$ servers without\nrevealing the demanded file index to any server. This paper formulates an\nextended model of PIR, referred to as multi-message private computation\n(MM-PC), where instead of retrieving a single file, the user wishes to retrieve\n$P>1$ linear combinations of files while preserving the privacy of the demand\ninformation. The MM-PC problem is a generalization of the private computation\n(PC) problem (where the user requests one linear combination of the files), and\nthe multi-message private information retrieval (MM-PIR) problem (where the\nuser requests $P>1$ files). A baseline achievable scheme repeats the optimal PC\nscheme by Sun and Jafar $P$ times, or treats each possible demanded linear\ncombination as an independent file and then uses the near optimal MM-PIR scheme\nby Banawan and Ulukus. In this paper, we propose a new MM-PC scheme that\nsignificantly improves upon the baseline schemes. In doing so, we design the\nqueries inspired by the structure in the cache-aided scalar linear function\nretrieval scheme by Wan {\\it et al.}, which leverages the dependency between\nlinear functions to reduce the amount of communications. To ensure the\ndecodability of our scheme, we propose a new method to benefit from the\nexisting dependency, referred to as the sign assignment step. In the end, we\nuse Maximum Distance Separable matrices to code the queries, which allows the\nreduction of download from the servers, while preserving privacy. By the\nproposed schemes, we characterize the capacity within a multiplicative factor\nof $2$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a typical formulation of the private information retrieval (PIR) problem,\na single user wishes to retrieve one out of $ K$ files from $N$ servers without\nrevealing the demanded file index to any server. This paper formulates an\nextended model of PIR, referred to as multi-message private computation\n(MM-PC), where instead of retrieving a single file, the user wishes to retrieve\n$P>1$ linear combinations of files while preserving the privacy of the demand\ninformation. The MM-PC problem is a generalization of the private computation\n(PC) problem (where the user requests one linear combination of the files), and\nthe multi-message private information retrieval (MM-PIR) problem (where the\nuser requests $P>1$ files). A baseline achievable scheme repeats the optimal PC\nscheme by Sun and Jafar $P$ times, or treats each possible demanded linear\ncombination as an independent file and then uses the near optimal MM-PIR scheme\nby Banawan and Ulukus. In this paper, we propose a new MM-PC scheme that\nsignificantly improves upon the baseline schemes. In doing so, we design the\nqueries inspired by the structure in the cache-aided scalar linear function\nretrieval scheme by Wan {\\it et al.}, which leverages the dependency between\nlinear functions to reduce the amount of communications. To ensure the\ndecodability of our scheme, we propose a new method to benefit from the\nexisting dependency, referred to as the sign assignment step. In the end, we\nuse Maximum Distance Separable matrices to code the queries, which allows the\nreduction of download from the servers, while preserving privacy. By the\nproposed schemes, we characterize the capacity within a multiplicative factor\nof $2$."
                },
                "authors": [
                    {
                        "name": "Ali Gholami"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Tayyebeh Jahani-Nezhad"
                    },
                    {
                        "name": "Hua Sun"
                    },
                    {
                        "name": "Mingyue Ji"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "arxiv_comment": "A version of this paper is submitted to IEEE Transactions on\n  Communications. A short version was accepted and presented at ISIT 2024 in\n  Athens",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.05332v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.05332v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12947v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12947v1",
                "updated": "2024-08-23T09:54:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    54,
                    22,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T09:54:22Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    54,
                    22,
                    4,
                    236,
                    0
                ],
                "title": "Which Part of the Heap is Useful? Improving Heap Liveness Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Which Part of the Heap is Useful? Improving Heap Liveness Analysis"
                },
                "summary": "With the growing sizes of data structures allocated in heap, understanding\nthe actual use of heap memory is critically important for minimizing cache\nmisses and reclaiming unused memory. A static analysis aimed at this is\ndifficult because the heap locations are unnamed. Using allocation sites to\nname them creates very few distinctions making it difficult to identify\nallocated heap locations that are not used. Heap liveness analysis using access\ngraphs solves this problem by (a) using a storeless model of heap memory by\nnaming the locations with access paths, and (b) representing the unbounded sets\nof access paths (which are regular languages) as finite automata.\n  We improve the scalability and efficiency of heap liveness analysis, and\nreduce the amount of computed heap liveness information by using deterministic\nautomata and by minimizing the inclusion of aliased access paths in the\nlanguage. Practically, our field-, flow-, context-sensitive liveness analysis\non SPEC CPU2006 benchmarks scales to 36 kLoC (existing analysis scales to 10.5\nkLoC) and improves efficiency even up to 99%. For some of the benchmarks, our\ntechnique shows multifold reduction in the computed liveness information,\nranging from 2 to 100 times (in terms of the number of live access paths),\nwithout compromising on soundness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing sizes of data structures allocated in heap, understanding\nthe actual use of heap memory is critically important for minimizing cache\nmisses and reclaiming unused memory. A static analysis aimed at this is\ndifficult because the heap locations are unnamed. Using allocation sites to\nname them creates very few distinctions making it difficult to identify\nallocated heap locations that are not used. Heap liveness analysis using access\ngraphs solves this problem by (a) using a storeless model of heap memory by\nnaming the locations with access paths, and (b) representing the unbounded sets\nof access paths (which are regular languages) as finite automata.\n  We improve the scalability and efficiency of heap liveness analysis, and\nreduce the amount of computed heap liveness information by using deterministic\nautomata and by minimizing the inclusion of aliased access paths in the\nlanguage. Practically, our field-, flow-, context-sensitive liveness analysis\non SPEC CPU2006 benchmarks scales to 36 kLoC (existing analysis scales to 10.5\nkLoC) and improves efficiency even up to 99%. For some of the benchmarks, our\ntechnique shows multifold reduction in the computed liveness information,\nranging from 2 to 100 times (in terms of the number of live access paths),\nwithout compromising on soundness."
                },
                "authors": [
                    {
                        "name": "Vini Kanvar"
                    },
                    {
                        "name": "Uday P. Khedker"
                    }
                ],
                "author_detail": {
                    "name": "Uday P. Khedker"
                },
                "author": "Uday P. Khedker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12947v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12947v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12592v1",
                "updated": "2024-08-22T17:56:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    56,
                    29,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T17:56:29Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    56,
                    29,
                    3,
                    235,
                    0
                ],
                "title": "Exposing Shadow Branches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exposing Shadow Branches"
                },
                "summary": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation."
                },
                "authors": [
                    {
                        "name": "Chrysanthos Pepi"
                    },
                    {
                        "name": "Bhargav Reddy Godala"
                    },
                    {
                        "name": "Krishnam Tibrewala"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Paul V. Gratz"
                    },
                    {
                        "name": "Daniel A. Jiménez"
                    },
                    {
                        "name": "Gilles A. Pokam"
                    },
                    {
                        "name": "David I. August"
                    }
                ],
                "author_detail": {
                    "name": "David I. August"
                },
                "author": "David I. August",
                "arxiv_comment": "13 pages, 16 figures, Submitted to ASPLOS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.14533v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.14533v2",
                "updated": "2024-08-22T17:47:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    47,
                    49,
                    3,
                    235,
                    0
                ],
                "published": "2023-09-25T21:17:17Z",
                "published_parsed": [
                    2023,
                    9,
                    25,
                    21,
                    17,
                    17,
                    0,
                    268,
                    0
                ],
                "title": "Stable CoO$_2$ Nanoscrolls With Outstanding Electrical Properties",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stable CoO$_2$ Nanoscrolls With Outstanding Electrical Properties"
                },
                "summary": "Layered CoO$_2$ is of great interest for its promising properties but is\nmeta-stable in its bulk form. CoO$_2$ was synthesized by converting the\nquasi-one-dimensional crystal structure of bulk Ca$_3$Co$_2$O$_6$ via a\nhydrothermal treatment. The resulting nanostructures were predominantly\nnanoscrolls with very thin walls, which exhibit long-term stability. A detailed\nstructural investigation reveals that the CoO$_2$ is found to crystallize in\nmonoclinic form, similar to the related CaCoO$_2$-CoO$_2$ misfit structure.\nIndividual nanoscrolls are characterized electrically and show a p-type\nsemiconducting nature with a high current-carrying capacity of 4$\\cdot$10$^5$ A\ncm$^{-2}$ and an extremely high breakdown voltage of up to 270 kV/cm. The\nresults demonstrate the possibility to stabilize meta-stable materials in\nlow-dimensional forms and a promising application of the nanoscrolls as\ninterconnect in high-voltage electronic circuitry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Layered CoO$_2$ is of great interest for its promising properties but is\nmeta-stable in its bulk form. CoO$_2$ was synthesized by converting the\nquasi-one-dimensional crystal structure of bulk Ca$_3$Co$_2$O$_6$ via a\nhydrothermal treatment. The resulting nanostructures were predominantly\nnanoscrolls with very thin walls, which exhibit long-term stability. A detailed\nstructural investigation reveals that the CoO$_2$ is found to crystallize in\nmonoclinic form, similar to the related CaCoO$_2$-CoO$_2$ misfit structure.\nIndividual nanoscrolls are characterized electrically and show a p-type\nsemiconducting nature with a high current-carrying capacity of 4$\\cdot$10$^5$ A\ncm$^{-2}$ and an extremely high breakdown voltage of up to 270 kV/cm. The\nresults demonstrate the possibility to stabilize meta-stable materials in\nlow-dimensional forms and a promising application of the nanoscrolls as\ninterconnect in high-voltage electronic circuitry."
                },
                "authors": [
                    {
                        "name": "Simon Hettler"
                    },
                    {
                        "name": "Kankona Singha Roy"
                    },
                    {
                        "name": "Raul Arenal"
                    },
                    {
                        "name": "Leela S. Panchakarla"
                    }
                ],
                "author_detail": {
                    "name": "Leela S. Panchakarla"
                },
                "author": "Leela S. Panchakarla",
                "arxiv_doi": "10.1002/admi.202400317",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1002/admi.202400317",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2309.14533v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.14533v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Adv. Mater. Interfaces 2024, 2400317",
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11506v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11506v1",
                "updated": "2024-08-21T10:26:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    26,
                    26,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T10:26:26Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    26,
                    26,
                    2,
                    234,
                    0
                ],
                "title": "Rheological behavior of molybdenum disulfide (MoS2) inks under electric\n  fields: influence of concentration and voltage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rheological behavior of molybdenum disulfide (MoS2) inks under electric\n  fields: influence of concentration and voltage"
                },
                "summary": "This work provides a complete rheological characterization of molybdenum\ndisulfide (MoS2) inks in the presence of electric fields. Several\nconcentrations of MoS2 are studied and dispersed in a viscoelastic fluid. The\nlubrication effects are present in the ink when the MoS2 concentration is\nhigher than 0.10% w/w. The dielectric properties show the impossibility of a\npositive electrorheological effect for all MoS2-inks studied. The formation of\nvortices and electromigration of MoS2 particles occur under the influence of an\nexternal electric field. These two phenomena affect the rheological behavior of\nMoS2-inks under shear flow condition. Relatively to the extensional rheology\nexperiments, the particle migration and the vortex formation promote anisotropy\non the rheological properties of the inks which affects the relaxation time,\nthe formation of beads-on-a-string and the uniaxial elongational flow condition\nis no longer valid. When the electric field strength is 1.5 kV/mm, the\nformation of Taylor's cone is observed and independent of MoS2 concentration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work provides a complete rheological characterization of molybdenum\ndisulfide (MoS2) inks in the presence of electric fields. Several\nconcentrations of MoS2 are studied and dispersed in a viscoelastic fluid. The\nlubrication effects are present in the ink when the MoS2 concentration is\nhigher than 0.10% w/w. The dielectric properties show the impossibility of a\npositive electrorheological effect for all MoS2-inks studied. The formation of\nvortices and electromigration of MoS2 particles occur under the influence of an\nexternal electric field. These two phenomena affect the rheological behavior of\nMoS2-inks under shear flow condition. Relatively to the extensional rheology\nexperiments, the particle migration and the vortex formation promote anisotropy\non the rheological properties of the inks which affects the relaxation time,\nthe formation of beads-on-a-string and the uniaxial elongational flow condition\nis no longer valid. When the electric field strength is 1.5 kV/mm, the\nformation of Taylor's cone is observed and independent of MoS2 concentration."
                },
                "authors": [
                    {
                        "name": "Pedro C Rijo"
                    },
                    {
                        "name": "Francisco J. Galindo-Rosales"
                    }
                ],
                "author_detail": {
                    "name": "Francisco J. Galindo-Rosales"
                },
                "author": "Francisco J. Galindo-Rosales",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11506v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11506v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.10685v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.10685v2",
                "updated": "2024-08-21T06:10:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    6,
                    10,
                    2,
                    2,
                    234,
                    0
                ],
                "published": "2024-01-19T13:32:55Z",
                "published_parsed": [
                    2024,
                    1,
                    19,
                    13,
                    32,
                    55,
                    4,
                    19,
                    0
                ],
                "title": "Towards End-to-End GPS Localization with Neural Pseudorange Correction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards End-to-End GPS Localization with Neural Pseudorange Correction"
                },
                "summary": "The pseudorange error is one of the root causes of localization inaccuracy in\nGPS. Previous data-driven methods regress and eliminate pseudorange errors\nusing handcrafted intermediate labels. Unlike them, we propose an end-to-end\nGPS localization framework, E2E-PrNet, to train a neural network for\npseudorange correction (PrNet) directly using the final task loss calculated\nwith the ground truth of GPS receiver states. The gradients of the loss with\nrespect to learnable parameters are backpropagated through a Differentiable\nNonlinear Least Squares (DNLS) optimizer to PrNet. The feasibility of fusing\nthe data-driven neural network and the model-based DNLS module is verified with\nGPS data collected by Android phones, showing that E2E-PrNet outperforms the\nbaseline weighted least squares method and the state-of-the-art end-to-end\ndata-driven approach. Finally, we discuss the explainability of E2E-PrNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The pseudorange error is one of the root causes of localization inaccuracy in\nGPS. Previous data-driven methods regress and eliminate pseudorange errors\nusing handcrafted intermediate labels. Unlike them, we propose an end-to-end\nGPS localization framework, E2E-PrNet, to train a neural network for\npseudorange correction (PrNet) directly using the final task loss calculated\nwith the ground truth of GPS receiver states. The gradients of the loss with\nrespect to learnable parameters are backpropagated through a Differentiable\nNonlinear Least Squares (DNLS) optimizer to PrNet. The feasibility of fusing\nthe data-driven neural network and the model-based DNLS module is verified with\nGPS data collected by Android phones, showing that E2E-PrNet outperforms the\nbaseline weighted least squares method and the state-of-the-art end-to-end\ndata-driven approach. Finally, we discuss the explainability of E2E-PrNet."
                },
                "authors": [
                    {
                        "name": "Xu Weng"
                    },
                    {
                        "name": "KV Ling"
                    },
                    {
                        "name": "Haochen Liu"
                    },
                    {
                        "name": "Kun Cao"
                    }
                ],
                "author_detail": {
                    "name": "Kun Cao"
                },
                "author": "Kun Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.10685v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.10685v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11325v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11325v1",
                "updated": "2024-08-21T04:16:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    4,
                    16,
                    49,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T04:16:49Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    4,
                    16,
                    49,
                    2,
                    234,
                    0
                ],
                "title": "Telepathic Datacenters: Fast RPCs using Shared CXL Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Telepathic Datacenters: Fast RPCs using Shared CXL Memory"
                },
                "summary": "Datacenter applications often rely on remote procedure calls (RPCs) for fast,\nefficient, and secure communication. However, RPCs are slow, inefficient, and\nhard to use as they require expensive serialization and compression to\ncommunicate over a packetized serial network link. Compute Express Link 3.0\n(CXL) offers an alternative solution, allowing applications to share data using\na cache-coherent, shared-memory interface across clusters of machines.\n  RPCool is a new framework that exploits CXL's shared memory capabilities.\nRPCool avoids serialization by passing pointers to data structures in shared\nmemory. While avoiding serialization is useful, directly sharing pointer-rich\ndata eliminates the isolation that copying data over traditional networks\nprovides, leaving the receiver vulnerable to invalid pointers and concurrent\nupdates to shared data by the sender. RPCool restores this safety with careful\nand efficient management of memory permissions. Another significant challenge\nwith CXL shared memory capabilities is that they are unlikely to scale to an\nentire datacenter. RPCool addresses this by falling back to RDMA-based\ncommunication.\n  Overall, RPCool reduces the round-trip latency by 1.93$\\times$ and\n7.2$\\times$ compared to state-of-the-art RDMA and CXL-based RPC mechanisms,\nrespectively. Moreover, RPCool performs either comparably or better than other\nRPC mechanisms across a range of workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Datacenter applications often rely on remote procedure calls (RPCs) for fast,\nefficient, and secure communication. However, RPCs are slow, inefficient, and\nhard to use as they require expensive serialization and compression to\ncommunicate over a packetized serial network link. Compute Express Link 3.0\n(CXL) offers an alternative solution, allowing applications to share data using\na cache-coherent, shared-memory interface across clusters of machines.\n  RPCool is a new framework that exploits CXL's shared memory capabilities.\nRPCool avoids serialization by passing pointers to data structures in shared\nmemory. While avoiding serialization is useful, directly sharing pointer-rich\ndata eliminates the isolation that copying data over traditional networks\nprovides, leaving the receiver vulnerable to invalid pointers and concurrent\nupdates to shared data by the sender. RPCool restores this safety with careful\nand efficient management of memory permissions. Another significant challenge\nwith CXL shared memory capabilities is that they are unlikely to scale to an\nentire datacenter. RPCool addresses this by falling back to RDMA-based\ncommunication.\n  Overall, RPCool reduces the round-trip latency by 1.93$\\times$ and\n7.2$\\times$ compared to state-of-the-art RDMA and CXL-based RPC mechanisms,\nrespectively. Moreover, RPCool performs either comparably or better than other\nRPC mechanisms across a range of workloads."
                },
                "authors": [
                    {
                        "name": "Suyash Mahar"
                    },
                    {
                        "name": "Ehsan Hajyjasini"
                    },
                    {
                        "name": "Seungjin Lee"
                    },
                    {
                        "name": "Zifeng Zhang"
                    },
                    {
                        "name": "Mingyao Shen"
                    },
                    {
                        "name": "Steven Swanson"
                    }
                ],
                "author_detail": {
                    "name": "Steven Swanson"
                },
                "author": "Steven Swanson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11325v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11325v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10970v1",
                "updated": "2024-08-20T16:02:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    16,
                    2,
                    54,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T16:02:54Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    16,
                    2,
                    54,
                    1,
                    233,
                    0
                ],
                "title": "Hybrid Recurrent Models Support Emergent Descriptions for Hierarchical\n  Planning and Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Recurrent Models Support Emergent Descriptions for Hierarchical\n  Planning and Control"
                },
                "summary": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work has demonstrated that a class of hybrid state-space\nmodel known as recurrent switching linear dynamical systems (rSLDS) discover\nmeaningful behavioural units via the piecewise linear decomposition of complex\ncontinuous dynamics (Linderman et al., 2016). Furthermore, they model how the\nunderlying continuous states drive these discrete mode switches. We propose\nthat the rich representations formed by an rSLDS can provide useful\nabstractions for planning and control. We present a novel hierarchical\nmodel-based algorithm inspired by Active Inference in which a discrete MDP sits\nabove a low-level linear-quadratic controller. The recurrent transition\ndynamics learned by the rSLDS allow us to (1) specify temporally-abstracted\nsub-goals in a method reminiscent of the options framework, (2) lift the\nexploration into discrete space allowing us to exploit information-theoretic\nexploration bonuses and (3) `cache' the approximate solutions to low-level\nproblems in the discrete planner. We successfully apply our model to the sparse\nContinuous Mountain Car task, demonstrating fast system identification via\nenhanced exploration and non-trivial planning through the delineation of\nabstract sub-goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work has demonstrated that a class of hybrid state-space\nmodel known as recurrent switching linear dynamical systems (rSLDS) discover\nmeaningful behavioural units via the piecewise linear decomposition of complex\ncontinuous dynamics (Linderman et al., 2016). Furthermore, they model how the\nunderlying continuous states drive these discrete mode switches. We propose\nthat the rich representations formed by an rSLDS can provide useful\nabstractions for planning and control. We present a novel hierarchical\nmodel-based algorithm inspired by Active Inference in which a discrete MDP sits\nabove a low-level linear-quadratic controller. The recurrent transition\ndynamics learned by the rSLDS allow us to (1) specify temporally-abstracted\nsub-goals in a method reminiscent of the options framework, (2) lift the\nexploration into discrete space allowing us to exploit information-theoretic\nexploration bonuses and (3) `cache' the approximate solutions to low-level\nproblems in the discrete planner. We successfully apply our model to the sparse\nContinuous Mountain Car task, demonstrating fast system identification via\nenhanced exploration and non-trivial planning through the delineation of\nabstract sub-goals."
                },
                "authors": [
                    {
                        "name": "Poppy Collis"
                    },
                    {
                        "name": "Ryan Singh"
                    },
                    {
                        "name": "Paul F Kinghorn"
                    },
                    {
                        "name": "Christopher L Buckley"
                    }
                ],
                "author_detail": {
                    "name": "Christopher L Buckley"
                },
                "author": "Christopher L Buckley",
                "arxiv_comment": "4 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10746v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10746v1",
                "updated": "2024-08-20T11:30:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    11,
                    30,
                    12,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T11:30:12Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    11,
                    30,
                    12,
                    1,
                    233,
                    0
                ],
                "title": "Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI\n  Framework for Personal LLMs Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI\n  Framework for Personal LLMs Fine-Tuning"
                },
                "summary": "Large language models (LLMs) have unlocked a plethora of powerful\napplications at the network edge, such as intelligent personal assistants. Data\nprivacy and security concerns have prompted a shift towards edge-based\nfine-tuning of personal LLMs, away from cloud reliance. However, this raises\nissues of computational intensity and resource scarcity, hindering training\nefficiency and feasibility. While current studies investigate\nparameter-efficient fine-tuning (PEFT) techniques to mitigate resource\nconstraints, our analysis indicates that these techniques are not sufficiently\nresource-efficient for edge devices. To tackle these challenges, we propose\nPluto and Charon (PAC), a time and memory efficient collaborative edge AI\nframework for personal LLMs fine-tuning. PAC breaks the resource wall of\npersonal LLMs fine-tuning with a sophisticated algorithm-system co-design. (1)\nAlgorithmically, PAC implements a personal LLMs fine-tuning technique that is\nefficient in terms of parameters, time, and memory. It utilizes Parallel\nAdapters to circumvent the need for a full backward pass through the LLM\nbackbone. Additionally, an activation cache mechanism further streamlining the\nprocess by negating the necessity for repeated forward passes across multiple\nepochs. (2) Systematically, PAC leverages edge devices in close proximity,\npooling them as a collective resource for in-situ personal LLMs fine-tuning,\nutilizing a hybrid data and pipeline parallelism to orchestrate distributed\ntraining. The use of the activation cache eliminates the need for forward pass\nthrough the LLM backbone,enabling exclusive fine-tuning of the Parallel\nAdapters using data parallelism. Extensive evaluation based on prototype\nimplementation demonstrates that PAC remarkably outperforms state-of-the-art\napproaches, achieving up to 8.64x end-to-end speedup and up to 88.16% reduction\nin memory footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have unlocked a plethora of powerful\napplications at the network edge, such as intelligent personal assistants. Data\nprivacy and security concerns have prompted a shift towards edge-based\nfine-tuning of personal LLMs, away from cloud reliance. However, this raises\nissues of computational intensity and resource scarcity, hindering training\nefficiency and feasibility. While current studies investigate\nparameter-efficient fine-tuning (PEFT) techniques to mitigate resource\nconstraints, our analysis indicates that these techniques are not sufficiently\nresource-efficient for edge devices. To tackle these challenges, we propose\nPluto and Charon (PAC), a time and memory efficient collaborative edge AI\nframework for personal LLMs fine-tuning. PAC breaks the resource wall of\npersonal LLMs fine-tuning with a sophisticated algorithm-system co-design. (1)\nAlgorithmically, PAC implements a personal LLMs fine-tuning technique that is\nefficient in terms of parameters, time, and memory. It utilizes Parallel\nAdapters to circumvent the need for a full backward pass through the LLM\nbackbone. Additionally, an activation cache mechanism further streamlining the\nprocess by negating the necessity for repeated forward passes across multiple\nepochs. (2) Systematically, PAC leverages edge devices in close proximity,\npooling them as a collective resource for in-situ personal LLMs fine-tuning,\nutilizing a hybrid data and pipeline parallelism to orchestrate distributed\ntraining. The use of the activation cache eliminates the need for forward pass\nthrough the LLM backbone,enabling exclusive fine-tuning of the Parallel\nAdapters using data parallelism. Extensive evaluation based on prototype\nimplementation demonstrates that PAC remarkably outperforms state-of-the-art\napproaches, achieving up to 8.64x end-to-end speedup and up to 88.16% reduction\nin memory footprint."
                },
                "authors": [
                    {
                        "name": "Bei Ouyang"
                    },
                    {
                        "name": "Shengyuan Ye"
                    },
                    {
                        "name": "Liekang Zeng"
                    },
                    {
                        "name": "Tianyi Qian"
                    },
                    {
                        "name": "Jingyi Li"
                    },
                    {
                        "name": "Xu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xu Chen"
                },
                "author": "Xu Chen",
                "arxiv_comment": "Accepted by The 53rd International Conference on Parallel Processing\n  (ICPP'24)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10746v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10746v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09697v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09697v2",
                "updated": "2024-08-20T04:46:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    4,
                    46,
                    18,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-19T04:43:56Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    4,
                    43,
                    56,
                    0,
                    232,
                    0
                ],
                "title": "Heta: Distributed Training of Heterogeneous Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heta: Distributed Training of Heterogeneous Graph Neural Networks"
                },
                "summary": "Heterogeneous Graph Neural Networks (HGNNs) leverage diverse semantic\nrelationships in Heterogeneous Graphs (HetGs) and have demonstrated remarkable\nlearning performance in various applications. However, current distributed GNN\ntraining systems often overlook unique characteristics of HetGs, such as\nvarying feature dimensions and the prevalence of missing features among nodes,\nleading to suboptimal performance or even incompatibility with distributed HGNN\ntraining. We introduce Heta, a framework designed to address the communication\nbottleneck in distributed HGNN training. Heta leverages the inherent structure\nof HGNNs - independent relation-specific aggregations for each relation,\nfollowed by a cross-relation aggregation - and advocates for a novel\nRelation-Aggregation-First computation paradigm. It performs relation-specific\naggregations within graph partitions and then exchanges partial aggregations.\nThis design, coupled with a new graph partitioning method that divides a HetG\nbased on its graph schema and HGNN computation dependency, substantially\nreduces communication overhead. Heta further incorporates an innovative GPU\nfeature caching strategy that accounts for the different cache miss-penalties\nassociated with diverse node types. Comprehensive evaluations of various HGNN\nmodels and large heterogeneous graph datasets demonstrate that Heta outperforms\nstate-of-the-art systems like DGL and GraphLearn by up to 5.8x and 2.3x in\nend-to-end epoch time, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous Graph Neural Networks (HGNNs) leverage diverse semantic\nrelationships in Heterogeneous Graphs (HetGs) and have demonstrated remarkable\nlearning performance in various applications. However, current distributed GNN\ntraining systems often overlook unique characteristics of HetGs, such as\nvarying feature dimensions and the prevalence of missing features among nodes,\nleading to suboptimal performance or even incompatibility with distributed HGNN\ntraining. We introduce Heta, a framework designed to address the communication\nbottleneck in distributed HGNN training. Heta leverages the inherent structure\nof HGNNs - independent relation-specific aggregations for each relation,\nfollowed by a cross-relation aggregation - and advocates for a novel\nRelation-Aggregation-First computation paradigm. It performs relation-specific\naggregations within graph partitions and then exchanges partial aggregations.\nThis design, coupled with a new graph partitioning method that divides a HetG\nbased on its graph schema and HGNN computation dependency, substantially\nreduces communication overhead. Heta further incorporates an innovative GPU\nfeature caching strategy that accounts for the different cache miss-penalties\nassociated with diverse node types. Comprehensive evaluations of various HGNN\nmodels and large heterogeneous graph datasets demonstrate that Heta outperforms\nstate-of-the-art systems like DGL and GraphLearn by up to 5.8x and 2.3x in\nend-to-end epoch time, respectively."
                },
                "authors": [
                    {
                        "name": "Yuchen Zhong"
                    },
                    {
                        "name": "Junwei Su"
                    },
                    {
                        "name": "Chuan Wu"
                    },
                    {
                        "name": "Minjie Wang"
                    }
                ],
                "author_detail": {
                    "name": "Minjie Wang"
                },
                "author": "Minjie Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09697v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09697v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10104v1",
                "updated": "2024-08-19T15:47:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    15,
                    47,
                    17,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T15:47:17Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    15,
                    47,
                    17,
                    0,
                    232,
                    0
                ],
                "title": "Multi-Mode Lens for Momentum Microscopy and XPEEM: Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Mode Lens for Momentum Microscopy and XPEEM: Theory"
                },
                "summary": "The strong electric field between the sample and the extractor is the core of\ncathode lenses and a pivotal determinant of high resolution. Nevertheless,\nfields in the range of 3-8 kV/mm can be a source of complications. Local field\nenhancement at sharp edges or microscopic protrusions of cleaved samples may\nresult in field emission or flashovers. Moreover, slow background electrons are\ndrawn into the microscope column, where they contribute to space charge\neffects. A novel front lens configuration, optimized through ray-tracing\nsimulations, significantly reduces the field at the sample and allows even for\nzero field or retarding field, which serves to suppress space charge effects.\nOne or several annular electrodes, situated in a concentric position relative\nto the extractor, serve to form an additional lens within the gap between the\nsample and the extractor. The refractory power of this lens, and consequently\nthe field at the sample surface, can be modified by adjusting the potentials of\nthe annular electrodes. The imaging properties and aberrations of this gap lens\nhave been investigated with regard to momentum imaging and XPEEM. The study\nencompasses the energy range from the few-eV level for laser-ARPES to 6 keV,\nfor hard X-ray ARPES. The additional converging lens situated in close\nproximity to the sample exhibits a reduced field curvature of the k-image in\nthe backfocal plane. This allows for the acquisition of larger fields of view\nin both momentum and real-space imaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The strong electric field between the sample and the extractor is the core of\ncathode lenses and a pivotal determinant of high resolution. Nevertheless,\nfields in the range of 3-8 kV/mm can be a source of complications. Local field\nenhancement at sharp edges or microscopic protrusions of cleaved samples may\nresult in field emission or flashovers. Moreover, slow background electrons are\ndrawn into the microscope column, where they contribute to space charge\neffects. A novel front lens configuration, optimized through ray-tracing\nsimulations, significantly reduces the field at the sample and allows even for\nzero field or retarding field, which serves to suppress space charge effects.\nOne or several annular electrodes, situated in a concentric position relative\nto the extractor, serve to form an additional lens within the gap between the\nsample and the extractor. The refractory power of this lens, and consequently\nthe field at the sample surface, can be modified by adjusting the potentials of\nthe annular electrodes. The imaging properties and aberrations of this gap lens\nhave been investigated with regard to momentum imaging and XPEEM. The study\nencompasses the energy range from the few-eV level for laser-ARPES to 6 keV,\nfor hard X-ray ARPES. The additional converging lens situated in close\nproximity to the sample exhibits a reduced field curvature of the k-image in\nthe backfocal plane. This allows for the acquisition of larger fields of view\nin both momentum and real-space imaging."
                },
                "authors": [
                    {
                        "name": "Olena Tkach"
                    },
                    {
                        "name": "Gerd Schoenhense"
                    }
                ],
                "author_detail": {
                    "name": "Gerd Schoenhense"
                },
                "author": "Gerd Schoenhense",
                "arxiv_comment": "17 pages, 4 figures, 44 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09848v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09848v1",
                "updated": "2024-08-19T09:50:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    50,
                    35,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T09:50:35Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    50,
                    35,
                    0,
                    232,
                    0
                ],
                "title": "Abstract Environment Trimming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Abstract Environment Trimming"
                },
                "summary": "Variable sharing is a fundamental property in the static analysis of logic\nprograms, since it is instrumental for ensuring correctness and increasing\nprecision while inferring many useful program properties. Such properties\ninclude modes, determinacy, non-failure, cost, etc. This has motivated\nsignificant work on developing abstract domains to improve the precision and\nperformance of sharing analyses. Much of this work has centered around the\nfamily of set-sharing domains, because of the high precision they offer.\nHowever, this comes at a price: their scalability to a wide set of realistic\nprograms remains challenging and this hinders their wider adoption. In this\nwork, rather than defining new sharing abstract domains, we focus instead on\ndeveloping techniques which can be incorporated in the analyzers to address\naspects that are known to affect the efficiency of these domains, such as the\nnumber of variables, without affecting precision. These techniques are inspired\nin others used in the context of compiler optimizations, such as expression\nreassociation and variable trimming. We present several such techniques and\nprovide an extensive experimental evaluation of over 1100 program modules taken\nfrom both production code and classical benchmarks. This includes the\nSpectector cache analyzer, the s(CASP) system, the libraries of the Ciao\nsystem, the LPdoc documenter, the PLAI analyzer itself, etc. The experimental\nresults are quite encouraging: we have obtained significant speed-ups, and,\nmore importantly, the number of modules that require a timeout was cut in half.\nAs a result, many more programs can be analyzed precisely in reasonable times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variable sharing is a fundamental property in the static analysis of logic\nprograms, since it is instrumental for ensuring correctness and increasing\nprecision while inferring many useful program properties. Such properties\ninclude modes, determinacy, non-failure, cost, etc. This has motivated\nsignificant work on developing abstract domains to improve the precision and\nperformance of sharing analyses. Much of this work has centered around the\nfamily of set-sharing domains, because of the high precision they offer.\nHowever, this comes at a price: their scalability to a wide set of realistic\nprograms remains challenging and this hinders their wider adoption. In this\nwork, rather than defining new sharing abstract domains, we focus instead on\ndeveloping techniques which can be incorporated in the analyzers to address\naspects that are known to affect the efficiency of these domains, such as the\nnumber of variables, without affecting precision. These techniques are inspired\nin others used in the context of compiler optimizations, such as expression\nreassociation and variable trimming. We present several such techniques and\nprovide an extensive experimental evaluation of over 1100 program modules taken\nfrom both production code and classical benchmarks. This includes the\nSpectector cache analyzer, the s(CASP) system, the libraries of the Ciao\nsystem, the LPdoc documenter, the PLAI analyzer itself, etc. The experimental\nresults are quite encouraging: we have obtained significant speed-ups, and,\nmore importantly, the number of modules that require a timeout was cut in half.\nAs a result, many more programs can be analyzed precisely in reasonable times."
                },
                "authors": [
                    {
                        "name": "Daniel Jurjo-Rivas"
                    },
                    {
                        "name": "Jose F. Morales"
                    },
                    {
                        "name": "Pedro López-García"
                    },
                    {
                        "name": "Manuel V. Hermenegildo"
                    }
                ],
                "author_detail": {
                    "name": "Manuel V. Hermenegildo"
                },
                "author": "Manuel V. Hermenegildo",
                "arxiv_comment": "61 pages, 10 figures, 7 tables, submitted to ICLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09848v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10284v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10284v1",
                "updated": "2024-08-19T03:27:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    27,
                    15,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T03:27:15Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    27,
                    15,
                    0,
                    232,
                    0
                ],
                "title": "AdapMoE: Adaptive Sensitivity-based Expert Gating and Management for\n  Efficient MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdapMoE: Adaptive Sensitivity-based Expert Gating and Management for\n  Efficient MoE Inference"
                },
                "summary": "Mixture-of-Experts (MoE) models are designed to enhance the efficiency of\nlarge language models (LLMs) without proportionally increasing the\ncomputational demands. However, their deployment on edge devices still faces\nsignificant challenges due to high on-demand loading overheads from managing\nsparsely activated experts. This paper introduces AdapMoE, an algorithm-system\nco-design framework for efficient MoE inference. AdapMoE features adaptive\nexpert gating and management to reduce the on-demand loading overheads. We\nobserve the heterogeneity of experts loading across layers and tokens, based on\nwhich we propose a sensitivity-based strategy to adjust the number of activated\nexperts dynamically. Meanwhile, we also integrate advanced prefetching and\ncache management techniques to further reduce the loading latency. Through\ncomprehensive evaluations on various platforms, we demonstrate AdapMoE\nconsistently outperforms existing techniques, reducing the average number of\nactivated experts by 25% and achieving a 1.35x speedup without accuracy\ndegradation. Code is available at: https://github.com/PKU-SEC-Lab/AdapMoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) models are designed to enhance the efficiency of\nlarge language models (LLMs) without proportionally increasing the\ncomputational demands. However, their deployment on edge devices still faces\nsignificant challenges due to high on-demand loading overheads from managing\nsparsely activated experts. This paper introduces AdapMoE, an algorithm-system\nco-design framework for efficient MoE inference. AdapMoE features adaptive\nexpert gating and management to reduce the on-demand loading overheads. We\nobserve the heterogeneity of experts loading across layers and tokens, based on\nwhich we propose a sensitivity-based strategy to adjust the number of activated\nexperts dynamically. Meanwhile, we also integrate advanced prefetching and\ncache management techniques to further reduce the loading latency. Through\ncomprehensive evaluations on various platforms, we demonstrate AdapMoE\nconsistently outperforms existing techniques, reducing the average number of\nactivated experts by 25% and achieving a 1.35x speedup without accuracy\ndegradation. Code is available at: https://github.com/PKU-SEC-Lab/AdapMoE."
                },
                "authors": [
                    {
                        "name": "Shuzhang Zhong"
                    },
                    {
                        "name": "Ling Liang"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Ru Huang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_doi": "10.1145/3676536.3676741",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676536.3676741",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.10284v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10284v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07092v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07092v2",
                "updated": "2024-08-18T17:27:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    18,
                    17,
                    27,
                    17,
                    6,
                    231,
                    0
                ],
                "published": "2024-08-11T18:40:36Z",
                "published_parsed": [
                    2024,
                    8,
                    11,
                    18,
                    40,
                    36,
                    6,
                    224,
                    0
                ],
                "title": "Post-Training Sparse Attention with Double Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-Training Sparse Attention with Double Sparsity"
                },
                "summary": "The inference process for large language models is slow and memory-intensive,\nwith one of the most critical bottlenecks being excessive Key-Value (KV) cache\naccesses. This paper introduces \"Double Sparsity,\" a novel post-training sparse\nattention technique designed to alleviate this bottleneck by reducing KV cache\naccess. Double Sparsity combines token sparsity, which focuses on utilizing\nonly the important tokens for computing self-attention, with channel sparsity,\nan approach that uses important feature channels for identifying important\ntokens. Our key insight is that the pattern of channel sparsity is relatively\nstatic, allowing us to use offline calibration to make it efficient at runtime,\nthereby enabling accurate and efficient identification of important tokens.\nMoreover, this method can be combined with offloading to achieve significant\nmemory usage reduction. Experimental results demonstrate that Double Sparsity\ncan achieve $\\frac{1}{16}$ token and channel sparsity with minimal impact on\naccuracy across various tasks, including wiki-2 perplexity, key-value\nretrieval, and long context benchmarks with models including Llama-2-7B,\nLlama-2-70B, and Mixtral-8x7B. It brings up to a 14.1$\\times$ acceleration in\nattention operations and a 1.9$\\times$ improvement in end-to-end inference on\nGPUs. With offloading, it achieves a decoding speed acceleration of\n16.3$\\times$ compared to state-of-the-art solutions at a sequence length of\n256K. Our code is publicly available at\nhttps://github.com/andy-yang-1/DoubleSparse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inference process for large language models is slow and memory-intensive,\nwith one of the most critical bottlenecks being excessive Key-Value (KV) cache\naccesses. This paper introduces \"Double Sparsity,\" a novel post-training sparse\nattention technique designed to alleviate this bottleneck by reducing KV cache\naccess. Double Sparsity combines token sparsity, which focuses on utilizing\nonly the important tokens for computing self-attention, with channel sparsity,\nan approach that uses important feature channels for identifying important\ntokens. Our key insight is that the pattern of channel sparsity is relatively\nstatic, allowing us to use offline calibration to make it efficient at runtime,\nthereby enabling accurate and efficient identification of important tokens.\nMoreover, this method can be combined with offloading to achieve significant\nmemory usage reduction. Experimental results demonstrate that Double Sparsity\ncan achieve $\\frac{1}{16}$ token and channel sparsity with minimal impact on\naccuracy across various tasks, including wiki-2 perplexity, key-value\nretrieval, and long context benchmarks with models including Llama-2-7B,\nLlama-2-70B, and Mixtral-8x7B. It brings up to a 14.1$\\times$ acceleration in\nattention operations and a 1.9$\\times$ improvement in end-to-end inference on\nGPUs. With offloading, it achieves a decoding speed acceleration of\n16.3$\\times$ compared to state-of-the-art solutions at a sequence length of\n256K. Our code is publicly available at\nhttps://github.com/andy-yang-1/DoubleSparse."
                },
                "authors": [
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Ying Sheng"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Lianmin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Lianmin Zheng"
                },
                "author": "Lianmin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07092v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07092v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09483v1",
                "updated": "2024-08-18T13:54:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    18,
                    13,
                    54,
                    46,
                    6,
                    231,
                    0
                ],
                "published": "2024-08-18T13:54:46Z",
                "published_parsed": [
                    2024,
                    8,
                    18,
                    13,
                    54,
                    46,
                    6,
                    231,
                    0
                ],
                "title": "CMD: A Cache-assisted GPU Memory Deduplication Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CMD: A Cache-assisted GPU Memory Deduplication Architecture"
                },
                "summary": "Massive off-chip accesses in GPUs are the main performance bottleneck, and we\ndivided these accesses into three types: (1) Write, (2) Data-Read, and (3)\nRead-Only. Besides, We find that many writes are duplicate, and the duplication\ncan be inter-dup and intra-dup. While inter-dup means different memory blocks\nare identical, and intra-dup means all the 4B elements in a line are the same.\nIn this work, we propose a cache-assisted GPU memory deduplication architecture\nnamed CMD to reduce the off-chip accesses via utilizing the data duplication in\nGPU applications. CMD includes three key design contributions which aim to\nreduce the three kinds of accesses: (1) A novel GPU memory deduplication\narchitecture that removes the inter-dup and inter-dup lines. As for the\ninter-dup detection, we reduce the extra read requests caused by the\ntraditional read-verify hash process. Besides, we design several techniques to\nmanage duplicate blocks. (2) We propose a cache-assisted read scheme to reduce\nthe reads to duplicate data. When an L2 cache miss wants to read the duplicate\nblock, if the reference block has been fetched to L2 and it is clean, we can\ncopy it to the L2 missed block without accessing off-chip DRAM. As for the\nreads to intra-dup data, CMD uses the on-chip metadata cache to get the data.\n(3) When a cache line is evicted, the clean sectors in the line are invalidated\nwhile the dirty sectors are written back. However, most read-only victims are\nre-referenced from DRAM more than twice. Therefore, we add a full-associate\nFIFO to accommodate the read-only (it is also clean) victims to reduce the\nre-reference counts. Experiments show that CMD can decrease the off-chip\naccesses by 31.01%, reduce the energy by 32.78% and improve performance by\n37.79%. Besides, CMD can improve the performance of memory-intensive workloads\nby 50.18%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Massive off-chip accesses in GPUs are the main performance bottleneck, and we\ndivided these accesses into three types: (1) Write, (2) Data-Read, and (3)\nRead-Only. Besides, We find that many writes are duplicate, and the duplication\ncan be inter-dup and intra-dup. While inter-dup means different memory blocks\nare identical, and intra-dup means all the 4B elements in a line are the same.\nIn this work, we propose a cache-assisted GPU memory deduplication architecture\nnamed CMD to reduce the off-chip accesses via utilizing the data duplication in\nGPU applications. CMD includes three key design contributions which aim to\nreduce the three kinds of accesses: (1) A novel GPU memory deduplication\narchitecture that removes the inter-dup and inter-dup lines. As for the\ninter-dup detection, we reduce the extra read requests caused by the\ntraditional read-verify hash process. Besides, we design several techniques to\nmanage duplicate blocks. (2) We propose a cache-assisted read scheme to reduce\nthe reads to duplicate data. When an L2 cache miss wants to read the duplicate\nblock, if the reference block has been fetched to L2 and it is clean, we can\ncopy it to the L2 missed block without accessing off-chip DRAM. As for the\nreads to intra-dup data, CMD uses the on-chip metadata cache to get the data.\n(3) When a cache line is evicted, the clean sectors in the line are invalidated\nwhile the dirty sectors are written back. However, most read-only victims are\nre-referenced from DRAM more than twice. Therefore, we add a full-associate\nFIFO to accommodate the read-only (it is also clean) victims to reduce the\nre-reference counts. Experiments show that CMD can decrease the off-chip\naccesses by 31.01%, reduce the energy by 32.78% and improve performance by\n37.79%. Besides, CMD can improve the performance of memory-intensive workloads\nby 50.18%."
                },
                "authors": [
                    {
                        "name": "Wei Zhao"
                    },
                    {
                        "name": "Dan Feng"
                    },
                    {
                        "name": "Wei Tong"
                    },
                    {
                        "name": "Xueliang Wei"
                    },
                    {
                        "name": "Bing Wu"
                    }
                ],
                "author_detail": {
                    "name": "Bing Wu"
                },
                "author": "Bing Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11550v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11550v3",
                "updated": "2024-08-16T08:46:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    8,
                    46,
                    33,
                    4,
                    229,
                    0
                ],
                "published": "2024-07-16T09:53:32Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    9,
                    53,
                    32,
                    1,
                    198,
                    0
                ],
                "title": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference"
                },
                "summary": "Large Language Models have excelled in various fields but encounter\nchallenges in memory and time efficiency due to the expanding Key-Value (KV)\ncache required for long-sequence inference. Recent efforts try to reduce KV\ncache size to a given memory budget by evicting vast non-critical cache\nelements during runtime, while preserving generation quality. Our revisiting of\ncurrent eviction methods reveals that they fundamentally minimize an upper\nbound of the $L_1$ eviction loss between the pre- and post-eviction outputs of\nmulti-head self-attention mechanisms. Moreover, our analysis indicates that the\ncommon practices of uniformly assigning budgets across attention heads harm\ntheir post-eviction generation quality. In light of these findings, we propose\na simple yet effective adaptive budget allocation algorithm. This algorithm not\nonly optimizes the theoretical loss upper bound but also reduces the $L_1$\neviction loss in practice by aligning with the varied characteristics across\ndifferent heads. By integrating this algorithm into two state-of-the-art\nmethods, we demonstrate the effectiveness of using adaptive budget allocation\nto optimize KV cache eviction. Extensive evaluations on 16 datasets and the\nNeedle-in-a-Haystack test confirm significant performance improvements across\nvarious tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have excelled in various fields but encounter\nchallenges in memory and time efficiency due to the expanding Key-Value (KV)\ncache required for long-sequence inference. Recent efforts try to reduce KV\ncache size to a given memory budget by evicting vast non-critical cache\nelements during runtime, while preserving generation quality. Our revisiting of\ncurrent eviction methods reveals that they fundamentally minimize an upper\nbound of the $L_1$ eviction loss between the pre- and post-eviction outputs of\nmulti-head self-attention mechanisms. Moreover, our analysis indicates that the\ncommon practices of uniformly assigning budgets across attention heads harm\ntheir post-eviction generation quality. In light of these findings, we propose\na simple yet effective adaptive budget allocation algorithm. This algorithm not\nonly optimizes the theoretical loss upper bound but also reduces the $L_1$\neviction loss in practice by aligning with the varied characteristics across\ndifferent heads. By integrating this algorithm into two state-of-the-art\nmethods, we demonstrate the effectiveness of using adaptive budget allocation\nto optimize KV cache eviction. Extensive evaluations on 16 datasets and the\nNeedle-in-a-Haystack test confirm significant performance improvements across\nvarious tasks."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S. Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S. Kevin Zhou"
                },
                "author": "S. Kevin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11550v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11550v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08545v1",
                "updated": "2024-08-16T06:11:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T06:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have gained increased popularity due to their\nremarkable success across various tasks, which has led to the active\ndevelopment of a large set of diverse LLMs. However, individual LLMs have\nlimitations when applied to complex tasks because of such factors as training\nbiases, model sizes, and the datasets used. A promising approach is to\nefficiently harness the diverse capabilities of LLMs to overcome these\nindividual limitations. Towards this goal, we introduce a novel LLM selection\nalgorithm called SelectLLM. This algorithm directs input queries to the most\nsuitable subset of LLMs from a large pool, ensuring they collectively provide\nthe correct response efficiently. SelectLLM uses a multi-label classifier,\nutilizing the classifier's predictions and confidence scores to design optimal\npolicies for selecting an optimal, query-aware, and lightweight subset of LLMs.\nOur findings show that the proposed model outperforms individual LLMs and\nachieves competitive performance compared to similarly sized, computationally\nexpensive top-performing LLM subsets. Specifically, with a similarly sized\ntop-performing LLM subset, we achieve a significant reduction in latency on two\nstandard reasoning benchmarks: 13% lower latency for GSM8K and 70% lower\nlatency for MMLU. Additionally, we conduct comprehensive analyses and ablation\nstudies, which validate the robustness of the proposed model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have gained increased popularity due to their\nremarkable success across various tasks, which has led to the active\ndevelopment of a large set of diverse LLMs. However, individual LLMs have\nlimitations when applied to complex tasks because of such factors as training\nbiases, model sizes, and the datasets used. A promising approach is to\nefficiently harness the diverse capabilities of LLMs to overcome these\nindividual limitations. Towards this goal, we introduce a novel LLM selection\nalgorithm called SelectLLM. This algorithm directs input queries to the most\nsuitable subset of LLMs from a large pool, ensuring they collectively provide\nthe correct response efficiently. SelectLLM uses a multi-label classifier,\nutilizing the classifier's predictions and confidence scores to design optimal\npolicies for selecting an optimal, query-aware, and lightweight subset of LLMs.\nOur findings show that the proposed model outperforms individual LLMs and\nachieves competitive performance compared to similarly sized, computationally\nexpensive top-performing LLM subsets. Specifically, with a similarly sized\ntop-performing LLM subset, we achieve a significant reduction in latency on two\nstandard reasoning benchmarks: 13% lower latency for GSM8K and 70% lower\nlatency for MMLU. Additionally, we conduct comprehensive analyses and ablation\nstudies, which validate the robustness of the proposed model."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19291v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19291v2",
                "updated": "2024-08-16T04:12:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    4,
                    12,
                    25,
                    4,
                    229,
                    0
                ],
                "published": "2024-07-27T16:20:21Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    16,
                    20,
                    21,
                    5,
                    209,
                    0
                ],
                "title": "Symmetric Locality: Definition and Initial Results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Locality: Definition and Initial Results"
                },
                "summary": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs."
                },
                "authors": [
                    {
                        "name": "Giordan Escalona"
                    },
                    {
                        "name": "Dylan McKellips"
                    },
                    {
                        "name": "Chen Ding"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ding"
                },
                "author": "Chen Ding",
                "arxiv_comment": "6 pages, 2nd ver",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19291v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19291v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04870v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04870v3",
                "updated": "2024-08-15T05:24:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    5,
                    24,
                    19,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-09T05:20:05Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    20,
                    5,
                    4,
                    222,
                    0
                ],
                "title": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs"
                },
                "summary": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Ayush RoyChowdhury"
                    },
                    {
                        "name": "Mulong Luo"
                    },
                    {
                        "name": "Prateek Sahu"
                    },
                    {
                        "name": "Sarbartha Banerjee"
                    },
                    {
                        "name": "Mohit Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Tiwari"
                },
                "author": "Mohit Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04870v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04870v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07853v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07853v1",
                "updated": "2024-08-14T23:42:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    23,
                    42,
                    46,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T23:42:46Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    23,
                    42,
                    46,
                    2,
                    227,
                    0
                ],
                "title": "A Case for Enabling Delegation of 5G Core Decisions to the RAN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Case for Enabling Delegation of 5G Core Decisions to the RAN"
                },
                "summary": "Under conventional 5G system design, the authentication and continuous\nmonitoring of user equipment (UE) demands a reliable backhaul connection\nbetween the radio access network (RAN) and the core network functions (AMF,\nAUSF, UDM, etc.). This is not a given, especially in disaster response and\nmilitary operations. We propose that, in these scenarios, decisions made by\ncore functions can be effectively delegated to the RAN by leveraging the RAN's\ncomputing resources and the micro-service programmability of the O-RAN system\narchitecture. This paper presents several concrete designs of core-RAN decision\ndelegation, including caching of core decisions and replicating some of the\ncore decision logic. Each design has revealed interesting performance and\nsecurity trade-offs that warrant further investigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Under conventional 5G system design, the authentication and continuous\nmonitoring of user equipment (UE) demands a reliable backhaul connection\nbetween the radio access network (RAN) and the core network functions (AMF,\nAUSF, UDM, etc.). This is not a given, especially in disaster response and\nmilitary operations. We propose that, in these scenarios, decisions made by\ncore functions can be effectively delegated to the RAN by leveraging the RAN's\ncomputing resources and the micro-service programmability of the O-RAN system\narchitecture. This paper presents several concrete designs of core-RAN decision\ndelegation, including caching of core decisions and replicating some of the\ncore decision logic. Each design has revealed interesting performance and\nsecurity trade-offs that warrant further investigation."
                },
                "authors": [
                    {
                        "name": "Lucas Vancina"
                    },
                    {
                        "name": "Geoffrey Xie"
                    }
                ],
                "author_detail": {
                    "name": "Geoffrey Xie"
                },
                "author": "Geoffrey Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07853v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07853v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15440v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15440v2",
                "updated": "2024-08-14T09:18:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    9,
                    18,
                    2,
                    2,
                    227,
                    0
                ],
                "published": "2024-07-22T07:42:57Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    7,
                    42,
                    57,
                    0,
                    204,
                    0
                ],
                "title": "The Bicameral Cache: a split cache for vector architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache: a split cache for vector architectures"
                },
                "summary": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value."
                },
                "authors": [
                    {
                        "name": "Susana Rebolledo"
                    },
                    {
                        "name": "Borja Perez"
                    },
                    {
                        "name": "Jose Luis Bosque"
                    },
                    {
                        "name": "Peter Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Peter Hsu"
                },
                "author": "Peter Hsu",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15440v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15440v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07304v1",
                "updated": "2024-08-14T05:42:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    5,
                    42,
                    35,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T05:42:35Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    5,
                    42,
                    35,
                    2,
                    227,
                    0
                ],
                "title": "At Least Factor-of-Two Optimization for RWLE-Based Homomorphic\n  Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "At Least Factor-of-Two Optimization for RWLE-Based Homomorphic\n  Encryption"
                },
                "summary": "Many modern applications that deal with sensitive data, such as healthcare\nand government services, outsource computation to cloud platforms. In such\nuntrusted environments, privacy is of vital importance. One solution to this\nproblem is homomorphic encryption (HE), a family of cryptographic schemes that\nsupport certain algebraic operations on encrypted data without the need for\ndecryption. However, despite major advancements, encryption in modern HE\nschemes still comes with a non-trivial computational overhead that can hamper\ndata-intensive workloads. To resolve this, recent research has shown that\nleveraging caching techniques, such as Rache, can significantly enhance the\nperformance of HE schemes while maintaining security. Rache unfortunately\ndisplays a key limitation in the time complexity of its caching procedure,\nwhich scales with the size of the plaintext space. Smuche is another caching\nscheme that simultaneously improves the scalability of the caching procedure\nand turns the encryption process into a constant-time operation, utilizing only\na single scalar multiplication. Even still, more can be done. In this paper, we\npresent an encryption method we call ``Zinc\" which entirely forgoes the\nmultiple caching process, replacing it with a single scalar addition, and then\ninjecting randomness that takes constant time with respect to the plaintext\nspace. This injection of randomness is similar to Smuche, and a great\nimprovement from Rache, allowing Zinc to achieve efficiency without\ncompromising security. We implement the scheme using Microsoft SEAL and compare\nits performance to vanilla CKKS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many modern applications that deal with sensitive data, such as healthcare\nand government services, outsource computation to cloud platforms. In such\nuntrusted environments, privacy is of vital importance. One solution to this\nproblem is homomorphic encryption (HE), a family of cryptographic schemes that\nsupport certain algebraic operations on encrypted data without the need for\ndecryption. However, despite major advancements, encryption in modern HE\nschemes still comes with a non-trivial computational overhead that can hamper\ndata-intensive workloads. To resolve this, recent research has shown that\nleveraging caching techniques, such as Rache, can significantly enhance the\nperformance of HE schemes while maintaining security. Rache unfortunately\ndisplays a key limitation in the time complexity of its caching procedure,\nwhich scales with the size of the plaintext space. Smuche is another caching\nscheme that simultaneously improves the scalability of the caching procedure\nand turns the encryption process into a constant-time operation, utilizing only\na single scalar multiplication. Even still, more can be done. In this paper, we\npresent an encryption method we call ``Zinc\" which entirely forgoes the\nmultiple caching process, replacing it with a single scalar addition, and then\ninjecting randomness that takes constant time with respect to the plaintext\nspace. This injection of randomness is similar to Smuche, and a great\nimprovement from Rache, allowing Zinc to achieve efficiency without\ncompromising security. We implement the scheme using Microsoft SEAL and compare\nits performance to vanilla CKKS."
                },
                "authors": [
                    {
                        "name": "Jonathan Ly"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Ly"
                },
                "author": "Jonathan Ly",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15743v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15743v2",
                "updated": "2024-08-13T13:56:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    56,
                    14,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-22T15:42:59Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    15,
                    42,
                    59,
                    0,
                    204,
                    0
                ],
                "title": "Cache-Aided MIMO Communications: DoF Analysis and Transmitter\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Aided MIMO Communications: DoF Analysis and Transmitter\n  Optimization"
                },
                "summary": "Cache-aided MIMO communications aims to jointly exploit both coded\ncaching~(CC) and spatial multiplexing gains to enhance communication\nefficiency. In this paper, we first analyze the achievable degrees of\nfreedom~(DoF) in a MIMO-CC system with CC gain \\(t\\), where a server with \\(L\\)\ntransmit antennas communicates with \\(K\\) users, each equipped with \\(G\\)\nreceive antennas. We demonstrate that the enhanced achievable DoF is\n\\(\\max_{\\beta, \\Omega} \\Omega \\beta\\), where the number of users \\(\\Omega\\)\nserved in each transmission is fine-tuned to maximize DoF, and \\(\\beta \\le\n\\min\\big(G, \\nicefrac{L \\binom{\\Omega-1}{t}}{1 + (\\Omega - t -\n1)\\binom{\\Omega-1}{t}}\\big)\\) represents the number of parallel streams decoded\nby each user. Second, we introduce an effective transmit covariance matrix\ndesign aimed at maximizing the symmetric rate, solved iteratively via\nsuccessive convex approximation. Third, we propose a new class of MIMO-CC\nschemes using a novel scheduling mechanism leveraging maximal multicasting\nopportunities to maximize delivery rates at given SNR levels while adhering to\nlinear processing constraints. Lastly, we devise linear multicast beamforming\nstrategies tailored for the flexible scheduling schemes in MIMO-CC systems and\npresent an iterative solution for the efficient design of beamformers.\nExtensive numerical simulations are used to verify the results of the paper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-aided MIMO communications aims to jointly exploit both coded\ncaching~(CC) and spatial multiplexing gains to enhance communication\nefficiency. In this paper, we first analyze the achievable degrees of\nfreedom~(DoF) in a MIMO-CC system with CC gain \\(t\\), where a server with \\(L\\)\ntransmit antennas communicates with \\(K\\) users, each equipped with \\(G\\)\nreceive antennas. We demonstrate that the enhanced achievable DoF is\n\\(\\max_{\\beta, \\Omega} \\Omega \\beta\\), where the number of users \\(\\Omega\\)\nserved in each transmission is fine-tuned to maximize DoF, and \\(\\beta \\le\n\\min\\big(G, \\nicefrac{L \\binom{\\Omega-1}{t}}{1 + (\\Omega - t -\n1)\\binom{\\Omega-1}{t}}\\big)\\) represents the number of parallel streams decoded\nby each user. Second, we introduce an effective transmit covariance matrix\ndesign aimed at maximizing the symmetric rate, solved iteratively via\nsuccessive convex approximation. Third, we propose a new class of MIMO-CC\nschemes using a novel scheduling mechanism leveraging maximal multicasting\nopportunities to maximize delivery rates at given SNR levels while adhering to\nlinear processing constraints. Lastly, we devise linear multicast beamforming\nstrategies tailored for the flexible scheduling schemes in MIMO-CC systems and\npresent an iterative solution for the efficient design of beamformers.\nExtensive numerical simulations are used to verify the results of the paper."
                },
                "authors": [
                    {
                        "name": "Mohammad NaseriTehrani"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Antti Tölli"
                    }
                ],
                "author_detail": {
                    "name": "Antti Tölli"
                },
                "author": "Antti Tölli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15743v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15743v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04043v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04043v3",
                "updated": "2024-08-13T13:31:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    31,
                    34,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-07T18:51:07Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    18,
                    51,
                    7,
                    2,
                    220,
                    0
                ],
                "title": "Ownership in low-level intermediate representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ownership in low-level intermediate representation"
                },
                "summary": "The concept of ownership in high level languages can aid both the programmer\nand the compiler to reason about the validity of memory operations. Previously,\nownership semantics has been used successfully in high level automatic program\nverification to model a reference to data by a first order logic (FOL)\nrepresentation of data instead of maintaining an address map. However,\nownership semantics is not used in low level program verification. We have\nidentified two challenges. First, ownership information is lost when a program\nis compiled to a low level intermediate representation (e.g., in LLVM IR).\nSecond, pointers in low level programs point to bytes using an address map\n(e.g., in unsafe Rust) and thus the verification condition (VC) cannot always\nreplace a pointer by its FOL abstraction. To remedy the situation, we develop\nownership semantics for an LLVM like low level intermediate representation.\nUsing these semantics, the VC can opportunistically model some memory accesses\nby a direct access of a pointer cache that stores byte representation of data.\nThis scheme reduces instances where an address map must be maintained,\nespecially for mostly safe programs that follow ownership semantics. For unsafe\nfunctionality, memory accesses are modelled by operations on an address map and\nwe provide mechanisms to keep the address map and pointer cache in sync. We\nimplement these semantics in SEABMC, a bit precise bounded model checker for\nLLVM. For evaluation, the source programs are assumed to be written in C. Since\nC does not have ownership built in, suitable macros are added that introduce\nand preserve ownership during translation to LLVM like IR for verification.\nThis approach is evaluated on mature open source C code. For both handcrafted\nbenchmarks and practical programs, we observe a speedup of $1.3x-5x$ during SMT\nsolving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The concept of ownership in high level languages can aid both the programmer\nand the compiler to reason about the validity of memory operations. Previously,\nownership semantics has been used successfully in high level automatic program\nverification to model a reference to data by a first order logic (FOL)\nrepresentation of data instead of maintaining an address map. However,\nownership semantics is not used in low level program verification. We have\nidentified two challenges. First, ownership information is lost when a program\nis compiled to a low level intermediate representation (e.g., in LLVM IR).\nSecond, pointers in low level programs point to bytes using an address map\n(e.g., in unsafe Rust) and thus the verification condition (VC) cannot always\nreplace a pointer by its FOL abstraction. To remedy the situation, we develop\nownership semantics for an LLVM like low level intermediate representation.\nUsing these semantics, the VC can opportunistically model some memory accesses\nby a direct access of a pointer cache that stores byte representation of data.\nThis scheme reduces instances where an address map must be maintained,\nespecially for mostly safe programs that follow ownership semantics. For unsafe\nfunctionality, memory accesses are modelled by operations on an address map and\nwe provide mechanisms to keep the address map and pointer cache in sync. We\nimplement these semantics in SEABMC, a bit precise bounded model checker for\nLLVM. For evaluation, the source programs are assumed to be written in C. Since\nC does not have ownership built in, suitable macros are added that introduce\nand preserve ownership during translation to LLVM like IR for verification.\nThis approach is evaluated on mature open source C code. For both handcrafted\nbenchmarks and practical programs, we observe a speedup of $1.3x-5x$ during SMT\nsolving."
                },
                "authors": [
                    {
                        "name": "Siddharth Priya"
                    },
                    {
                        "name": "Arie Gurfinkel"
                    }
                ],
                "author_detail": {
                    "name": "Arie Gurfinkel"
                },
                "author": "Arie Gurfinkel",
                "arxiv_comment": "FMCAD 2024 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04043v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04043v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18003v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18003v3",
                "updated": "2024-08-13T09:55:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    55,
                    43,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-25T12:56:22Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    12,
                    56,
                    22,
                    3,
                    207,
                    0
                ],
                "title": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption"
                },
                "summary": "Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture' s struggle with handling long texts. KV-Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV-Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV-Cache and elaborate on various\nmethods currently used to optimize the KV-Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture' s struggle with handling long texts. KV-Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV-Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV-Cache and elaborate on various\nmethods currently used to optimize the KV-Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field."
                },
                "authors": [
                    {
                        "name": "Luohe Shi"
                    },
                    {
                        "name": "Hongyi Zhang"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "to be published in CoLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18003v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18003v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00167v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00167v2",
                "updated": "2024-08-13T09:08:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    8,
                    55,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-31T21:33:56Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    21,
                    33,
                    56,
                    2,
                    213,
                    0
                ],
                "title": "Finch: Prompt-guided Key-Value Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finch: Prompt-guided Key-Value Cache Compression"
                },
                "summary": "Recent large language model applications, such as Retrieval-Augmented\nGeneration and chatbots, have led to an increased need to process longer input\ncontexts. However, this requirement is hampered by inherent limitations.\nArchitecturally, models are constrained by a context window defined during\ntraining. Additionally, processing extensive texts requires substantial GPU\nmemory. We propose a novel approach, Finch, to compress the input context by\nleveraging the pre-trained model weights of the self-attention. Given a prompt\nand a long text, Finch iteratively identifies the most relevant Key (K) and\nValue (V) pairs over chunks of the text conditioned on the prompt. Only such\npairs are stored in the KV cache, which, within the space constrained by the\ncontext window, ultimately contains a compressed version of the long text. Our\nproposal enables models to consume large inputs even with high compression (up\nto 93x) while preserving semantic integrity without the need for fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language model applications, such as Retrieval-Augmented\nGeneration and chatbots, have led to an increased need to process longer input\ncontexts. However, this requirement is hampered by inherent limitations.\nArchitecturally, models are constrained by a context window defined during\ntraining. Additionally, processing extensive texts requires substantial GPU\nmemory. We propose a novel approach, Finch, to compress the input context by\nleveraging the pre-trained model weights of the self-attention. Given a prompt\nand a long text, Finch iteratively identifies the most relevant Key (K) and\nValue (V) pairs over chunks of the text conditioned on the prompt. Only such\npairs are stored in the KV cache, which, within the space constrained by the\ncontext window, ultimately contains a compressed version of the long text. Our\nproposal enables models to consume large inputs even with high compression (up\nto 93x) while preserving semantic integrity without the need for fine-tuning."
                },
                "authors": [
                    {
                        "name": "Giulio Corallo"
                    },
                    {
                        "name": "Paolo Papotti"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Papotti"
                },
                "author": "Paolo Papotti",
                "arxiv_comment": "Accepted for publication at TACL - pre-MIT Press publication version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00167v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00167v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05996v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05996v1",
                "updated": "2024-08-12T08:46:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    46,
                    30,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T08:46:30Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    46,
                    30,
                    0,
                    225,
                    0
                ],
                "title": "Value-based Proactive Caching for Sensing Data in Internet of Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value-based Proactive Caching for Sensing Data in Internet of Vehicles"
                },
                "summary": "Sensing data (SD) plays an important role in safe-related applications for\nInternet of Vehicles. Proactively caching required sensing data (SD) is a\npivotal strategy for alleviating network congestion and improving data\naccessibility. Despite merits, existing studies predominantly address SD\ncaching within a single time slot, which may not be scalable to scenarios\ninvolving multi-slots. Furthermore, the oversight of service capacity at\ncaching nodes could lead to significant queuing delays in SD reception. To\ntackle these limitations, we jointly consider the problem of anchoring caching\nplacement and requests allocation for SD. A value model incorporating both\ntemporal and spacial characteristics is first proposed to estimate the\nsignificance of different caching decisions. Subsequently, a stochastic integer\nnonlinear programming model is provided to optimize the long-term system\nperformance, which is converted into a series of online optimization problem by\nleveraging the Lyapunov method and linearized via introducing auxiliary\nvariables. To expedite the solution, we provide a binary quantum particle swarm\noptimization based algorithm with quadratic time complexity. Numerical\ninvestigations demonstrate the superiority of proposed algorithms compared with\nother schemes in terms of energy consumption, response latency, and cache-hit\nratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sensing data (SD) plays an important role in safe-related applications for\nInternet of Vehicles. Proactively caching required sensing data (SD) is a\npivotal strategy for alleviating network congestion and improving data\naccessibility. Despite merits, existing studies predominantly address SD\ncaching within a single time slot, which may not be scalable to scenarios\ninvolving multi-slots. Furthermore, the oversight of service capacity at\ncaching nodes could lead to significant queuing delays in SD reception. To\ntackle these limitations, we jointly consider the problem of anchoring caching\nplacement and requests allocation for SD. A value model incorporating both\ntemporal and spacial characteristics is first proposed to estimate the\nsignificance of different caching decisions. Subsequently, a stochastic integer\nnonlinear programming model is provided to optimize the long-term system\nperformance, which is converted into a series of online optimization problem by\nleveraging the Lyapunov method and linearized via introducing auxiliary\nvariables. To expedite the solution, we provide a binary quantum particle swarm\noptimization based algorithm with quadratic time complexity. Numerical\ninvestigations demonstrate the superiority of proposed algorithms compared with\nother schemes in terms of energy consumption, response latency, and cache-hit\nratio."
                },
                "authors": [
                    {
                        "name": "Yantong Wang"
                    },
                    {
                        "name": "Ke Liu"
                    },
                    {
                        "name": "Hui Ji"
                    },
                    {
                        "name": "Jiande Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jiande Sun"
                },
                "author": "Jiande Sun",
                "arxiv_comment": "14 pages,10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05996v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05996v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19895v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19895v2",
                "updated": "2024-08-12T07:47:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    7,
                    47,
                    28,
                    0,
                    225,
                    0
                ],
                "published": "2024-07-29T11:17:26Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    11,
                    17,
                    26,
                    0,
                    211,
                    0
                ],
                "title": "Culsans: An Efficient Snoop-based Coherency Unit for the CVA6 Open\n  Source RISC-V application processor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Culsans: An Efficient Snoop-based Coherency Unit for the CVA6 Open\n  Source RISC-V application processor"
                },
                "summary": "Symmetric Multi-Processing (SMP) based on cache coherency is crucial for\nhigh-end embedded systems like automotive applications. RISC-V is gaining\ntraction, and open-source hardware (OSH) platforms offer solutions to issues\nsuch as IP costs and vendor dependency. Existing multi-core cache-coherent\nRISC-V platforms are complex and not efficient for small embedded core\nclusters. We propose an open-source SystemVerilog implementation of a\nlightweight snoop-based cache-coherent cluster of Linux-capable CVA6 cores. Our\ndesign uses the MOESI protocol via the Arm's AMBA ACE protocol. Evaluated with\nSplash-3 benchmarks, our solution shows up to 32.87% faster performance in a\ndual-core setup and an average improvement of 15.8% over OpenPiton. Synthesized\nusing GF 22nm FDSOI technology, the Cache Coherency Unit occupies only 1.6% of\nthe system area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Multi-Processing (SMP) based on cache coherency is crucial for\nhigh-end embedded systems like automotive applications. RISC-V is gaining\ntraction, and open-source hardware (OSH) platforms offer solutions to issues\nsuch as IP costs and vendor dependency. Existing multi-core cache-coherent\nRISC-V platforms are complex and not efficient for small embedded core\nclusters. We propose an open-source SystemVerilog implementation of a\nlightweight snoop-based cache-coherent cluster of Linux-capable CVA6 cores. Our\ndesign uses the MOESI protocol via the Arm's AMBA ACE protocol. Evaluated with\nSplash-3 benchmarks, our solution shows up to 32.87% faster performance in a\ndual-core setup and an average improvement of 15.8% over OpenPiton. Synthesized\nusing GF 22nm FDSOI technology, the Cache Coherency Unit occupies only 1.6% of\nthe system area."
                },
                "authors": [
                    {
                        "name": "Riccardo Tedeschi"
                    },
                    {
                        "name": "Luca Valente"
                    },
                    {
                        "name": "Gianmarco Ottavi"
                    },
                    {
                        "name": "Enrico Zelioli"
                    },
                    {
                        "name": "Nils Wistoff"
                    },
                    {
                        "name": "Massimiliano Giacometti"
                    },
                    {
                        "name": "Abdul Basit Sajjad"
                    },
                    {
                        "name": "Luca Benini"
                    },
                    {
                        "name": "Davide Rossi"
                    }
                ],
                "author_detail": {
                    "name": "Davide Rossi"
                },
                "author": "Davide Rossi",
                "arxiv_comment": "4 pages, 4 figures, DSD2024 and SEAA2024 Works in Progress Session\n  AUG 2024; Updated the acknowledgments",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19895v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19895v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05912v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05912v1",
                "updated": "2024-08-12T03:53:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    3,
                    53,
                    51,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T03:53:51Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    3,
                    53,
                    51,
                    0,
                    225,
                    0
                ],
                "title": "Correct Wrong Path",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Correct Wrong Path"
                },
                "summary": "Modern OOO CPUs have very deep pipelines with large branch misprediction\nrecovery penalties. Speculatively executed instructions on the wrong path can\nsignificantly change cache state, depending on speculation levels. Architects\noften employ trace-driven simulation models in the design exploration stage,\nwhich sacrifice precision for speed. Trace-driven simulators are orders of\nmagnitude faster than execution-driven models, reducing the often hundreds of\nthousands of simulation hours needed to explore new micro-architectural ideas.\nDespite this strong benefit of trace-driven simulation, these often fail to\nadequately model the consequences of wrong path because obtaining them is\nnontrivial. Prior works consider either a positive or negative impact of wrong\npath but not both. Here, we examine wrong path execution in simulation results\nand design a set of infrastructure for enabling wrong-path execution in a trace\ndriven simulator. Our analysis shows the wrong path affects structures on both\nthe instruction and data sides extensively, resulting in performance variations\nranging from $-3.05$\\% to $20.9$\\% when ignoring wrong path. To benefit the\nresearch community and enhance the accuracy of simulators, we opened our traces\nand tracing utility in the hopes that industry can provide wrong-path traces\ngenerated by their internal simulators, enabling academic simulation without\nexposing industry IP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern OOO CPUs have very deep pipelines with large branch misprediction\nrecovery penalties. Speculatively executed instructions on the wrong path can\nsignificantly change cache state, depending on speculation levels. Architects\noften employ trace-driven simulation models in the design exploration stage,\nwhich sacrifice precision for speed. Trace-driven simulators are orders of\nmagnitude faster than execution-driven models, reducing the often hundreds of\nthousands of simulation hours needed to explore new micro-architectural ideas.\nDespite this strong benefit of trace-driven simulation, these often fail to\nadequately model the consequences of wrong path because obtaining them is\nnontrivial. Prior works consider either a positive or negative impact of wrong\npath but not both. Here, we examine wrong path execution in simulation results\nand design a set of infrastructure for enabling wrong-path execution in a trace\ndriven simulator. Our analysis shows the wrong path affects structures on both\nthe instruction and data sides extensively, resulting in performance variations\nranging from $-3.05$\\% to $20.9$\\% when ignoring wrong path. To benefit the\nresearch community and enhance the accuracy of simulators, we opened our traces\nand tracing utility in the hopes that industry can provide wrong-path traces\ngenerated by their internal simulators, enabling academic simulation without\nexposing industry IP."
                },
                "authors": [
                    {
                        "name": "Bhargav Reddy Godala"
                    },
                    {
                        "name": "Sankara Prasad Ramesh"
                    },
                    {
                        "name": "Krishnam Tibrewala"
                    },
                    {
                        "name": "Chrysanthos Pepi"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Svilen Kanev"
                    },
                    {
                        "name": "Gilles A. Pokam"
                    },
                    {
                        "name": "Daniel A. Jiménez"
                    },
                    {
                        "name": "Paul V. Gratz"
                    },
                    {
                        "name": "David I. August"
                    }
                ],
                "author_detail": {
                    "name": "David I. August"
                },
                "author": "David I. August",
                "arxiv_comment": "5 pages, 7 Figures, Submited to Computer Architecture Letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05912v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05912v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12747v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12747v2",
                "updated": "2024-08-11T16:35:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    16,
                    35,
                    10,
                    6,
                    224,
                    0
                ],
                "published": "2024-05-21T12:59:59Z",
                "published_parsed": [
                    2024,
                    5,
                    21,
                    12,
                    59,
                    59,
                    1,
                    142,
                    0
                ],
                "title": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay"
                },
                "summary": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "Added Section IV - (performance analysis of proposed HPDA\n  construction). The term 'coding delay' is formally defined (page no. 5). 14\n  pages, 10 figures and 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.12747v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12747v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.19410v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.19410v2",
                "updated": "2024-08-11T08:07:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    8,
                    7,
                    28,
                    6,
                    224,
                    0
                ],
                "published": "2024-02-29T18:07:58Z",
                "published_parsed": [
                    2024,
                    2,
                    29,
                    18,
                    7,
                    58,
                    3,
                    60,
                    0
                ],
                "title": "Genie: Smart ROS-based Caching for Connected Autonomous Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Genie: Smart ROS-based Caching for Connected Autonomous Robots"
                },
                "summary": "Despite the promising future of autonomous robots, several key issues\ncurrently remain that can lead to compromised performance and safety. One such\nissue is latency, where we find that even the latest embedded platforms from\nNVIDIA fail to execute intelligence tasks (e.g., object detection) of\nautonomous vehicles in a real-time fashion. One remedy to this problem is the\npromising paradigm of edge computing. Through collaboration with our industry\npartner, we identify key prohibitive limitations of the current edge mindset:\n(1) servers are not distributed enough and thus, are not close enough to\nvehicles, (2) current proposed edge solutions do not provide substantially\nbetter performance and extra information specific to autonomous vehicles to\nwarrant their cost to the user, and (3) the state-of-the-art solutions are not\ncompatible with popular frameworks used in autonomous systems, particularly the\nRobot Operating System (ROS).\n  To remedy these issues, we provide Genie, an encapsulation technique that can\nenable transparent caching in ROS in a non-intrusive way (i.e., without\nmodifying the source code), can build the cache in a distributed manner (in\ncontrast to traditional central caching methods), and can construct a\ncollective three-dimensional object map to provide substantially better latency\n(even on low-power edge servers) and higher quality data to all vehicles in a\ncertain locality. We fully implement our design on state-of-the-art\nindustry-adopted embedded and edge platforms, using the prominent autonomous\ndriving software Autoware, and find that Genie can enhance the latency of\nAutoware Vision Detector by 82% on average, enable object reusability 31% of\nthe time on average and as much as 67% for the incoming requests, and boost the\nconfidence in its object map considerably over time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the promising future of autonomous robots, several key issues\ncurrently remain that can lead to compromised performance and safety. One such\nissue is latency, where we find that even the latest embedded platforms from\nNVIDIA fail to execute intelligence tasks (e.g., object detection) of\nautonomous vehicles in a real-time fashion. One remedy to this problem is the\npromising paradigm of edge computing. Through collaboration with our industry\npartner, we identify key prohibitive limitations of the current edge mindset:\n(1) servers are not distributed enough and thus, are not close enough to\nvehicles, (2) current proposed edge solutions do not provide substantially\nbetter performance and extra information specific to autonomous vehicles to\nwarrant their cost to the user, and (3) the state-of-the-art solutions are not\ncompatible with popular frameworks used in autonomous systems, particularly the\nRobot Operating System (ROS).\n  To remedy these issues, we provide Genie, an encapsulation technique that can\nenable transparent caching in ROS in a non-intrusive way (i.e., without\nmodifying the source code), can build the cache in a distributed manner (in\ncontrast to traditional central caching methods), and can construct a\ncollective three-dimensional object map to provide substantially better latency\n(even on low-power edge servers) and higher quality data to all vehicles in a\ncertain locality. We fully implement our design on state-of-the-art\nindustry-adopted embedded and edge platforms, using the prominent autonomous\ndriving software Autoware, and find that Genie can enhance the latency of\nAutoware Vision Detector by 82% on average, enable object reusability 31% of\nthe time on average and as much as 67% for the incoming requests, and boost the\nconfidence in its object map considerably over time."
                },
                "authors": [
                    {
                        "name": "Zexin Li"
                    },
                    {
                        "name": "Soroush Bateni"
                    },
                    {
                        "name": "Cong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Cong Liu"
                },
                "author": "Cong Liu",
                "arxiv_comment": "Submitted to ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.19410v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.19410v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05646v1",
                "updated": "2024-08-10T22:47:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "published": "2024-08-10T22:47:12Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "title": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression"
                },
                "summary": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Gobinda Saha"
                    },
                    {
                        "name": "Sakshi Choudhary"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "12 page, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05614v1",
                "updated": "2024-08-10T19:17:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    10,
                    19,
                    17,
                    46,
                    5,
                    223,
                    0
                ],
                "published": "2024-08-10T19:17:46Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    19,
                    17,
                    46,
                    5,
                    223,
                    0
                ],
                "title": "ICGMM: CXL-enabled Memory Expansion with Intelligent Caching Using\n  Gaussian Mixture Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ICGMM: CXL-enabled Memory Expansion with Intelligent Caching Using\n  Gaussian Mixture Model"
                },
                "summary": "Compute Express Link (CXL) emerges as a solution for wide gap between\ncomputational speed and data communication rates among host and multiple\ndevices. It fosters a unified and coherent memory space between host and CXL\nstorage devices such as such as Solid-state drive (SSD) for memory expansion,\nwith a corresponding DRAM implemented as the device cache. However, this\nintroduces challenges such as substantial cache miss penalties, sub-optimal\ncaching due to data access granularity mismatch between the DRAM \"cache\" and\nSSD \"memory\", and inefficient hardware cache management. To address these\nissues, we propose a novel solution, named ICGMM, which optimizes caching and\neviction directly on hardware, employing a Gaussian Mixture Model (GMM)-based\napproach. We prototype our solution on an FPGA board, which demonstrates a\nnoteworthy improvement compared to the classic Least Recently Used (LRU) cache\nstrategy. We observe a decrease in the cache miss rate ranging from 0.32% to\n6.14%, leading to a substantial 16.23% to 39.14% reduction in the average SSD\naccess latency. Furthermore, when compared to the state-of-the-art Long\nShort-Term Memory (LSTM)-based cache policies, our GMM algorithm on FPGA\nshowcases an impressive latency reduction of over 10,000 times. Remarkably,\nthis is achieved while demanding much fewer hardware resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Express Link (CXL) emerges as a solution for wide gap between\ncomputational speed and data communication rates among host and multiple\ndevices. It fosters a unified and coherent memory space between host and CXL\nstorage devices such as such as Solid-state drive (SSD) for memory expansion,\nwith a corresponding DRAM implemented as the device cache. However, this\nintroduces challenges such as substantial cache miss penalties, sub-optimal\ncaching due to data access granularity mismatch between the DRAM \"cache\" and\nSSD \"memory\", and inefficient hardware cache management. To address these\nissues, we propose a novel solution, named ICGMM, which optimizes caching and\neviction directly on hardware, employing a Gaussian Mixture Model (GMM)-based\napproach. We prototype our solution on an FPGA board, which demonstrates a\nnoteworthy improvement compared to the classic Least Recently Used (LRU) cache\nstrategy. We observe a decrease in the cache miss rate ranging from 0.32% to\n6.14%, leading to a substantial 16.23% to 39.14% reduction in the average SSD\naccess latency. Furthermore, when compared to the state-of-the-art Long\nShort-Term Memory (LSTM)-based cache policies, our GMM algorithm on FPGA\nshowcases an impressive latency reduction of over 10,000 times. Remarkably,\nthis is achieved while demanding much fewer hardware resources."
                },
                "authors": [
                    {
                        "name": "Hanqiu Chen"
                    },
                    {
                        "name": "Yitu Wang"
                    },
                    {
                        "name": "Luis Vitorio Cargnini"
                    },
                    {
                        "name": "Mohammadreza Soltaniyeh"
                    },
                    {
                        "name": "Dongyang Li"
                    },
                    {
                        "name": "Gongjin Sun"
                    },
                    {
                        "name": "Pradeep Subedi"
                    },
                    {
                        "name": "Andrew Chang"
                    },
                    {
                        "name": "Yiran Chen"
                    },
                    {
                        "name": "Cong Hao"
                    }
                ],
                "author_detail": {
                    "name": "Cong Hao"
                },
                "author": "Cong Hao",
                "arxiv_comment": "This paper is accepted by DAC2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05171v1",
                "updated": "2024-08-09T16:48:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    48,
                    1,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T16:48:01Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    48,
                    1,
                    4,
                    222,
                    0
                ],
                "title": "Time-resolved measurement of neutron energy isotropy in a\n  sheared-flow-stabilized Z pinch",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-resolved measurement of neutron energy isotropy in a\n  sheared-flow-stabilized Z pinch"
                },
                "summary": "Previous measurements of neutron energy using fast plastic scintillators\nwhile operating the Fusion Z Pinch Experiment (FuZE) constrained the energy of\nany yield-producing deuteron beams to less than $4.65 keV$. FuZE has since been\noperated at increasingly higher input power, resulting in increased plasma\ncurrent and larger fusion neutron yields. A detailed experimental study of the\nneutron energy isotropy in these regimes applies more stringent limits to\npossible contributions from beam-target fusion. The FuZE device operated at\n$-25~kV$ charge voltage has resulted in average plasma currents of $370~kA$ and\nD-D fusion neutron yields of $4\\times10^7$ neutrons per discharge. Measurements\nof the neutron energy isotropy under these operating conditions demonstrates\nthe energy of deuteron beams is less than $7.4 \\pm 5.6^\\mathrm{(stat)} \\pm\n3.7^\\mathrm{(syst)}~keV$. Characterization of the detector response has reduced\nthe number of free parameters in the fit of the neutron energy distribution,\nimproving the confidence in the forward-fit method. Gamma backgrounds have been\nmeasured and the impact of these contributions on the isotropy results have\nbeen studied. Additionally, a time dependent measurement of the isotropy has\nbeen resolved for the first time, indicating increases to possible deuteron\nbeam energies at late times. This suggests the possible growth of $m$=0\ninstabilities at the end of the main radiation event but confirms that the\nmajority of the neutron production exhibits isotropy consistent with\nthermonuclear origin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous measurements of neutron energy using fast plastic scintillators\nwhile operating the Fusion Z Pinch Experiment (FuZE) constrained the energy of\nany yield-producing deuteron beams to less than $4.65 keV$. FuZE has since been\noperated at increasingly higher input power, resulting in increased plasma\ncurrent and larger fusion neutron yields. A detailed experimental study of the\nneutron energy isotropy in these regimes applies more stringent limits to\npossible contributions from beam-target fusion. The FuZE device operated at\n$-25~kV$ charge voltage has resulted in average plasma currents of $370~kA$ and\nD-D fusion neutron yields of $4\\times10^7$ neutrons per discharge. Measurements\nof the neutron energy isotropy under these operating conditions demonstrates\nthe energy of deuteron beams is less than $7.4 \\pm 5.6^\\mathrm{(stat)} \\pm\n3.7^\\mathrm{(syst)}~keV$. Characterization of the detector response has reduced\nthe number of free parameters in the fit of the neutron energy distribution,\nimproving the confidence in the forward-fit method. Gamma backgrounds have been\nmeasured and the impact of these contributions on the isotropy results have\nbeen studied. Additionally, a time dependent measurement of the isotropy has\nbeen resolved for the first time, indicating increases to possible deuteron\nbeam energies at late times. This suggests the possible growth of $m$=0\ninstabilities at the end of the main radiation event but confirms that the\nmajority of the neutron production exhibits isotropy consistent with\nthermonuclear origin."
                },
                "authors": [
                    {
                        "name": "R. A. Ryan"
                    },
                    {
                        "name": "P. E. Tsai"
                    },
                    {
                        "name": "A. R. Johansen"
                    },
                    {
                        "name": "A. Youmans"
                    },
                    {
                        "name": "D. P. Higginson"
                    },
                    {
                        "name": "J. M. Mitrani"
                    },
                    {
                        "name": "C. S. Adams"
                    },
                    {
                        "name": "D. A. Sutherland"
                    },
                    {
                        "name": "B. Levitt"
                    },
                    {
                        "name": "U. Shumlak"
                    }
                ],
                "author_detail": {
                    "name": "U. Shumlak"
                },
                "author": "U. Shumlak",
                "arxiv_comment": "16 pages, 11 figures, submitted to Journal of Nuclear Fusion",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03675v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03675v2",
                "updated": "2024-08-08T01:20:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    8,
                    1,
                    20,
                    13,
                    3,
                    221,
                    0
                ],
                "published": "2024-08-07T10:31:07Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    10,
                    31,
                    7,
                    2,
                    220,
                    0
                ],
                "title": "NACL: A General and Effective KV Cache Eviction Framework for LLMs at\n  Inference Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NACL: A General and Effective KV Cache Eviction Framework for LLMs at\n  Inference Time"
                },
                "summary": "Large Language Models (LLMs) have ignited an innovative surge of AI\napplications, marking a new era of exciting possibilities equipped with\nextended context windows. However, hosting these models is cost-prohibitive\nmainly due to the extensive memory consumption of KV Cache involving\nlong-context modeling. Despite several works proposing to evict unnecessary\ntokens from the KV Cache, most of them rely on the biased local statistics of\naccumulated attention scores and report performance using unconvincing metric\nlike perplexity on inadequate short-text evaluation. In this paper, we propose\nNACL, a general framework for long-context KV cache eviction that achieves more\noptimal and efficient eviction in a single operation during the encoding phase.\nDue to NACL's efficiency, we combine more accurate attention score statistics\nin PROXY TOKENS EVICTION with the diversified random eviction strategy of\nRANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance\nthe robustness in maintaining pivotal tokens for long-context modeling tasks.\nNotably, our method significantly improves the performance on short- and\nlong-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50%\nwith over 95% performance maintenance. The code is available at\nhttps://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have ignited an innovative surge of AI\napplications, marking a new era of exciting possibilities equipped with\nextended context windows. However, hosting these models is cost-prohibitive\nmainly due to the extensive memory consumption of KV Cache involving\nlong-context modeling. Despite several works proposing to evict unnecessary\ntokens from the KV Cache, most of them rely on the biased local statistics of\naccumulated attention scores and report performance using unconvincing metric\nlike perplexity on inadequate short-text evaluation. In this paper, we propose\nNACL, a general framework for long-context KV cache eviction that achieves more\noptimal and efficient eviction in a single operation during the encoding phase.\nDue to NACL's efficiency, we combine more accurate attention score statistics\nin PROXY TOKENS EVICTION with the diversified random eviction strategy of\nRANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance\nthe robustness in maintaining pivotal tokens for long-context modeling tasks.\nNotably, our method significantly improves the performance on short- and\nlong-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50%\nwith over 95% performance maintenance. The code is available at\nhttps://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL."
                },
                "authors": [
                    {
                        "name": "Yilong Chen"
                    },
                    {
                        "name": "Guoxia Wang"
                    },
                    {
                        "name": "Junyuan Shang"
                    },
                    {
                        "name": "Shiyao Cui"
                    },
                    {
                        "name": "Zhenyu Zhang"
                    },
                    {
                        "name": "Tingwen Liu"
                    },
                    {
                        "name": "Shuohuan Wang"
                    },
                    {
                        "name": "Yu Sun"
                    },
                    {
                        "name": "Dianhai Yu"
                    },
                    {
                        "name": "Hua Wu"
                    }
                ],
                "author_detail": {
                    "name": "Hua Wu"
                },
                "author": "Hua Wu",
                "arxiv_comment": "Accepted by ACL 2024 (main conference, long paper)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03675v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03675v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2210.10978v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2210.10978v2",
                "updated": "2024-08-07T23:48:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    23,
                    48,
                    59,
                    2,
                    220,
                    0
                ],
                "published": "2022-10-20T02:58:36Z",
                "published_parsed": [
                    2022,
                    10,
                    20,
                    2,
                    58,
                    36,
                    3,
                    293,
                    0
                ],
                "title": "A Comprehensive Survey on Edge Data Integrity Verification: Fundamentals\n  and Future Trends",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Survey on Edge Data Integrity Verification: Fundamentals\n  and Future Trends"
                },
                "summary": "Recent advances in edge computing~(EC) have pushed cloud-based data caching\nservices to edge, however, such emerging edge storage comes with numerous\nchallenging and unique security issues. One of them is the problem of edge data\nintegrity verification (EDIV) which coordinates multiple participants (e.g.,\ndata owners and edge nodes) to inspect whether data cached on edge is\nauthentic. To date, various solutions have been proposed to address the EDIV\nproblem, while there is no systematic review. Thus, we offer a comprehensive\nsurvey for the first time, aiming to show current research status, open\nproblems, and potentially promising insights for readers to further investigate\nthis under-explored field. Specifically, we begin by stating the significance\nof the EDIV problem, the integrity verification difference between data cached\non cloud and edge, and three typical system models with corresponding\ninspection processes. To thoroughly assess prior research efforts, we\nsynthesize a universal criteria framework that an effective verification\napproach should satisfy. On top of it, a schematic development timeline is\ndeveloped to reveal the research advance on EDIV in a sequential manner,\nfollowed by a detailed review of the existing EDIV solutions. Finally, we\nhighlight intriguing research challenges and possible directions for future\nwork, along with a discussion on how forthcoming technology, e.g., machine\nlearning and context-aware security, can augment security in EC. Given our\nfindings, some major observations are: there is a noticeable trend to equip\nEDIV solutions with various functions and diversify study scenarios; completing\nEDIV within two types of participants (i.e., data owner and edge nodes) is\ngarnering escalating interest among researchers; although the majority of\nexisting methods rely on cryptography, emerging technology is being explored to\nhandle the EDIV problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in edge computing~(EC) have pushed cloud-based data caching\nservices to edge, however, such emerging edge storage comes with numerous\nchallenging and unique security issues. One of them is the problem of edge data\nintegrity verification (EDIV) which coordinates multiple participants (e.g.,\ndata owners and edge nodes) to inspect whether data cached on edge is\nauthentic. To date, various solutions have been proposed to address the EDIV\nproblem, while there is no systematic review. Thus, we offer a comprehensive\nsurvey for the first time, aiming to show current research status, open\nproblems, and potentially promising insights for readers to further investigate\nthis under-explored field. Specifically, we begin by stating the significance\nof the EDIV problem, the integrity verification difference between data cached\non cloud and edge, and three typical system models with corresponding\ninspection processes. To thoroughly assess prior research efforts, we\nsynthesize a universal criteria framework that an effective verification\napproach should satisfy. On top of it, a schematic development timeline is\ndeveloped to reveal the research advance on EDIV in a sequential manner,\nfollowed by a detailed review of the existing EDIV solutions. Finally, we\nhighlight intriguing research challenges and possible directions for future\nwork, along with a discussion on how forthcoming technology, e.g., machine\nlearning and context-aware security, can augment security in EC. Given our\nfindings, some major observations are: there is a noticeable trend to equip\nEDIV solutions with various functions and diversify study scenarios; completing\nEDIV within two types of participants (i.e., data owner and edge nodes) is\ngarnering escalating interest among researchers; although the majority of\nexisting methods rely on cryptography, emerging technology is being explored to\nhandle the EDIV problem."
                },
                "authors": [
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Youyang Qu"
                    },
                    {
                        "name": "Yong Xiang"
                    },
                    {
                        "name": "Md Palash Uddin"
                    },
                    {
                        "name": "Dezhong Peng"
                    },
                    {
                        "name": "Longxiang Gao"
                    }
                ],
                "author_detail": {
                    "name": "Longxiang Gao"
                },
                "author": "Longxiang Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2210.10978v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2210.10978v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04107v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04107v1",
                "updated": "2024-08-07T22:10:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    22,
                    10,
                    26,
                    2,
                    220,
                    0
                ],
                "published": "2024-08-07T22:10:26Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    22,
                    10,
                    26,
                    2,
                    220,
                    0
                ],
                "title": "Zero-Delay QKV Compression for Mitigating KV Cache and Network\n  Bottlenecks in LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Delay QKV Compression for Mitigating KV Cache and Network\n  Bottlenecks in LLM Inference"
                },
                "summary": "In large-language models, memory constraints in the key-value cache (KVC)\npose a challenge during inference, especially with long prompts. In this work,\nwe observed that compressing KV values is more effective than compressing the\nmodel regarding accuracy and job completion time (JCT). However, quantizing KV\nvalues and dropping less-important tokens incur significant runtime\ncomputational time overhead, delaying JCT. These methods also cannot reduce\ncomputation time or high network communication time overhead in\nsequence-parallelism (SP) frameworks for long prompts. To tackle these issues,\nbased on our insightful observations from experimental analysis, we propose\nZeroC, a Zero-delay QKV Compression system that eliminates time overhead and\neven reduces computation and communication time of the model operations. ZeroC\ninnovatively embeds compression and decompression operations within model\noperations and adaptively determines compression ratios at a hybrid layer-token\nlevel. Further, it enables a communication-efficient SP inference framework.\nTrace-driven experiments demonstrate that ZeroC achieves up to 80% lower\naverage JCT, 35% lower average perplexity, and 2.8x higher throughput with the\nsame latency compared to state-of-the-art compression methods. ZeroC also\nreduces the average JCT of current LLM serving systems by up to 91% with the\nconstraint of 0.1 perplexity increase. We open-sourced the code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large-language models, memory constraints in the key-value cache (KVC)\npose a challenge during inference, especially with long prompts. In this work,\nwe observed that compressing KV values is more effective than compressing the\nmodel regarding accuracy and job completion time (JCT). However, quantizing KV\nvalues and dropping less-important tokens incur significant runtime\ncomputational time overhead, delaying JCT. These methods also cannot reduce\ncomputation time or high network communication time overhead in\nsequence-parallelism (SP) frameworks for long prompts. To tackle these issues,\nbased on our insightful observations from experimental analysis, we propose\nZeroC, a Zero-delay QKV Compression system that eliminates time overhead and\neven reduces computation and communication time of the model operations. ZeroC\ninnovatively embeds compression and decompression operations within model\noperations and adaptively determines compression ratios at a hybrid layer-token\nlevel. Further, it enables a communication-efficient SP inference framework.\nTrace-driven experiments demonstrate that ZeroC achieves up to 80% lower\naverage JCT, 35% lower average perplexity, and 2.8x higher throughput with the\nsame latency compared to state-of-the-art compression methods. ZeroC also\nreduces the average JCT of current LLM serving systems by up to 91% with the\nconstraint of 0.1 perplexity increase. We open-sourced the code."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Haiying Shen"
                    }
                ],
                "author_detail": {
                    "name": "Haiying Shen"
                },
                "author": "Haiying Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04107v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04107v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19547v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19547v2",
                "updated": "2024-08-07T20:43:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    20,
                    43,
                    10,
                    2,
                    220,
                    0
                ],
                "published": "2024-07-28T17:46:15Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    17,
                    46,
                    15,
                    6,
                    210,
                    0
                ],
                "title": "Temporal Feature Matters: A Framework for Diffusion Model Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Feature Matters: A Framework for Diffusion Model Quantization"
                },
                "summary": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration..",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration.."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Yuhang Li"
                    },
                    {
                        "name": "Jiwen Lu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2311.16503",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19547v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19547v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03652v1",
                "updated": "2024-08-07T09:34:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    9,
                    34,
                    55,
                    2,
                    220,
                    0
                ],
                "published": "2024-08-07T09:34:55Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    9,
                    34,
                    55,
                    2,
                    220,
                    0
                ],
                "title": "mucAI at WojoodNER 2024: Arabic Named Entity Recognition with Nearest\n  Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "mucAI at WojoodNER 2024: Arabic Named Entity Recognition with Nearest\n  Neighbor Search"
                },
                "summary": "Named Entity Recognition (NER) is a task in Natural Language Processing (NLP)\nthat aims to identify and classify entities in text into predefined categories.\nHowever, when applied to Arabic data, NER encounters unique challenges stemming\nfrom the language's rich morphological inflections, absence of capitalization\ncues, and spelling variants, where a single word can comprise multiple\nmorphemes. In this paper, we introduce Arabic KNN-NER, our submission to the\nWojood NER Shared Task 2024 (ArabicNLP 2024). We have participated in the\nshared sub-task 1 Flat NER. In this shared sub-task, we tackle fine-grained\nflat-entity recognition for Arabic text, where we identify a single main entity\nand possibly zero or multiple sub-entities for each word. Arabic KNN-NER\naugments the probability distribution of a fine-tuned model with another label\nprobability distribution derived from performing a KNN search over the cached\ntraining data. Our submission achieved 91% on the test set on the WojoodFine\ndataset, placing Arabic KNN-NER on top of the leaderboard for the shared task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Named Entity Recognition (NER) is a task in Natural Language Processing (NLP)\nthat aims to identify and classify entities in text into predefined categories.\nHowever, when applied to Arabic data, NER encounters unique challenges stemming\nfrom the language's rich morphological inflections, absence of capitalization\ncues, and spelling variants, where a single word can comprise multiple\nmorphemes. In this paper, we introduce Arabic KNN-NER, our submission to the\nWojood NER Shared Task 2024 (ArabicNLP 2024). We have participated in the\nshared sub-task 1 Flat NER. In this shared sub-task, we tackle fine-grained\nflat-entity recognition for Arabic text, where we identify a single main entity\nand possibly zero or multiple sub-entities for each word. Arabic KNN-NER\naugments the probability distribution of a fine-tuned model with another label\nprobability distribution derived from performing a KNN search over the cached\ntraining data. Our submission achieved 91% on the test set on the WojoodFine\ndataset, placing Arabic KNN-NER on top of the leaderboard for the shared task."
                },
                "authors": [
                    {
                        "name": "Ahmed Abdou"
                    },
                    {
                        "name": "Tasneem Mohsen"
                    }
                ],
                "author_detail": {
                    "name": "Tasneem Mohsen"
                },
                "author": "Tasneem Mohsen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02999v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02999v1",
                "updated": "2024-08-06T07:12:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    7,
                    12,
                    9,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T07:12:09Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    7,
                    12,
                    9,
                    1,
                    219,
                    0
                ],
                "title": "LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning"
                },
                "summary": "The emergence of intelligence in large language models (LLMs) has inspired\ninvestigations into their integration into automata learning. This paper\nintroduces the probabilistic Minimally Adequate Teacher (pMAT) formulation,\nwhich leverages a probabilistic oracle that could give persistent errors\nrandomly during answering the membership queries for deterministic finite\nautomata (DFA) learning. Given the tendency of LLMs to produce hallucinatory\ncontent, we have developed techniques to improve answer accuracy and ensure the\ncorrectness of the learned automata. We propose the $\\mathtt{Discrimination}$\nprompt as well as the $\\mathtt{Verification}$ prompt and explore their\nadvantages over common prompts. Additionally, we compare DFA learning\nperformance between the TTT algorithm and common active learning algorithms. To\naddress the exponential number of persistent errors, we implement a dynamic\nquery cache refinement algorithm that identifies and corrects conflicting\nqueries by combining the active and passive learning algorithms. The empirical\nresults demonstrate the robustness and efficiency of our approach, providing a\ntheoretical foundation for automata learning with LLMs in the loop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of intelligence in large language models (LLMs) has inspired\ninvestigations into their integration into automata learning. This paper\nintroduces the probabilistic Minimally Adequate Teacher (pMAT) formulation,\nwhich leverages a probabilistic oracle that could give persistent errors\nrandomly during answering the membership queries for deterministic finite\nautomata (DFA) learning. Given the tendency of LLMs to produce hallucinatory\ncontent, we have developed techniques to improve answer accuracy and ensure the\ncorrectness of the learned automata. We propose the $\\mathtt{Discrimination}$\nprompt as well as the $\\mathtt{Verification}$ prompt and explore their\nadvantages over common prompts. Additionally, we compare DFA learning\nperformance between the TTT algorithm and common active learning algorithms. To\naddress the exponential number of persistent errors, we implement a dynamic\nquery cache refinement algorithm that identifies and corrects conflicting\nqueries by combining the active and passive learning algorithms. The empirical\nresults demonstrate the robustness and efficiency of our approach, providing a\ntheoretical foundation for automata learning with LLMs in the loop."
                },
                "authors": [
                    {
                        "name": "Lekai Chen"
                    },
                    {
                        "name": "Ashutosh Trivedi"
                    },
                    {
                        "name": "Alvaro Velasquez"
                    }
                ],
                "author_detail": {
                    "name": "Alvaro Velasquez"
                },
                "author": "Alvaro Velasquez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02999v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02999v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.FL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02911v1",
                "updated": "2024-08-06T02:51:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    2,
                    51,
                    22,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T02:51:22Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    2,
                    51,
                    22,
                    1,
                    219,
                    0
                ],
                "title": "NVPC: A Transparent NVM Page Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NVPC: A Transparent NVM Page Cache"
                },
                "summary": "Towards a compatible utilization of NVM, NVM-specialized kernel file systems\nand NVM-based disk file system accelerators have been proposed. However, these\nstudies only focus on one or several characteristics of NVM, while failing to\nexploit its best practice by putting NVM in the proper position of the whole\nstorage stack. In this paper, we present NVPC, a transparent acceleration to\nexisting kernel file systems with an NVM-enhanced page cache. The acceleration\nlies in two aspects, respectively matching the desperate needs of existing disk\nfile systems: sync writes and cache-missed operations. Besides, the fast DRAM\npage cache is preserved for cache-hit operations. For sync writes, a\nhigh-performance log-based sync absorbing area is provided to redirect data\ndestination from the slow disk to the fast NVM. Meanwhile, the byte-addressable\nfeature of NVM is used to prevent write amplification. For cache-missed\noperations, NVPC makes use of the idle space on NVM to extend the DRAM page\ncache, so that more and larger workloads can fit into the cache. NVPC is\nentirely implemented as a page cache, thus can provide efficient speed-up to\ndisk file systems with full transparency to users and full compatibility to\nlower file systems.\n  In Filebench macro-benchmarks, NVPC achieves at most 3.55x, 2.84x, and 2.64x\nfaster than NOVA, Ext-4, and SPFS. In RocksDB workloads with working set larger\nthan DRAM, NVPC achieves 1.12x, 2.59x, and 2.11x faster than NOVA, Ext-4, and\nSPFS. Meanwhile, NVPC gains positive revenue from NOVA, Ext-4, and SPFS in\n62.5% of the tested cases in our read/write/sync mixed evaluation,\ndemonstrating that NVPC is more balanced and adaptive to complex real-world\nworkloads. Experimental results also show that NVPC is the only method that\naccelerates Ext-4 in particular cases for up to 15.19x, with no slow-down to\nany other use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a compatible utilization of NVM, NVM-specialized kernel file systems\nand NVM-based disk file system accelerators have been proposed. However, these\nstudies only focus on one or several characteristics of NVM, while failing to\nexploit its best practice by putting NVM in the proper position of the whole\nstorage stack. In this paper, we present NVPC, a transparent acceleration to\nexisting kernel file systems with an NVM-enhanced page cache. The acceleration\nlies in two aspects, respectively matching the desperate needs of existing disk\nfile systems: sync writes and cache-missed operations. Besides, the fast DRAM\npage cache is preserved for cache-hit operations. For sync writes, a\nhigh-performance log-based sync absorbing area is provided to redirect data\ndestination from the slow disk to the fast NVM. Meanwhile, the byte-addressable\nfeature of NVM is used to prevent write amplification. For cache-missed\noperations, NVPC makes use of the idle space on NVM to extend the DRAM page\ncache, so that more and larger workloads can fit into the cache. NVPC is\nentirely implemented as a page cache, thus can provide efficient speed-up to\ndisk file systems with full transparency to users and full compatibility to\nlower file systems.\n  In Filebench macro-benchmarks, NVPC achieves at most 3.55x, 2.84x, and 2.64x\nfaster than NOVA, Ext-4, and SPFS. In RocksDB workloads with working set larger\nthan DRAM, NVPC achieves 1.12x, 2.59x, and 2.11x faster than NOVA, Ext-4, and\nSPFS. Meanwhile, NVPC gains positive revenue from NOVA, Ext-4, and SPFS in\n62.5% of the tested cases in our read/write/sync mixed evaluation,\ndemonstrating that NVPC is more balanced and adaptive to complex real-world\nworkloads. Experimental results also show that NVPC is the only method that\naccelerates Ext-4 in particular cases for up to 15.19x, with no slow-down to\nany other use cases."
                },
                "authors": [
                    {
                        "name": "Guoyu Wang"
                    },
                    {
                        "name": "Xilong Che"
                    },
                    {
                        "name": "Haoyang Wei"
                    },
                    {
                        "name": "Shuo Chen"
                    },
                    {
                        "name": "Puyi He"
                    },
                    {
                        "name": "Juncheng Hu"
                    }
                ],
                "author_detail": {
                    "name": "Juncheng Hu"
                },
                "author": "Juncheng Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02409v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02409v1",
                "updated": "2024-08-05T12:09:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    5,
                    12,
                    9,
                    50,
                    0,
                    218,
                    0
                ],
                "published": "2024-08-05T12:09:50Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    12,
                    9,
                    50,
                    0,
                    218,
                    0
                ],
                "title": "Electron-beam-induced modification of gold microparticles in an SEM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron-beam-induced modification of gold microparticles in an SEM"
                },
                "summary": "Electron-beam-induced conversion of materials in a transmission electron\nmicroscope uses the high power density of a localized electron beam of\nacceleration voltages above 100 kV as an energy source to transform matter at\nthe sub-micron scale. Here, the e-beam-induced transformation of precursor\nmicroparticles employing a low-energy e-beam with an acceleration voltage of 30\nkV in a scanning electron microscope is developed to increase the versatility\nand efficiency of the technique. Under these conditions, the technique can be\nclassified between e-beam lithography, where the e-beam is used to mill holes\nin or grow some different material onto a substrate, and e-beam welding, where\nmatter can be welded together when overcoming the melting phase. Modifying gold\nmicroparticles on an amorphous SiOx substrate reveals the dominant role of\ninelastic electron-matter interaction and subsequent localized heating for the\nobserved melting and vaporization of the precursor microparticles under the\nelectron beam. Monte-Carlo scattering simulations and thermodynamic modeling\nfurther support the findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron-beam-induced conversion of materials in a transmission electron\nmicroscope uses the high power density of a localized electron beam of\nacceleration voltages above 100 kV as an energy source to transform matter at\nthe sub-micron scale. Here, the e-beam-induced transformation of precursor\nmicroparticles employing a low-energy e-beam with an acceleration voltage of 30\nkV in a scanning electron microscope is developed to increase the versatility\nand efficiency of the technique. Under these conditions, the technique can be\nclassified between e-beam lithography, where the e-beam is used to mill holes\nin or grow some different material onto a substrate, and e-beam welding, where\nmatter can be welded together when overcoming the melting phase. Modifying gold\nmicroparticles on an amorphous SiOx substrate reveals the dominant role of\ninelastic electron-matter interaction and subsequent localized heating for the\nobserved melting and vaporization of the precursor microparticles under the\nelectron beam. Monte-Carlo scattering simulations and thermodynamic modeling\nfurther support the findings."
                },
                "authors": [
                    {
                        "name": "Kristina Weinel"
                    },
                    {
                        "name": "Marc Benjamin Hahn"
                    },
                    {
                        "name": "Axel Lubk"
                    },
                    {
                        "name": "Wen Feng"
                    },
                    {
                        "name": "Ignacio Gonzalez Martinez"
                    },
                    {
                        "name": "Bernd Büchner"
                    },
                    {
                        "name": "Leonardo Agudo Jácome"
                    }
                ],
                "author_detail": {
                    "name": "Leonardo Agudo Jácome"
                },
                "author": "Leonardo Agudo Jácome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02409v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02409v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05235v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05235v1",
                "updated": "2024-08-05T09:07:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    5,
                    9,
                    7,
                    6,
                    0,
                    218,
                    0
                ],
                "published": "2024-08-05T09:07:06Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    9,
                    7,
                    6,
                    0,
                    218,
                    0
                ],
                "title": "SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference\n  Serving"
                },
                "summary": "As Large Language Models (LLMs) gain traction, their reliance on power-hungry\nGPUs places ever-increasing energy demands, raising environmental and monetary\nconcerns. Inference dominates LLM workloads, presenting a critical challenge\nfor providers: minimizing energy costs under Service-Level Objectives (SLOs)\nthat ensure optimal user experience. In this paper, we present\n\\textit{throttLL'eM}, a framework that reduces energy consumption while meeting\nSLOs through the use of instance and GPU frequency scaling.\n\\textit{throttLL'eM} features mechanisms that project future KV cache usage and\nbatch size. Leveraging a Machine-Learning (ML) model that receives these\nprojections as inputs, \\textit{throttLL'eM} manages performance at the\niteration level to satisfy SLOs with reduced frequencies and instance sizes. We\nshow that the proposed ML model achieves $R^2$ scores greater than 0.97 and\nmiss-predicts performance by less than 1 iteration per second on average.\nExperimental results on LLM inference traces show that \\textit{throttLL'eM}\nachieves up to 43.8\\% lower energy consumption and an energy efficiency\nimprovement of at least $1.71\\times$ under SLOs, when compared to NVIDIA's\nTriton server.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) gain traction, their reliance on power-hungry\nGPUs places ever-increasing energy demands, raising environmental and monetary\nconcerns. Inference dominates LLM workloads, presenting a critical challenge\nfor providers: minimizing energy costs under Service-Level Objectives (SLOs)\nthat ensure optimal user experience. In this paper, we present\n\\textit{throttLL'eM}, a framework that reduces energy consumption while meeting\nSLOs through the use of instance and GPU frequency scaling.\n\\textit{throttLL'eM} features mechanisms that project future KV cache usage and\nbatch size. Leveraging a Machine-Learning (ML) model that receives these\nprojections as inputs, \\textit{throttLL'eM} manages performance at the\niteration level to satisfy SLOs with reduced frequencies and instance sizes. We\nshow that the proposed ML model achieves $R^2$ scores greater than 0.97 and\nmiss-predicts performance by less than 1 iteration per second on average.\nExperimental results on LLM inference traces show that \\textit{throttLL'eM}\nachieves up to 43.8\\% lower energy consumption and an energy efficiency\nimprovement of at least $1.71\\times$ under SLOs, when compared to NVIDIA's\nTriton server."
                },
                "authors": [
                    {
                        "name": "Andreas Kosmas Kakolyris"
                    },
                    {
                        "name": "Dimosthenis Masouros"
                    },
                    {
                        "name": "Petros Vavaroutsos"
                    },
                    {
                        "name": "Sotirios Xydis"
                    },
                    {
                        "name": "Dimitrios Soudris"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Soudris"
                },
                "author": "Dimitrios Soudris",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05235v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05235v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11912v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11912v3",
                "updated": "2024-08-04T00:58:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    58,
                    4,
                    6,
                    217,
                    0
                ],
                "published": "2024-04-18T05:25:54Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    5,
                    25,
                    54,
                    3,
                    109,
                    0
                ],
                "title": "TriForce: Lossless Acceleration of Long Sequence Generation with\n  Hierarchical Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TriForce: Lossless Acceleration of Long Sequence Generation with\n  Hierarchical Speculative Decoding"
                },
                "summary": "With large language models (LLMs) widely deployed in long content generation\nrecently, there has emerged an increasing demand for efficient long-sequence\ninference support. However, key-value (KV) cache, which is stored to avoid\nre-computation, has emerged as a critical bottleneck by growing linearly in\nsize with the sequence length. Due to the auto-regressive nature of LLMs, the\nentire KV cache will be loaded for every generated token, resulting in low\nutilization of computational cores and high latency. While various compression\nmethods for KV cache have been proposed to alleviate this issue, they suffer\nfrom degradation in generation quality. We introduce TriForce, a hierarchical\nspeculative decoding system that is scalable for long sequence generation. This\napproach leverages the original model weights and dynamic sparse KV cache via\nretrieval as a draft model, which serves as an intermediate layer in the\nhierarchy and is further speculated by a smaller model to reduce its drafting\nlatency. TriForce not only facilitates impressive speedups for Llama2-7B-128K,\nachieving up to 2.31$\\times$ on an A100 GPU but also showcases scalability in\nhandling even longer contexts. For the offloading setting on two RTX 4090 GPUs,\nTriForce achieves 0.108s/token$\\unicode{x2014}$only half as slow as the\nauto-regressive baseline on an A100, which attains 7.78$\\times$ on our\noptimized offloading system. Additionally, TriForce performs 4.86$\\times$ than\nDeepSpeed-Zero-Inference on a single RTX 4090 GPU. TriForce's robustness is\nhighlighted by its consistently outstanding performance across various\ntemperatures. The code is available at\nhttps://github.com/Infini-AI-Lab/TriForce.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With large language models (LLMs) widely deployed in long content generation\nrecently, there has emerged an increasing demand for efficient long-sequence\ninference support. However, key-value (KV) cache, which is stored to avoid\nre-computation, has emerged as a critical bottleneck by growing linearly in\nsize with the sequence length. Due to the auto-regressive nature of LLMs, the\nentire KV cache will be loaded for every generated token, resulting in low\nutilization of computational cores and high latency. While various compression\nmethods for KV cache have been proposed to alleviate this issue, they suffer\nfrom degradation in generation quality. We introduce TriForce, a hierarchical\nspeculative decoding system that is scalable for long sequence generation. This\napproach leverages the original model weights and dynamic sparse KV cache via\nretrieval as a draft model, which serves as an intermediate layer in the\nhierarchy and is further speculated by a smaller model to reduce its drafting\nlatency. TriForce not only facilitates impressive speedups for Llama2-7B-128K,\nachieving up to 2.31$\\times$ on an A100 GPU but also showcases scalability in\nhandling even longer contexts. For the offloading setting on two RTX 4090 GPUs,\nTriForce achieves 0.108s/token$\\unicode{x2014}$only half as slow as the\nauto-regressive baseline on an A100, which attains 7.78$\\times$ on our\noptimized offloading system. Additionally, TriForce performs 4.86$\\times$ than\nDeepSpeed-Zero-Inference on a single RTX 4090 GPU. TriForce's robustness is\nhighlighted by its consistently outstanding performance across various\ntemperatures. The code is available at\nhttps://github.com/Infini-AI-Lab/TriForce."
                },
                "authors": [
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "arxiv_comment": "COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11912v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11912v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01890v1",
                "updated": "2024-08-04T00:38:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    38,
                    34,
                    6,
                    217,
                    0
                ],
                "published": "2024-08-04T00:38:34Z",
                "published_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    38,
                    34,
                    6,
                    217,
                    0
                ],
                "title": "Cross-layer Attention Sharing for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-layer Attention Sharing for Large Language Models"
                },
                "summary": "As large language models (LLMs) evolve, the increase in model depth and\nparameter number leads to substantial redundancy. To enhance the efficiency of\nthe attention mechanism, previous works primarily compress the KV cache or\ngroup attention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to save the computation by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights. Driven by these\ninsights, we introduce LiSA, a lightweight substitute for self-attention in\nwell-trained LLMs. LiSA employs tiny feed-forward networks to align attention\nheads between adjacent layers and low-rank matrices to approximate differences\nin layer-wise attention weights. Evaluations encompassing 13 typical benchmarks\ndemonstrate that LiSA maintains high response quality in terms of accuracy and\nperplexity while reducing redundant attention calculations within 53-84% of the\ntotal layers. Our implementations of LiSA achieve a 6X compression of Q and K,\nwith maximum throughput improvements of 19.5% for LLaMA3-8B and 32.3% for\nLLaMA2-7B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) evolve, the increase in model depth and\nparameter number leads to substantial redundancy. To enhance the efficiency of\nthe attention mechanism, previous works primarily compress the KV cache or\ngroup attention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to save the computation by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights. Driven by these\ninsights, we introduce LiSA, a lightweight substitute for self-attention in\nwell-trained LLMs. LiSA employs tiny feed-forward networks to align attention\nheads between adjacent layers and low-rank matrices to approximate differences\nin layer-wise attention weights. Evaluations encompassing 13 typical benchmarks\ndemonstrate that LiSA maintains high response quality in terms of accuracy and\nperplexity while reducing redundant attention calculations within 53-84% of the\ntotal layers. Our implementations of LiSA achieve a 6X compression of Q and K,\nwith maximum throughput improvements of 19.5% for LLaMA3-8B and 32.3% for\nLLaMA2-7B."
                },
                "authors": [
                    {
                        "name": "Yongyu Mu"
                    },
                    {
                        "name": "Yuzhang Wu"
                    },
                    {
                        "name": "Yuchun Fan"
                    },
                    {
                        "name": "Chenglong Wang"
                    },
                    {
                        "name": "Hengyu Li"
                    },
                    {
                        "name": "Qiaozhi He"
                    },
                    {
                        "name": "Murun Yang"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "arxiv_comment": "Working in process",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01519v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01519v1",
                "updated": "2024-08-02T18:25:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    2,
                    18,
                    25,
                    57,
                    4,
                    215,
                    0
                ],
                "published": "2024-08-02T18:25:57Z",
                "published_parsed": [
                    2024,
                    8,
                    2,
                    18,
                    25,
                    57,
                    4,
                    215,
                    0
                ],
                "title": "Multi-Material Decomposition Using Spectral Diffusion Posterior Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Material Decomposition Using Spectral Diffusion Posterior Sampling"
                },
                "summary": "Many spectral CT applications require accurate material decomposition.\nExisting material decomposition algorithms are often susceptible to significant\nnoise magnification or, in the case of one-step model-based approaches,\nhampered by slow convergence rates and large computational requirements. In\nthis work, we proposed a novel framework - spectral diffusion posterior\nsampling (spectral DPS) - for one-step reconstruction and multi-material\ndecomposition, which combines sophisticated prior information captured by\none-time unsupervised learning and an arbitrary analytic physical system model.\nSpectral DPS is built upon a general DPS framework for nonlinear inverse\nproblems. Several strategies developed in previous work, including jumpstart\nsampling, Jacobian approximation, and multi-step likelihood updates are applied\nfacilitate stable and accurate decompositions. The effectiveness of spectral\nDPS was evaluated on a simulated dual-layer and a kV-switching spectral system\nas well as on a physical cone-beam CT (CBCT) test bench. In simulation studies,\nspectral DPS improved PSNR by 27.49% to 71.93% over baseline DPS and by 26.53%\nto 57.30% over MBMD, depending on the the region of interest. In physical\nphantom study, spectral DPS achieved a <1% error in estimating the mean density\nin a homogeneous region. Compared with baseline DPS, spectral DPS effectively\navoided generating false structures in the homogeneous phantom and reduced the\nvariability around edges. Both simulation and physical phantom studies\ndemonstrated the superior performance of spectral DPS for stable and accurate\nmaterial decomposition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many spectral CT applications require accurate material decomposition.\nExisting material decomposition algorithms are often susceptible to significant\nnoise magnification or, in the case of one-step model-based approaches,\nhampered by slow convergence rates and large computational requirements. In\nthis work, we proposed a novel framework - spectral diffusion posterior\nsampling (spectral DPS) - for one-step reconstruction and multi-material\ndecomposition, which combines sophisticated prior information captured by\none-time unsupervised learning and an arbitrary analytic physical system model.\nSpectral DPS is built upon a general DPS framework for nonlinear inverse\nproblems. Several strategies developed in previous work, including jumpstart\nsampling, Jacobian approximation, and multi-step likelihood updates are applied\nfacilitate stable and accurate decompositions. The effectiveness of spectral\nDPS was evaluated on a simulated dual-layer and a kV-switching spectral system\nas well as on a physical cone-beam CT (CBCT) test bench. In simulation studies,\nspectral DPS improved PSNR by 27.49% to 71.93% over baseline DPS and by 26.53%\nto 57.30% over MBMD, depending on the the region of interest. In physical\nphantom study, spectral DPS achieved a <1% error in estimating the mean density\nin a homogeneous region. Compared with baseline DPS, spectral DPS effectively\navoided generating false structures in the homogeneous phantom and reduced the\nvariability around edges. Both simulation and physical phantom studies\ndemonstrated the superior performance of spectral DPS for stable and accurate\nmaterial decomposition."
                },
                "authors": [
                    {
                        "name": "Xiao Jiang"
                    },
                    {
                        "name": "Grace J. Gang"
                    },
                    {
                        "name": "J. Webster Stayman"
                    }
                ],
                "author_detail": {
                    "name": "J. Webster Stayman"
                },
                "author": "J. Webster Stayman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01519v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01519v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00327v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00327v2",
                "updated": "2024-08-02T07:37:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    2,
                    7,
                    37,
                    51,
                    4,
                    215,
                    0
                ],
                "published": "2024-08-01T07:00:18Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    7,
                    0,
                    18,
                    3,
                    214,
                    0
                ],
                "title": "Search-in-Memory (SiM): Reliable, Versatile, and Efficient Data Matching\n  in SSD's NAND Flash Memory Chip for Data Indexing Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search-in-Memory (SiM): Reliable, Versatile, and Efficient Data Matching\n  in SSD's NAND Flash Memory Chip for Data Indexing Acceleration"
                },
                "summary": "To index the increasing volume of data, modern data indexes are typically\nstored on SSDs and cached in DRAM. However, searching such an index has\nresulted in significant I/O traffic due to limited access locality and\ninefficient cache utilization. At the heart of index searching is the operation\nof filtering through vast data spans to isolate a small, relevant subset, which\ninvolves basic equality tests rather than the complex arithmetic provided by\nmodern CPUs. This paper introduces the Search-in-Memory (SiM) chip, which\ndemonstrates the feasibility of performing data filtering directly within a\nNAND flash memory chip, transmitting only relevant search results rather than\ncomplete pages. Instead of adding complex circuits, we propose repurposing\nexisting circuitry for efficient and accurate bitwise parallel matching. We\ndemonstrate how different data structures can use our flexible SIMD command\ninterface to offload index searches. This strategy not only frees up the CPU\nfor more computationally demanding tasks, but it also optimizes DRAM usage for\nwrite buffering, significantly lowering energy consumption associated with I/O\ntransmission between the CPU and DRAM. Extensive testing across a wide range of\nworkloads reveals up to a 9X speedup in write-heavy workloads and up to 45%\nenergy savings due to reduced read and write I/O. Furthermore, we achieve\nsignificant reductions in median and tail read latencies of up to 89% and 85%\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To index the increasing volume of data, modern data indexes are typically\nstored on SSDs and cached in DRAM. However, searching such an index has\nresulted in significant I/O traffic due to limited access locality and\ninefficient cache utilization. At the heart of index searching is the operation\nof filtering through vast data spans to isolate a small, relevant subset, which\ninvolves basic equality tests rather than the complex arithmetic provided by\nmodern CPUs. This paper introduces the Search-in-Memory (SiM) chip, which\ndemonstrates the feasibility of performing data filtering directly within a\nNAND flash memory chip, transmitting only relevant search results rather than\ncomplete pages. Instead of adding complex circuits, we propose repurposing\nexisting circuitry for efficient and accurate bitwise parallel matching. We\ndemonstrate how different data structures can use our flexible SIMD command\ninterface to offload index searches. This strategy not only frees up the CPU\nfor more computationally demanding tasks, but it also optimizes DRAM usage for\nwrite buffering, significantly lowering energy consumption associated with I/O\ntransmission between the CPU and DRAM. Extensive testing across a wide range of\nworkloads reveals up to a 9X speedup in write-heavy workloads and up to 45%\nenergy savings due to reduced read and write I/O. Furthermore, we achieve\nsignificant reductions in median and tail read latencies of up to 89% and 85%\nrespectively."
                },
                "authors": [
                    {
                        "name": "Yun-Chih Chen"
                    },
                    {
                        "name": "Yuan-Hao Chang"
                    },
                    {
                        "name": "Tei-Wei Kuo"
                    }
                ],
                "author_detail": {
                    "name": "Tei-Wei Kuo"
                },
                "author": "Tei-Wei Kuo",
                "arxiv_comment": "This paper has been accepted for presentation at the The\n  International Conference on Hardware/Software Codesign and System Synthesis\n  (CODES+ISSS) in September, 2024. An extended abstract of this paper was\n  presented in Design, Automation & Test in Europe Conference & Exhibition\n  (DATE), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00327v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00327v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00957v1",
                "updated": "2024-08-01T23:52:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    23,
                    52,
                    43,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T23:52:43Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    23,
                    52,
                    43,
                    3,
                    214,
                    0
                ],
                "title": "Caching Aided Multi-Tenant Serverless Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching Aided Multi-Tenant Serverless Computing"
                },
                "summary": "One key to enabling high-performance serverless computing is to mitigate\ncold-starts. Current solutions utilize a warm pool to keep function alive: a\nwarm-start can be analogous to a CPU cache-hit. However, modern cache has\nmultiple hierarchies and the last-level cache is shared among cores, whereas\nthe warm pool is limited to a single tenant for security concerns. Also, the\nwarm pool keep-alive policy can be further optimized using cache replacement\nalgorithms. In this paper, we borrow practical optimizations from caching, and\ndesign FaasCamp, a caching-aided multi-tenant serverless computing framework.\nFaasCamp extends the single-tier warm pool into multi-tiers, with a reclaim\npool introduced enabling secure function instance sharing among tenants. Also,\nFaasCamp leverages machine learning to approximate the optimal cache\nreplacement policy to improve the warm rate. We have implemented a prototype\nand conducted extensive experiments under multiple scenarios. The results show\nthat FaasCamp can outperform existing platforms with minimal overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One key to enabling high-performance serverless computing is to mitigate\ncold-starts. Current solutions utilize a warm pool to keep function alive: a\nwarm-start can be analogous to a CPU cache-hit. However, modern cache has\nmultiple hierarchies and the last-level cache is shared among cores, whereas\nthe warm pool is limited to a single tenant for security concerns. Also, the\nwarm pool keep-alive policy can be further optimized using cache replacement\nalgorithms. In this paper, we borrow practical optimizations from caching, and\ndesign FaasCamp, a caching-aided multi-tenant serverless computing framework.\nFaasCamp extends the single-tier warm pool into multi-tiers, with a reclaim\npool introduced enabling secure function instance sharing among tenants. Also,\nFaasCamp leverages machine learning to approximate the optimal cache\nreplacement policy to improve the warm rate. We have implemented a prototype\nand conducted extensive experiments under multiple scenarios. The results show\nthat FaasCamp can outperform existing platforms with minimal overhead."
                },
                "authors": [
                    {
                        "name": "Chu Qiao"
                    },
                    {
                        "name": "Cong Wang"
                    },
                    {
                        "name": "Zhenkai Zhang"
                    },
                    {
                        "name": "Yuede Ji"
                    },
                    {
                        "name": "Xing Gao"
                    }
                ],
                "author_detail": {
                    "name": "Xing Gao"
                },
                "author": "Xing Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00859v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00859v2",
                "updated": "2024-08-01T21:21:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    21,
                    21,
                    28,
                    3,
                    214,
                    0
                ],
                "published": "2024-04-01T02:01:28Z",
                "published_parsed": [
                    2024,
                    4,
                    1,
                    2,
                    1,
                    28,
                    0,
                    92,
                    0
                ],
                "title": "Do language models plan ahead for future tokens?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do language models plan ahead for future tokens?"
                },
                "summary": "Do transformers \"think ahead\" during inference at a given position? It is\nknown transformers prepare information in the hidden states of the forward pass\nat time step $t$ that is then used in future forward passes $t+\\tau$. We posit\ntwo explanations for this phenomenon: pre-caching, in which off-diagonal\ngradient terms present during training result in the model computing features\nat $t$ irrelevant to the present inference task but useful for the future, and\nbreadcrumbs, in which features most relevant to time step $t$ are already the\nsame as those that would most benefit inference at time $t+\\tau$. We test these\nhypotheses by training language models without propagating gradients to past\ntimesteps, a scheme we formalize as myopic training. In a constructed synthetic\ndata setting, we find clear evidence for pre-caching. In the autoregressive\nlanguage modeling setting, our experiments are more suggestive of the\nbreadcrumbs hypothesis, though pre-caching increases with model scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do transformers \"think ahead\" during inference at a given position? It is\nknown transformers prepare information in the hidden states of the forward pass\nat time step $t$ that is then used in future forward passes $t+\\tau$. We posit\ntwo explanations for this phenomenon: pre-caching, in which off-diagonal\ngradient terms present during training result in the model computing features\nat $t$ irrelevant to the present inference task but useful for the future, and\nbreadcrumbs, in which features most relevant to time step $t$ are already the\nsame as those that would most benefit inference at time $t+\\tau$. We test these\nhypotheses by training language models without propagating gradients to past\ntimesteps, a scheme we formalize as myopic training. In a constructed synthetic\ndata setting, we find clear evidence for pre-caching. In the autoregressive\nlanguage modeling setting, our experiments are more suggestive of the\nbreadcrumbs hypothesis, though pre-caching increases with model scale."
                },
                "authors": [
                    {
                        "name": "Wilson Wu"
                    },
                    {
                        "name": "John X. Morris"
                    },
                    {
                        "name": "Lionel Levine"
                    }
                ],
                "author_detail": {
                    "name": "Lionel Levine"
                },
                "author": "Lionel Levine",
                "arxiv_comment": "24 pages, 11 figures. Camera-ready for COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00859v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00859v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2409.07453v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07453v1",
                "updated": "2024-09-11T17:59:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    17,
                    59,
                    1,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T17:59:01Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    17,
                    59,
                    1,
                    2,
                    255,
                    0
                ],
                "title": "\"My Grade is Wrong!\": A Contestable AI Framework for Interactive\n  Feedback in Evaluating Student Essays",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"My Grade is Wrong!\": A Contestable AI Framework for Interactive\n  Feedback in Evaluating Student Essays"
                },
                "summary": "Interactive feedback, where feedback flows in both directions between teacher\nand student, is more effective than traditional one-way feedback. However, it\nis often too time-consuming for widespread use in educational practice. While\nLarge Language Models (LLMs) have potential for automating feedback, they\nstruggle with reasoning and interaction in an interactive setting. This paper\nintroduces CAELF, a Contestable AI Empowered LLM Framework for automating\ninteractive feedback. CAELF allows students to query, challenge, and clarify\ntheir feedback by integrating a multi-agent system with computational\nargumentation. Essays are first assessed by multiple Teaching-Assistant Agents\n(TA Agents), and then a Teacher Agent aggregates the evaluations through formal\nreasoning to generate feedback and grades. Students can further engage with the\nfeedback to refine their understanding. A case study on 500 critical thinking\nessays with user studies demonstrates that CAELF significantly improves\ninteractive feedback, enhancing the reasoning and interaction capabilities of\nLLMs. This approach offers a promising solution to overcoming the time and\nresource barriers that have limited the adoption of interactive feedback in\neducational settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interactive feedback, where feedback flows in both directions between teacher\nand student, is more effective than traditional one-way feedback. However, it\nis often too time-consuming for widespread use in educational practice. While\nLarge Language Models (LLMs) have potential for automating feedback, they\nstruggle with reasoning and interaction in an interactive setting. This paper\nintroduces CAELF, a Contestable AI Empowered LLM Framework for automating\ninteractive feedback. CAELF allows students to query, challenge, and clarify\ntheir feedback by integrating a multi-agent system with computational\nargumentation. Essays are first assessed by multiple Teaching-Assistant Agents\n(TA Agents), and then a Teacher Agent aggregates the evaluations through formal\nreasoning to generate feedback and grades. Students can further engage with the\nfeedback to refine their understanding. A case study on 500 critical thinking\nessays with user studies demonstrates that CAELF significantly improves\ninteractive feedback, enhancing the reasoning and interaction capabilities of\nLLMs. This approach offers a promising solution to overcoming the time and\nresource barriers that have limited the adoption of interactive feedback in\neducational settings."
                },
                "authors": [
                    {
                        "name": "Shengxin Hong"
                    },
                    {
                        "name": "Chang Cai"
                    },
                    {
                        "name": "Sixuan Du"
                    },
                    {
                        "name": "Haiyue Feng"
                    },
                    {
                        "name": "Siyuan Liu"
                    },
                    {
                        "name": "Xiuyi Fan"
                    }
                ],
                "author_detail": {
                    "name": "Xiuyi Fan"
                },
                "author": "Xiuyi Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07453v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07453v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07440v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07440v1",
                "updated": "2024-09-11T17:37:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    17,
                    37,
                    48,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T17:37:48Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    17,
                    37,
                    48,
                    2,
                    255,
                    0
                ],
                "title": "SUPER: Evaluating Agents on Setting Up and Executing Tasks from Research\n  Repositories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SUPER: Evaluating Agents on Setting Up and Executing Tasks from Research\n  Repositories"
                },
                "summary": "Given that Large Language Models (LLMs) have made significant progress in\nwriting code, can they now be used to autonomously reproduce results from\nresearch repositories? Such a capability would be a boon to the research\ncommunity, helping researchers validate, understand, and extend prior work. To\nadvance towards this goal, we introduce SUPER, the first benchmark designed to\nevaluate the capability of LLMs in setting up and executing tasks from research\nrepositories. SUPERaims to capture the realistic challenges faced by\nresearchers working with Machine Learning (ML) and Natural Language Processing\n(NLP) research repositories. Our benchmark comprises three distinct problem\nsets: 45 end-to-end problems with annotated expert solutions, 152 sub problems\nderived from the expert set that focus on specific challenges (e.g.,\nconfiguring a trainer), and 602 automatically generated problems for\nlarger-scale development. We introduce various evaluation measures to assess\nboth task success and progress, utilizing gold solutions when available or\napproximations otherwise. We show that state-of-the-art approaches struggle to\nsolve these problems with the best model (GPT-4o) solving only 16.3% of the\nend-to-end set, and 46.1% of the scenarios. This illustrates the challenge of\nthis task, and suggests that SUPER can serve as a valuable resource for the\ncommunity to make and measure progress.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given that Large Language Models (LLMs) have made significant progress in\nwriting code, can they now be used to autonomously reproduce results from\nresearch repositories? Such a capability would be a boon to the research\ncommunity, helping researchers validate, understand, and extend prior work. To\nadvance towards this goal, we introduce SUPER, the first benchmark designed to\nevaluate the capability of LLMs in setting up and executing tasks from research\nrepositories. SUPERaims to capture the realistic challenges faced by\nresearchers working with Machine Learning (ML) and Natural Language Processing\n(NLP) research repositories. Our benchmark comprises three distinct problem\nsets: 45 end-to-end problems with annotated expert solutions, 152 sub problems\nderived from the expert set that focus on specific challenges (e.g.,\nconfiguring a trainer), and 602 automatically generated problems for\nlarger-scale development. We introduce various evaluation measures to assess\nboth task success and progress, utilizing gold solutions when available or\napproximations otherwise. We show that state-of-the-art approaches struggle to\nsolve these problems with the best model (GPT-4o) solving only 16.3% of the\nend-to-end set, and 46.1% of the scenarios. This illustrates the challenge of\nthis task, and suggests that SUPER can serve as a valuable resource for the\ncommunity to make and measure progress."
                },
                "authors": [
                    {
                        "name": "Ben Bogin"
                    },
                    {
                        "name": "Kejuan Yang"
                    },
                    {
                        "name": "Shashank Gupta"
                    },
                    {
                        "name": "Kyle Richardson"
                    },
                    {
                        "name": "Erin Bransom"
                    },
                    {
                        "name": "Peter Clark"
                    },
                    {
                        "name": "Ashish Sabharwal"
                    },
                    {
                        "name": "Tushar Khot"
                    }
                ],
                "author_detail": {
                    "name": "Tushar Khot"
                },
                "author": "Tushar Khot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07440v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07440v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07434v1",
                "updated": "2024-09-11T17:28:38Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    17,
                    28,
                    38,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T17:28:38Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    17,
                    28,
                    38,
                    2,
                    255,
                    0
                ],
                "title": "Asymptotics of Stochastic Gradient Descent with Dropout Regularization\n  in Linear Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asymptotics of Stochastic Gradient Descent with Dropout Regularization\n  in Linear Models"
                },
                "summary": "This paper proposes an asymptotic theory for online inference of the\nstochastic gradient descent (SGD) iterates with dropout regularization in\nlinear regression. Specifically, we establish the geometric-moment contraction\n(GMC) for constant step-size SGD dropout iterates to show the existence of a\nunique stationary distribution of the dropout recursive function. By the GMC\nproperty, we provide quenched central limit theorems (CLT) for the difference\nbetween dropout and $\\ell^2$-regularized iterates, regardless of\ninitialization. The CLT for the difference between the Ruppert-Polyak averaged\nSGD (ASGD) with dropout and $\\ell^2$-regularized iterates is also presented.\nBased on these asymptotic normality results, we further introduce an online\nestimator for the long-run covariance matrix of ASGD dropout to facilitate\ninference in a recursive manner with efficiency in computational time and\nmemory. The numerical experiments demonstrate that for sufficiently large\nsamples, the proposed confidence intervals for ASGD with dropout nearly achieve\nthe nominal coverage probability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes an asymptotic theory for online inference of the\nstochastic gradient descent (SGD) iterates with dropout regularization in\nlinear regression. Specifically, we establish the geometric-moment contraction\n(GMC) for constant step-size SGD dropout iterates to show the existence of a\nunique stationary distribution of the dropout recursive function. By the GMC\nproperty, we provide quenched central limit theorems (CLT) for the difference\nbetween dropout and $\\ell^2$-regularized iterates, regardless of\ninitialization. The CLT for the difference between the Ruppert-Polyak averaged\nSGD (ASGD) with dropout and $\\ell^2$-regularized iterates is also presented.\nBased on these asymptotic normality results, we further introduce an online\nestimator for the long-run covariance matrix of ASGD dropout to facilitate\ninference in a recursive manner with efficiency in computational time and\nmemory. The numerical experiments demonstrate that for sufficiently large\nsamples, the proposed confidence intervals for ASGD with dropout nearly achieve\nthe nominal coverage probability."
                },
                "authors": [
                    {
                        "name": "Jiaqi Li"
                    },
                    {
                        "name": "Johannes Schmidt-Hieber"
                    },
                    {
                        "name": "Wei Biao Wu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Biao Wu"
                },
                "author": "Wei Biao Wu",
                "arxiv_comment": "77 pages, 5 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62E20, 62F12, 68W27",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07431v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07431v1",
                "updated": "2024-09-11T17:21:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    17,
                    21,
                    59,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T17:21:59Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    17,
                    21,
                    59,
                    2,
                    255,
                    0
                ],
                "title": "Synthetic continued pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic continued pretraining"
                },
                "summary": "Pretraining on large-scale, unstructured internet text has enabled language\nmodels to acquire a significant amount of world knowledge. However, this\nknowledge acquisition is data-inefficient -- to learn a given fact, models must\nbe trained on hundreds to thousands of diverse representations of it. This\nposes a challenge when adapting a pretrained model to a small corpus of\ndomain-specific documents, where each fact may appear rarely or only once. We\npropose to bridge this gap with synthetic continued pretraining: using the\nsmall domain-specific corpus to synthesize a large corpus more amenable to\nlearning, and then performing continued pretraining on the synthesized corpus.\nWe instantiate this proposal with EntiGraph, a synthetic data augmentation\nalgorithm that extracts salient entities from the source documents and then\ngenerates diverse text by drawing connections between the sampled entities.\nSynthetic continued pretraining using EntiGraph enables a language model to\nanswer questions and follow generic instructions related to the source\ndocuments without access to them. If instead, the source documents are\navailable at inference time, we show that the knowledge acquired through our\napproach compounds with retrieval-augmented generation. To better understand\nthese results, we build a simple mathematical model of EntiGraph, and show how\nsynthetic data augmentation can \"rearrange\" knowledge to enable more\ndata-efficient learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pretraining on large-scale, unstructured internet text has enabled language\nmodels to acquire a significant amount of world knowledge. However, this\nknowledge acquisition is data-inefficient -- to learn a given fact, models must\nbe trained on hundreds to thousands of diverse representations of it. This\nposes a challenge when adapting a pretrained model to a small corpus of\ndomain-specific documents, where each fact may appear rarely or only once. We\npropose to bridge this gap with synthetic continued pretraining: using the\nsmall domain-specific corpus to synthesize a large corpus more amenable to\nlearning, and then performing continued pretraining on the synthesized corpus.\nWe instantiate this proposal with EntiGraph, a synthetic data augmentation\nalgorithm that extracts salient entities from the source documents and then\ngenerates diverse text by drawing connections between the sampled entities.\nSynthetic continued pretraining using EntiGraph enables a language model to\nanswer questions and follow generic instructions related to the source\ndocuments without access to them. If instead, the source documents are\navailable at inference time, we show that the knowledge acquired through our\napproach compounds with retrieval-augmented generation. To better understand\nthese results, we build a simple mathematical model of EntiGraph, and show how\nsynthetic data augmentation can \"rearrange\" knowledge to enable more\ndata-efficient learning."
                },
                "authors": [
                    {
                        "name": "Zitong Yang"
                    },
                    {
                        "name": "Neil Band"
                    },
                    {
                        "name": "Shuangping Li"
                    },
                    {
                        "name": "Emmanuel Candès"
                    },
                    {
                        "name": "Tatsunori Hashimoto"
                    }
                ],
                "author_detail": {
                    "name": "Tatsunori Hashimoto"
                },
                "author": "Tatsunori Hashimoto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07431v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07431v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11691v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11691v2",
                "updated": "2024-09-11T17:10:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    17,
                    10,
                    36,
                    2,
                    255,
                    0
                ],
                "published": "2024-07-16T13:06:15Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    13,
                    6,
                    15,
                    1,
                    198,
                    0
                ],
                "title": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality\n  Models"
                },
                "summary": "We present VLMEvalKit: an open-source toolkit for evaluating large\nmulti-modality models based on PyTorch. The toolkit aims to provide a\nuser-friendly and comprehensive framework for researchers and developers to\nevaluate existing multi-modality models and publish reproducible evaluation\nresults. In VLMEvalKit, we implement over 70 different large multi-modality\nmodels, including both proprietary APIs and open-source models, as well as more\nthan 20 different multi-modal benchmarks. By implementing a single interface,\nnew models can be easily added to the toolkit, while the toolkit automatically\nhandles the remaining workloads, including data preparation, distributed\ninference, prediction post-processing, and metric calculation. Although the\ntoolkit is currently mainly used for evaluating large vision-language models,\nits design is compatible with future updates that incorporate additional\nmodalities, such as audio and video. Based on the evaluation results obtained\nwith the toolkit, we host OpenVLM Leaderboard, a comprehensive leaderboard to\ntrack the progress of multi-modality learning research. The toolkit is released\nat https://github.com/open-compass/VLMEvalKit and is actively maintained.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present VLMEvalKit: an open-source toolkit for evaluating large\nmulti-modality models based on PyTorch. The toolkit aims to provide a\nuser-friendly and comprehensive framework for researchers and developers to\nevaluate existing multi-modality models and publish reproducible evaluation\nresults. In VLMEvalKit, we implement over 70 different large multi-modality\nmodels, including both proprietary APIs and open-source models, as well as more\nthan 20 different multi-modal benchmarks. By implementing a single interface,\nnew models can be easily added to the toolkit, while the toolkit automatically\nhandles the remaining workloads, including data preparation, distributed\ninference, prediction post-processing, and metric calculation. Although the\ntoolkit is currently mainly used for evaluating large vision-language models,\nits design is compatible with future updates that incorporate additional\nmodalities, such as audio and video. Based on the evaluation results obtained\nwith the toolkit, we host OpenVLM Leaderboard, a comprehensive leaderboard to\ntrack the progress of multi-modality learning research. The toolkit is released\nat https://github.com/open-compass/VLMEvalKit and is actively maintained."
                },
                "authors": [
                    {
                        "name": "Haodong Duan"
                    },
                    {
                        "name": "Junming Yang"
                    },
                    {
                        "name": "Yuxuan Qiao"
                    },
                    {
                        "name": "Xinyu Fang"
                    },
                    {
                        "name": "Lin Chen"
                    },
                    {
                        "name": "Yuan Liu"
                    },
                    {
                        "name": "Amit Agarwal"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Mo Li"
                    },
                    {
                        "name": "Yubo Ma"
                    },
                    {
                        "name": "Hailong Sun"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    },
                    {
                        "name": "Junbo Cui"
                    },
                    {
                        "name": "Xiaoyi Dong"
                    },
                    {
                        "name": "Yuhang Zang"
                    },
                    {
                        "name": "Pan Zhang"
                    },
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "arxiv_comment": "Updated on 2024.09.12",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11691v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11691v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07424v1",
                "updated": "2024-09-11T17:10:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    17,
                    10,
                    20,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T17:10:20Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    17,
                    10,
                    20,
                    2,
                    255,
                    0
                ],
                "title": "Towards Fairer Health Recommendations: finding informative unbiased\n  samples via Word Sense Disambiguation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Fairer Health Recommendations: finding informative unbiased\n  samples via Word Sense Disambiguation"
                },
                "summary": "There have been growing concerns around high-stake applications that rely on\nmodels trained with biased data, which consequently produce biased predictions,\noften harming the most vulnerable. In particular, biased medical data could\ncause health-related applications and recommender systems to create outputs\nthat jeopardize patient care and widen disparities in health outcomes. A recent\nframework titled Fairness via AI posits that, instead of attempting to correct\nmodel biases, researchers must focus on their root causes by using AI to debias\ndata. Inspired by this framework, we tackle bias detection in medical curricula\nusing NLP models, including LLMs, and evaluate them on a gold standard dataset\ncontaining 4,105 excerpts annotated by medical experts for bias from a large\ncorpus. We build on previous work by coauthors which augments the set of\nnegative samples with non-annotated text containing social identifier terms.\nHowever, some of these terms, especially those related to race and ethnicity,\ncan carry different meanings (e.g., \"white matter of spinal cord\"). To address\nthis issue, we propose the use of Word Sense Disambiguation models to refine\ndataset quality by removing irrelevant sentences. We then evaluate fine-tuned\nvariations of BERT models as well as GPT models with zero- and few-shot\nprompting. We found LLMs, considered SOTA on many NLP tasks, unsuitable for\nbias detection, while fine-tuned BERT models generally perform well across all\nevaluated metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There have been growing concerns around high-stake applications that rely on\nmodels trained with biased data, which consequently produce biased predictions,\noften harming the most vulnerable. In particular, biased medical data could\ncause health-related applications and recommender systems to create outputs\nthat jeopardize patient care and widen disparities in health outcomes. A recent\nframework titled Fairness via AI posits that, instead of attempting to correct\nmodel biases, researchers must focus on their root causes by using AI to debias\ndata. Inspired by this framework, we tackle bias detection in medical curricula\nusing NLP models, including LLMs, and evaluate them on a gold standard dataset\ncontaining 4,105 excerpts annotated by medical experts for bias from a large\ncorpus. We build on previous work by coauthors which augments the set of\nnegative samples with non-annotated text containing social identifier terms.\nHowever, some of these terms, especially those related to race and ethnicity,\ncan carry different meanings (e.g., \"white matter of spinal cord\"). To address\nthis issue, we propose the use of Word Sense Disambiguation models to refine\ndataset quality by removing irrelevant sentences. We then evaluate fine-tuned\nvariations of BERT models as well as GPT models with zero- and few-shot\nprompting. We found LLMs, considered SOTA on many NLP tasks, unsuitable for\nbias detection, while fine-tuned BERT models generally perform well across all\nevaluated metrics."
                },
                "authors": [
                    {
                        "name": "Gavin Butts"
                    },
                    {
                        "name": "Pegah Emdad"
                    },
                    {
                        "name": "Jethro Lee"
                    },
                    {
                        "name": "Shannon Song"
                    },
                    {
                        "name": "Chiman Salavati"
                    },
                    {
                        "name": "Willmar Sosa Diaz"
                    },
                    {
                        "name": "Shiri Dori-Hacohen"
                    },
                    {
                        "name": "Fabricio Murai"
                    }
                ],
                "author_detail": {
                    "name": "Fabricio Murai"
                },
                "author": "Fabricio Murai",
                "arxiv_comment": "Accepted for long presentation at the FAcctRec @ Recsys 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; J.3; K.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07423v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07423v1",
                "updated": "2024-09-11T17:09:49Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    17,
                    9,
                    49,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T17:09:49Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    17,
                    9,
                    49,
                    2,
                    255,
                    0
                ],
                "title": "Enhancing adversarial robustness in Natural Language Inference using\n  explanations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing adversarial robustness in Natural Language Inference using\n  explanations"
                },
                "summary": "The surge of state-of-the-art Transformer-based models has undoubtedly pushed\nthe limits of NLP model performance, excelling in a variety of tasks. We cast\nthe spotlight on the underexplored task of Natural Language Inference (NLI),\nsince models trained on popular well-suited datasets are susceptible to\nadversarial attacks, allowing subtle input interventions to mislead the model.\nIn this work, we validate the usage of natural language explanation as a\nmodel-agnostic defence strategy through extensive experimentation: only by\nfine-tuning a classifier on the explanation rather than premise-hypothesis\ninputs, robustness under various adversarial attacks is achieved in comparison\nto explanation-free baselines. Moreover, since there is no standard strategy of\ntesting the semantic validity of the generated explanations, we research the\ncorrelation of widely used language generation metrics with human perception,\nin order for them to serve as a proxy towards robust NLI models. Our approach\nis resource-efficient and reproducible without significant computational\nlimitations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The surge of state-of-the-art Transformer-based models has undoubtedly pushed\nthe limits of NLP model performance, excelling in a variety of tasks. We cast\nthe spotlight on the underexplored task of Natural Language Inference (NLI),\nsince models trained on popular well-suited datasets are susceptible to\nadversarial attacks, allowing subtle input interventions to mislead the model.\nIn this work, we validate the usage of natural language explanation as a\nmodel-agnostic defence strategy through extensive experimentation: only by\nfine-tuning a classifier on the explanation rather than premise-hypothesis\ninputs, robustness under various adversarial attacks is achieved in comparison\nto explanation-free baselines. Moreover, since there is no standard strategy of\ntesting the semantic validity of the generated explanations, we research the\ncorrelation of widely used language generation metrics with human perception,\nin order for them to serve as a proxy towards robust NLI models. Our approach\nis resource-efficient and reproducible without significant computational\nlimitations."
                },
                "authors": [
                    {
                        "name": "Alexandros Koulakos"
                    },
                    {
                        "name": "Maria Lymperaiou"
                    },
                    {
                        "name": "Giorgos Filandrianos"
                    },
                    {
                        "name": "Giorgos Stamou"
                    }
                ],
                "author_detail": {
                    "name": "Giorgos Stamou"
                },
                "author": "Giorgos Stamou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07423v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07423v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10296v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10296v3",
                "updated": "2024-09-11T17:08:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    17,
                    8,
                    45,
                    2,
                    255,
                    0
                ],
                "published": "2024-04-16T05:40:30Z",
                "published_parsed": [
                    2024,
                    4,
                    16,
                    5,
                    40,
                    30,
                    1,
                    107,
                    0
                ],
                "title": "Engineering software 2.0 by Interpolating Neural Networks: Unifying\n  Training, Solving, and Calibration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Engineering software 2.0 by Interpolating Neural Networks: Unifying\n  Training, Solving, and Calibration"
                },
                "summary": "The evolution of artificial intelligence (AI) and neural network theories has\nrevolutionized the way software is programmed, shifting from a hard-coded\nseries of codes, Software 1.0, to a vast neural network, Software 2.0. However,\nthis transition in engineering software has faced challenges such as data\nscarcity, multi-modality of data, low model accuracy, and slow inference. Here,\nwe propose a new network based on interpolation theories and tensor\ndecomposition, the interpolating neural network (INN) to open the new era of\nEngineering Software 2.0 that unifies training, solving, and calibration.\nInstead of interpolating training data, a common notion in computer science,\nINN interpolates grid points in the physical space whose coordinates and values\nare trainable. INN features orders of magnitude fewer trainable parameters (or\ndegrees of freedom for solving), faster training/solving, less inference cost,\nsmaller memory footprint, and higher model accuracy compared to multi-layer\nperceptron (MLP) or physics-informed neural networks (PINN). Various numerical\nexperiments that cover computer science and engineering domains demonstrate\nthat INN can solve over Zetta scale (10^{21}) partial differential equations\nand train/calibrate a dataset with extraordinary accuracy but fewer parameters\nusing only a single graphics processing unit (GPU).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evolution of artificial intelligence (AI) and neural network theories has\nrevolutionized the way software is programmed, shifting from a hard-coded\nseries of codes, Software 1.0, to a vast neural network, Software 2.0. However,\nthis transition in engineering software has faced challenges such as data\nscarcity, multi-modality of data, low model accuracy, and slow inference. Here,\nwe propose a new network based on interpolation theories and tensor\ndecomposition, the interpolating neural network (INN) to open the new era of\nEngineering Software 2.0 that unifies training, solving, and calibration.\nInstead of interpolating training data, a common notion in computer science,\nINN interpolates grid points in the physical space whose coordinates and values\nare trainable. INN features orders of magnitude fewer trainable parameters (or\ndegrees of freedom for solving), faster training/solving, less inference cost,\nsmaller memory footprint, and higher model accuracy compared to multi-layer\nperceptron (MLP) or physics-informed neural networks (PINN). Various numerical\nexperiments that cover computer science and engineering domains demonstrate\nthat INN can solve over Zetta scale (10^{21}) partial differential equations\nand train/calibrate a dataset with extraordinary accuracy but fewer parameters\nusing only a single graphics processing unit (GPU)."
                },
                "authors": [
                    {
                        "name": "Chanwook Park"
                    },
                    {
                        "name": "Sourav Saha"
                    },
                    {
                        "name": "Jiachen Guo"
                    },
                    {
                        "name": "Hantao Zhang"
                    },
                    {
                        "name": "Xiaoyu Xie"
                    },
                    {
                        "name": "Miguel A. Bessa"
                    },
                    {
                        "name": "Dong Qian"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Gregory J. Wagner"
                    },
                    {
                        "name": "Jian Cao"
                    },
                    {
                        "name": "Wing Kam Liu"
                    }
                ],
                "author_detail": {
                    "name": "Wing Kam Liu"
                },
                "author": "Wing Kam Liu",
                "arxiv_comment": "9 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.10296v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10296v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.09262v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.09262v2",
                "updated": "2024-09-11T17:01:10Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    17,
                    1,
                    10,
                    2,
                    255,
                    0
                ],
                "published": "2024-07-12T13:47:18Z",
                "published_parsed": [
                    2024,
                    7,
                    12,
                    13,
                    47,
                    18,
                    4,
                    194,
                    0
                ],
                "title": "Prospect of Precision Cosmology and Testing General Relativity using\n  Binary Black Holes- Galaxies Cross-correlation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prospect of Precision Cosmology and Testing General Relativity using\n  Binary Black Holes- Galaxies Cross-correlation"
                },
                "summary": "Modified theories of gravity predict deviations from General Relativity (GR)\nin the propagation of gravitational waves (GW) across cosmological distances. A\nkey prediction is that the GW luminosity distance will vary with redshift,\ndiffering from the electromagnetic (EM) luminosity distance due to varying\neffective Planck mass. We introduce a model-independent, data-driven approach\nto explore these deviations using multi-messenger observations of dark standard\nsirens (Binary Black Holes, BBH). By combining GW luminosity distance\nmeasurements from dark sirens with Baryon Acoustic Oscillation (BAO)\nmeasurements, BBH redshifts inferred from cross-correlation with spectroscopic\nor photometric galaxy surveys, and sound horizon measurements from the Cosmic\nMicrowave Background (CMB), we can make a data-driven test of GR (jointly with\nthe Hubble constant) as a function of redshift. Using the multi-messenger\ntechnique with the spectroscopic DESI galaxy survey, we achieve precise\nmeasurements of deviations in the effective Planck mass variation with\nredshift. For the Cosmic Explorer and Einstein Telescope (CEET), the best\nprecision is approximately 3.6\\%, and for LIGO-Virgo-KAGRA (LVK), it is 7.4\\%\nat a redshift of $\\rm{z = 0.425}$. Additionally, we can measure the Hubble\nconstant with a precision of about 1.1\\% from CEET and 7\\% from LVK over five\nyears of observation with a 75\\% duty cycle. We also explore the potential of\ncross-correlation with photometric galaxy surveys from the Rubin Observatory,\nextending measurements up to a redshift of $\\rm{z \\sim 2.5}$. This approach can\nreveal potential deviations from models affecting GW propagation using numerous\ndark standard sirens in synergy with DESI and the Rubin Observatory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modified theories of gravity predict deviations from General Relativity (GR)\nin the propagation of gravitational waves (GW) across cosmological distances. A\nkey prediction is that the GW luminosity distance will vary with redshift,\ndiffering from the electromagnetic (EM) luminosity distance due to varying\neffective Planck mass. We introduce a model-independent, data-driven approach\nto explore these deviations using multi-messenger observations of dark standard\nsirens (Binary Black Holes, BBH). By combining GW luminosity distance\nmeasurements from dark sirens with Baryon Acoustic Oscillation (BAO)\nmeasurements, BBH redshifts inferred from cross-correlation with spectroscopic\nor photometric galaxy surveys, and sound horizon measurements from the Cosmic\nMicrowave Background (CMB), we can make a data-driven test of GR (jointly with\nthe Hubble constant) as a function of redshift. Using the multi-messenger\ntechnique with the spectroscopic DESI galaxy survey, we achieve precise\nmeasurements of deviations in the effective Planck mass variation with\nredshift. For the Cosmic Explorer and Einstein Telescope (CEET), the best\nprecision is approximately 3.6\\%, and for LIGO-Virgo-KAGRA (LVK), it is 7.4\\%\nat a redshift of $\\rm{z = 0.425}$. Additionally, we can measure the Hubble\nconstant with a precision of about 1.1\\% from CEET and 7\\% from LVK over five\nyears of observation with a 75\\% duty cycle. We also explore the potential of\ncross-correlation with photometric galaxy surveys from the Rubin Observatory,\nextending measurements up to a redshift of $\\rm{z \\sim 2.5}$. This approach can\nreveal potential deviations from models affecting GW propagation using numerous\ndark standard sirens in synergy with DESI and the Rubin Observatory."
                },
                "authors": [
                    {
                        "name": "Samsuzzaman Afroz"
                    },
                    {
                        "name": "Suvodip Mukherjee"
                    }
                ],
                "author_detail": {
                    "name": "Suvodip Mukherjee"
                },
                "author": "Suvodip Mukherjee",
                "arxiv_comment": "16 Pages, 14 figures, Accepted for publication in MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.09262v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.09262v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07407v1",
                "updated": "2024-09-11T16:49:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    16,
                    49,
                    46,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T16:49:46Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    16,
                    49,
                    46,
                    2,
                    255,
                    0
                ],
                "title": "CLNX: Bridging Code and Natural Language for C/C++\n  Vulnerability-Contributing Commits Identification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLNX: Bridging Code and Natural Language for C/C++\n  Vulnerability-Contributing Commits Identification"
                },
                "summary": "Large Language Models (LLMs) have shown great promise in vulnerability\nidentification. As C/C++ comprises half of the Open-Source Software (OSS)\nvulnerabilities over the past decade and updates in OSS mainly occur through\ncommits, enhancing LLMs' ability to identify C/C++ Vulnerability-Contributing\nCommits (VCCs) is essential. However, current studies primarily focus on\nfurther pre-training LLMs on massive code datasets, which is resource-intensive\nand poses efficiency challenges. In this paper, we enhance the ability of\nBERT-based LLMs to identify C/C++ VCCs in a lightweight manner. We propose\nCodeLinguaNexus (CLNX) as a bridge facilitating communication between C/C++\nprograms and LLMs. Based on commits, CLNX efficiently converts the source code\ninto a more natural representation while preserving key details. Specifically,\nCLNX first applies structure-level naturalization to decompose complex\nprograms, followed by token-level naturalization to interpret complex symbols.\nWe evaluate CLNX on public datasets of 25,872 C/C++ functions with their\ncommits. The results show that CLNX significantly enhances the performance of\nLLMs on identifying C/C++ VCCs. Moreover, CLNX-equipped CodeBERT achieves new\nstate-of-the-art and identifies 38 OSS vulnerabilities in the real world.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown great promise in vulnerability\nidentification. As C/C++ comprises half of the Open-Source Software (OSS)\nvulnerabilities over the past decade and updates in OSS mainly occur through\ncommits, enhancing LLMs' ability to identify C/C++ Vulnerability-Contributing\nCommits (VCCs) is essential. However, current studies primarily focus on\nfurther pre-training LLMs on massive code datasets, which is resource-intensive\nand poses efficiency challenges. In this paper, we enhance the ability of\nBERT-based LLMs to identify C/C++ VCCs in a lightweight manner. We propose\nCodeLinguaNexus (CLNX) as a bridge facilitating communication between C/C++\nprograms and LLMs. Based on commits, CLNX efficiently converts the source code\ninto a more natural representation while preserving key details. Specifically,\nCLNX first applies structure-level naturalization to decompose complex\nprograms, followed by token-level naturalization to interpret complex symbols.\nWe evaluate CLNX on public datasets of 25,872 C/C++ functions with their\ncommits. The results show that CLNX significantly enhances the performance of\nLLMs on identifying C/C++ VCCs. Moreover, CLNX-equipped CodeBERT achieves new\nstate-of-the-art and identifies 38 OSS vulnerabilities in the real world."
                },
                "authors": [
                    {
                        "name": "Zeqing Qin"
                    },
                    {
                        "name": "Yiwei Wu"
                    },
                    {
                        "name": "Lansheng Han"
                    }
                ],
                "author_detail": {
                    "name": "Lansheng Han"
                },
                "author": "Lansheng Han",
                "arxiv_comment": "8 pages, 2 figures, conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M25",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07394v1",
                "updated": "2024-09-11T16:35:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    16,
                    35,
                    18,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T16:35:18Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    16,
                    35,
                    18,
                    2,
                    255,
                    0
                ],
                "title": "AdaCAD: Adaptively Decoding to Balance Conflicts between Contextual and\n  Parametric Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaCAD: Adaptively Decoding to Balance Conflicts between Contextual and\n  Parametric Knowledge"
                },
                "summary": "Knowledge conflict arises from discrepancies between information in the\ncontext of a large language model (LLM) and the knowledge stored in its\nparameters. This can hurt performance when using standard decoding techniques,\nwhich tend to ignore the context. Existing test-time contrastive methods seek\nto address this by comparing the LLM's output distribution with and without the\ncontext and adjust the model according to the contrast between them. However,\nwe find that these methods frequently misjudge the degree of conflict and\nstruggle to handle instances that vary in their amount of conflict, with static\nmethods over-adjusting when conflict is absent. We propose a fine-grained,\ninstance-level approach called AdaCAD, which dynamically infers the weight of\nadjustment based on the degree of conflict, as measured by the Jensen-Shannon\ndivergence between distributions representing contextual and parametric\nknowledge. Our experiments across four models on six diverse question-answering\n(QA) datasets and three summarization tasks demonstrate that our training-free\nadaptive method consistently outperforms other decoding methods on QA, with\naverage accuracy gains of 14.21% (absolute) over a static contrastive baseline,\nand improves the factuality of summaries by 5.59 (AlignScore). Furthermore, our\nanalysis shows that while decoding with contrastive baselines hurts performance\nwhen conflict is absent, AdaCAD mitigates these losses, making it more\napplicable to real-world datasets in which some examples have conflict and\nothers do not.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge conflict arises from discrepancies between information in the\ncontext of a large language model (LLM) and the knowledge stored in its\nparameters. This can hurt performance when using standard decoding techniques,\nwhich tend to ignore the context. Existing test-time contrastive methods seek\nto address this by comparing the LLM's output distribution with and without the\ncontext and adjust the model according to the contrast between them. However,\nwe find that these methods frequently misjudge the degree of conflict and\nstruggle to handle instances that vary in their amount of conflict, with static\nmethods over-adjusting when conflict is absent. We propose a fine-grained,\ninstance-level approach called AdaCAD, which dynamically infers the weight of\nadjustment based on the degree of conflict, as measured by the Jensen-Shannon\ndivergence between distributions representing contextual and parametric\nknowledge. Our experiments across four models on six diverse question-answering\n(QA) datasets and three summarization tasks demonstrate that our training-free\nadaptive method consistently outperforms other decoding methods on QA, with\naverage accuracy gains of 14.21% (absolute) over a static contrastive baseline,\nand improves the factuality of summaries by 5.59 (AlignScore). Furthermore, our\nanalysis shows that while decoding with contrastive baselines hurts performance\nwhen conflict is absent, AdaCAD mitigates these losses, making it more\napplicable to real-world datasets in which some examples have conflict and\nothers do not."
                },
                "authors": [
                    {
                        "name": "Han Wang"
                    },
                    {
                        "name": "Archiki Prasad"
                    },
                    {
                        "name": "Elias Stengel-Eskin"
                    },
                    {
                        "name": "Mohit Bansal"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Bansal"
                },
                "author": "Mohit Bansal",
                "arxiv_comment": "16 pages, Code: https://github.com/HanNight/AdaCAD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02076v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02076v3",
                "updated": "2024-09-11T16:35:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    16,
                    35,
                    0,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-03T17:25:54Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    17,
                    25,
                    54,
                    1,
                    247,
                    0
                ],
                "title": "LongGenbench: Benchmarking Long-Form Generation in Long Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongGenbench: Benchmarking Long-Form Generation in Long Context LLMs"
                },
                "summary": "The abilities of long-context language models (LMs) are often evaluated using\nthe \"Needle-in-a-Haystack\" (NIAH) test, which comprises tasks designed to\nassess a model's ability to identify specific information (\"needle\") within\nlarge text sequences (\"haystack\"). While these benchmarks measure how well\nmodels understand long-context input sequences, they do not effectively gauge\nthe quality of long-form text generation--a critical aspect for applications\nsuch as design proposals and creative writing. To address this gap, we have\nintroduced a new long-form text evaluation benchmark, LongGenbench, which tests\nmodels' ability to identify specific events within generated long text\nsequences. In this benchmark, we prompt long-context LMs to create long-form\ntext that must include particular events or constraints and evaluate their\nability to incorporate these elements. We evaluated ten long-context LMs across\nfour distinct scenarios, three types of prompt instructions, and two different\ngeneration-length settings (16K and 32K). Although these models perform well on\nNIAH benchmarks, none demonstrated satisfactory performance on the\nLongGenbench, raising concerns about their ability to generate coherent\nlong-form text that follows instructions. Additionally, as the length of the\ngenerated text increases, all models exhibit a significant drop in performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The abilities of long-context language models (LMs) are often evaluated using\nthe \"Needle-in-a-Haystack\" (NIAH) test, which comprises tasks designed to\nassess a model's ability to identify specific information (\"needle\") within\nlarge text sequences (\"haystack\"). While these benchmarks measure how well\nmodels understand long-context input sequences, they do not effectively gauge\nthe quality of long-form text generation--a critical aspect for applications\nsuch as design proposals and creative writing. To address this gap, we have\nintroduced a new long-form text evaluation benchmark, LongGenbench, which tests\nmodels' ability to identify specific events within generated long text\nsequences. In this benchmark, we prompt long-context LMs to create long-form\ntext that must include particular events or constraints and evaluate their\nability to incorporate these elements. We evaluated ten long-context LMs across\nfour distinct scenarios, three types of prompt instructions, and two different\ngeneration-length settings (16K and 32K). Although these models perform well on\nNIAH benchmarks, none demonstrated satisfactory performance on the\nLongGenbench, raising concerns about their ability to generate coherent\nlong-form text that follows instructions. Additionally, as the length of the\ngenerated text increases, all models exhibit a significant drop in performance."
                },
                "authors": [
                    {
                        "name": "Yuhao Wu"
                    },
                    {
                        "name": "Ming Shan Hee"
                    },
                    {
                        "name": "Zhiqing Hu"
                    },
                    {
                        "name": "Roy Ka-Wei Lee"
                    }
                ],
                "author_detail": {
                    "name": "Roy Ka-Wei Lee"
                },
                "author": "Roy Ka-Wei Lee",
                "arxiv_comment": "work in progress. arXiv admin note: text overlap with\n  arXiv:2404.06654 by other authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02076v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02076v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.00711v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.00711v3",
                "updated": "2024-09-11T16:32:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    16,
                    32,
                    15,
                    2,
                    255,
                    0
                ],
                "published": "2024-02-01T16:06:35Z",
                "published_parsed": [
                    2024,
                    2,
                    1,
                    16,
                    6,
                    35,
                    3,
                    32,
                    0
                ],
                "title": "Explaining Text Classifiers with Counterfactual Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explaining Text Classifiers with Counterfactual Representations"
                },
                "summary": "One well motivated explanation method for classifiers leverages\ncounterfactuals which are hypothetical events identical to real observations in\nall aspects except for one feature. Constructing such counterfactual poses\nspecific challenges for texts, however, as some attribute values may not\nnecessarily align with plausible real-world events. In this paper we propose a\nsimple method for generating counterfactuals by intervening in the space of\ntext representations which bypasses this limitation. We argue that our\ninterventions are minimally disruptive and that they are theoretically sound as\nthey align with counterfactuals as defined in Pearl's causal inference\nframework. To validate our method, we conducted experiments first on a\nsynthetic dataset and then on a realistic dataset of counterfactuals. This\nallows for a direct comparison between classifier predictions based on ground\ntruth counterfactuals - obtained through explicit text interventions - and our\ncounterfactuals, derived through interventions in the representation space.\nEventually, we study a real world scenario where our counterfactuals can be\nleveraged both for explaining a classifier and for bias mitigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One well motivated explanation method for classifiers leverages\ncounterfactuals which are hypothetical events identical to real observations in\nall aspects except for one feature. Constructing such counterfactual poses\nspecific challenges for texts, however, as some attribute values may not\nnecessarily align with plausible real-world events. In this paper we propose a\nsimple method for generating counterfactuals by intervening in the space of\ntext representations which bypasses this limitation. We argue that our\ninterventions are minimally disruptive and that they are theoretically sound as\nthey align with counterfactuals as defined in Pearl's causal inference\nframework. To validate our method, we conducted experiments first on a\nsynthetic dataset and then on a realistic dataset of counterfactuals. This\nallows for a direct comparison between classifier predictions based on ground\ntruth counterfactuals - obtained through explicit text interventions - and our\ncounterfactuals, derived through interventions in the representation space.\nEventually, we study a real world scenario where our counterfactuals can be\nleveraged both for explaining a classifier and for bias mitigation."
                },
                "authors": [
                    {
                        "name": "Pirmin Lemberger"
                    },
                    {
                        "name": "Antoine Saillenfest"
                    }
                ],
                "author_detail": {
                    "name": "Antoine Saillenfest"
                },
                "author": "Antoine Saillenfest",
                "arxiv_comment": "24 pages, 4 figures, accepted for publication in ECAI 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.00711v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.00711v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62Fxx",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.18334v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.18334v3",
                "updated": "2024-09-11T16:28:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    16,
                    28,
                    29,
                    2,
                    255,
                    0
                ],
                "published": "2024-02-28T13:54:57Z",
                "published_parsed": [
                    2024,
                    2,
                    28,
                    13,
                    54,
                    57,
                    2,
                    59,
                    0
                ],
                "title": "Learning to Generate Instruction Tuning Datasets for Zero-Shot Task\n  Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Generate Instruction Tuning Datasets for Zero-Shot Task\n  Adaptation"
                },
                "summary": "We introduce Bonito, an open-source model for conditional task generation\nthat converts unannotated text into task-specific training datasets for\ninstruction tuning. We aim to enable zero-shot task adaptation of large\nlanguage models on users' specialized, private data. We train Bonito by\nfine-tuning a pretrained large language model on a new large-scale dataset with\n1.65M examples created by remixing existing instruction tuning datasets into\nmeta-templates. The meta-templates for a dataset produce training examples\nwhere the input is the unannotated text and the task attribute and the output\nconsists of the instruction and the response. We use Bonito to generate\nsynthetic tasks for seven datasets from specialized domains with unannotated\ntext across three task types -- yes-no question answering, extractive question\nanswering, and natural language inference -- and adapt language models. We show\nthat Bonito significantly improves the average performance of pretrained and\ninstruction tuned models over the de facto self supervised baseline. For\nexample, adapting Mistral-Instruct-v2 and instruction tuned variants of Mistral\nand Llama2 with Bonito improves the strong zero-shot performance by 22.1 F1\npoints whereas the next word prediction objective undoes some of the benefits\nof instruction tuning and reduces the average performance by 0.8 F1 points. We\nconduct additional experiments with Bonito to understand the effects of the\ndomain, the size of the training set, and the choice of alternative synthetic\ntask generators. Overall, we show that learning with synthetic instruction\ntuning datasets is an effective way to adapt language models to new domains.\nThe model, dataset, and code are available at\nhttps://github.com/BatsResearch/bonito.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Bonito, an open-source model for conditional task generation\nthat converts unannotated text into task-specific training datasets for\ninstruction tuning. We aim to enable zero-shot task adaptation of large\nlanguage models on users' specialized, private data. We train Bonito by\nfine-tuning a pretrained large language model on a new large-scale dataset with\n1.65M examples created by remixing existing instruction tuning datasets into\nmeta-templates. The meta-templates for a dataset produce training examples\nwhere the input is the unannotated text and the task attribute and the output\nconsists of the instruction and the response. We use Bonito to generate\nsynthetic tasks for seven datasets from specialized domains with unannotated\ntext across three task types -- yes-no question answering, extractive question\nanswering, and natural language inference -- and adapt language models. We show\nthat Bonito significantly improves the average performance of pretrained and\ninstruction tuned models over the de facto self supervised baseline. For\nexample, adapting Mistral-Instruct-v2 and instruction tuned variants of Mistral\nand Llama2 with Bonito improves the strong zero-shot performance by 22.1 F1\npoints whereas the next word prediction objective undoes some of the benefits\nof instruction tuning and reduces the average performance by 0.8 F1 points. We\nconduct additional experiments with Bonito to understand the effects of the\ndomain, the size of the training set, and the choice of alternative synthetic\ntask generators. Overall, we show that learning with synthetic instruction\ntuning datasets is an effective way to adapt language models to new domains.\nThe model, dataset, and code are available at\nhttps://github.com/BatsResearch/bonito."
                },
                "authors": [
                    {
                        "name": "Nihal V. Nayak"
                    },
                    {
                        "name": "Yiyang Nan"
                    },
                    {
                        "name": "Avi Trost"
                    },
                    {
                        "name": "Stephen H. Bach"
                    }
                ],
                "author_detail": {
                    "name": "Stephen H. Bach"
                },
                "author": "Stephen H. Bach",
                "arxiv_comment": "ACL Findings 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.18334v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.18334v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07380v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07380v1",
                "updated": "2024-09-11T16:13:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    16,
                    13,
                    20,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T16:13:20Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    16,
                    13,
                    20,
                    2,
                    255,
                    0
                ],
                "title": "Multi-source Stable Variable Importance Measure via Adversarial Machine\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-source Stable Variable Importance Measure via Adversarial Machine\n  Learning"
                },
                "summary": "As part of enhancing the interpretability of machine learning, it is of\nrenewed interest to quantify and infer the predictive importance of certain\nexposure covariates. Modern scientific studies often collect data from multiple\nsources with distributional heterogeneity. Thus, measuring and inferring stable\nassociations across multiple environments is crucial in reliable and\ngeneralizable decision-making. In this paper, we propose MIMAL, a novel\nstatistical framework for Multi-source stable Importance Measure via\nAdversarial Learning. MIMAL measures the importance of some exposure variables\nby maximizing the worst-case predictive reward over the source mixture. Our\nframework allows various machine learning methods for confounding adjustment\nand exposure effect characterization. For inferential analysis, the asymptotic\nnormality of our introduced statistic is established under a general machine\nlearning framework that requires no stronger learning accuracy conditions than\nthose for single source variable importance. Numerical studies with various\ntypes of data generation setups and machine learning implementation are\nconducted to justify the finite-sample performance of MIMAL. We also illustrate\nour method through a real-world study of Beijing air pollution in multiple\nlocations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As part of enhancing the interpretability of machine learning, it is of\nrenewed interest to quantify and infer the predictive importance of certain\nexposure covariates. Modern scientific studies often collect data from multiple\nsources with distributional heterogeneity. Thus, measuring and inferring stable\nassociations across multiple environments is crucial in reliable and\ngeneralizable decision-making. In this paper, we propose MIMAL, a novel\nstatistical framework for Multi-source stable Importance Measure via\nAdversarial Learning. MIMAL measures the importance of some exposure variables\nby maximizing the worst-case predictive reward over the source mixture. Our\nframework allows various machine learning methods for confounding adjustment\nand exposure effect characterization. For inferential analysis, the asymptotic\nnormality of our introduced statistic is established under a general machine\nlearning framework that requires no stronger learning accuracy conditions than\nthose for single source variable importance. Numerical studies with various\ntypes of data generation setups and machine learning implementation are\nconducted to justify the finite-sample performance of MIMAL. We also illustrate\nour method through a real-world study of Beijing air pollution in multiple\nlocations."
                },
                "authors": [
                    {
                        "name": "Zitao Wang"
                    },
                    {
                        "name": "Nian Si"
                    },
                    {
                        "name": "Zijian Guo"
                    },
                    {
                        "name": "Molei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Molei Liu"
                },
                "author": "Molei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07380v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07380v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.13574v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.13574v3",
                "updated": "2024-09-11T16:01:08Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    16,
                    1,
                    8,
                    2,
                    255,
                    0
                ],
                "published": "2023-07-25T15:27:52Z",
                "published_parsed": [
                    2023,
                    7,
                    25,
                    15,
                    27,
                    52,
                    1,
                    206,
                    0
                ],
                "title": "Unraveling the early universe's equation of state and primordial black\n  hole production with PTA, BBN, and CMB observations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unraveling the early universe's equation of state and primordial black\n  hole production with PTA, BBN, and CMB observations"
                },
                "summary": "Pulsar timing array (PTA) data releases showed strong evidence for a\nstochastic gravitational-wave background in the nanohertz band. When the signal\nis interpreted by a scenario of scalar-induced gravitational waves (SIGWs), we\nencounter overproduction of primordial black holes (PBHs). We wonder if varying\nthe equation of state (EoS) of the early Universe can resolve this issue and\nthereby lead to a consistent interpretation of the PTA data. Analyzing a data\ncombination of PTA, big-bang nucleosynthesis, and cosmic microwave background,\nwe find that an epoch with EoS $w\\sim\\mathcal{O}(10^{-2})$ between the end of\ninflation and the onset of radiation domination can significantly suppress the\nproduction of PBHs, leading to alleviation of the PBH-overproduction issue.\nWith the inferred interval $w=0.44_{-0.40}^{+0.52}$ at 95\\% confidence level,\nour scenario can interpret the PTA data just as well as the conventional\nscenario of SIGWs produced during the radiation domination.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pulsar timing array (PTA) data releases showed strong evidence for a\nstochastic gravitational-wave background in the nanohertz band. When the signal\nis interpreted by a scenario of scalar-induced gravitational waves (SIGWs), we\nencounter overproduction of primordial black holes (PBHs). We wonder if varying\nthe equation of state (EoS) of the early Universe can resolve this issue and\nthereby lead to a consistent interpretation of the PTA data. Analyzing a data\ncombination of PTA, big-bang nucleosynthesis, and cosmic microwave background,\nwe find that an epoch with EoS $w\\sim\\mathcal{O}(10^{-2})$ between the end of\ninflation and the onset of radiation domination can significantly suppress the\nproduction of PBHs, leading to alleviation of the PBH-overproduction issue.\nWith the inferred interval $w=0.44_{-0.40}^{+0.52}$ at 95\\% confidence level,\nour scenario can interpret the PTA data just as well as the conventional\nscenario of SIGWs produced during the radiation domination."
                },
                "authors": [
                    {
                        "name": "Qing-Hua Zhu"
                    },
                    {
                        "name": "Zhi-Chao Zhao"
                    },
                    {
                        "name": "Sai Wang"
                    },
                    {
                        "name": "Xin Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Zhang"
                },
                "author": "Xin Zhang",
                "arxiv_doi": "10.1088/1674-1137/ad79d5",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1088/1674-1137/ad79d5",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2307.13574v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.13574v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "9 pages, 5 figures, 2 columns, minor revision. To be published in\n  Chinese Physics C",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07369v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07369v1",
                "updated": "2024-09-11T15:58:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    58,
                    9,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T15:58:09Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    58,
                    9,
                    2,
                    255,
                    0
                ],
                "title": "Constraining Genetic Symbolic Regression via Semantic Backpropagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constraining Genetic Symbolic Regression via Semantic Backpropagation"
                },
                "summary": "Evolutionary symbolic regression approaches are powerful tools that can\napproximate an explicit mapping between input features and observation for\nvarious problems. However, ensuring that explored expressions maintain\nconsistency with domain-specific constraints remains a crucial challenge. While\nneural networks are able to employ additional information like conservation\nlaws to achieve more appropriate and robust approximations, the potential\nremains unrealized within genetic algorithms. This disparity is rooted in the\ninherent discrete randomness of recombining and mutating to generate new\nmapping expressions, making it challenging to maintain and preserve inferred\nconstraints or restrictions in the course of the exploration. To address this\nlimitation, we propose an approach centered on semantic backpropagation\nincorporated into the Gene Expression Programming (GEP), which integrates\ndomain-specific properties in a vector representation as corrective feedback\nduring the evolutionary process. By creating backward rules akin to algorithmic\ndifferentiation and leveraging pre-computed subsolutions, the mechanism allows\nthe enforcement of any constraint within an expression tree by determining the\nmisalignment and propagating desired changes back. To illustrate the\neffectiveness of constraining GEP through semantic backpropagation, we take the\nconstraint of physical dimension as an example. This framework is applied to\ndiscovering physical equations from the Feynman lectures. Results have shown\nnot only an increased likelihood of recovering the original equation but also\nnotable robustness in the presence of noisy data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolutionary symbolic regression approaches are powerful tools that can\napproximate an explicit mapping between input features and observation for\nvarious problems. However, ensuring that explored expressions maintain\nconsistency with domain-specific constraints remains a crucial challenge. While\nneural networks are able to employ additional information like conservation\nlaws to achieve more appropriate and robust approximations, the potential\nremains unrealized within genetic algorithms. This disparity is rooted in the\ninherent discrete randomness of recombining and mutating to generate new\nmapping expressions, making it challenging to maintain and preserve inferred\nconstraints or restrictions in the course of the exploration. To address this\nlimitation, we propose an approach centered on semantic backpropagation\nincorporated into the Gene Expression Programming (GEP), which integrates\ndomain-specific properties in a vector representation as corrective feedback\nduring the evolutionary process. By creating backward rules akin to algorithmic\ndifferentiation and leveraging pre-computed subsolutions, the mechanism allows\nthe enforcement of any constraint within an expression tree by determining the\nmisalignment and propagating desired changes back. To illustrate the\neffectiveness of constraining GEP through semantic backpropagation, we take the\nconstraint of physical dimension as an example. This framework is applied to\ndiscovering physical equations from the Feynman lectures. Results have shown\nnot only an increased likelihood of recovering the original equation but also\nnotable robustness in the presence of noisy data."
                },
                "authors": [
                    {
                        "name": "Maximilian Reissmann"
                    },
                    {
                        "name": "Yuan Fang"
                    },
                    {
                        "name": "Andrew Ooi"
                    },
                    {
                        "name": "Richard Sandberg"
                    }
                ],
                "author_detail": {
                    "name": "Richard Sandberg"
                },
                "author": "Richard Sandberg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07369v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07369v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.MP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07368v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07368v1",
                "updated": "2024-09-11T15:56:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    56,
                    15,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T15:56:15Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    56,
                    15,
                    2,
                    255,
                    0
                ],
                "title": "Demo: SGCode: A Flexible Prompt-Optimizing System for Secure Generation\n  of Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demo: SGCode: A Flexible Prompt-Optimizing System for Secure Generation\n  of Code"
                },
                "summary": "This paper introduces SGCode, a flexible prompt-optimizing system to generate\nsecure code with large language models (LLMs). SGCode integrates recent\nprompt-optimization approaches with LLMs in a unified system accessible through\nfront-end and back-end APIs, enabling users to 1) generate secure code, which\nis free of vulnerabilities, 2) review and share security analysis, and 3)\neasily switch from one prompt optimization approach to another, while providing\ninsights on model and system performance. We populated SGCode on an AWS server\nwith PromSec, an approach that optimizes prompts by combining an LLM and\nsecurity tools with a lightweight generative adversarial graph neural network\nto detect and fix security vulnerabilities in the generated code. Extensive\nexperiments show that SGCode is practical as a public tool to gain insights\ninto the trade-offs between model utility, secure code generation, and system\ncost. SGCode has only a marginal cost compared with prompting LLMs. SGCode is\navailable at: http://3.131.141.63:8501/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces SGCode, a flexible prompt-optimizing system to generate\nsecure code with large language models (LLMs). SGCode integrates recent\nprompt-optimization approaches with LLMs in a unified system accessible through\nfront-end and back-end APIs, enabling users to 1) generate secure code, which\nis free of vulnerabilities, 2) review and share security analysis, and 3)\neasily switch from one prompt optimization approach to another, while providing\ninsights on model and system performance. We populated SGCode on an AWS server\nwith PromSec, an approach that optimizes prompts by combining an LLM and\nsecurity tools with a lightweight generative adversarial graph neural network\nto detect and fix security vulnerabilities in the generated code. Extensive\nexperiments show that SGCode is practical as a public tool to gain insights\ninto the trade-offs between model utility, secure code generation, and system\ncost. SGCode has only a marginal cost compared with prompting LLMs. SGCode is\navailable at: http://3.131.141.63:8501/."
                },
                "authors": [
                    {
                        "name": "Khiem Ton"
                    },
                    {
                        "name": "Nhi Nguyen"
                    },
                    {
                        "name": "Mahmoud Nazzal"
                    },
                    {
                        "name": "Abdallah Khreishah"
                    },
                    {
                        "name": "Cristian Borcea"
                    },
                    {
                        "name": "NhatHai Phan"
                    },
                    {
                        "name": "Ruoming Jin"
                    },
                    {
                        "name": "Issa Khalil"
                    },
                    {
                        "name": "Yelong Shen"
                    }
                ],
                "author_detail": {
                    "name": "Yelong Shen"
                },
                "author": "Yelong Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07368v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07368v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.13764v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.13764v4",
                "updated": "2024-09-11T15:47:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    47,
                    11,
                    2,
                    255,
                    0
                ],
                "published": "2024-02-21T12:38:59Z",
                "published_parsed": [
                    2024,
                    2,
                    21,
                    12,
                    38,
                    59,
                    2,
                    52,
                    0
                ],
                "title": "CriticEval: Evaluating Large Language Model as Critic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CriticEval: Evaluating Large Language Model as Critic"
                },
                "summary": "Critique ability, i.e., the capability of Large Language Models (LLMs) to\nidentify and rectify flaws in responses, is crucial for their applications in\nself-improvement and scalable oversight. While numerous studies have been\nproposed to evaluate critique ability of LLMs, their comprehensiveness and\nreliability are still limited. To overcome this problem, we introduce\nCriticEval, a novel benchmark designed to comprehensively and reliably evaluate\ncritique ability of LLMs. Specifically, to ensure the comprehensiveness,\nCriticEval evaluates critique ability from four dimensions across nine diverse\ntask scenarios. It evaluates both scalar-valued and textual critiques,\ntargeting responses of varying quality. To ensure the reliability, a large\nnumber of critiques are annotated to serve as references, enabling GPT-4 to\nevaluate textual critiques reliably. Extensive evaluations of open-source and\nclosed-source LLMs first validate the reliability of evaluation in CriticEval.\nThen, experimental results demonstrate the promising potential of open-source\nLLMs, the effectiveness of critique datasets and several intriguing\nrelationships between the critique ability and some critical factors, including\ntask types, response qualities and critique dimensions. Datasets and evaluation\ntoolkit for CriticEval will be publicly released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Critique ability, i.e., the capability of Large Language Models (LLMs) to\nidentify and rectify flaws in responses, is crucial for their applications in\nself-improvement and scalable oversight. While numerous studies have been\nproposed to evaluate critique ability of LLMs, their comprehensiveness and\nreliability are still limited. To overcome this problem, we introduce\nCriticEval, a novel benchmark designed to comprehensively and reliably evaluate\ncritique ability of LLMs. Specifically, to ensure the comprehensiveness,\nCriticEval evaluates critique ability from four dimensions across nine diverse\ntask scenarios. It evaluates both scalar-valued and textual critiques,\ntargeting responses of varying quality. To ensure the reliability, a large\nnumber of critiques are annotated to serve as references, enabling GPT-4 to\nevaluate textual critiques reliably. Extensive evaluations of open-source and\nclosed-source LLMs first validate the reliability of evaluation in CriticEval.\nThen, experimental results demonstrate the promising potential of open-source\nLLMs, the effectiveness of critique datasets and several intriguing\nrelationships between the critique ability and some critical factors, including\ntask types, response qualities and critique dimensions. Datasets and evaluation\ntoolkit for CriticEval will be publicly released."
                },
                "authors": [
                    {
                        "name": "Tian Lan"
                    },
                    {
                        "name": "Wenwei Zhang"
                    },
                    {
                        "name": "Chen Xu"
                    },
                    {
                        "name": "Heyan Huang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Xian-ling Mao"
                    }
                ],
                "author_detail": {
                    "name": "Xian-ling Mao"
                },
                "author": "Xian-ling Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.13764v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.13764v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07355v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07355v1",
                "updated": "2024-09-11T15:40:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    40,
                    7,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T15:40:07Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    40,
                    7,
                    2,
                    255,
                    0
                ],
                "title": "Think Together and Work Better: Combining Humans' and LLMs' Think-Aloud\n  Outcomes for Effective Text Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think Together and Work Better: Combining Humans' and LLMs' Think-Aloud\n  Outcomes for Effective Text Evaluation"
                },
                "summary": "This study introduces \\textbf{InteractEval}, a framework that integrates\nhuman expertise and Large Language Models (LLMs) using the Think-Aloud (TA)\nmethod to generate attributes for checklist-based text evaluation. By combining\nhuman flexibility and reasoning with LLM consistency, InteractEval outperforms\ntraditional non-LLM-based and LLM-based baselines across four distinct\ndimensions, consisting of Coherence, Fluency, Consistency, and Relevance. The\nexperiment also investigates the effectiveness of the TA method, showing that\nit promotes divergent thinking in both humans and LLMs, leading to the\ngeneration of a wider range of relevant attributes and enhance text evaluation\nperformance. Comparative analysis reveals that humans excel at identifying\nattributes related to internal quality (Coherence and Fluency), but LLMs\nperform better at those attributes related to external alignment (Consistency\nand Relevance). Consequently, leveraging both humans and LLMs together produces\nthe best evaluation outcomes. In other words, this study emphasizes the\nnecessity of effectively combining humans and LLMs in an automated\nchecklist-based text evaluation framework. The code is available at\n\\textbf{\\url{https://github.com/BBeeChu/InteractEval.git}}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study introduces \\textbf{InteractEval}, a framework that integrates\nhuman expertise and Large Language Models (LLMs) using the Think-Aloud (TA)\nmethod to generate attributes for checklist-based text evaluation. By combining\nhuman flexibility and reasoning with LLM consistency, InteractEval outperforms\ntraditional non-LLM-based and LLM-based baselines across four distinct\ndimensions, consisting of Coherence, Fluency, Consistency, and Relevance. The\nexperiment also investigates the effectiveness of the TA method, showing that\nit promotes divergent thinking in both humans and LLMs, leading to the\ngeneration of a wider range of relevant attributes and enhance text evaluation\nperformance. Comparative analysis reveals that humans excel at identifying\nattributes related to internal quality (Coherence and Fluency), but LLMs\nperform better at those attributes related to external alignment (Consistency\nand Relevance). Consequently, leveraging both humans and LLMs together produces\nthe best evaluation outcomes. In other words, this study emphasizes the\nnecessity of effectively combining humans and LLMs in an automated\nchecklist-based text evaluation framework. The code is available at\n\\textbf{\\url{https://github.com/BBeeChu/InteractEval.git}}."
                },
                "authors": [
                    {
                        "name": "SeongYeub Chu"
                    },
                    {
                        "name": "JongWoo Kim"
                    },
                    {
                        "name": "MunYong Yi"
                    }
                ],
                "author_detail": {
                    "name": "MunYong Yi"
                },
                "author": "MunYong Yi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07355v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07355v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07353v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07353v1",
                "updated": "2024-09-11T15:39:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    39,
                    42,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T15:39:42Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    39,
                    42,
                    2,
                    255,
                    0
                ],
                "title": "Securing Vision-Language Models with a Robust Encoder Against Jailbreak\n  and Adversarial Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Securing Vision-Language Models with a Robust Encoder Against Jailbreak\n  and Adversarial Attacks"
                },
                "summary": "Large Vision-Language Models (LVLMs), trained on multimodal big datasets,\nhave significantly advanced AI by excelling in vision-language tasks. However,\nthese models remain vulnerable to adversarial attacks, particularly jailbreak\nattacks, which bypass safety protocols and cause the model to generate\nmisleading or harmful responses. This vulnerability stems from both the\ninherent susceptibilities of LLMs and the expanded attack surface introduced by\nthe visual modality. We propose Sim-CLIP+, a novel defense mechanism that\nadversarially fine-tunes the CLIP vision encoder by leveraging a Siamese\narchitecture. This approach maximizes cosine similarity between perturbed and\nclean samples, facilitating resilience against adversarial manipulations.\nSim-CLIP+ offers a plug-and-play solution, allowing seamless integration into\nexisting LVLM architectures as a robust vision encoder. Unlike previous\ndefenses, our method requires no structural modifications to the LVLM and\nincurs minimal computational overhead. Sim-CLIP+ demonstrates effectiveness\nagainst both gradient-based adversarial attacks and various jailbreak\ntechniques. We evaluate Sim-CLIP+ against three distinct jailbreak attack\nstrategies and perform clean evaluations using standard downstream datasets,\nincluding COCO for image captioning and OKVQA for visual question answering.\nExtensive experiments demonstrate that Sim-CLIP+ maintains high clean accuracy\nwhile substantially improving robustness against both gradient-based\nadversarial attacks and jailbreak techniques. Our code and robust vision\nencoders are available at\nhttps://github.com/speedlab-git/Robust-Encoder-against-Jailbreak-attack.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision-Language Models (LVLMs), trained on multimodal big datasets,\nhave significantly advanced AI by excelling in vision-language tasks. However,\nthese models remain vulnerable to adversarial attacks, particularly jailbreak\nattacks, which bypass safety protocols and cause the model to generate\nmisleading or harmful responses. This vulnerability stems from both the\ninherent susceptibilities of LLMs and the expanded attack surface introduced by\nthe visual modality. We propose Sim-CLIP+, a novel defense mechanism that\nadversarially fine-tunes the CLIP vision encoder by leveraging a Siamese\narchitecture. This approach maximizes cosine similarity between perturbed and\nclean samples, facilitating resilience against adversarial manipulations.\nSim-CLIP+ offers a plug-and-play solution, allowing seamless integration into\nexisting LVLM architectures as a robust vision encoder. Unlike previous\ndefenses, our method requires no structural modifications to the LVLM and\nincurs minimal computational overhead. Sim-CLIP+ demonstrates effectiveness\nagainst both gradient-based adversarial attacks and various jailbreak\ntechniques. We evaluate Sim-CLIP+ against three distinct jailbreak attack\nstrategies and perform clean evaluations using standard downstream datasets,\nincluding COCO for image captioning and OKVQA for visual question answering.\nExtensive experiments demonstrate that Sim-CLIP+ maintains high clean accuracy\nwhile substantially improving robustness against both gradient-based\nadversarial attacks and jailbreak techniques. Our code and robust vision\nencoders are available at\nhttps://github.com/speedlab-git/Robust-Encoder-against-Jailbreak-attack.git."
                },
                "authors": [
                    {
                        "name": "Md Zarif Hossain"
                    },
                    {
                        "name": "Ahmed Imteaj"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed Imteaj"
                },
                "author": "Ahmed Imteaj",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07353v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07353v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07331v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07331v1",
                "updated": "2024-09-11T15:11:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    11,
                    39,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T15:11:39Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    11,
                    39,
                    2,
                    255,
                    0
                ],
                "title": "Learning to Compress Contexts for Efficient Knowledge-based Visual\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Compress Contexts for Efficient Knowledge-based Visual\n  Question Answering"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have demonstrated great zero-shot\nperformance on visual question answering (VQA). However, when it comes to\nknowledge-based VQA (KB-VQA), MLLMs may lack human commonsense or specialized\ndomain knowledge to answer such questions and require obtaining necessary\ninformation from external knowledge sources. Previous works like\nRetrival-Augmented VQA-v2 (RAVQA-v2) focus on utilizing as much input\ninformation, such as image-based textual descriptions and retrieved knowledge,\nas possible to improve performance, but they all overlook the issue that with\nthe number of input tokens increasing, inference efficiency significantly\ndecreases, which contradicts the demands of practical applications. To address\nthis issue, we propose Retrieval-Augmented MLLM with Compressed Contexts\n(RACC). RACC learns to compress and aggregate retrieved contexts, from which it\ngenerates a compact modulation in the form of Key-Value (KV) cache. This\nmodulation is then used to adapt the downstream frozen MLLM, thereby achieving\neffective and efficient inference. RACC achieves a state-of-the-art (SOTA)\nperformance of 62.9% on OK-VQA. Moreover, it significantly reduces inference\nlatency by 22.0%-59.7% compared to the prominent RAVQA-v2. Abundant experiments\nshow RACC's broad applicability. It is compatible with various off-the-shelf\nMLLMs and can also handle different knowledge sources including textual and\nmultimodal documents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have demonstrated great zero-shot\nperformance on visual question answering (VQA). However, when it comes to\nknowledge-based VQA (KB-VQA), MLLMs may lack human commonsense or specialized\ndomain knowledge to answer such questions and require obtaining necessary\ninformation from external knowledge sources. Previous works like\nRetrival-Augmented VQA-v2 (RAVQA-v2) focus on utilizing as much input\ninformation, such as image-based textual descriptions and retrieved knowledge,\nas possible to improve performance, but they all overlook the issue that with\nthe number of input tokens increasing, inference efficiency significantly\ndecreases, which contradicts the demands of practical applications. To address\nthis issue, we propose Retrieval-Augmented MLLM with Compressed Contexts\n(RACC). RACC learns to compress and aggregate retrieved contexts, from which it\ngenerates a compact modulation in the form of Key-Value (KV) cache. This\nmodulation is then used to adapt the downstream frozen MLLM, thereby achieving\neffective and efficient inference. RACC achieves a state-of-the-art (SOTA)\nperformance of 62.9% on OK-VQA. Moreover, it significantly reduces inference\nlatency by 22.0%-59.7% compared to the prominent RAVQA-v2. Abundant experiments\nshow RACC's broad applicability. It is compatible with various off-the-shelf\nMLLMs and can also handle different knowledge sources including textual and\nmultimodal documents."
                },
                "authors": [
                    {
                        "name": "Weixi Weng"
                    },
                    {
                        "name": "Jieming Zhu"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Xiaojun Meng"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Chun Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Chun Yuan"
                },
                "author": "Chun Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07331v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07331v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07315v1",
                "updated": "2024-09-11T14:48:14Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    14,
                    48,
                    14,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T14:48:14Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    14,
                    48,
                    14,
                    2,
                    255,
                    0
                ],
                "title": "Integrating Bayesian Approaches and Expert Knowledge for Forecasting\n  Continuous Glucose Monitoring Values in Type 2 Diabetes Mellitus",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Bayesian Approaches and Expert Knowledge for Forecasting\n  Continuous Glucose Monitoring Values in Type 2 Diabetes Mellitus"
                },
                "summary": "Precise and timely forecasting of blood glucose levels is essential for\neffective diabetes management. While extensive research has been conducted on\nType 1 diabetes mellitus, Type 2 diabetes mellitus (T2DM) presents unique\nchallenges due to its heterogeneity, underscoring the need for specialized\nblood glucose forecasting systems. This study introduces a novel blood glucose\nforecasting system, applied to a dataset of 100 patients from the ShanghaiT2DM\nstudy. Our study uniquely integrates knowledge-driven and data-driven\napproaches, leveraging expert knowledge to validate and interpret the\nrelationships among diabetes-related variables and deploying the data-driven\napproach to provide accurate forecast blood glucose levels. The Bayesian\nnetwork approach facilitates the analysis of dependencies among various\ndiabetes-related variables, thus enabling the inference of continuous glucose\nmonitoring (CGM) trajectories in similar individuals with T2DM. By\nincorporating past CGM data including inference CGM trajectories, dietary\nrecords, and individual-specific information, the Bayesian structural time\nseries (BSTS) model effectively forecasts glucose levels across time intervals\nranging from 15 to 60 minutes. Forecast results show a mean absolute error of\n6.41 mg/dL, a root mean square error of 8.29 mg/dL, and a mean absolute\npercentage error of 5.28%, for a 15-minute prediction horizon. This study makes\nthe first application of the ShanghaiT2DM dataset for glucose level\nforecasting, considering the influences of diabetes-related variables. Its\nfindings establish a foundational framework for developing personalized\ndiabetes management strategies, potentially enhancing diabetes care through\nmore accurate and timely interventions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Precise and timely forecasting of blood glucose levels is essential for\neffective diabetes management. While extensive research has been conducted on\nType 1 diabetes mellitus, Type 2 diabetes mellitus (T2DM) presents unique\nchallenges due to its heterogeneity, underscoring the need for specialized\nblood glucose forecasting systems. This study introduces a novel blood glucose\nforecasting system, applied to a dataset of 100 patients from the ShanghaiT2DM\nstudy. Our study uniquely integrates knowledge-driven and data-driven\napproaches, leveraging expert knowledge to validate and interpret the\nrelationships among diabetes-related variables and deploying the data-driven\napproach to provide accurate forecast blood glucose levels. The Bayesian\nnetwork approach facilitates the analysis of dependencies among various\ndiabetes-related variables, thus enabling the inference of continuous glucose\nmonitoring (CGM) trajectories in similar individuals with T2DM. By\nincorporating past CGM data including inference CGM trajectories, dietary\nrecords, and individual-specific information, the Bayesian structural time\nseries (BSTS) model effectively forecasts glucose levels across time intervals\nranging from 15 to 60 minutes. Forecast results show a mean absolute error of\n6.41 mg/dL, a root mean square error of 8.29 mg/dL, and a mean absolute\npercentage error of 5.28%, for a 15-minute prediction horizon. This study makes\nthe first application of the ShanghaiT2DM dataset for glucose level\nforecasting, considering the influences of diabetes-related variables. Its\nfindings establish a foundational framework for developing personalized\ndiabetes management strategies, potentially enhancing diabetes care through\nmore accurate and timely interventions."
                },
                "authors": [
                    {
                        "name": "Yuyang Sun"
                    },
                    {
                        "name": "Panagiotis Kosmas"
                    }
                ],
                "author_detail": {
                    "name": "Panagiotis Kosmas"
                },
                "author": "Panagiotis Kosmas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07314v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07314v1",
                "updated": "2024-09-11T14:44:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    14,
                    44,
                    51,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T14:44:51Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    14,
                    44,
                    51,
                    2,
                    255,
                    0
                ],
                "title": "MEDIC: Towards a Comprehensive Framework for Evaluating LLMs in Clinical\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEDIC: Towards a Comprehensive Framework for Evaluating LLMs in Clinical\n  Applications"
                },
                "summary": "The rapid development of Large Language Models (LLMs) for healthcare\napplications has spurred calls for holistic evaluation beyond frequently-cited\nbenchmarks like USMLE, to better reflect real-world performance. While\nreal-world assessments are valuable indicators of utility, they often lag\nbehind the pace of LLM evolution, likely rendering findings obsolete upon\ndeployment. This temporal disconnect necessitates a comprehensive upfront\nevaluation that can guide model selection for specific clinical applications.\nWe introduce MEDIC, a framework assessing LLMs across five critical dimensions\nof clinical competence: medical reasoning, ethics and bias, data and language\nunderstanding, in-context learning, and clinical safety. MEDIC features a novel\ncross-examination framework quantifying LLM performance across areas like\ncoverage and hallucination detection, without requiring reference outputs. We\napply MEDIC to evaluate LLMs on medical question-answering, safety,\nsummarization, note generation, and other tasks. Our results show performance\ndisparities across model sizes, baseline vs medically finetuned models, and\nhave implications on model selection for applications requiring specific model\nstrengths, such as low hallucination or lower cost of inference. MEDIC's\nmultifaceted evaluation reveals these performance trade-offs, bridging the gap\nbetween theoretical capabilities and practical implementation in healthcare\nsettings, ensuring that the most promising models are identified and adapted\nfor diverse healthcare applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of Large Language Models (LLMs) for healthcare\napplications has spurred calls for holistic evaluation beyond frequently-cited\nbenchmarks like USMLE, to better reflect real-world performance. While\nreal-world assessments are valuable indicators of utility, they often lag\nbehind the pace of LLM evolution, likely rendering findings obsolete upon\ndeployment. This temporal disconnect necessitates a comprehensive upfront\nevaluation that can guide model selection for specific clinical applications.\nWe introduce MEDIC, a framework assessing LLMs across five critical dimensions\nof clinical competence: medical reasoning, ethics and bias, data and language\nunderstanding, in-context learning, and clinical safety. MEDIC features a novel\ncross-examination framework quantifying LLM performance across areas like\ncoverage and hallucination detection, without requiring reference outputs. We\napply MEDIC to evaluate LLMs on medical question-answering, safety,\nsummarization, note generation, and other tasks. Our results show performance\ndisparities across model sizes, baseline vs medically finetuned models, and\nhave implications on model selection for applications requiring specific model\nstrengths, such as low hallucination or lower cost of inference. MEDIC's\nmultifaceted evaluation reveals these performance trade-offs, bridging the gap\nbetween theoretical capabilities and practical implementation in healthcare\nsettings, ensuring that the most promising models are identified and adapted\nfor diverse healthcare applications."
                },
                "authors": [
                    {
                        "name": "Praveen K Kanithi"
                    },
                    {
                        "name": "Clément Christophe"
                    },
                    {
                        "name": "Marco AF Pimentel"
                    },
                    {
                        "name": "Tathagata Raha"
                    },
                    {
                        "name": "Nada Saadi"
                    },
                    {
                        "name": "Hamza Javed"
                    },
                    {
                        "name": "Svetlana Maslenkova"
                    },
                    {
                        "name": "Nasir Hayat"
                    },
                    {
                        "name": "Ronnie Rajan"
                    },
                    {
                        "name": "Shadab Khan"
                    }
                ],
                "author_detail": {
                    "name": "Shadab Khan"
                },
                "author": "Shadab Khan",
                "arxiv_comment": "Technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07314v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07314v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.02043v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.02043v3",
                "updated": "2024-09-11T14:43:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    14,
                    43,
                    56,
                    2,
                    255,
                    0
                ],
                "published": "2023-11-03T17:19:31Z",
                "published_parsed": [
                    2023,
                    11,
                    3,
                    17,
                    19,
                    31,
                    4,
                    307,
                    0
                ],
                "title": "Bayesian Quantile Regression with Subset Selection: A Posterior\n  Summarization Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Quantile Regression with Subset Selection: A Posterior\n  Summarization Perspective"
                },
                "summary": "Quantile regression is a powerful tool in epidemiological studies where\ninterest lies in inferring how different exposures affect specific percentiles\nof the distribution of a health or life outcome. Existing methods either\nestimate conditional quantiles separately for each quantile of interest or\nestimate the entire conditional distribution using semi- or non-parametric\nmodels. The former often produce inadequate models for real data and do not\nshare information across quantiles, while the latter are characterized by\ncomplex and constrained models that can be difficult to interpret and\ncomputationally inefficient. Further, neither approach is well-suited for\nquantile-specific subset selection. Instead, we pose the fundamental problems\nof linear quantile estimation, uncertainty quantification, and subset selection\nfrom a Bayesian decision analysis perspective. For any Bayesian regression\nmodel, we derive optimal and interpretable linear estimates and uncertainty\nquantification for each model-based conditional quantile. Our approach\nintroduces a quantile-focused squared error loss, which enables efficient,\nclosed-form computing and maintains a close relationship with Wasserstein-based\ndensity estimation. In an extensive simulation study, our methods demonstrate\nsubstantial gains in quantile estimation accuracy, variable selection, and\ninference over frequentist and Bayesian competitors. We use these tools to\nidentify and quantify the heterogeneous impacts of multiple social stressors\nand environmental exposures on educational outcomes across the full spectrum of\nlow-, medium-, and high-achieving students in North Carolina.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantile regression is a powerful tool in epidemiological studies where\ninterest lies in inferring how different exposures affect specific percentiles\nof the distribution of a health or life outcome. Existing methods either\nestimate conditional quantiles separately for each quantile of interest or\nestimate the entire conditional distribution using semi- or non-parametric\nmodels. The former often produce inadequate models for real data and do not\nshare information across quantiles, while the latter are characterized by\ncomplex and constrained models that can be difficult to interpret and\ncomputationally inefficient. Further, neither approach is well-suited for\nquantile-specific subset selection. Instead, we pose the fundamental problems\nof linear quantile estimation, uncertainty quantification, and subset selection\nfrom a Bayesian decision analysis perspective. For any Bayesian regression\nmodel, we derive optimal and interpretable linear estimates and uncertainty\nquantification for each model-based conditional quantile. Our approach\nintroduces a quantile-focused squared error loss, which enables efficient,\nclosed-form computing and maintains a close relationship with Wasserstein-based\ndensity estimation. In an extensive simulation study, our methods demonstrate\nsubstantial gains in quantile estimation accuracy, variable selection, and\ninference over frequentist and Bayesian competitors. We use these tools to\nidentify and quantify the heterogeneous impacts of multiple social stressors\nand environmental exposures on educational outcomes across the full spectrum of\nlow-, medium-, and high-achieving students in North Carolina."
                },
                "authors": [
                    {
                        "name": "Joseph Feldman"
                    },
                    {
                        "name": "Daniel Kowal"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Kowal"
                },
                "author": "Daniel Kowal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.02043v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.02043v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.05993v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.05993v2",
                "updated": "2024-09-11T14:42:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    14,
                    42,
                    29,
                    2,
                    255,
                    0
                ],
                "published": "2024-04-09T03:54:28Z",
                "published_parsed": [
                    2024,
                    4,
                    9,
                    3,
                    54,
                    28,
                    1,
                    100,
                    0
                ],
                "title": "AEGIS: Online Adaptive AI Content Safety Moderation with Ensemble of LLM\n  Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AEGIS: Online Adaptive AI Content Safety Moderation with Ensemble of LLM\n  Experts"
                },
                "summary": "As Large Language Models (LLMs) and generative AI become more widespread, the\ncontent safety risks associated with their use also increase. We find a notable\ndeficiency in high-quality content safety datasets and benchmarks that\ncomprehensively cover a wide range of critical safety areas. To address this,\nwe define a broad content safety risk taxonomy, comprising 13 critical risk and\n9 sparse risk categories. Additionally, we curate AEGISSAFETYDATASET, a new\ndataset of approximately 26, 000 human-LLM interaction instances, complete with\nhuman annotations adhering to the taxonomy. We plan to release this dataset to\nthe community to further research and to help benchmark LLM models for safety.\nTo demonstrate the effectiveness of the dataset, we instruction-tune multiple\nLLM-based safety models. We show that our models (named AEGISSAFETYEXPERTS),\nnot only surpass or perform competitively with the state-of-the-art LLM-based\nsafety models and general purpose LLMs, but also exhibit robustness across\nmultiple jail-break attack categories. We also show how using\nAEGISSAFETYDATASET during the LLM alignment phase does not negatively impact\nthe performance of the aligned models on MT Bench scores. Furthermore, we\npropose AEGIS, a novel application of a no-regret online adaptation framework\nwith strong theoretical guarantees, to perform content moderation with an\nensemble of LLM content safety experts in deployment",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) and generative AI become more widespread, the\ncontent safety risks associated with their use also increase. We find a notable\ndeficiency in high-quality content safety datasets and benchmarks that\ncomprehensively cover a wide range of critical safety areas. To address this,\nwe define a broad content safety risk taxonomy, comprising 13 critical risk and\n9 sparse risk categories. Additionally, we curate AEGISSAFETYDATASET, a new\ndataset of approximately 26, 000 human-LLM interaction instances, complete with\nhuman annotations adhering to the taxonomy. We plan to release this dataset to\nthe community to further research and to help benchmark LLM models for safety.\nTo demonstrate the effectiveness of the dataset, we instruction-tune multiple\nLLM-based safety models. We show that our models (named AEGISSAFETYEXPERTS),\nnot only surpass or perform competitively with the state-of-the-art LLM-based\nsafety models and general purpose LLMs, but also exhibit robustness across\nmultiple jail-break attack categories. We also show how using\nAEGISSAFETYDATASET during the LLM alignment phase does not negatively impact\nthe performance of the aligned models on MT Bench scores. Furthermore, we\npropose AEGIS, a novel application of a no-regret online adaptation framework\nwith strong theoretical guarantees, to perform content moderation with an\nensemble of LLM content safety experts in deployment"
                },
                "authors": [
                    {
                        "name": "Shaona Ghosh"
                    },
                    {
                        "name": "Prasoon Varshney"
                    },
                    {
                        "name": "Erick Galinkin"
                    },
                    {
                        "name": "Christopher Parisien"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Parisien"
                },
                "author": "Christopher Parisien",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.05993v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.05993v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.13430v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.13430v2",
                "updated": "2024-09-11T14:40:19Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    14,
                    40,
                    19,
                    2,
                    255,
                    0
                ],
                "published": "2023-03-23T16:50:19Z",
                "published_parsed": [
                    2023,
                    3,
                    23,
                    16,
                    50,
                    19,
                    3,
                    82,
                    0
                ],
                "title": "Medical diffusion on a budget: Textual Inversion for medical image\n  generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical diffusion on a budget: Textual Inversion for medical image\n  generation"
                },
                "summary": "Diffusion models for text-to-image generation, known for their efficiency,\naccessibility, and quality, have gained popularity. While inference with these\nsystems on consumer-grade GPUs is increasingly feasible, training from scratch\nrequires large captioned datasets and significant computational resources. In\nmedical image generation, the limited availability of large, publicly\naccessible datasets with text reports poses challenges due to legal and ethical\nconcerns. This work shows that adapting pre-trained Stable Diffusion models to\nmedical imaging modalities is achievable by training text embeddings using\nTextual Inversion. In this study, we experimented with small medical datasets\n(100 samples each from three modalities) and trained within hours to generate\ndiagnostically accurate images, as judged by an expert radiologist. Experiments\nwith Textual Inversion training and inference parameters reveal the necessity\nof larger embeddings and more examples in the medical domain. Classification\nexperiments show an increase in diagnostic accuracy (AUC) for detecting\nprostate cancer on MRI, from 0.78 to 0.80. Further experiments demonstrate\nembedding flexibility through disease interpolation, combining pathologies, and\ninpainting for precise disease appearance control. The trained embeddings are\ncompact (less than 1 MB), enabling easy data sharing with reduced privacy\nconcerns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models for text-to-image generation, known for their efficiency,\naccessibility, and quality, have gained popularity. While inference with these\nsystems on consumer-grade GPUs is increasingly feasible, training from scratch\nrequires large captioned datasets and significant computational resources. In\nmedical image generation, the limited availability of large, publicly\naccessible datasets with text reports poses challenges due to legal and ethical\nconcerns. This work shows that adapting pre-trained Stable Diffusion models to\nmedical imaging modalities is achievable by training text embeddings using\nTextual Inversion. In this study, we experimented with small medical datasets\n(100 samples each from three modalities) and trained within hours to generate\ndiagnostically accurate images, as judged by an expert radiologist. Experiments\nwith Textual Inversion training and inference parameters reveal the necessity\nof larger embeddings and more examples in the medical domain. Classification\nexperiments show an increase in diagnostic accuracy (AUC) for detecting\nprostate cancer on MRI, from 0.78 to 0.80. Further experiments demonstrate\nembedding flexibility through disease interpolation, combining pathologies, and\ninpainting for precise disease appearance control. The trained embeddings are\ncompact (less than 1 MB), enabling easy data sharing with reduced privacy\nconcerns."
                },
                "authors": [
                    {
                        "name": "Bram de Wilde"
                    },
                    {
                        "name": "Anindo Saha"
                    },
                    {
                        "name": "Maarten de Rooij"
                    },
                    {
                        "name": "Henkjan Huisman"
                    },
                    {
                        "name": "Geert Litjens"
                    }
                ],
                "author_detail": {
                    "name": "Geert Litjens"
                },
                "author": "Geert Litjens",
                "arxiv_comment": "Accepted for publication at MIDL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2303.13430v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.13430v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2212.04400v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2212.04400v2",
                "updated": "2024-09-11T14:37:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    14,
                    37,
                    18,
                    2,
                    255,
                    0
                ],
                "published": "2022-12-08T16:57:52Z",
                "published_parsed": [
                    2022,
                    12,
                    8,
                    16,
                    57,
                    52,
                    3,
                    342,
                    0
                ],
                "title": "The Lifebelt Particle Filter for robust estimation from low-valued count\n  data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Lifebelt Particle Filter for robust estimation from low-valued count\n  data"
                },
                "summary": "Particle filtering methods can be applied to estimation problems in discrete\nspaces on bounded domains, to sample from and marginalise over unknown hidden\nstates. As in continuous settings, problems such as particle degradation can\narise: proposed particles can be incompatible with the data, lying in low\nprobability regions or outside the boundary constraints, and the discrete\nsystem could result in all particles having weights of zero. In this paper we\nintroduce the Lifebelt Particle Filter (LBPF), a novel method for robust\nlikelihood estimation in low-valued count problems. The LBPF combines a\nstandard particle filter with one (or more) lifebelt particles which, by\nconstruction, lie within the boundaries of the discrete random variables, and\ntherefore are compatible with the data. A mixture of resampled and\nnon-resampled particles allows for the preservation of the lifebelt particle,\nwhich, together with the remaining particle swarm, provides samples from the\nfiltering distribution, and can be used to generate unbiased estimates of the\nlikelihood. The main benefit of the LBPF is that only one or few, wisely\nchosen, particles are sufficient to prevent particle collapse. Differently from\nother methods, there is no need to increase the number of particles, and\ntherefore the computational effort, in regions of the parameter space that\ngenerate less likely hidden states. The LBPF can be used within a\npseudo-marginal scheme to draw inferences on static parameters, $\n\\boldsymbol{\\theta} $, governing the system. We address here the estimation of\na parameter governing probabilities of deaths and recoveries of hospitalised\npatients during an epidemic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Particle filtering methods can be applied to estimation problems in discrete\nspaces on bounded domains, to sample from and marginalise over unknown hidden\nstates. As in continuous settings, problems such as particle degradation can\narise: proposed particles can be incompatible with the data, lying in low\nprobability regions or outside the boundary constraints, and the discrete\nsystem could result in all particles having weights of zero. In this paper we\nintroduce the Lifebelt Particle Filter (LBPF), a novel method for robust\nlikelihood estimation in low-valued count problems. The LBPF combines a\nstandard particle filter with one (or more) lifebelt particles which, by\nconstruction, lie within the boundaries of the discrete random variables, and\ntherefore are compatible with the data. A mixture of resampled and\nnon-resampled particles allows for the preservation of the lifebelt particle,\nwhich, together with the remaining particle swarm, provides samples from the\nfiltering distribution, and can be used to generate unbiased estimates of the\nlikelihood. The main benefit of the LBPF is that only one or few, wisely\nchosen, particles are sufficient to prevent particle collapse. Differently from\nother methods, there is no need to increase the number of particles, and\ntherefore the computational effort, in regions of the parameter space that\ngenerate less likely hidden states. The LBPF can be used within a\npseudo-marginal scheme to draw inferences on static parameters, $\n\\boldsymbol{\\theta} $, governing the system. We address here the estimation of\na parameter governing probabilities of deaths and recoveries of hospitalised\npatients during an epidemic."
                },
                "authors": [
                    {
                        "name": "Alice Corbella"
                    },
                    {
                        "name": "Trevelyan J. McKinley"
                    },
                    {
                        "name": "Paul J. Birrell"
                    },
                    {
                        "name": "Daniela De Angelis"
                    },
                    {
                        "name": "Anne M. Presanis"
                    },
                    {
                        "name": "Gareth O. Roberts"
                    },
                    {
                        "name": "Simon E. F. Spencer"
                    }
                ],
                "author_detail": {
                    "name": "Simon E. F. Spencer"
                },
                "author": "Simon E. F. Spencer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2212.04400v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2212.04400v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.12813v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.12813v2",
                "updated": "2024-09-11T14:27:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    14,
                    27,
                    43,
                    2,
                    255,
                    0
                ],
                "published": "2023-05-22T08:13:46Z",
                "published_parsed": [
                    2023,
                    5,
                    22,
                    8,
                    13,
                    46,
                    0,
                    142,
                    0
                ],
                "title": "Robustly Learning Regions of Attraction from Fixed Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robustly Learning Regions of Attraction from Fixed Data"
                },
                "summary": "While stability analysis is a mainstay for control science, especially\ncomputing regions of attraction of equilibrium points, until recently most\nstability analysis tools always required explicit knowledge of the model or a\nhigh-fidelity simulator representing the system at hand. In this work, a new\ndata-driven Lyapunov analysis framework is proposed. Without using the model or\nits simulator, the proposed approach can learn a piece-wise affine Lyapunov\nfunction with a finite and fixed off-line dataset. The learnt Lyapunov function\nis robust to any dynamics that are consistent with the off-line dataset, and\nits computation is based on second order cone programming. Along with the\ndevelopment of the proposed scheme, a slight generalization of classical\nLyapunov stability criteria is derived, enabling an iterative inference\nalgorithm to augment the region of attraction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While stability analysis is a mainstay for control science, especially\ncomputing regions of attraction of equilibrium points, until recently most\nstability analysis tools always required explicit knowledge of the model or a\nhigh-fidelity simulator representing the system at hand. In this work, a new\ndata-driven Lyapunov analysis framework is proposed. Without using the model or\nits simulator, the proposed approach can learn a piece-wise affine Lyapunov\nfunction with a finite and fixed off-line dataset. The learnt Lyapunov function\nis robust to any dynamics that are consistent with the off-line dataset, and\nits computation is based on second order cone programming. Along with the\ndevelopment of the proposed scheme, a slight generalization of classical\nLyapunov stability criteria is derived, enabling an iterative inference\nalgorithm to augment the region of attraction."
                },
                "authors": [
                    {
                        "name": "Matteo Tacchi"
                    },
                    {
                        "name": "Yingzhao Lian"
                    },
                    {
                        "name": "Colin Jones"
                    }
                ],
                "author_detail": {
                    "name": "Colin Jones"
                },
                "author": "Colin Jones",
                "arxiv_comment": "33 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.12813v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.12813v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18140v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18140v2",
                "updated": "2024-09-11T14:27:41Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    14,
                    27,
                    41,
                    2,
                    255,
                    0
                ],
                "published": "2024-06-26T07:44:27Z",
                "published_parsed": [
                    2024,
                    6,
                    26,
                    7,
                    44,
                    27,
                    2,
                    178,
                    0
                ],
                "title": "Exclusive Style Removal for Cross Domain Novel Class Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exclusive Style Removal for Cross Domain Novel Class Discovery"
                },
                "summary": "As a promising field in open-world learning, \\textit{Novel Class Discovery}\n(NCD) is usually a task to cluster unseen novel classes in an unlabeled set\nbased on the prior knowledge of labeled data within the same domain. However,\nthe performance of existing NCD methods could be severely compromised when\nnovel classes are sampled from a different distribution with the labeled ones.\nIn this paper, we explore and establish the solvability of NCD in cross domain\nsetting with the necessary condition that style information must be removed.\nBased on the theoretical analysis, we introduce an exclusive style removal\nmodule for extracting style information that is distinctive from the baseline\nfeatures, thereby facilitating inference. Moreover, this module is easy to\nintegrate with other NCD methods, acting as a plug-in to improve performance on\nnovel classes with different distributions compared to the seen labeled set.\nAdditionally, recognizing the non-negligible influence of different backbones\nand pre-training strategies on the performance of the NCD methods, we build a\nfair benchmark for future NCD research. Extensive experiments on three common\ndatasets demonstrate the effectiveness of our proposed module.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a promising field in open-world learning, \\textit{Novel Class Discovery}\n(NCD) is usually a task to cluster unseen novel classes in an unlabeled set\nbased on the prior knowledge of labeled data within the same domain. However,\nthe performance of existing NCD methods could be severely compromised when\nnovel classes are sampled from a different distribution with the labeled ones.\nIn this paper, we explore and establish the solvability of NCD in cross domain\nsetting with the necessary condition that style information must be removed.\nBased on the theoretical analysis, we introduce an exclusive style removal\nmodule for extracting style information that is distinctive from the baseline\nfeatures, thereby facilitating inference. Moreover, this module is easy to\nintegrate with other NCD methods, acting as a plug-in to improve performance on\nnovel classes with different distributions compared to the seen labeled set.\nAdditionally, recognizing the non-negligible influence of different backbones\nand pre-training strategies on the performance of the NCD methods, we build a\nfair benchmark for future NCD research. Extensive experiments on three common\ndatasets demonstrate the effectiveness of our proposed module."
                },
                "authors": [
                    {
                        "name": "Yicheng Wang"
                    },
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Junmin Liu"
                    },
                    {
                        "name": "Zhen Fang"
                    },
                    {
                        "name": "Kai Sun"
                    }
                ],
                "author_detail": {
                    "name": "Kai Sun"
                },
                "author": "Kai Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18140v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18140v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07291v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07291v1",
                "updated": "2024-09-11T14:20:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    14,
                    20,
                    47,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T14:20:47Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    14,
                    20,
                    47,
                    2,
                    255,
                    0
                ],
                "title": "Exploring User-level Gradient Inversion with a Diffusion Prior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring User-level Gradient Inversion with a Diffusion Prior"
                },
                "summary": "We explore user-level gradient inversion as a new attack surface in\ndistributed learning. We first investigate existing attacks on their ability to\nmake inferences about private information beyond training data reconstruction.\nMotivated by the low reconstruction quality of existing methods, we propose a\nnovel gradient inversion attack that applies a denoising diffusion model as a\nstrong image prior in order to enhance recovery in the large batch setting.\nUnlike traditional attacks, which aim to reconstruct individual samples and\nsuffer at large batch and image sizes, our approach instead aims to recover a\nrepresentative image that captures the sensitive shared semantic information\ncorresponding to the underlying user. Our experiments with face images\ndemonstrate the ability of our methods to recover realistic facial images along\nwith private user attributes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore user-level gradient inversion as a new attack surface in\ndistributed learning. We first investigate existing attacks on their ability to\nmake inferences about private information beyond training data reconstruction.\nMotivated by the low reconstruction quality of existing methods, we propose a\nnovel gradient inversion attack that applies a denoising diffusion model as a\nstrong image prior in order to enhance recovery in the large batch setting.\nUnlike traditional attacks, which aim to reconstruct individual samples and\nsuffer at large batch and image sizes, our approach instead aims to recover a\nrepresentative image that captures the sensitive shared semantic information\ncorresponding to the underlying user. Our experiments with face images\ndemonstrate the ability of our methods to recover realistic facial images along\nwith private user attributes."
                },
                "authors": [
                    {
                        "name": "Zhuohang Li"
                    },
                    {
                        "name": "Andrew Lowy"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Toshiaki Koike-Akino"
                    },
                    {
                        "name": "Bradley Malin"
                    },
                    {
                        "name": "Kieran Parsons"
                    },
                    {
                        "name": "Ye Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ye Wang"
                },
                "author": "Ye Wang",
                "arxiv_comment": "Presented at the International Workshop on Federated Learning in the\n  Age of Foundation Models in conjunction with NeurIPS 2023",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07291v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07276v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07276v1",
                "updated": "2024-09-11T13:49:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    13,
                    49,
                    48,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T13:49:48Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    13,
                    49,
                    48,
                    2,
                    255,
                    0
                ],
                "title": "STORE: Streamlining Semantic Tokenization and Generative Recommendation\n  with A Single LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STORE: Streamlining Semantic Tokenization and Generative Recommendation\n  with A Single LLM"
                },
                "summary": "Traditional recommendation models often rely on unique item identifiers (IDs)\nto distinguish between items, which can hinder their ability to effectively\nleverage item content information and generalize to long-tail or cold-start\nitems. Recently, semantic tokenization has been proposed as a promising\nsolution that aims to tokenize each item's semantic representation into a\nsequence of discrete tokens. In this way, it preserves the item's semantics\nwithin these tokens and ensures that semantically similar items are represented\nby similar tokens. These semantic tokens have become fundamental in training\ngenerative recommendation models. However, existing generative recommendation\nmethods typically involve multiple sub-models for embedding, quantization, and\nrecommendation, leading to an overly complex system. In this paper, we propose\nto streamline the semantic tokenization and generative recommendation process\nwith a unified framework, dubbed STORE, which leverages a single large language\nmodel (LLM) for both tasks. Specifically, we formulate semantic tokenization as\na text-to-token task and generative recommendation as a token-to-token task,\nsupplemented by a token-to-text reconstruction task and a text-to-token\nauxiliary task. All these tasks are framed in a generative manner and trained\nusing a single LLM backbone. Extensive experiments have been conducted to\nvalidate the effectiveness of our STORE framework across various recommendation\ntasks and datasets. We will release the source code and configurations for\nreproducible research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional recommendation models often rely on unique item identifiers (IDs)\nto distinguish between items, which can hinder their ability to effectively\nleverage item content information and generalize to long-tail or cold-start\nitems. Recently, semantic tokenization has been proposed as a promising\nsolution that aims to tokenize each item's semantic representation into a\nsequence of discrete tokens. In this way, it preserves the item's semantics\nwithin these tokens and ensures that semantically similar items are represented\nby similar tokens. These semantic tokens have become fundamental in training\ngenerative recommendation models. However, existing generative recommendation\nmethods typically involve multiple sub-models for embedding, quantization, and\nrecommendation, leading to an overly complex system. In this paper, we propose\nto streamline the semantic tokenization and generative recommendation process\nwith a unified framework, dubbed STORE, which leverages a single large language\nmodel (LLM) for both tasks. Specifically, we formulate semantic tokenization as\na text-to-token task and generative recommendation as a token-to-token task,\nsupplemented by a token-to-text reconstruction task and a text-to-token\nauxiliary task. All these tasks are framed in a generative manner and trained\nusing a single LLM backbone. Extensive experiments have been conducted to\nvalidate the effectiveness of our STORE framework across various recommendation\ntasks and datasets. We will release the source code and configurations for\nreproducible research."
                },
                "authors": [
                    {
                        "name": "Qijiong Liu"
                    },
                    {
                        "name": "Jieming Zhu"
                    },
                    {
                        "name": "Lu Fan"
                    },
                    {
                        "name": "Zhou Zhao"
                    },
                    {
                        "name": "Xiao-Ming Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xiao-Ming Wu"
                },
                "author": "Xiao-Ming Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07276v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07269v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07269v1",
                "updated": "2024-09-11T13:43:53Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    13,
                    43,
                    53,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T13:43:53Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    13,
                    43,
                    53,
                    2,
                    255,
                    0
                ],
                "title": "Realistic and Efficient Face Swapping: A Unified Approach with Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Realistic and Efficient Face Swapping: A Unified Approach with Diffusion\n  Models"
                },
                "summary": "Despite promising progress in face swapping task, realistic swapped images\nremain elusive, often marred by artifacts, particularly in scenarios involving\nhigh pose variation, color differences, and occlusion. To address these issues,\nwe propose a novel approach that better harnesses diffusion models for\nface-swapping by making following core contributions. (a) We propose to\nre-frame the face-swapping task as a self-supervised, train-time inpainting\nproblem, enhancing the identity transfer while blending with the target image.\n(b) We introduce a multi-step Denoising Diffusion Implicit Model (DDIM)\nsampling during training, reinforcing identity and perceptual similarities. (c)\nThird, we introduce CLIP feature disentanglement to extract pose, expression,\nand lighting information from the target image, improving fidelity. (d)\nFurther, we introduce a mask shuffling technique during inpainting training,\nwhich allows us to create a so-called universal model for swapping, with an\nadditional feature of head swapping. Ours can swap hair and even accessories,\nbeyond traditional face swapping. Unlike prior works reliant on multiple\noff-the-shelf models, ours is a relatively unified approach and so it is\nresilient to errors in other off-the-shelf models. Extensive experiments on\nFFHQ and CelebA datasets validate the efficacy and robustness of our approach,\nshowcasing high-fidelity, realistic face-swapping with minimal inference time.\nOur code is available at https://github.com/Sanoojan/REFace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite promising progress in face swapping task, realistic swapped images\nremain elusive, often marred by artifacts, particularly in scenarios involving\nhigh pose variation, color differences, and occlusion. To address these issues,\nwe propose a novel approach that better harnesses diffusion models for\nface-swapping by making following core contributions. (a) We propose to\nre-frame the face-swapping task as a self-supervised, train-time inpainting\nproblem, enhancing the identity transfer while blending with the target image.\n(b) We introduce a multi-step Denoising Diffusion Implicit Model (DDIM)\nsampling during training, reinforcing identity and perceptual similarities. (c)\nThird, we introduce CLIP feature disentanglement to extract pose, expression,\nand lighting information from the target image, improving fidelity. (d)\nFurther, we introduce a mask shuffling technique during inpainting training,\nwhich allows us to create a so-called universal model for swapping, with an\nadditional feature of head swapping. Ours can swap hair and even accessories,\nbeyond traditional face swapping. Unlike prior works reliant on multiple\noff-the-shelf models, ours is a relatively unified approach and so it is\nresilient to errors in other off-the-shelf models. Extensive experiments on\nFFHQ and CelebA datasets validate the efficacy and robustness of our approach,\nshowcasing high-fidelity, realistic face-swapping with minimal inference time.\nOur code is available at https://github.com/Sanoojan/REFace."
                },
                "authors": [
                    {
                        "name": "Sanoojan Baliah"
                    },
                    {
                        "name": "Qinliang Lin"
                    },
                    {
                        "name": "Shengcai Liao"
                    },
                    {
                        "name": "Xiaodan Liang"
                    },
                    {
                        "name": "Muhammad Haris Khan"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Haris Khan"
                },
                "author": "Muhammad Haris Khan",
                "arxiv_comment": "Accepted as a conference paper at WACV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07269v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07267v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07267v1",
                "updated": "2024-09-11T13:43:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    13,
                    43,
                    1,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T13:43:01Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    13,
                    43,
                    1,
                    2,
                    255,
                    0
                ],
                "title": "MiniDrive: More Efficient Vision-Language Models with Multi-Level 2D\n  Features as Text Tokens for Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiniDrive: More Efficient Vision-Language Models with Multi-Level 2D\n  Features as Text Tokens for Autonomous Driving"
                },
                "summary": "Vision-language models (VLMs) serve as general-purpose end-to-end models in\nautonomous driving, performing subtasks such as prediction, planning, and\nperception through question-and-answer interactions. However, most existing\nmethods rely on computationally expensive visual encoders and large language\nmodels (LLMs), making them difficult to deploy in real-world scenarios and\nreal-time applications. Meanwhile, most existing VLMs lack the ability to\nprocess multiple images, making it difficult to adapt to multi-camera\nperception in autonomous driving. To address these issues, we propose a novel\nframework called MiniDrive, which incorporates our proposed Feature Engineering\nMixture of Experts (FE-MoE) module and Dynamic Instruction Adapter\n(DI-Adapter). The FE-MoE effectively maps 2D features into visual token\nembeddings before being input into the language model. The DI-Adapter enables\nthe visual token embeddings to dynamically change with the instruction text\nembeddings, resolving the issue of static visual token embeddings for the same\nimage in previous approaches. Compared to previous works, MiniDrive achieves\nstate-of-the-art performance in terms of parameter size, floating point\noperations, and response efficiency, with the smallest version containing only\n83M parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) serve as general-purpose end-to-end models in\nautonomous driving, performing subtasks such as prediction, planning, and\nperception through question-and-answer interactions. However, most existing\nmethods rely on computationally expensive visual encoders and large language\nmodels (LLMs), making them difficult to deploy in real-world scenarios and\nreal-time applications. Meanwhile, most existing VLMs lack the ability to\nprocess multiple images, making it difficult to adapt to multi-camera\nperception in autonomous driving. To address these issues, we propose a novel\nframework called MiniDrive, which incorporates our proposed Feature Engineering\nMixture of Experts (FE-MoE) module and Dynamic Instruction Adapter\n(DI-Adapter). The FE-MoE effectively maps 2D features into visual token\nembeddings before being input into the language model. The DI-Adapter enables\nthe visual token embeddings to dynamically change with the instruction text\nembeddings, resolving the issue of static visual token embeddings for the same\nimage in previous approaches. Compared to previous works, MiniDrive achieves\nstate-of-the-art performance in terms of parameter size, floating point\noperations, and response efficiency, with the smallest version containing only\n83M parameters."
                },
                "authors": [
                    {
                        "name": "Enming Zhang"
                    },
                    {
                        "name": "Xingyuan Dai"
                    },
                    {
                        "name": "Yisheng Lv"
                    },
                    {
                        "name": "Qianghai Miao"
                    }
                ],
                "author_detail": {
                    "name": "Qianghai Miao"
                },
                "author": "Qianghai Miao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07267v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07267v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07263v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07263v1",
                "updated": "2024-09-11T13:35:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    13,
                    35,
                    9,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T13:35:09Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    13,
                    35,
                    9,
                    2,
                    255,
                    0
                ],
                "title": "Order selection in GARMA models for count time series: a Bayesian\n  perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Order selection in GARMA models for count time series: a Bayesian\n  perspective"
                },
                "summary": "Estimation in GARMA models has traditionally been carried out under the\nfrequentist approach. To date, Bayesian approaches for such estimation have\nbeen relatively limited. In the context of GARMA models for count time series,\nBayesian estimation achieves satisfactory results in terms of point estimation.\nModel selection in this context often relies on the use of information\ncriteria. Despite its prominence in the literature, the use of information\ncriteria for model selection in GARMA models for count time series have been\nshown to present poor performance in simulations, especially in terms of their\nability to correctly identify models, even under large sample sizes. In this\nstudy, we study the problem of order selection in GARMA models for count time\nseries, adopting a Bayesian perspective through the application of the\nReversible Jump Markov Chain Monte Carlo approach. Monte Carlo simulation\nstudies are conducted to assess the finite sample performance of the developed\nideas, including point and interval inference, sensitivity analysis, effects of\nburn-in and thinning, as well as the choice of related priors and\nhyperparameters. Two real-data applications are presented, one considering\nautomobile production in Brazil and the other considering bus exportation in\nBrazil before and after the COVID-19 pandemic, showcasing the method's\ncapabilities and further exploring its flexibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimation in GARMA models has traditionally been carried out under the\nfrequentist approach. To date, Bayesian approaches for such estimation have\nbeen relatively limited. In the context of GARMA models for count time series,\nBayesian estimation achieves satisfactory results in terms of point estimation.\nModel selection in this context often relies on the use of information\ncriteria. Despite its prominence in the literature, the use of information\ncriteria for model selection in GARMA models for count time series have been\nshown to present poor performance in simulations, especially in terms of their\nability to correctly identify models, even under large sample sizes. In this\nstudy, we study the problem of order selection in GARMA models for count time\nseries, adopting a Bayesian perspective through the application of the\nReversible Jump Markov Chain Monte Carlo approach. Monte Carlo simulation\nstudies are conducted to assess the finite sample performance of the developed\nideas, including point and interval inference, sensitivity analysis, effects of\nburn-in and thinning, as well as the choice of related priors and\nhyperparameters. Two real-data applications are presented, one considering\nautomobile production in Brazil and the other considering bus exportation in\nBrazil before and after the COVID-19 pandemic, showcasing the method's\ncapabilities and further exploring its flexibility."
                },
                "authors": [
                    {
                        "name": "Katerine Zuniga Lastra"
                    },
                    {
                        "name": "Guilherme Pumi"
                    },
                    {
                        "name": "Taiane Schaedler Prass"
                    }
                ],
                "author_detail": {
                    "name": "Taiane Schaedler Prass"
                },
                "author": "Taiane Schaedler Prass",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07263v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07263v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62M10, 62F15, 62J02, 62F10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20441v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20441v2",
                "updated": "2024-09-11T13:11:16Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    13,
                    11,
                    16,
                    2,
                    255,
                    0
                ],
                "published": "2024-05-30T19:35:06Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    19,
                    35,
                    6,
                    3,
                    151,
                    0
                ],
                "title": "SECURE: Benchmarking Large Language Models for Cybersecurity Advisory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SECURE: Benchmarking Large Language Models for Cybersecurity Advisory"
                },
                "summary": "Large Language Models (LLMs) have demonstrated potential in cybersecurity\napplications but have also caused lower confidence due to problems like\nhallucinations and a lack of truthfulness. Existing benchmarks provide general\nevaluations but do not sufficiently address the practical and applied aspects\nof LLM performance in cybersecurity-specific tasks. To address this gap, we\nintroduce the SECURE (Security Extraction, Understanding \\& Reasoning\nEvaluation), a benchmark designed to assess LLMs performance in realistic\ncybersecurity scenarios. SECURE includes six datasets focussed on the\nIndustrial Control System sector to evaluate knowledge extraction,\nunderstanding, and reasoning based on industry-standard sources. Our study\nevaluates seven state-of-the-art models on these tasks, providing insights into\ntheir strengths and weaknesses in cybersecurity contexts, and offer\nrecommendations for improving LLMs reliability as cyber advisory tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated potential in cybersecurity\napplications but have also caused lower confidence due to problems like\nhallucinations and a lack of truthfulness. Existing benchmarks provide general\nevaluations but do not sufficiently address the practical and applied aspects\nof LLM performance in cybersecurity-specific tasks. To address this gap, we\nintroduce the SECURE (Security Extraction, Understanding \\& Reasoning\nEvaluation), a benchmark designed to assess LLMs performance in realistic\ncybersecurity scenarios. SECURE includes six datasets focussed on the\nIndustrial Control System sector to evaluate knowledge extraction,\nunderstanding, and reasoning based on industry-standard sources. Our study\nevaluates seven state-of-the-art models on these tasks, providing insights into\ntheir strengths and weaknesses in cybersecurity contexts, and offer\nrecommendations for improving LLMs reliability as cyber advisory tools."
                },
                "authors": [
                    {
                        "name": "Dipkamal Bhusal"
                    },
                    {
                        "name": "Md Tanvirul Alam"
                    },
                    {
                        "name": "Le Nguyen"
                    },
                    {
                        "name": "Ashim Mahara"
                    },
                    {
                        "name": "Zachary Lightcap"
                    },
                    {
                        "name": "Rodney Frazier"
                    },
                    {
                        "name": "Romy Fieblinger"
                    },
                    {
                        "name": "Grace Long Torales"
                    },
                    {
                        "name": "Nidhi Rastogi"
                    }
                ],
                "author_detail": {
                    "name": "Nidhi Rastogi"
                },
                "author": "Nidhi Rastogi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20441v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20441v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07246v1",
                "updated": "2024-09-11T13:04:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    13,
                    4,
                    34,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T13:04:34Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    13,
                    4,
                    34,
                    2,
                    255,
                    0
                ],
                "title": "Propaganda to Hate: A Multimodal Analysis of Arabic Memes with\n  Multi-Agent LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Propaganda to Hate: A Multimodal Analysis of Arabic Memes with\n  Multi-Agent LLMs"
                },
                "summary": "In the past decade, social media platforms have been used for information\ndissemination and consumption. While a major portion of the content is posted\nto promote citizen journalism and public awareness, some content is posted to\nmislead users. Among different content types such as text, images, and videos,\nmemes (text overlaid on images) are particularly prevalent and can serve as\npowerful vehicles for propaganda, hate, and humor. In the current literature,\nthere have been efforts to individually detect such content in memes. However,\nthe study of their intersection is very limited. In this study, we explore the\nintersection between propaganda and hate in memes using a multi-agent LLM-based\napproach. We extend the propagandistic meme dataset with coarse and\nfine-grained hate labels. Our finding suggests that there is an association\nbetween propaganda and hate in memes. We provide detailed experimental results\nthat can serve as a baseline for future studies. We will make the experimental\nresources publicly available to the community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the past decade, social media platforms have been used for information\ndissemination and consumption. While a major portion of the content is posted\nto promote citizen journalism and public awareness, some content is posted to\nmislead users. Among different content types such as text, images, and videos,\nmemes (text overlaid on images) are particularly prevalent and can serve as\npowerful vehicles for propaganda, hate, and humor. In the current literature,\nthere have been efforts to individually detect such content in memes. However,\nthe study of their intersection is very limited. In this study, we explore the\nintersection between propaganda and hate in memes using a multi-agent LLM-based\napproach. We extend the propagandistic meme dataset with coarse and\nfine-grained hate labels. Our finding suggests that there is an association\nbetween propaganda and hate in memes. We provide detailed experimental results\nthat can serve as a baseline for future studies. We will make the experimental\nresources publicly available to the community."
                },
                "authors": [
                    {
                        "name": "Firoj Alam"
                    },
                    {
                        "name": "Md. Rafiul Biswas"
                    },
                    {
                        "name": "Uzair Shah"
                    },
                    {
                        "name": "Wajdi Zaghouani"
                    },
                    {
                        "name": "Georgios Mikros"
                    }
                ],
                "author_detail": {
                    "name": "Georgios Mikros"
                },
                "author": "Georgios Mikros",
                "arxiv_comment": "propaganda, hate-speech, disinformation, misinformation, fake news,\n  LLMs, GPT-4, multimodality, multimodal LLMs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07239v1",
                "updated": "2024-09-11T12:53:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    12,
                    53,
                    7,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T12:53:07Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    12,
                    53,
                    7,
                    2,
                    255,
                    0
                ],
                "title": "PiTe: Pixel-Temporal Alignment for Large Video-Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PiTe: Pixel-Temporal Alignment for Large Video-Language Model"
                },
                "summary": "Fueled by the Large Language Models (LLMs) wave, Large Visual-Language Models\n(LVLMs) have emerged as a pivotal advancement, bridging the gap between image\nand text. However, video making it challenging for LVLMs to perform adequately\ndue to the complexity of the relationship between language and spatial-temporal\ndata structure. Recent Large Video-Language Models (LVidLMs) align feature of\nstatic visual data like image into latent space of language feature, by general\nmulti-modal tasks to leverage abilities of LLMs sufficiently. In this paper, we\nexplore fine-grained alignment approach via object trajectory for different\nmodalities across both spatial and temporal dimensions simultaneously. Thus, we\npropose a novel LVidLM by trajectory-guided Pixel-Temporal Alignment, dubbed\nPiTe, that exhibits promising applicable model property. To achieve\nfine-grained video-language alignment, we curate a multi-modal pre-training\ndataset PiTe-143k, the dataset provision of moving trajectories in pixel level\nfor all individual objects, that appear and mention in the video and caption\nboth, by our automatic annotation pipeline. Meanwhile, PiTe demonstrates\nastounding capabilities on myriad video-related multi-modal tasks through beat\nthe state-of-the-art methods by a large margin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fueled by the Large Language Models (LLMs) wave, Large Visual-Language Models\n(LVLMs) have emerged as a pivotal advancement, bridging the gap between image\nand text. However, video making it challenging for LVLMs to perform adequately\ndue to the complexity of the relationship between language and spatial-temporal\ndata structure. Recent Large Video-Language Models (LVidLMs) align feature of\nstatic visual data like image into latent space of language feature, by general\nmulti-modal tasks to leverage abilities of LLMs sufficiently. In this paper, we\nexplore fine-grained alignment approach via object trajectory for different\nmodalities across both spatial and temporal dimensions simultaneously. Thus, we\npropose a novel LVidLM by trajectory-guided Pixel-Temporal Alignment, dubbed\nPiTe, that exhibits promising applicable model property. To achieve\nfine-grained video-language alignment, we curate a multi-modal pre-training\ndataset PiTe-143k, the dataset provision of moving trajectories in pixel level\nfor all individual objects, that appear and mention in the video and caption\nboth, by our automatic annotation pipeline. Meanwhile, PiTe demonstrates\nastounding capabilities on myriad video-related multi-modal tasks through beat\nthe state-of-the-art methods by a large margin."
                },
                "authors": [
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Pengxiang Ding"
                    },
                    {
                        "name": "Siteng Huang"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Han Zhao"
                    },
                    {
                        "name": "Donglin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Donglin Wang"
                },
                "author": "Donglin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.00374v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.00374v3",
                "updated": "2024-09-11T12:48:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    12,
                    48,
                    42,
                    2,
                    255,
                    0
                ],
                "published": "2023-12-01T06:36:17Z",
                "published_parsed": [
                    2023,
                    12,
                    1,
                    6,
                    36,
                    17,
                    4,
                    335,
                    0
                ],
                "title": "The Philosopher's Stone: Trojaning Plugins of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Philosopher's Stone: Trojaning Plugins of Large Language Models"
                },
                "summary": "Open-source Large Language Models (LLMs) have recently gained popularity\nbecause of their comparable performance to proprietary LLMs. To efficiently\nfulfill domain-specialized tasks, open-source LLMs can be refined, without\nexpensive accelerators, using low-rank adapters. However, it is still unknown\nwhether low-rank adapters can be exploited to control LLMs. To address this\ngap, we demonstrate that an infected adapter can induce, on specific\ntriggers,an LLM to output content defined by an adversary and to even\nmaliciously use tools. To train a Trojan adapter, we propose two novel attacks,\nPOLISHED and FUSION, that improve over prior approaches. POLISHED uses a\nsuperior LLM to align na\\\"ively poisoned data based on our insight that it can\nbetter inject poisoning knowledge during training. In contrast, FUSION\nleverages a novel over-poisoning procedure to transform a benign adapter into a\nmalicious one by magnifying the attention between trigger and target in model\nweights. In our experiments, we first conduct two case studies to demonstrate\nthat a compromised LLM agent can use malware to control the system (e.g., a\nLLM-driven robot) or to launch a spear-phishing attack. Then, in terms of\ntargeted misinformation, we show that our attacks provide higher attack\neffectiveness than the existing baseline and, for the purpose of attracting\ndownloads, preserve or improve the adapter's utility. Finally, we designed and\nevaluated three potential defenses. However, none proved entirely effective in\nsafeguarding against our attacks, highlighting the need for more robust\ndefenses supporting a secure LLM supply chain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-source Large Language Models (LLMs) have recently gained popularity\nbecause of their comparable performance to proprietary LLMs. To efficiently\nfulfill domain-specialized tasks, open-source LLMs can be refined, without\nexpensive accelerators, using low-rank adapters. However, it is still unknown\nwhether low-rank adapters can be exploited to control LLMs. To address this\ngap, we demonstrate that an infected adapter can induce, on specific\ntriggers,an LLM to output content defined by an adversary and to even\nmaliciously use tools. To train a Trojan adapter, we propose two novel attacks,\nPOLISHED and FUSION, that improve over prior approaches. POLISHED uses a\nsuperior LLM to align na\\\"ively poisoned data based on our insight that it can\nbetter inject poisoning knowledge during training. In contrast, FUSION\nleverages a novel over-poisoning procedure to transform a benign adapter into a\nmalicious one by magnifying the attention between trigger and target in model\nweights. In our experiments, we first conduct two case studies to demonstrate\nthat a compromised LLM agent can use malware to control the system (e.g., a\nLLM-driven robot) or to launch a spear-phishing attack. Then, in terms of\ntargeted misinformation, we show that our attacks provide higher attack\neffectiveness than the existing baseline and, for the purpose of attracting\ndownloads, preserve or improve the adapter's utility. Finally, we designed and\nevaluated three potential defenses. However, none proved entirely effective in\nsafeguarding against our attacks, highlighting the need for more robust\ndefenses supporting a secure LLM supply chain."
                },
                "authors": [
                    {
                        "name": "Tian Dong"
                    },
                    {
                        "name": "Minhui Xue"
                    },
                    {
                        "name": "Guoxing Chen"
                    },
                    {
                        "name": "Rayne Holland"
                    },
                    {
                        "name": "Yan Meng"
                    },
                    {
                        "name": "Shaofeng Li"
                    },
                    {
                        "name": "Zhen Liu"
                    },
                    {
                        "name": "Haojin Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Haojin Zhu"
                },
                "author": "Haojin Zhu",
                "arxiv_comment": "Accepted by NDSS Symposium 2025. Please cite this paper as \"Tian\n  Dong, Minhui Xue, Guoxing Chen, Rayne Holland, Yan Meng, Shaofeng Li, Zhen\n  Liu, Haojin Zhu. The Philosopher's Stone: Trojaning Plugins of Large Language\n  Models. In the 32nd Annual Network and Distributed System Security Symposium\n  (NDSS 2025).\"",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.00374v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.00374v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07233v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07233v1",
                "updated": "2024-09-11T12:45:53Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    12,
                    45,
                    53,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T12:45:53Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    12,
                    45,
                    53,
                    2,
                    255,
                    0
                ],
                "title": "Extended-support beta regression for $[0, 1]$ responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extended-support beta regression for $[0, 1]$ responses"
                },
                "summary": "We introduce the XBX regression model, a continuous mixture of\nextended-support beta regressions for modeling bounded responses with or\nwithout boundary observations. The core building block of the new model is the\nextended-support beta distribution, which is a censored version of a\nfour-parameter beta distribution with the same exceedance on the left and right\nof $(0, 1)$. Hence, XBX regression is a direct extension of beta regression. We\nprove that both beta regression with dispersion effects and heteroscedastic\nnormal regression with censoring at both $0$ and $1$ -- known as the\nheteroscedastic two-limit tobit model in the econometrics literature -- are\nspecial cases of the extended-support beta regression model, depending on\nwhether a single extra parameter is zero or infinity, respectively. To overcome\nidentifiability issues that may arise in estimating the extra parameter due to\nthe similarity of the beta and normal distribution for certain parameter\nsettings, we assume that the additional parameter has an exponential\ndistribution with an unknown mean. The associated marginal likelihood can be\nconveniently and accurately approximated using a Gauss-Laguerre quadrature\nrule, resulting in efficient estimation and inference procedures. The new model\nis used to analyze investment decisions in a behavioral economics experiment,\nwhere the occurrence and extent of loss aversion is of interest. In contrast to\nstandard approaches, XBX regression can simultaneously capture the probability\nof rational behavior as well as the mean amount of loss aversion. Moreover, the\neffectiveness of the new model is illustrated through extensive numerical\ncomparisons with alternative models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the XBX regression model, a continuous mixture of\nextended-support beta regressions for modeling bounded responses with or\nwithout boundary observations. The core building block of the new model is the\nextended-support beta distribution, which is a censored version of a\nfour-parameter beta distribution with the same exceedance on the left and right\nof $(0, 1)$. Hence, XBX regression is a direct extension of beta regression. We\nprove that both beta regression with dispersion effects and heteroscedastic\nnormal regression with censoring at both $0$ and $1$ -- known as the\nheteroscedastic two-limit tobit model in the econometrics literature -- are\nspecial cases of the extended-support beta regression model, depending on\nwhether a single extra parameter is zero or infinity, respectively. To overcome\nidentifiability issues that may arise in estimating the extra parameter due to\nthe similarity of the beta and normal distribution for certain parameter\nsettings, we assume that the additional parameter has an exponential\ndistribution with an unknown mean. The associated marginal likelihood can be\nconveniently and accurately approximated using a Gauss-Laguerre quadrature\nrule, resulting in efficient estimation and inference procedures. The new model\nis used to analyze investment decisions in a behavioral economics experiment,\nwhere the occurrence and extent of loss aversion is of interest. In contrast to\nstandard approaches, XBX regression can simultaneously capture the probability\nof rational behavior as well as the mean amount of loss aversion. Moreover, the\neffectiveness of the new model is illustrated through extensive numerical\ncomparisons with alternative models."
                },
                "authors": [
                    {
                        "name": "Ioannis Kosmidis"
                    },
                    {
                        "name": "Achim Zeileis"
                    }
                ],
                "author_detail": {
                    "name": "Achim Zeileis"
                },
                "author": "Achim Zeileis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07233v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07233v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62J02, 62P20, 62F10, 62F03, 62E99",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13555v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13555v2",
                "updated": "2024-09-11T12:19:14Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    12,
                    19,
                    14,
                    2,
                    255,
                    0
                ],
                "published": "2024-06-19T13:44:56Z",
                "published_parsed": [
                    2024,
                    6,
                    19,
                    13,
                    44,
                    56,
                    2,
                    171,
                    0
                ],
                "title": "BiLD: Bi-directional Logits Difference Loss for Large Language Model\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BiLD: Bi-directional Logits Difference Loss for Large Language Model\n  Distillation"
                },
                "summary": "In recent years, large language models (LLMs) have shown exceptional\ncapabilities across various natural language processing (NLP) tasks. However,\nsuch impressive performance often comes with the trade-off of an increased\nparameter size, posing significant challenges for widespread deployment.\nKnowledge distillation (KD) provides a solution by transferring knowledge from\na large teacher model to a smaller student model. In this paper, we explore the\ntask-specific distillation of LLMs at the logit level. Our investigation\nreveals that the logits of fine-tuned LLMs exhibit a more extreme long-tail\ndistribution than those from vision models, with hidden \"noise\" in the long\ntail affecting distillation performance. Furthermore, existing logits\ndistillation methods often struggle to effectively utilize the internal ranking\ninformation from the logits. To address these, we propose the Bi-directional\nLogits Difference (BiLD) loss. The BiLD loss filters out the long-tail noise by\nutilizing only top-$k$ teacher and student logits, and leverages the internal\nlogits ranking information by constructing logits differences. To evaluate BiLD\nloss, we conduct comprehensive experiments on 13 datasets using two types of\nLLMs. Our results show that the BiLD loss, with only the top-8 logits,\noutperforms supervised fine-tuning (SFT), vanilla KL loss, and five other\ndistillation methods from both NLP and CV fields.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, large language models (LLMs) have shown exceptional\ncapabilities across various natural language processing (NLP) tasks. However,\nsuch impressive performance often comes with the trade-off of an increased\nparameter size, posing significant challenges for widespread deployment.\nKnowledge distillation (KD) provides a solution by transferring knowledge from\na large teacher model to a smaller student model. In this paper, we explore the\ntask-specific distillation of LLMs at the logit level. Our investigation\nreveals that the logits of fine-tuned LLMs exhibit a more extreme long-tail\ndistribution than those from vision models, with hidden \"noise\" in the long\ntail affecting distillation performance. Furthermore, existing logits\ndistillation methods often struggle to effectively utilize the internal ranking\ninformation from the logits. To address these, we propose the Bi-directional\nLogits Difference (BiLD) loss. The BiLD loss filters out the long-tail noise by\nutilizing only top-$k$ teacher and student logits, and leverages the internal\nlogits ranking information by constructing logits differences. To evaluate BiLD\nloss, we conduct comprehensive experiments on 13 datasets using two types of\nLLMs. Our results show that the BiLD loss, with only the top-8 logits,\noutperforms supervised fine-tuning (SFT), vanilla KL loss, and five other\ndistillation methods from both NLP and CV fields."
                },
                "authors": [
                    {
                        "name": "Minchong Li"
                    },
                    {
                        "name": "Feng Zhou"
                    },
                    {
                        "name": "Xiaohui Song"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohui Song"
                },
                "author": "Xiaohui Song",
                "arxiv_comment": "Submitted to COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13555v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13555v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11522v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11522v2",
                "updated": "2024-09-11T12:00:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    12,
                    0,
                    30,
                    2,
                    255,
                    0
                ],
                "published": "2024-06-17T13:23:52Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    13,
                    23,
                    52,
                    0,
                    169,
                    0
                ],
                "title": "FullCert: Deterministic End-to-End Certification for Training and\n  Inference of Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FullCert: Deterministic End-to-End Certification for Training and\n  Inference of Neural Networks"
                },
                "summary": "Modern machine learning models are sensitive to the manipulation of both the\ntraining data (poisoning attacks) and inference data (adversarial examples).\nRecognizing this issue, the community has developed many empirical defenses\nagainst both attacks and, more recently, certification methods with provable\nguarantees against inference-time attacks. However, such guarantees are still\nlargely lacking for training-time attacks. In this work, we present FullCert,\nthe first end-to-end certifier with sound, deterministic bounds, which proves\nrobustness against both training-time and inference-time attacks. We first\nbound all possible perturbations an adversary can make to the training data\nunder the considered threat model. Using these constraints, we bound the\nperturbations' influence on the model's parameters. Finally, we bound the\nimpact of these parameter changes on the model's prediction, resulting in joint\nrobustness guarantees against poisoning and adversarial examples. To facilitate\nthis novel certification paradigm, we combine our theoretical work with a new\nopen-source library BoundFlow, which enables model training on bounded\ndatasets. We experimentally demonstrate FullCert's feasibility on two datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern machine learning models are sensitive to the manipulation of both the\ntraining data (poisoning attacks) and inference data (adversarial examples).\nRecognizing this issue, the community has developed many empirical defenses\nagainst both attacks and, more recently, certification methods with provable\nguarantees against inference-time attacks. However, such guarantees are still\nlargely lacking for training-time attacks. In this work, we present FullCert,\nthe first end-to-end certifier with sound, deterministic bounds, which proves\nrobustness against both training-time and inference-time attacks. We first\nbound all possible perturbations an adversary can make to the training data\nunder the considered threat model. Using these constraints, we bound the\nperturbations' influence on the model's parameters. Finally, we bound the\nimpact of these parameter changes on the model's prediction, resulting in joint\nrobustness guarantees against poisoning and adversarial examples. To facilitate\nthis novel certification paradigm, we combine our theoretical work with a new\nopen-source library BoundFlow, which enables model training on bounded\ndatasets. We experimentally demonstrate FullCert's feasibility on two datasets."
                },
                "authors": [
                    {
                        "name": "Tobias Lorenz"
                    },
                    {
                        "name": "Marta Kwiatkowska"
                    },
                    {
                        "name": "Mario Fritz"
                    }
                ],
                "author_detail": {
                    "name": "Mario Fritz"
                },
                "author": "Mario Fritz",
                "arxiv_comment": "This preprint has not undergone peer review or any post-submission\n  improvements or corrections. The Version of Record of this contribution is\n  published in DAGM GCPR 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11522v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11522v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02616v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02616v5",
                "updated": "2024-09-11T11:59:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    11,
                    59,
                    25,
                    2,
                    255,
                    0
                ],
                "published": "2024-06-03T09:41:42Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    9,
                    41,
                    42,
                    0,
                    155,
                    0
                ],
                "title": "Adaptive Layer Splitting for Wireless LLM Inference in Edge Computing: A\n  Model-Based Reinforcement Learning Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Layer Splitting for Wireless LLM Inference in Edge Computing: A\n  Model-Based Reinforcement Learning Approach"
                },
                "summary": "Optimizing the deployment of large language models (LLMs) in edge computing\nenvironments is critical for enhancing privacy and computational efficiency.\nToward efficient wireless LLM inference in edge computing, this study\ncomprehensively analyzes the impact of different splitting points in mainstream\nopen-source LLMs. On this basis, this study introduces a framework taking\ninspiration from model-based reinforcement learning (MBRL) to determine the\noptimal splitting point across the edge and user equipment (UE). By\nincorporating a reward surrogate model, our approach significantly reduces the\ncomputational cost of frequent performance evaluations. Extensive simulations\ndemonstrate that this method effectively balances inference performance and\ncomputational load under varying network conditions, providing a robust\nsolution for LLM deployment in decentralized settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing the deployment of large language models (LLMs) in edge computing\nenvironments is critical for enhancing privacy and computational efficiency.\nToward efficient wireless LLM inference in edge computing, this study\ncomprehensively analyzes the impact of different splitting points in mainstream\nopen-source LLMs. On this basis, this study introduces a framework taking\ninspiration from model-based reinforcement learning (MBRL) to determine the\noptimal splitting point across the edge and user equipment (UE). By\nincorporating a reward surrogate model, our approach significantly reduces the\ncomputational cost of frequent performance evaluations. Extensive simulations\ndemonstrate that this method effectively balances inference performance and\ncomputational load under varying network conditions, providing a robust\nsolution for LLM deployment in decentralized settings."
                },
                "authors": [
                    {
                        "name": "Yuxuan Chen"
                    },
                    {
                        "name": "Rongpeng Li"
                    },
                    {
                        "name": "Xiaoxue Yu"
                    },
                    {
                        "name": "Zhifeng Zhao"
                    },
                    {
                        "name": "Honggang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Honggang Zhang"
                },
                "author": "Honggang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02616v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02616v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07204v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07204v1",
                "updated": "2024-09-11T11:50:16Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    11,
                    50,
                    16,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T11:50:16Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    11,
                    50,
                    16,
                    2,
                    255,
                    0
                ],
                "title": "Online Graph Filtering Over Expanding Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Graph Filtering Over Expanding Graphs"
                },
                "summary": "Graph filters are a staple tool for processing signals over graphs in a\nmultitude of downstream tasks. However, they are commonly designed for graphs\nwith a fixed number of nodes, despite real-world networks typically grow over\ntime. This topological evolution is often known up to a stochastic model, thus,\nmaking conventional graph filters ill-equipped to withstand such topological\nchanges, their uncertainty, as well as the dynamic nature of the incoming data.\nTo tackle these issues, we propose an online graph filtering framework by\nrelying on online learning principles. We design filters for scenarios where\nthe topology is both known and unknown, including a learner adaptive to such\nevolution. We conduct a regret analysis to highlight the role played by the\ndifferent components such as the online algorithm, the filter order, and the\ngrowing graph model. Numerical experiments with synthetic and real data\ncorroborate the proposed approach for graph signal inference tasks and show a\ncompetitive performance w.r.t. baselines and state-of-the-art alternatives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph filters are a staple tool for processing signals over graphs in a\nmultitude of downstream tasks. However, they are commonly designed for graphs\nwith a fixed number of nodes, despite real-world networks typically grow over\ntime. This topological evolution is often known up to a stochastic model, thus,\nmaking conventional graph filters ill-equipped to withstand such topological\nchanges, their uncertainty, as well as the dynamic nature of the incoming data.\nTo tackle these issues, we propose an online graph filtering framework by\nrelying on online learning principles. We design filters for scenarios where\nthe topology is both known and unknown, including a learner adaptive to such\nevolution. We conduct a regret analysis to highlight the role played by the\ndifferent components such as the online algorithm, the filter order, and the\ngrowing graph model. Numerical experiments with synthetic and real data\ncorroborate the proposed approach for graph signal inference tasks and show a\ncompetitive performance w.r.t. baselines and state-of-the-art alternatives."
                },
                "authors": [
                    {
                        "name": "Bishwadeep Das"
                    },
                    {
                        "name": "Elvin Isufi"
                    }
                ],
                "author_detail": {
                    "name": "Elvin Isufi"
                },
                "author": "Elvin Isufi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07204v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07204v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.01480v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.01480v2",
                "updated": "2024-09-11T11:34:35Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    11,
                    34,
                    35,
                    2,
                    255,
                    0
                ],
                "published": "2023-09-04T09:35:53Z",
                "published_parsed": [
                    2023,
                    9,
                    4,
                    9,
                    35,
                    53,
                    0,
                    247,
                    0
                ],
                "title": "EventTrojan: Manipulating Non-Intrusive Speech Quality Assessment via\n  Imperceptible Events",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EventTrojan: Manipulating Non-Intrusive Speech Quality Assessment via\n  Imperceptible Events"
                },
                "summary": "Non-Intrusive speech quality assessment (NISQA) has gained significant\nattention for predicting speech's mean opinion score (MOS) without requiring\nthe reference speech. Researchers have gradually started to apply NISQA to\nvarious practical scenarios. However, little attention has been paid to the\nsecurity of NISQA models. Backdoor attacks represent the most serious threat to\ndeep neural networks (DNNs) due to the fact that backdoors possess a very high\nattack success rate once embedded. However, existing backdoor attacks assume\nthat the attacker actively feeds samples containing triggers into the model\nduring the inference phase. This is not adapted to the specific scenario of\nNISQA. And current backdoor attacks on regression tasks lack an objective\nmetric to measure the attack performance. To address these issues, we propose a\nnovel backdoor triggering approach (EventTrojan) that utilizes an event during\nthe usage of the NISQA model as a trigger. Moreover, we innovatively provide an\nobjective metric for backdoor attacks on regression tasks. Extensive\nexperiments on four benchmark datasets demonstrate the effectiveness of the\nEventTrojan attack. Besides, it also has good resistance to several defense\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-Intrusive speech quality assessment (NISQA) has gained significant\nattention for predicting speech's mean opinion score (MOS) without requiring\nthe reference speech. Researchers have gradually started to apply NISQA to\nvarious practical scenarios. However, little attention has been paid to the\nsecurity of NISQA models. Backdoor attacks represent the most serious threat to\ndeep neural networks (DNNs) due to the fact that backdoors possess a very high\nattack success rate once embedded. However, existing backdoor attacks assume\nthat the attacker actively feeds samples containing triggers into the model\nduring the inference phase. This is not adapted to the specific scenario of\nNISQA. And current backdoor attacks on regression tasks lack an objective\nmetric to measure the attack performance. To address these issues, we propose a\nnovel backdoor triggering approach (EventTrojan) that utilizes an event during\nthe usage of the NISQA model as a trigger. Moreover, we innovatively provide an\nobjective metric for backdoor attacks on regression tasks. Extensive\nexperiments on four benchmark datasets demonstrate the effectiveness of the\nEventTrojan attack. Besides, it also has good resistance to several defense\nmethods."
                },
                "authors": [
                    {
                        "name": "Ying Ren"
                    },
                    {
                        "name": "Kailai Shen"
                    },
                    {
                        "name": "Zhe Ye"
                    },
                    {
                        "name": "Diqun Yan"
                    }
                ],
                "author_detail": {
                    "name": "Diqun Yan"
                },
                "author": "Diqun Yan",
                "arxiv_comment": "Accepted by ICME2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.01480v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.01480v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.19997v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.19997v3",
                "updated": "2024-09-11T11:12:49Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    11,
                    12,
                    49,
                    2,
                    255,
                    0
                ],
                "published": "2024-03-29T06:24:16Z",
                "published_parsed": [
                    2024,
                    3,
                    29,
                    6,
                    24,
                    16,
                    4,
                    89,
                    0
                ],
                "title": "Size-dependent fracture in elastomers: experiments and continuum\n  modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Size-dependent fracture in elastomers: experiments and continuum\n  modeling"
                },
                "summary": "Elastomeric materials display a complicated set of stretchability and\nfracture properties that strongly depend on the flaw size, which has long been\nof interest to engineers and materials scientists. Here, we combine experiments\nand numerical simulations for a comprehensive understanding of the nonlocal,\nsize-dependent features of fracture in elastomers. We show the size-dependent\nfracture behavior is quantitatively described through a nonlocal continuum\nmodel. The key ingredient of the nonlocal model is the use of an intrinsic\nlength scale associated with a finite fracture process zone, which is inferred\nfrom experiments. Of particular importance, our experimental and theoretical\napproach passes the critical set of capturing key aspects of the size-dependent\nfracture in elastomers. Applications to a wide range of synthetic elastomers\nthat exhibit moderate (~100%) to extreme stretchability (~1000%) are presented,\nwhich is also used to demonstrate the applicability of our approach in\nelastomeric specimens with complex geometries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Elastomeric materials display a complicated set of stretchability and\nfracture properties that strongly depend on the flaw size, which has long been\nof interest to engineers and materials scientists. Here, we combine experiments\nand numerical simulations for a comprehensive understanding of the nonlocal,\nsize-dependent features of fracture in elastomers. We show the size-dependent\nfracture behavior is quantitatively described through a nonlocal continuum\nmodel. The key ingredient of the nonlocal model is the use of an intrinsic\nlength scale associated with a finite fracture process zone, which is inferred\nfrom experiments. Of particular importance, our experimental and theoretical\napproach passes the critical set of capturing key aspects of the size-dependent\nfracture in elastomers. Applications to a wide range of synthetic elastomers\nthat exhibit moderate (~100%) to extreme stretchability (~1000%) are presented,\nwhich is also used to demonstrate the applicability of our approach in\nelastomeric specimens with complex geometries."
                },
                "authors": [
                    {
                        "name": "Jaehee Lee"
                    },
                    {
                        "name": "Jeongun Lee"
                    },
                    {
                        "name": "Seounghee Yun"
                    },
                    {
                        "name": "Sanha Kim"
                    },
                    {
                        "name": "Howon Lee"
                    },
                    {
                        "name": "Shawn A. Chester"
                    },
                    {
                        "name": "Hansohl Cho"
                    }
                ],
                "author_detail": {
                    "name": "Hansohl Cho"
                },
                "author": "Hansohl Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.19997v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.19997v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.soft",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02331v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02331v2",
                "updated": "2024-09-11T10:57:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    10,
                    57,
                    46,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-03T23:04:28Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    23,
                    4,
                    28,
                    1,
                    247,
                    0
                ],
                "title": "A parameterization of anisotropic Gaussian fields with penalized\n  complexity priors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A parameterization of anisotropic Gaussian fields with penalized\n  complexity priors"
                },
                "summary": "Gaussian random fields (GFs) are fundamental tools in spatial modeling and\ncan be represented flexibly and efficiently as solutions to stochastic partial\ndifferential equations (SPDEs). The SPDEs depend on specific parameters, which\nenforce various field behaviors and can be estimated using Bayesian inference.\nHowever, the likelihood typically only provides limited insights into the\ncovariance structure under in-fill asymptotics. In response, it is essential to\nleverage priors to achieve appropriate, meaningful covariance structures in the\nposterior. This study introduces a smooth, invertible parameterization of the\ncorrelation length and diffusion matrix of an anisotropic GF and constructs\npenalized complexity (PC) priors for the model when the parameters are constant\nin space. The formulated prior is weakly informative, effectively penalizing\ncomplexity by pushing the correlation range toward infinity and the anisotropy\nto zero.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian random fields (GFs) are fundamental tools in spatial modeling and\ncan be represented flexibly and efficiently as solutions to stochastic partial\ndifferential equations (SPDEs). The SPDEs depend on specific parameters, which\nenforce various field behaviors and can be estimated using Bayesian inference.\nHowever, the likelihood typically only provides limited insights into the\ncovariance structure under in-fill asymptotics. In response, it is essential to\nleverage priors to achieve appropriate, meaningful covariance structures in the\nposterior. This study introduces a smooth, invertible parameterization of the\ncorrelation length and diffusion matrix of an anisotropic GF and constructs\npenalized complexity (PC) priors for the model when the parameters are constant\nin space. The formulated prior is weakly informative, effectively penalizing\ncomplexity by pushing the correlation range toward infinity and the anisotropy\nto zero."
                },
                "authors": [
                    {
                        "name": "Liam Llamazares-Elias"
                    },
                    {
                        "name": "Jonas Latz"
                    },
                    {
                        "name": "Finn Lindgren"
                    }
                ],
                "author_detail": {
                    "name": "Finn Lindgren"
                },
                "author": "Finn Lindgren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02331v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02331v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07181v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07181v1",
                "updated": "2024-09-11T10:50:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    10,
                    50,
                    51,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T10:50:51Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    10,
                    50,
                    51,
                    2,
                    255,
                    0
                ],
                "title": "A new analytic approach to infer the cosmic-ray ionization rate in hot\n  molecular cores from HCO$^+$, N$_2$H$^+$, and CO observations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A new analytic approach to infer the cosmic-ray ionization rate in hot\n  molecular cores from HCO$^+$, N$_2$H$^+$, and CO observations"
                },
                "summary": "The cosmic-ray ionization rate ($\\zeta_2$) is one of the key parameters in\nstar formation, since it regulates the chemical and dynamical evolution of\nmolecular clouds by ionizing molecules and determining the coupling between the\nmagnetic field and gas. However, measurements of $\\zeta_2$ in dense clouds\n(e.g., $n_{\\rm H} \\geq 10^4$ cm$^{-3}$) are difficult and sensitive to the\nmodel assumptions. The aim is to find a convenient analytic approach that can\nbe used in high-mass star-forming regions (HMSFRs), especially for warm gas\nenvironments such as hot molecular cores (HMCs). We propose a new analytic\napproach to calculate $\\zeta_2$ through HCO$^+$, N$_2$H$^+$, and CO\nmeasurements. Our method gives a good approximation, to within $50$\\%, of\n$\\zeta_2$ in dense and warm gas (e.g., $n_{\\rm H} \\geq 10^4$ cm$^{-3}$, $T =\n50, 100$ K) for $A_{\\rm V} \\geq 4$ mag and $t \\geq 2\\times10^4$ yr at Solar\nmetallicity. The analytic approach gives better results for higher densities.\nHowever, it starts to underestimate the CRIR at low metallicity ($Z =\n0.1Z_\\odot$) and high CRIR ($\\zeta_2 \\geq 3\\times10^{-15}$ s$^{-1}$). By\napplying our method to the OMC-2 FIR4 envelope and the L1157-B1 shock region,\nwe find $\\zeta_2$ values of $(1.0\\pm0.3)\\times10^{-14}$ s$^{-1}$ and\n$(2.2\\pm0.4)\\times10^{-16}$ s$^{-1}$, consistent with those previously\nreported. We calculate $\\zeta_2$ toward a total of 82 samples in HMSFRs,\nfinding that the average value of $\\zeta_2$ toward all HMC samples ($\\zeta_2$ =\n(7.4$\\pm$5.0)$\\times$10$^{-16}$ s$^{-1}$) is more than an order of magnitude\nhigher than the theoretical prediction of cosmic-ray attenuation models,\nfavoring the scenario that locally accelerated cosmic rays in embedded\nprotostars should be responsible for the observed high $\\zeta_2$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The cosmic-ray ionization rate ($\\zeta_2$) is one of the key parameters in\nstar formation, since it regulates the chemical and dynamical evolution of\nmolecular clouds by ionizing molecules and determining the coupling between the\nmagnetic field and gas. However, measurements of $\\zeta_2$ in dense clouds\n(e.g., $n_{\\rm H} \\geq 10^4$ cm$^{-3}$) are difficult and sensitive to the\nmodel assumptions. The aim is to find a convenient analytic approach that can\nbe used in high-mass star-forming regions (HMSFRs), especially for warm gas\nenvironments such as hot molecular cores (HMCs). We propose a new analytic\napproach to calculate $\\zeta_2$ through HCO$^+$, N$_2$H$^+$, and CO\nmeasurements. Our method gives a good approximation, to within $50$\\%, of\n$\\zeta_2$ in dense and warm gas (e.g., $n_{\\rm H} \\geq 10^4$ cm$^{-3}$, $T =\n50, 100$ K) for $A_{\\rm V} \\geq 4$ mag and $t \\geq 2\\times10^4$ yr at Solar\nmetallicity. The analytic approach gives better results for higher densities.\nHowever, it starts to underestimate the CRIR at low metallicity ($Z =\n0.1Z_\\odot$) and high CRIR ($\\zeta_2 \\geq 3\\times10^{-15}$ s$^{-1}$). By\napplying our method to the OMC-2 FIR4 envelope and the L1157-B1 shock region,\nwe find $\\zeta_2$ values of $(1.0\\pm0.3)\\times10^{-14}$ s$^{-1}$ and\n$(2.2\\pm0.4)\\times10^{-16}$ s$^{-1}$, consistent with those previously\nreported. We calculate $\\zeta_2$ toward a total of 82 samples in HMSFRs,\nfinding that the average value of $\\zeta_2$ toward all HMC samples ($\\zeta_2$ =\n(7.4$\\pm$5.0)$\\times$10$^{-16}$ s$^{-1}$) is more than an order of magnitude\nhigher than the theoretical prediction of cosmic-ray attenuation models,\nfavoring the scenario that locally accelerated cosmic rays in embedded\nprotostars should be responsible for the observed high $\\zeta_2$."
                },
                "authors": [
                    {
                        "name": "Gan Luo"
                    },
                    {
                        "name": "Thomas G. Bisbas"
                    },
                    {
                        "name": "Marco Padovani"
                    },
                    {
                        "name": "Brandt A. L. Gaches"
                    }
                ],
                "author_detail": {
                    "name": "Brandt A. L. Gaches"
                },
                "author": "Brandt A. L. Gaches",
                "arxiv_comment": "14 pages, 11 figures, accepted by A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07181v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07181v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07179v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07179v1",
                "updated": "2024-09-11T10:41:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    10,
                    41,
                    46,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T10:41:46Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    10,
                    41,
                    46,
                    2,
                    255,
                    0
                ],
                "title": "Phy124: Fast Physics-Driven 4D Content Generation from a Single Image",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phy124: Fast Physics-Driven 4D Content Generation from a Single Image"
                },
                "summary": "4D content generation focuses on creating dynamic 3D objects that change over\ntime. Existing methods primarily rely on pre-trained video diffusion models,\nutilizing sampling processes or reference videos. However, these approaches\nface significant challenges. Firstly, the generated 4D content often fails to\nadhere to real-world physics since video diffusion models do not incorporate\nphysical priors. Secondly, the extensive sampling process and the large number\nof parameters in diffusion models result in exceedingly time-consuming\ngeneration processes. To address these issues, we introduce Phy124, a novel,\nfast, and physics-driven method for controllable 4D content generation from a\nsingle image. Phy124 integrates physical simulation directly into the 4D\ngeneration process, ensuring that the resulting 4D content adheres to natural\nphysical laws. Phy124 also eliminates the use of diffusion models during the 4D\ndynamics generation phase, significantly speeding up the process. Phy124 allows\nfor the control of 4D dynamics, including movement speed and direction, by\nmanipulating external forces. Extensive experiments demonstrate that Phy124\ngenerates high-fidelity 4D content with significantly reduced inference times,\nachieving stateof-the-art performance. The code and generated 4D content are\navailable at the provided link: https://anonymous.4open.science/r/BBF2/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "4D content generation focuses on creating dynamic 3D objects that change over\ntime. Existing methods primarily rely on pre-trained video diffusion models,\nutilizing sampling processes or reference videos. However, these approaches\nface significant challenges. Firstly, the generated 4D content often fails to\nadhere to real-world physics since video diffusion models do not incorporate\nphysical priors. Secondly, the extensive sampling process and the large number\nof parameters in diffusion models result in exceedingly time-consuming\ngeneration processes. To address these issues, we introduce Phy124, a novel,\nfast, and physics-driven method for controllable 4D content generation from a\nsingle image. Phy124 integrates physical simulation directly into the 4D\ngeneration process, ensuring that the resulting 4D content adheres to natural\nphysical laws. Phy124 also eliminates the use of diffusion models during the 4D\ndynamics generation phase, significantly speeding up the process. Phy124 allows\nfor the control of 4D dynamics, including movement speed and direction, by\nmanipulating external forces. Extensive experiments demonstrate that Phy124\ngenerates high-fidelity 4D content with significantly reduced inference times,\nachieving stateof-the-art performance. The code and generated 4D content are\navailable at the provided link: https://anonymous.4open.science/r/BBF2/."
                },
                "authors": [
                    {
                        "name": "Jiajing Lin"
                    },
                    {
                        "name": "Zhenzhong Wang"
                    },
                    {
                        "name": "Yongjie Hou"
                    },
                    {
                        "name": "Yuzhou Tang"
                    },
                    {
                        "name": "Min Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Min Jiang"
                },
                "author": "Min Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07179v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07179v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07178v1",
                "updated": "2024-09-11T10:41:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    10,
                    41,
                    5,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T10:41:05Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    10,
                    41,
                    5,
                    2,
                    255,
                    0
                ],
                "title": "Identify Design Problems Through Questioning: Exploring Role-playing\n  Interactions with Large Language Models to Foster Design Questioning Skills",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identify Design Problems Through Questioning: Exploring Role-playing\n  Interactions with Large Language Models to Foster Design Questioning Skills"
                },
                "summary": "Identifying design problems is a crucial step for creating plausible\nsolutions, but it is challenging for design novices due to their limited\nknowledge and experience. Questioning is a promising skill that enables\nstudents to independently identify design problems without being passive or\nrelying on instructors. This study explores role-playing interactions with\nLarge Language Model (LLM)-powered Conversational Agents (CAs) to foster the\nquestioning skills of novice design students. We proposed an LLM-powered CA\nprototype and conducted a preliminary study with 16 novice design students\nengaged in a real-world design class to observe the interactions between\nstudents and the LLM-powered CAs. Our findings indicate that while the CAs\nstimulated questioning and reduced pressure to ask questions, it also\ninadvertently led to over-reliance on LLM responses. We proposed design\nconsiderations and future works for LLM-powered CA to foster questioning\nskills.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying design problems is a crucial step for creating plausible\nsolutions, but it is challenging for design novices due to their limited\nknowledge and experience. Questioning is a promising skill that enables\nstudents to independently identify design problems without being passive or\nrelying on instructors. This study explores role-playing interactions with\nLarge Language Model (LLM)-powered Conversational Agents (CAs) to foster the\nquestioning skills of novice design students. We proposed an LLM-powered CA\nprototype and conducted a preliminary study with 16 novice design students\nengaged in a real-world design class to observe the interactions between\nstudents and the LLM-powered CAs. Our findings indicate that while the CAs\nstimulated questioning and reduced pressure to ask questions, it also\ninadvertently led to over-reliance on LLM responses. We proposed design\nconsiderations and future works for LLM-powered CA to foster questioning\nskills."
                },
                "authors": [
                    {
                        "name": "Hyunseung Lim"
                    },
                    {
                        "name": "Dasom Choi"
                    },
                    {
                        "name": "Hwajung Hong"
                    }
                ],
                "author_detail": {
                    "name": "Hwajung Hong"
                },
                "author": "Hwajung Hong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07172v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07172v1",
                "updated": "2024-09-11T10:35:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    10,
                    35,
                    42,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T10:35:42Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    10,
                    35,
                    42,
                    2,
                    255,
                    0
                ],
                "title": "Swin-LiteMedSAM: A Lightweight Box-Based Segment Anything Model for\n  Large-Scale Medical Image Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Swin-LiteMedSAM: A Lightweight Box-Based Segment Anything Model for\n  Large-Scale Medical Image Datasets"
                },
                "summary": "Medical imaging is essential for the diagnosis and treatment of diseases,\nwith medical image segmentation as a subtask receiving high attention. However,\nautomatic medical image segmentation models are typically task-specific and\nstruggle to handle multiple scenarios, such as different imaging modalities and\nregions of interest. With the introduction of the Segment Anything Model (SAM),\ntraining a universal model for various clinical scenarios has become feasible.\nRecently, several Medical SAM (MedSAM) methods have been proposed, but these\nmodels often rely on heavy image encoders to achieve high performance, which\nmay not be practical for real-world applications due to their high\ncomputational demands and slow inference speed. To address this issue, a\nlightweight version of the MedSAM (LiteMedSAM) can provide a viable solution,\nachieving high performance while requiring fewer resources and less time. In\nthis work, we introduce Swin-LiteMedSAM, a new variant of LiteMedSAM. This\nmodel integrates the tiny Swin Transformer as the image encoder, incorporates\nmultiple types of prompts, including box-based points and scribble generated\nfrom a given bounding box, and establishes skip connections between the image\nencoder and the mask decoder. In the \\textit{Segment Anything in Medical Images\non Laptop} challenge (CVPR 2024), our approach strikes a good balance between\nsegmentation performance and speed, demonstrating significantly improved\noverall results across multiple modalities compared to the LiteMedSAM baseline\nprovided by the challenge organizers. Our proposed model achieved a DSC score\nof \\textbf{0.8678} and an NSD score of \\textbf{0.8844} on the validation set.\nOn the final test set, it attained a DSC score of \\textbf{0.8193} and an NSD\nscore of \\textbf{0.8461}, securing fourth place in the challenge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical imaging is essential for the diagnosis and treatment of diseases,\nwith medical image segmentation as a subtask receiving high attention. However,\nautomatic medical image segmentation models are typically task-specific and\nstruggle to handle multiple scenarios, such as different imaging modalities and\nregions of interest. With the introduction of the Segment Anything Model (SAM),\ntraining a universal model for various clinical scenarios has become feasible.\nRecently, several Medical SAM (MedSAM) methods have been proposed, but these\nmodels often rely on heavy image encoders to achieve high performance, which\nmay not be practical for real-world applications due to their high\ncomputational demands and slow inference speed. To address this issue, a\nlightweight version of the MedSAM (LiteMedSAM) can provide a viable solution,\nachieving high performance while requiring fewer resources and less time. In\nthis work, we introduce Swin-LiteMedSAM, a new variant of LiteMedSAM. This\nmodel integrates the tiny Swin Transformer as the image encoder, incorporates\nmultiple types of prompts, including box-based points and scribble generated\nfrom a given bounding box, and establishes skip connections between the image\nencoder and the mask decoder. In the \\textit{Segment Anything in Medical Images\non Laptop} challenge (CVPR 2024), our approach strikes a good balance between\nsegmentation performance and speed, demonstrating significantly improved\noverall results across multiple modalities compared to the LiteMedSAM baseline\nprovided by the challenge organizers. Our proposed model achieved a DSC score\nof \\textbf{0.8678} and an NSD score of \\textbf{0.8844} on the validation set.\nOn the final test set, it attained a DSC score of \\textbf{0.8193} and an NSD\nscore of \\textbf{0.8461}, securing fourth place in the challenge."
                },
                "authors": [
                    {
                        "name": "Ruochen Gao"
                    },
                    {
                        "name": "Donghang Lyu"
                    },
                    {
                        "name": "Marius Staring"
                    }
                ],
                "author_detail": {
                    "name": "Marius Staring"
                },
                "author": "Marius Staring",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07172v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07172v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07162v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07162v1",
                "updated": "2024-09-11T10:21:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    10,
                    21,
                    13,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T10:21:13Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    10,
                    21,
                    13,
                    2,
                    255,
                    0
                ],
                "title": "A Fine-grained Sentiment Analysis of App Reviews using Large Language\n  Models: An Evaluation Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Fine-grained Sentiment Analysis of App Reviews using Large Language\n  Models: An Evaluation Study"
                },
                "summary": "Analyzing user reviews for sentiment towards app features can provide\nvaluable insights into users' perceptions of app functionality and their\nevolving needs. Given the volume of user reviews received daily, an automated\nmechanism to generate feature-level sentiment summaries of user reviews is\nneeded. Recent advances in Large Language Models (LLMs) such as ChatGPT have\nshown impressive performance on several new tasks without updating the model's\nparameters i.e. using zero or a few labeled examples. Despite these\nadvancements, LLMs' capabilities to perform feature-specific sentiment analysis\nof user reviews remain unexplored. This study compares the performance of\nstate-of-the-art LLMs, including GPT-4, ChatGPT, and LLama-2-chat variants, for\nextracting app features and associated sentiments under 0-shot, 1-shot, and\n5-shot scenarios. Results indicate the best-performing GPT-4 model outperforms\nrule-based approaches by 23.6% in f1-score with zero-shot feature extraction;\n5-shot further improving it by 6%. GPT-4 achieves a 74% f1-score for predicting\npositive sentiment towards correctly predicted app features, with 5-shot\nenhancing it by 7%. Our study suggests that LLM models are promising for\ngenerating feature-specific sentiment summaries of user reviews.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing user reviews for sentiment towards app features can provide\nvaluable insights into users' perceptions of app functionality and their\nevolving needs. Given the volume of user reviews received daily, an automated\nmechanism to generate feature-level sentiment summaries of user reviews is\nneeded. Recent advances in Large Language Models (LLMs) such as ChatGPT have\nshown impressive performance on several new tasks without updating the model's\nparameters i.e. using zero or a few labeled examples. Despite these\nadvancements, LLMs' capabilities to perform feature-specific sentiment analysis\nof user reviews remain unexplored. This study compares the performance of\nstate-of-the-art LLMs, including GPT-4, ChatGPT, and LLama-2-chat variants, for\nextracting app features and associated sentiments under 0-shot, 1-shot, and\n5-shot scenarios. Results indicate the best-performing GPT-4 model outperforms\nrule-based approaches by 23.6% in f1-score with zero-shot feature extraction;\n5-shot further improving it by 6%. GPT-4 achieves a 74% f1-score for predicting\npositive sentiment towards correctly predicted app features, with 5-shot\nenhancing it by 7%. Our study suggests that LLM models are promising for\ngenerating feature-specific sentiment summaries of user reviews."
                },
                "authors": [
                    {
                        "name": "Faiz Ali Shah"
                    },
                    {
                        "name": "Ahmed Sabir"
                    },
                    {
                        "name": "Rajesh Sharma"
                    }
                ],
                "author_detail": {
                    "name": "Rajesh Sharma"
                },
                "author": "Rajesh Sharma",
                "arxiv_comment": "The summary of the project is available at\n  https://ahmed.jp/project_page/App_LLMs_2024/app_llms.html",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07162v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07162v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01193v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01193v3",
                "updated": "2024-09-11T10:09:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    10,
                    9,
                    4,
                    2,
                    255,
                    0
                ],
                "published": "2024-04-01T15:50:09Z",
                "published_parsed": [
                    2024,
                    4,
                    1,
                    15,
                    50,
                    9,
                    0,
                    92,
                    0
                ],
                "title": "Inferring parameters and reconstruction of two-dimensional turbulent\n  flows with physics-informed neural networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring parameters and reconstruction of two-dimensional turbulent\n  flows with physics-informed neural networks"
                },
                "summary": "Obtaining system parameters and reconstructing the full flow state from\nlimited velocity observations using conventional fluid dynamics solvers can be\nprohibitively expensive. Here we employ machine learning algorithms to overcome\nthe challenge. As an example, we consider a moderately turbulent fluid flow,\nexcited by a stationary force and described by a two-dimensional Navier-Stokes\nequation with linear bottom friction. Using dense in time, spatially sparse and\nprobably noisy velocity data, we reconstruct the spatially dense velocity\nfield, infer the pressure and driving force up to a harmonic function and its\ngradient, respectively, and determine the unknown fluid viscosity and friction\ncoefficient. Both the root-mean-square errors of the reconstructions and their\nenergy spectra are addressed. We study the dependence of these metrics on the\ndegree of sparsity and noise in the velocity measurements. Our approach\ninvolves training a physics-informed neural network by minimizing the loss\nfunction, which penalizes deviations from the provided data and violations of\nthe governing equations. The suggested technique extracts additional\ninformation from velocity measurements, potentially enhancing the capabilities\nof particle image/tracking velocimetry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Obtaining system parameters and reconstructing the full flow state from\nlimited velocity observations using conventional fluid dynamics solvers can be\nprohibitively expensive. Here we employ machine learning algorithms to overcome\nthe challenge. As an example, we consider a moderately turbulent fluid flow,\nexcited by a stationary force and described by a two-dimensional Navier-Stokes\nequation with linear bottom friction. Using dense in time, spatially sparse and\nprobably noisy velocity data, we reconstruct the spatially dense velocity\nfield, infer the pressure and driving force up to a harmonic function and its\ngradient, respectively, and determine the unknown fluid viscosity and friction\ncoefficient. Both the root-mean-square errors of the reconstructions and their\nenergy spectra are addressed. We study the dependence of these metrics on the\ndegree of sparsity and noise in the velocity measurements. Our approach\ninvolves training a physics-informed neural network by minimizing the loss\nfunction, which penalizes deviations from the provided data and violations of\nthe governing equations. The suggested technique extracts additional\ninformation from velocity measurements, potentially enhancing the capabilities\nof particle image/tracking velocimetry."
                },
                "authors": [
                    {
                        "name": "Vladimir Parfenyev"
                    },
                    {
                        "name": "Mark Blumenau"
                    },
                    {
                        "name": "Ilia Nikitin"
                    }
                ],
                "author_detail": {
                    "name": "Ilia Nikitin"
                },
                "author": "Ilia Nikitin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01193v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01193v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11346v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11346v3",
                "updated": "2024-09-11T10:05:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    10,
                    5,
                    37,
                    2,
                    255,
                    0
                ],
                "published": "2024-06-17T09:08:30Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    9,
                    8,
                    30,
                    0,
                    169,
                    0
                ],
                "title": "WaDec: Decompiling WebAssembly Using Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WaDec: Decompiling WebAssembly Using Large Language Model"
                },
                "summary": "WebAssembly (abbreviated Wasm) has emerged as a cornerstone of web\ndevelopment, offering a compact binary format that allows high-performance\napplications to run at near-native speeds in web browsers. Despite its\nadvantages, Wasm's binary nature presents significant challenges for developers\nand researchers, particularly regarding readability when debugging or analyzing\nweb applications. Therefore, effective decompilation becomes crucial.\nUnfortunately, traditional decompilers often struggle with producing readable\noutputs. While some large language model (LLM)-based decompilers have shown\ngood compatibility with general binary files, they still face specific\nchallenges when dealing with Wasm. In this paper, we introduce a novel\napproach, WaDec, which is the first use of a fine-tuned LLM to interpret and\ndecompile Wasm binary code into a higher-level, more comprehensible source code\nrepresentation. The LLM was meticulously fine-tuned using a specialized dataset\nof wat-c code snippets, employing self-supervised learning techniques. This\nenables WaDec to effectively decompile not only complete wat functions but also\nfiner-grained wat code snippets. Our experiments demonstrate that WaDec\nmarkedly outperforms current state-of-the-art tools, offering substantial\nimprovements across several metrics. It achieves a code inflation rate of only\n3.34%, a dramatic 97% reduction compared to the state-of-the-art's 116.94%.\nUnlike baselines' output that cannot be directly compiled or executed, WaDec\nmaintains a recompilability rate of 52.11%, a re-execution rate of 43.55%, and\nan output consistency of 27.15%. Additionally, it significantly exceeds\nstate-of-the-art performance in AST edit distance similarity by 185%,\ncyclomatic complexity by 8%, and cosine similarity by 41%, achieving an average\ncode similarity above 50%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WebAssembly (abbreviated Wasm) has emerged as a cornerstone of web\ndevelopment, offering a compact binary format that allows high-performance\napplications to run at near-native speeds in web browsers. Despite its\nadvantages, Wasm's binary nature presents significant challenges for developers\nand researchers, particularly regarding readability when debugging or analyzing\nweb applications. Therefore, effective decompilation becomes crucial.\nUnfortunately, traditional decompilers often struggle with producing readable\noutputs. While some large language model (LLM)-based decompilers have shown\ngood compatibility with general binary files, they still face specific\nchallenges when dealing with Wasm. In this paper, we introduce a novel\napproach, WaDec, which is the first use of a fine-tuned LLM to interpret and\ndecompile Wasm binary code into a higher-level, more comprehensible source code\nrepresentation. The LLM was meticulously fine-tuned using a specialized dataset\nof wat-c code snippets, employing self-supervised learning techniques. This\nenables WaDec to effectively decompile not only complete wat functions but also\nfiner-grained wat code snippets. Our experiments demonstrate that WaDec\nmarkedly outperforms current state-of-the-art tools, offering substantial\nimprovements across several metrics. It achieves a code inflation rate of only\n3.34%, a dramatic 97% reduction compared to the state-of-the-art's 116.94%.\nUnlike baselines' output that cannot be directly compiled or executed, WaDec\nmaintains a recompilability rate of 52.11%, a re-execution rate of 43.55%, and\nan output consistency of 27.15%. Additionally, it significantly exceeds\nstate-of-the-art performance in AST edit distance similarity by 185%,\ncyclomatic complexity by 8%, and cosine similarity by 41%, achieving an average\ncode similarity above 50%."
                },
                "authors": [
                    {
                        "name": "Xinyu She"
                    },
                    {
                        "name": "Yanjie Zhao"
                    },
                    {
                        "name": "Haoyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoyu Wang"
                },
                "author": "Haoyu Wang",
                "arxiv_comment": "This paper was accepted by ASE 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11346v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11346v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07146v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07146v1",
                "updated": "2024-09-11T09:49:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    9,
                    49,
                    50,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T09:49:50Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    9,
                    49,
                    50,
                    2,
                    255,
                    0
                ],
                "title": "Gated Slot Attention for Efficient Linear-Time Sequence Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gated Slot Attention for Efficient Linear-Time Sequence Modeling"
                },
                "summary": "Linear attention Transformers and their gated variants, celebrated for\nenabling parallel training and efficient recurrent inference, still fall short\nin recall-intensive tasks compared to traditional Transformers and demand\nsignificant resources for training from scratch. This paper introduces Gated\nSlot Attention (GSA), which enhances Attention with Bounded-memory-Control\n(ABC) by incorporating a gating mechanism inspired by Gated Linear Attention\n(GLA). Essentially, GSA comprises a two-layer GLA linked via softmax, utilizing\ncontext-aware memory reading and adaptive forgetting to improve memory capacity\nwhile maintaining compact recurrent state size. This design greatly enhances\nboth training and inference efficiency through GLA's hardware-efficient\ntraining algorithm and reduced state size. Additionally, retaining the softmax\noperation is particularly beneficial in \"finetuning pretrained Transformers to\nRNNs\" (T2R) settings, reducing the need for extensive training from scratch.\nExtensive experiments confirm GSA's superior performance in scenarios requiring\nin-context recall and in T2R settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear attention Transformers and their gated variants, celebrated for\nenabling parallel training and efficient recurrent inference, still fall short\nin recall-intensive tasks compared to traditional Transformers and demand\nsignificant resources for training from scratch. This paper introduces Gated\nSlot Attention (GSA), which enhances Attention with Bounded-memory-Control\n(ABC) by incorporating a gating mechanism inspired by Gated Linear Attention\n(GLA). Essentially, GSA comprises a two-layer GLA linked via softmax, utilizing\ncontext-aware memory reading and adaptive forgetting to improve memory capacity\nwhile maintaining compact recurrent state size. This design greatly enhances\nboth training and inference efficiency through GLA's hardware-efficient\ntraining algorithm and reduced state size. Additionally, retaining the softmax\noperation is particularly beneficial in \"finetuning pretrained Transformers to\nRNNs\" (T2R) settings, reducing the need for extensive training from scratch.\nExtensive experiments confirm GSA's superior performance in scenarios requiring\nin-context recall and in T2R settings."
                },
                "authors": [
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Songlin Yang"
                    },
                    {
                        "name": "Ruijie Zhu"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Leyang Cui"
                    },
                    {
                        "name": "Yiqiao Wang"
                    },
                    {
                        "name": "Bolun Wang"
                    },
                    {
                        "name": "Freda Shi"
                    },
                    {
                        "name": "Bailin Wang"
                    },
                    {
                        "name": "Wei Bi"
                    },
                    {
                        "name": "Peng Zhou"
                    },
                    {
                        "name": "Guohong Fu"
                    }
                ],
                "author_detail": {
                    "name": "Guohong Fu"
                },
                "author": "Guohong Fu",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07146v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07146v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.08669v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.08669v7",
                "updated": "2024-09-11T09:46:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    9,
                    46,
                    34,
                    2,
                    255,
                    0
                ],
                "published": "2023-06-14T17:59:03Z",
                "published_parsed": [
                    2023,
                    6,
                    14,
                    17,
                    59,
                    3,
                    2,
                    165,
                    0
                ],
                "title": "Plan B: New ${Z^\\prime}$ models for $b\\rightarrow sl^+l^-$ anomalies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plan B: New ${Z^\\prime}$ models for $b\\rightarrow sl^+l^-$ anomalies"
                },
                "summary": "Measurements of $b \\rightarrow s \\mu^+ \\mu^-$ transitions indicate that there\nmay be a new physics field coupling to di-muon pairs associated with the $b$ to\n$s$ flavour transition. Including the 2022 LHCb reanalysis of $R_K$ and\n$R_{K^\\ast}$, one infers that there may also be associated new physics in\n$b\\rightarrow e^+ e^-$ transitions. Here, we examine the extent of the\nstatistical preference for $Z^\\prime$ models coupling to di-electron pairs\ntaking into account the relevant constraints, in particular from experiments at\nLEP-2. We identify an anomaly-free set of models which interpolates between the\n$Z^\\prime$ not coupling to electrons at all, to one in which there is an equal\n$Z^\\prime$ coupling to muons and electrons (but where in all models in the set,\nthe $Z^\\prime$ boson can mediate $b\\rightarrow \\mu^+ \\mu^-$ transitions). A\n$3B_3-L_e-2L_\\mu$ model provides a close-to-optimal fit to the pertinent\nmeasurements along the line of interpolation. We have (re-)calculated\npredictions for the relevant LEP-2 observables in terms of dimension-6 SMEFT\noperators and put them into the ${\\tt flavio2.3.3}$ computer program, so that\nthey are available for global fits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measurements of $b \\rightarrow s \\mu^+ \\mu^-$ transitions indicate that there\nmay be a new physics field coupling to di-muon pairs associated with the $b$ to\n$s$ flavour transition. Including the 2022 LHCb reanalysis of $R_K$ and\n$R_{K^\\ast}$, one infers that there may also be associated new physics in\n$b\\rightarrow e^+ e^-$ transitions. Here, we examine the extent of the\nstatistical preference for $Z^\\prime$ models coupling to di-electron pairs\ntaking into account the relevant constraints, in particular from experiments at\nLEP-2. We identify an anomaly-free set of models which interpolates between the\n$Z^\\prime$ not coupling to electrons at all, to one in which there is an equal\n$Z^\\prime$ coupling to muons and electrons (but where in all models in the set,\nthe $Z^\\prime$ boson can mediate $b\\rightarrow \\mu^+ \\mu^-$ transitions). A\n$3B_3-L_e-2L_\\mu$ model provides a close-to-optimal fit to the pertinent\nmeasurements along the line of interpolation. We have (re-)calculated\npredictions for the relevant LEP-2 observables in terms of dimension-6 SMEFT\noperators and put them into the ${\\tt flavio2.3.3}$ computer program, so that\nthey are available for global fits."
                },
                "authors": [
                    {
                        "name": "Ben Allanach"
                    },
                    {
                        "name": "Anna Mullin"
                    }
                ],
                "author_detail": {
                    "name": "Anna Mullin"
                },
                "author": "Anna Mullin",
                "arxiv_comment": "26 pages, 7 figures. Erroneous Feynman diagram removed in Fig. 3",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.08669v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.08669v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07136v1",
                "updated": "2024-09-11T09:31:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    9,
                    31,
                    44,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T09:31:44Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    9,
                    31,
                    44,
                    2,
                    255,
                    0
                ],
                "title": "Leveraging Unstructured Text Data for Federated Instruction Tuning of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Unstructured Text Data for Federated Instruction Tuning of\n  Large Language Models"
                },
                "summary": "Federated instruction tuning enables multiple clients to collaboratively\nfine-tune a shared large language model (LLM) that can follow humans'\ninstructions without directly sharing raw data. However, existing literature\nimpractically requires that all the clients readily hold instruction-tuning\ndata (i.e., structured instruction-response pairs), which necessitates massive\nhuman annotations since clients' data is usually unstructured text instead.\nAddressing this, we propose a novel and flexible framework FedIT-U2S, which can\nautomatically transform unstructured corpus into structured data for federated\ninstruction tuning. FedIT-U2S consists two key steps: (1) few-shot\ninstruction-tuning data generation, where each unstructured data piece together\nwith several examples is combined to prompt an LLM in generating an\ninstruction-response pair. To further enhance the flexibility, a\nretrieval-based example selection technique is proposed, where the examples are\nautomatically selected based on the relatedness between the client's data piece\nand example pool, bypassing the need of determining examples in advance. (2) A\ntypical federated instruction tuning process based on the generated data.\nOverall, FedIT-U2S can be applied to diverse scenarios as long as the client\nholds valuable text corpus, broadening the application scope of federated\ninstruction tuning. We conduct a series of experiments on three domains\n(medicine, knowledge, and math), showing that our proposed FedIT-U2S can\nconsistently and significantly brings improvement over the base LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated instruction tuning enables multiple clients to collaboratively\nfine-tune a shared large language model (LLM) that can follow humans'\ninstructions without directly sharing raw data. However, existing literature\nimpractically requires that all the clients readily hold instruction-tuning\ndata (i.e., structured instruction-response pairs), which necessitates massive\nhuman annotations since clients' data is usually unstructured text instead.\nAddressing this, we propose a novel and flexible framework FedIT-U2S, which can\nautomatically transform unstructured corpus into structured data for federated\ninstruction tuning. FedIT-U2S consists two key steps: (1) few-shot\ninstruction-tuning data generation, where each unstructured data piece together\nwith several examples is combined to prompt an LLM in generating an\ninstruction-response pair. To further enhance the flexibility, a\nretrieval-based example selection technique is proposed, where the examples are\nautomatically selected based on the relatedness between the client's data piece\nand example pool, bypassing the need of determining examples in advance. (2) A\ntypical federated instruction tuning process based on the generated data.\nOverall, FedIT-U2S can be applied to diverse scenarios as long as the client\nholds valuable text corpus, broadening the application scope of federated\ninstruction tuning. We conduct a series of experiments on three domains\n(medicine, knowledge, and math), showing that our proposed FedIT-U2S can\nconsistently and significantly brings improvement over the base LLM."
                },
                "authors": [
                    {
                        "name": "Rui Ye"
                    },
                    {
                        "name": "Rui Ge"
                    },
                    {
                        "name": "Yuchi Fengting"
                    },
                    {
                        "name": "Jingyi Chai"
                    },
                    {
                        "name": "Yanfeng Wang"
                    },
                    {
                        "name": "Siheng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Siheng Chen"
                },
                "author": "Siheng Chen",
                "arxiv_comment": "11 pages, work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07133v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07133v1",
                "updated": "2024-09-11T09:30:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    9,
                    30,
                    1,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T09:30:01Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    9,
                    30,
                    1,
                    2,
                    255,
                    0
                ],
                "title": "Application of Quantum Graph Theory to Metamaterial Design: Negative\n  Refraction of Acoustic Waveguide Modes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application of Quantum Graph Theory to Metamaterial Design: Negative\n  Refraction of Acoustic Waveguide Modes"
                },
                "summary": "We leverage quantum graph theory to quickly and accurately characterise\nacoustic metamaterials comprising networks of interconnected pipes. Anisotropic\nbond lengths are incorporated in the model that correspond to space-coiled\nacoustic structures to exhibit dispersion spectra reminiscent of hyperbolic\nmetamaterials. We construct two metasurfaces with embedded graph structure and,\nmotivated by the graph theory, infer and fine-tune their dispersive properties\nto engineer non-resonant negative refraction of acoustic surface waves at their\ninterface. Agreement between the graph model, full wave simulations, and\nexperiments bolsters quantum graph theory as a new paradigm for metamaterial\ndesign.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We leverage quantum graph theory to quickly and accurately characterise\nacoustic metamaterials comprising networks of interconnected pipes. Anisotropic\nbond lengths are incorporated in the model that correspond to space-coiled\nacoustic structures to exhibit dispersion spectra reminiscent of hyperbolic\nmetamaterials. We construct two metasurfaces with embedded graph structure and,\nmotivated by the graph theory, infer and fine-tune their dispersive properties\nto engineer non-resonant negative refraction of acoustic surface waves at their\ninterface. Agreement between the graph model, full wave simulations, and\nexperiments bolsters quantum graph theory as a new paradigm for metamaterial\ndesign."
                },
                "authors": [
                    {
                        "name": "T. M. Lawrie"
                    },
                    {
                        "name": "T. A. Starkey"
                    },
                    {
                        "name": "G. Tanner"
                    },
                    {
                        "name": "D. B. Moore"
                    },
                    {
                        "name": "P. Savage"
                    },
                    {
                        "name": "G. J. Chaplain"
                    }
                ],
                "author_detail": {
                    "name": "G. J. Chaplain"
                },
                "author": "G. J. Chaplain",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07133v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07133v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.MP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07132v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07132v1",
                "updated": "2024-09-11T09:29:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    9,
                    29,
                    28,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T09:29:28Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    9,
                    29,
                    28,
                    2,
                    255,
                    0
                ],
                "title": "LLM-based feature generation from text for interpretable machine\n  learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based feature generation from text for interpretable machine\n  learning"
                },
                "summary": "Existing text representations such as embeddings and bag-of-words are not\nsuitable for rule learning due to their high dimensionality and absent or\nquestionable feature-level interpretability. This article explores whether\nlarge language models (LLMs) could address this by extracting a small number of\ninterpretable features from text. We demonstrate this process on two datasets\n(CORD-19 and M17+) containing several thousand scientific articles from\nmultiple disciplines and a target being a proxy for research impact. An\nevaluation based on testing for the statistically significant correlation with\nresearch impact has shown that LLama 2-generated features are semantically\nmeaningful. We consequently used these generated features in text\nclassification to predict the binary target variable representing the citation\nrate for the CORD-19 dataset and the ordinal 5-class target representing an\nexpert-awarded grade in the M17+ dataset. Machine-learning models trained on\nthe LLM-generated features provided similar predictive performance to the\nstate-of-the-art embedding model SciBERT for scientific text. The LLM used only\n62 features compared to 768 features in SciBERT embeddings, and these features\nwere directly interpretable, corresponding to notions such as article\nmethodological rigor, novelty, or grammatical correctness. As the final step,\nwe extract a small number of well-interpretable action rules. Consistently\ncompetitive results obtained with the same LLM feature set across both\nthematically diverse datasets show that this approach generalizes across\ndomains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing text representations such as embeddings and bag-of-words are not\nsuitable for rule learning due to their high dimensionality and absent or\nquestionable feature-level interpretability. This article explores whether\nlarge language models (LLMs) could address this by extracting a small number of\ninterpretable features from text. We demonstrate this process on two datasets\n(CORD-19 and M17+) containing several thousand scientific articles from\nmultiple disciplines and a target being a proxy for research impact. An\nevaluation based on testing for the statistically significant correlation with\nresearch impact has shown that LLama 2-generated features are semantically\nmeaningful. We consequently used these generated features in text\nclassification to predict the binary target variable representing the citation\nrate for the CORD-19 dataset and the ordinal 5-class target representing an\nexpert-awarded grade in the M17+ dataset. Machine-learning models trained on\nthe LLM-generated features provided similar predictive performance to the\nstate-of-the-art embedding model SciBERT for scientific text. The LLM used only\n62 features compared to 768 features in SciBERT embeddings, and these features\nwere directly interpretable, corresponding to notions such as article\nmethodological rigor, novelty, or grammatical correctness. As the final step,\nwe extract a small number of well-interpretable action rules. Consistently\ncompetitive results obtained with the same LLM feature set across both\nthematically diverse datasets show that this approach generalizes across\ndomains."
                },
                "authors": [
                    {
                        "name": "Vojtěch Balek"
                    },
                    {
                        "name": "Lukáš Sýkora"
                    },
                    {
                        "name": "Vilém Sklenák"
                    },
                    {
                        "name": "Tomáš Kliegr"
                    }
                ],
                "author_detail": {
                    "name": "Tomáš Kliegr"
                },
                "author": "Tomáš Kliegr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07132v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07132v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07131v1",
                "updated": "2024-09-11T09:27:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    9,
                    27,
                    50,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T09:27:50Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    9,
                    27,
                    50,
                    2,
                    255,
                    0
                ],
                "title": "Reranking Laws for Language Generation: A Communication-Theoretic\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reranking Laws for Language Generation: A Communication-Theoretic\n  Perspective"
                },
                "summary": "To ensure large language models (LLMs) are used safely, one must reduce their\npropensity to hallucinate or to generate unacceptable answers. A simple and\noften used strategy is to first let the LLM generate multiple hypotheses and\nthen employ a reranker to choose the best one. In this paper, we draw a\nparallel between this strategy and the use of redundancy to decrease the error\nrate in noisy communication channels. We conceptualize the generator as a\nsender transmitting multiple descriptions of a message through parallel noisy\nchannels. The receiver decodes the message by ranking the (potentially\ncorrupted) descriptions and selecting the one found to be most reliable. We\nprovide conditions under which this protocol is asymptotically error-free\n(i.e., yields an acceptable answer almost surely) even in scenarios where the\nreranker is imperfect (governed by Mallows or Zipf-Mandelbrot models) and the\nchannel distributions are statistically dependent. We use our framework to\nobtain reranking laws which we validate empirically on two real-world tasks\nusing LLMs: text-to-code generation with DeepSeek-Coder 7B and machine\ntranslation of medical data with TowerInstruct 13B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To ensure large language models (LLMs) are used safely, one must reduce their\npropensity to hallucinate or to generate unacceptable answers. A simple and\noften used strategy is to first let the LLM generate multiple hypotheses and\nthen employ a reranker to choose the best one. In this paper, we draw a\nparallel between this strategy and the use of redundancy to decrease the error\nrate in noisy communication channels. We conceptualize the generator as a\nsender transmitting multiple descriptions of a message through parallel noisy\nchannels. The receiver decodes the message by ranking the (potentially\ncorrupted) descriptions and selecting the one found to be most reliable. We\nprovide conditions under which this protocol is asymptotically error-free\n(i.e., yields an acceptable answer almost surely) even in scenarios where the\nreranker is imperfect (governed by Mallows or Zipf-Mandelbrot models) and the\nchannel distributions are statistically dependent. We use our framework to\nobtain reranking laws which we validate empirically on two real-world tasks\nusing LLMs: text-to-code generation with DeepSeek-Coder 7B and machine\ntranslation of medical data with TowerInstruct 13B."
                },
                "authors": [
                    {
                        "name": "António Farinhas"
                    },
                    {
                        "name": "Haau-Sing Li"
                    },
                    {
                        "name": "André F. T. Martins"
                    }
                ],
                "author_detail": {
                    "name": "André F. T. Martins"
                },
                "author": "André F. T. Martins",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07123v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07123v1",
                "updated": "2024-09-11T09:21:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    9,
                    21,
                    20,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T09:21:20Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    9,
                    21,
                    20,
                    2,
                    255,
                    0
                ],
                "title": "Cross-Refine: Improving Natural Language Explanation Generation by\n  Learning in Tandem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Refine: Improving Natural Language Explanation Generation by\n  Learning in Tandem"
                },
                "summary": "Natural language explanations (NLEs) are vital for elucidating the reasoning\nbehind large language model (LLM) decisions. Many techniques have been\ndeveloped to generate NLEs using LLMs. However, like humans, LLMs might not\nalways produce optimal NLEs on first attempt. Inspired by human learning\nprocesses, we introduce Cross-Refine, which employs role modeling by deploying\ntwo LLMs as generator and critic, respectively. The generator outputs a first\nNLE and then refines this initial explanation using feedback and suggestions\nprovided by the critic. Cross-Refine does not require any supervised training\ndata or additional training. We validate Cross-Refine across three NLP tasks\nusing three state-of-the-art open-source LLMs through automatic and human\nevaluation. We select Self-Refine (Madaan et al., 2023) as the baseline, which\nonly utilizes self-feedback to refine the explanations. Our findings from\nautomatic evaluation and a user study indicate that Cross-Refine outperforms\nSelf-Refine. Meanwhile, Cross-Refine can perform effectively with less powerful\nLLMs, whereas Self-Refine only yields strong results with ChatGPT.\nAdditionally, we conduct an ablation study to assess the importance of feedback\nand suggestions. Both of them play an important role in refining explanations.\nWe further evaluate Cross-Refine on a bilingual dataset in English and German.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural language explanations (NLEs) are vital for elucidating the reasoning\nbehind large language model (LLM) decisions. Many techniques have been\ndeveloped to generate NLEs using LLMs. However, like humans, LLMs might not\nalways produce optimal NLEs on first attempt. Inspired by human learning\nprocesses, we introduce Cross-Refine, which employs role modeling by deploying\ntwo LLMs as generator and critic, respectively. The generator outputs a first\nNLE and then refines this initial explanation using feedback and suggestions\nprovided by the critic. Cross-Refine does not require any supervised training\ndata or additional training. We validate Cross-Refine across three NLP tasks\nusing three state-of-the-art open-source LLMs through automatic and human\nevaluation. We select Self-Refine (Madaan et al., 2023) as the baseline, which\nonly utilizes self-feedback to refine the explanations. Our findings from\nautomatic evaluation and a user study indicate that Cross-Refine outperforms\nSelf-Refine. Meanwhile, Cross-Refine can perform effectively with less powerful\nLLMs, whereas Self-Refine only yields strong results with ChatGPT.\nAdditionally, we conduct an ablation study to assess the importance of feedback\nand suggestions. Both of them play an important role in refining explanations.\nWe further evaluate Cross-Refine on a bilingual dataset in English and German."
                },
                "authors": [
                    {
                        "name": "Qianli Wang"
                    },
                    {
                        "name": "Tatiana Anikina"
                    },
                    {
                        "name": "Nils Feldhus"
                    },
                    {
                        "name": "Simon Ostermann"
                    },
                    {
                        "name": "Sebastian Möller"
                    },
                    {
                        "name": "Vera Schmitt"
                    }
                ],
                "author_detail": {
                    "name": "Vera Schmitt"
                },
                "author": "Vera Schmitt",
                "arxiv_comment": "17 pages; under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07123v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07123v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07110v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07110v1",
                "updated": "2024-09-11T08:56:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    8,
                    56,
                    27,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T08:56:27Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    8,
                    56,
                    27,
                    2,
                    255,
                    0
                ],
                "title": "Bio-Eng-LMM AI Assist chatbot: A Comprehensive Tool for Research and\n  Education",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bio-Eng-LMM AI Assist chatbot: A Comprehensive Tool for Research and\n  Education"
                },
                "summary": "This article introduces Bio-Eng-LMM AI chatbot, a versatile platform designed\nto enhance user interaction for educational and research purposes. Leveraging\ncutting-edge open-source Large Language Models (LLMs), Bio-Eng-LMM operates as\na sophisticated AI assistant, exploiting the capabilities of traditional models\nlike ChatGPT. Central to Bio-Eng-LMM is its implementation of Retrieval\nAugmented Generation (RAG) through three primary methods: integration of\npreprocessed documents, real-time processing of user-uploaded files, and\ninformation retrieval from any specified website. Additionally, the chatbot\nincorporates image generation via a Stable Diffusion Model (SDM), image\nunderstanding and response generation through LLAVA, and search functionality\non the internet powered by secure search engine such as DuckDuckGo. To provide\ncomprehensive support, Bio-Eng-LMM offers text summarization, website content\nsummarization, and both text and voice interaction. The chatbot maintains\nsession memory to ensure contextually relevant and coherent responses. This\nintegrated platform builds upon the strengths of RAG-GPT and Web-Based RAG\nQuery (WBRQ) where the system fetches relevant information directly from the\nweb to enhance the LLMs response generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This article introduces Bio-Eng-LMM AI chatbot, a versatile platform designed\nto enhance user interaction for educational and research purposes. Leveraging\ncutting-edge open-source Large Language Models (LLMs), Bio-Eng-LMM operates as\na sophisticated AI assistant, exploiting the capabilities of traditional models\nlike ChatGPT. Central to Bio-Eng-LMM is its implementation of Retrieval\nAugmented Generation (RAG) through three primary methods: integration of\npreprocessed documents, real-time processing of user-uploaded files, and\ninformation retrieval from any specified website. Additionally, the chatbot\nincorporates image generation via a Stable Diffusion Model (SDM), image\nunderstanding and response generation through LLAVA, and search functionality\non the internet powered by secure search engine such as DuckDuckGo. To provide\ncomprehensive support, Bio-Eng-LMM offers text summarization, website content\nsummarization, and both text and voice interaction. The chatbot maintains\nsession memory to ensure contextually relevant and coherent responses. This\nintegrated platform builds upon the strengths of RAG-GPT and Web-Based RAG\nQuery (WBRQ) where the system fetches relevant information directly from the\nweb to enhance the LLMs response generation."
                },
                "authors": [
                    {
                        "name": "Ali Forootani"
                    },
                    {
                        "name": "Danial Esmaeili Aliabadi"
                    },
                    {
                        "name": "Daniela Thraen"
                    }
                ],
                "author_detail": {
                    "name": "Daniela Thraen"
                },
                "author": "Daniela Thraen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07110v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07110v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07101v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07101v1",
                "updated": "2024-09-11T08:45:12Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    8,
                    45,
                    12,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T08:45:12Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    8,
                    45,
                    12,
                    2,
                    255,
                    0
                ],
                "title": "Statistical Finite Elements via Interacting Particle Langevin Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical Finite Elements via Interacting Particle Langevin Dynamics"
                },
                "summary": "In this paper, we develop a class of interacting particle Langevin algorithms\nto solve inverse problems for partial differential equations (PDEs). In\nparticular, we leverage the statistical finite elements (statFEM) formulation\nto obtain a finite-dimensional latent variable statistical model where the\nparameter is that of the (discretised) forward map and the latent variable is\nthe statFEM solution of the PDE which is assumed to be partially observed. We\nthen adapt a recently proposed expectation-maximisation like scheme,\ninteracting particle Langevin algorithm (IPLA), for this problem and obtain a\njoint estimation procedure for the parameters and the latent variables. We\nconsider three main examples: (i) estimating the forcing for linear Poisson\nPDE, (ii) estimating the forcing for nonlinear Poisson PDE, and (iii)\nestimating diffusivity for linear Poisson PDE. We provide computational\ncomplexity estimates for forcing estimation in the linear case. We also provide\ncomprehensive numerical experiments and preconditioning strategies that\nsignificantly improve the performance, showing that the proposed class of\nmethods can be the choice for parameter inference in PDE models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we develop a class of interacting particle Langevin algorithms\nto solve inverse problems for partial differential equations (PDEs). In\nparticular, we leverage the statistical finite elements (statFEM) formulation\nto obtain a finite-dimensional latent variable statistical model where the\nparameter is that of the (discretised) forward map and the latent variable is\nthe statFEM solution of the PDE which is assumed to be partially observed. We\nthen adapt a recently proposed expectation-maximisation like scheme,\ninteracting particle Langevin algorithm (IPLA), for this problem and obtain a\njoint estimation procedure for the parameters and the latent variables. We\nconsider three main examples: (i) estimating the forcing for linear Poisson\nPDE, (ii) estimating the forcing for nonlinear Poisson PDE, and (iii)\nestimating diffusivity for linear Poisson PDE. We provide computational\ncomplexity estimates for forcing estimation in the linear case. We also provide\ncomprehensive numerical experiments and preconditioning strategies that\nsignificantly improve the performance, showing that the proposed class of\nmethods can be the choice for parameter inference in PDE models."
                },
                "authors": [
                    {
                        "name": "Alex Glyn-Davies"
                    },
                    {
                        "name": "Connor Duffin"
                    },
                    {
                        "name": "Ieva Kazlauskaite"
                    },
                    {
                        "name": "Mark Girolami"
                    },
                    {
                        "name": "Ö. Deniz Akyildiz"
                    }
                ],
                "author_detail": {
                    "name": "Ö. Deniz Akyildiz"
                },
                "author": "Ö. Deniz Akyildiz",
                "arxiv_comment": "23 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07101v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07101v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07100v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07100v1",
                "updated": "2024-09-11T08:44:10Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    8,
                    44,
                    10,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T08:44:10Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    8,
                    44,
                    10,
                    2,
                    255,
                    0
                ],
                "title": "Fast Medical Shape Reconstruction via Meta-learned Implicit Neural\n  Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Medical Shape Reconstruction via Meta-learned Implicit Neural\n  Representations"
                },
                "summary": "Efficient and fast reconstruction of anatomical structures plays a crucial\nrole in clinical practice. Minimizing retrieval and processing times not only\npotentially enhances swift response and decision-making in critical scenarios\nbut also supports interactive surgical planning and navigation. Recent methods\nattempt to solve the medical shape reconstruction problem by utilizing implicit\nneural functions. However, their performance suffers in terms of generalization\nand computation time, a critical metric for real-time applications. To address\nthese challenges, we propose to leverage meta-learning to improve the network\nparameters initialization, reducing inference time by an order of magnitude\nwhile maintaining high accuracy. We evaluate our approach on three public\ndatasets covering different anatomical shapes and modalities, namely CT and\nMRI. Our experimental results show that our model can handle various input\nconfigurations, such as sparse slices with different orientations and spacings.\nAdditionally, we demonstrate that our method exhibits strong transferable\ncapabilities in generalizing to shape domains unobserved at training time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient and fast reconstruction of anatomical structures plays a crucial\nrole in clinical practice. Minimizing retrieval and processing times not only\npotentially enhances swift response and decision-making in critical scenarios\nbut also supports interactive surgical planning and navigation. Recent methods\nattempt to solve the medical shape reconstruction problem by utilizing implicit\nneural functions. However, their performance suffers in terms of generalization\nand computation time, a critical metric for real-time applications. To address\nthese challenges, we propose to leverage meta-learning to improve the network\nparameters initialization, reducing inference time by an order of magnitude\nwhile maintaining high accuracy. We evaluate our approach on three public\ndatasets covering different anatomical shapes and modalities, namely CT and\nMRI. Our experimental results show that our model can handle various input\nconfigurations, such as sparse slices with different orientations and spacings.\nAdditionally, we demonstrate that our method exhibits strong transferable\ncapabilities in generalizing to shape domains unobserved at training time."
                },
                "authors": [
                    {
                        "name": "Gaia Romana De Paolis"
                    },
                    {
                        "name": "Dimitrios Lenis"
                    },
                    {
                        "name": "Johannes Novotny"
                    },
                    {
                        "name": "Maria Wimmer"
                    },
                    {
                        "name": "Astrid Berg"
                    },
                    {
                        "name": "Theresa Neubauer"
                    },
                    {
                        "name": "Philip Matthias Winter"
                    },
                    {
                        "name": "David Major"
                    },
                    {
                        "name": "Ariharasudhan Muthusami"
                    },
                    {
                        "name": "Gerald Schröcker"
                    },
                    {
                        "name": "Martin Mienkina"
                    },
                    {
                        "name": "Katja Bühler"
                    }
                ],
                "author_detail": {
                    "name": "Katja Bühler"
                },
                "author": "Katja Bühler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07100v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07100v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07091v1",
                "updated": "2024-09-11T08:24:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    8,
                    24,
                    34,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T08:24:34Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    8,
                    24,
                    34,
                    2,
                    255,
                    0
                ],
                "title": "Learning Task Specifications from Demonstrations as Probabilistic\n  Automata",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Task Specifications from Demonstrations as Probabilistic\n  Automata"
                },
                "summary": "Specifying tasks for robotic systems traditionally requires coding expertise,\ndeep domain knowledge, and significant time investment. While learning from\ndemonstration offers a promising alternative, existing methods often struggle\nwith tasks of longer horizons. To address this limitation, we introduce a\ncomputationally efficient approach for learning probabilistic deterministic\nfinite automata (PDFA) that capture task structures and expert preferences\ndirectly from demonstrations. Our approach infers sub-goals and their temporal\ndependencies, producing an interpretable task specification that domain experts\ncan easily understand and adjust. We validate our method through experiments\ninvolving object manipulation tasks, showcasing how our method enables a robot\narm to effectively replicate diverse expert strategies while adapting to\nchanging conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Specifying tasks for robotic systems traditionally requires coding expertise,\ndeep domain knowledge, and significant time investment. While learning from\ndemonstration offers a promising alternative, existing methods often struggle\nwith tasks of longer horizons. To address this limitation, we introduce a\ncomputationally efficient approach for learning probabilistic deterministic\nfinite automata (PDFA) that capture task structures and expert preferences\ndirectly from demonstrations. Our approach infers sub-goals and their temporal\ndependencies, producing an interpretable task specification that domain experts\ncan easily understand and adjust. We validate our method through experiments\ninvolving object manipulation tasks, showcasing how our method enables a robot\narm to effectively replicate diverse expert strategies while adapting to\nchanging conditions."
                },
                "authors": [
                    {
                        "name": "Mattijs Baert"
                    },
                    {
                        "name": "Sam Leroux"
                    },
                    {
                        "name": "Pieter Simoens"
                    }
                ],
                "author_detail": {
                    "name": "Pieter Simoens"
                },
                "author": "Pieter Simoens",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.16352v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.16352v2",
                "updated": "2024-09-11T08:23:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    8,
                    23,
                    58,
                    2,
                    255,
                    0
                ],
                "published": "2024-02-26T07:17:25Z",
                "published_parsed": [
                    2024,
                    2,
                    26,
                    7,
                    17,
                    25,
                    0,
                    57,
                    0
                ],
                "title": "MathGenie: Generating Synthetic Data with Question Back-translation for\n  Enhancing Mathematical Reasoning of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MathGenie: Generating Synthetic Data with Question Back-translation for\n  Enhancing Mathematical Reasoning of LLMs"
                },
                "summary": "Large language models (LLMs) have exhibited great potential in mathematical\nreasoning. However, there remains a performance gap in this area between\nexisting open-source models and closed-source models such as GPT-4. In this\npaper, we introduce MathGenie, a novel method for generating diverse and\nreliable math problems from a small-scale problem-solution dataset (denoted as\nseed data). We augment the ground-truth solutions of our seed data and train a\nback-translation model to translate the augmented solutions back into new\nquestions. Subsequently, we generate code-integrated solutions for the new\nquestions. To ensure the correctness of the code-integrated solutions, we\nemploy rationale-based strategy for solution verification. Various pretrained\nmodels, ranging from 7B to 70B, are trained on the newly curated data to test\nthe effectiveness of the proposed augmentation technique, resulting in a family\nof models known as MathGenieLM. These models consistently outperform previous\nopen-source models across five representative mathematical reasoning datasets,\nachieving state-of-the-art performance. In particular, MathGenieLM-InternLM2\nachieves an accuracy of 87.7% on GSM8K and 55.7% on MATH, securing the best\noverall score among open-source language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have exhibited great potential in mathematical\nreasoning. However, there remains a performance gap in this area between\nexisting open-source models and closed-source models such as GPT-4. In this\npaper, we introduce MathGenie, a novel method for generating diverse and\nreliable math problems from a small-scale problem-solution dataset (denoted as\nseed data). We augment the ground-truth solutions of our seed data and train a\nback-translation model to translate the augmented solutions back into new\nquestions. Subsequently, we generate code-integrated solutions for the new\nquestions. To ensure the correctness of the code-integrated solutions, we\nemploy rationale-based strategy for solution verification. Various pretrained\nmodels, ranging from 7B to 70B, are trained on the newly curated data to test\nthe effectiveness of the proposed augmentation technique, resulting in a family\nof models known as MathGenieLM. These models consistently outperform previous\nopen-source models across five representative mathematical reasoning datasets,\nachieving state-of-the-art performance. In particular, MathGenieLM-InternLM2\nachieves an accuracy of 87.7% on GSM8K and 55.7% on MATH, securing the best\noverall score among open-source language models."
                },
                "authors": [
                    {
                        "name": "Zimu Lu"
                    },
                    {
                        "name": "Aojun Zhou"
                    },
                    {
                        "name": "Houxing Ren"
                    },
                    {
                        "name": "Ke Wang"
                    },
                    {
                        "name": "Weikang Shi"
                    },
                    {
                        "name": "Junting Pan"
                    },
                    {
                        "name": "Mingjie Zhan"
                    },
                    {
                        "name": "Hongsheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongsheng Li"
                },
                "author": "Hongsheng Li",
                "arxiv_comment": "ACL 2024 camera ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.16352v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.16352v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07088v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07088v1",
                "updated": "2024-09-11T08:16:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    8,
                    16,
                    20,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T08:16:20Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    8,
                    16,
                    20,
                    2,
                    255,
                    0
                ],
                "title": "Ontology-Free General-Domain Knowledge Graph-to-Text Generation Dataset\n  Synthesis using Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ontology-Free General-Domain Knowledge Graph-to-Text Generation Dataset\n  Synthesis using Large Language Model"
                },
                "summary": "Knowledge Graph-to-Text (G2T) generation involves verbalizing structured\nknowledge graphs into natural language text. Recent advancements in Pretrained\nLanguage Models (PLMs) have improved G2T performance, but their effectiveness\ndepends on datasets with precise graph-text alignment. However, the scarcity of\nhigh-quality, general-domain G2T generation datasets restricts progress in the\ngeneral-domain G2T generation research. To address this issue, we introduce\nWikipedia Ontology-Free Graph-text dataset (WikiOFGraph), a new large-scale G2T\ndataset generated using a novel method that leverages Large Language Model\n(LLM) and Data-QuestEval. Our new dataset, which contains 5.85M general-domain\ngraph-text pairs, offers high graph-text consistency without relying on\nexternal ontologies. Experimental results demonstrate that PLM fine-tuned on\nWikiOFGraph outperforms those trained on other datasets across various\nevaluation metrics. Our method proves to be a scalable and effective solution\nfor generating high-quality G2T data, significantly advancing the field of G2T\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Graph-to-Text (G2T) generation involves verbalizing structured\nknowledge graphs into natural language text. Recent advancements in Pretrained\nLanguage Models (PLMs) have improved G2T performance, but their effectiveness\ndepends on datasets with precise graph-text alignment. However, the scarcity of\nhigh-quality, general-domain G2T generation datasets restricts progress in the\ngeneral-domain G2T generation research. To address this issue, we introduce\nWikipedia Ontology-Free Graph-text dataset (WikiOFGraph), a new large-scale G2T\ndataset generated using a novel method that leverages Large Language Model\n(LLM) and Data-QuestEval. Our new dataset, which contains 5.85M general-domain\ngraph-text pairs, offers high graph-text consistency without relying on\nexternal ontologies. Experimental results demonstrate that PLM fine-tuned on\nWikiOFGraph outperforms those trained on other datasets across various\nevaluation metrics. Our method proves to be a scalable and effective solution\nfor generating high-quality G2T data, significantly advancing the field of G2T\ngeneration."
                },
                "authors": [
                    {
                        "name": "Daehee Kim"
                    },
                    {
                        "name": "Deokhyung Kang"
                    },
                    {
                        "name": "Sangwon Ryu"
                    },
                    {
                        "name": "Gary Geunbae Lee"
                    }
                ],
                "author_detail": {
                    "name": "Gary Geunbae Lee"
                },
                "author": "Gary Geunbae Lee",
                "arxiv_comment": "18 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07088v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07088v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07085v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07085v1",
                "updated": "2024-09-11T08:11:16Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    8,
                    11,
                    16,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T08:11:16Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    8,
                    11,
                    16,
                    2,
                    255,
                    0
                ],
                "title": "Understanding Knowledge Drift in LLMs through Misinformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Knowledge Drift in LLMs through Misinformation"
                },
                "summary": "Large Language Models (LLMs) have revolutionized numerous applications,\nmaking them an integral part of our digital ecosystem. However, their\nreliability becomes critical, especially when these models are exposed to\nmisinformation. We primarily analyze the susceptibility of state-of-the-art\nLLMs to factual inaccuracies when they encounter false information in a QnA\nscenario, an issue that can lead to a phenomenon we refer to as *knowledge\ndrift*, which significantly undermines the trustworthiness of these models. We\nevaluate the factuality and the uncertainty of the models' responses relying on\nEntropy, Perplexity, and Token Probability metrics. Our experiments reveal that\nan LLM's uncertainty can increase up to 56.6% when the question is answered\nincorrectly due to the exposure to false information. At the same time,\nrepeated exposure to the same false information can decrease the models\nuncertainty again (-52.8% w.r.t. the answers on the untainted prompts),\npotentially manipulating the underlying model's beliefs and introducing a drift\nfrom its original knowledge. These findings provide insights into LLMs'\nrobustness and vulnerability to adversarial inputs, paving the way for\ndeveloping more reliable LLM applications across various domains. The code is\navailable at https://github.com/afastowski/knowledge_drift.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized numerous applications,\nmaking them an integral part of our digital ecosystem. However, their\nreliability becomes critical, especially when these models are exposed to\nmisinformation. We primarily analyze the susceptibility of state-of-the-art\nLLMs to factual inaccuracies when they encounter false information in a QnA\nscenario, an issue that can lead to a phenomenon we refer to as *knowledge\ndrift*, which significantly undermines the trustworthiness of these models. We\nevaluate the factuality and the uncertainty of the models' responses relying on\nEntropy, Perplexity, and Token Probability metrics. Our experiments reveal that\nan LLM's uncertainty can increase up to 56.6% when the question is answered\nincorrectly due to the exposure to false information. At the same time,\nrepeated exposure to the same false information can decrease the models\nuncertainty again (-52.8% w.r.t. the answers on the untainted prompts),\npotentially manipulating the underlying model's beliefs and introducing a drift\nfrom its original knowledge. These findings provide insights into LLMs'\nrobustness and vulnerability to adversarial inputs, paving the way for\ndeveloping more reliable LLM applications across various domains. The code is\navailable at https://github.com/afastowski/knowledge_drift."
                },
                "authors": [
                    {
                        "name": "Alina Fastowski"
                    },
                    {
                        "name": "Gjergji Kasneci"
                    }
                ],
                "author_detail": {
                    "name": "Gjergji Kasneci"
                },
                "author": "Gjergji Kasneci",
                "arxiv_comment": "13 pages, 3 figures. Accepted at DELTA workshop at KDD 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07085v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07085v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00060v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00060v2",
                "updated": "2024-09-11T07:51:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    7,
                    51,
                    43,
                    2,
                    255,
                    0
                ],
                "published": "2024-08-22T04:25:06Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    4,
                    25,
                    6,
                    3,
                    235,
                    0
                ],
                "title": "Understanding Literary Texts by LLMs: A Case Study of Ancient Chinese\n  Poetry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Literary Texts by LLMs: A Case Study of Ancient Chinese\n  Poetry"
                },
                "summary": "The birth and rapid development of large language models (LLMs) have caused\nquite a stir in the field of literature. Once considered unattainable, AI's\nrole in literary creation is increasingly becoming a reality. In genres such as\npoetry, jokes, and short stories, numerous AI tools have emerged, offering\nrefreshing new perspectives. However, it's difficult to further improve the\nquality of these works. This is primarily because understanding and\nappreciating a good literary work involves a considerable threshold, such as\nknowledge of literary theory, aesthetic sensibility, interdisciplinary\nknowledge. Therefore, authoritative data in this area is quite lacking.\nAdditionally, evaluating literary works is often complex and hard to fully\nquantify, which directly hinders the further development of AI creation.\n  To address this issue, this paper attempts to explore the mysteries of\nliterary texts from the perspective of LLMs, using ancient Chinese poetry as an\nexample for experimentation. First, we collected a variety of ancient poems\nfrom different sources and had experts annotate a small portion of them. Then,\nwe designed a range of comprehension metrics based on LLMs to evaluate all\nthese poems. Finally, we analyzed the correlations and differences between\nvarious poem collections to identify literary patterns. Through our\nexperiments, we observed a series of enlightening phenomena that provide\ntechnical support for the future development of high-level literary creation\nbased on LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The birth and rapid development of large language models (LLMs) have caused\nquite a stir in the field of literature. Once considered unattainable, AI's\nrole in literary creation is increasingly becoming a reality. In genres such as\npoetry, jokes, and short stories, numerous AI tools have emerged, offering\nrefreshing new perspectives. However, it's difficult to further improve the\nquality of these works. This is primarily because understanding and\nappreciating a good literary work involves a considerable threshold, such as\nknowledge of literary theory, aesthetic sensibility, interdisciplinary\nknowledge. Therefore, authoritative data in this area is quite lacking.\nAdditionally, evaluating literary works is often complex and hard to fully\nquantify, which directly hinders the further development of AI creation.\n  To address this issue, this paper attempts to explore the mysteries of\nliterary texts from the perspective of LLMs, using ancient Chinese poetry as an\nexample for experimentation. First, we collected a variety of ancient poems\nfrom different sources and had experts annotate a small portion of them. Then,\nwe designed a range of comprehension metrics based on LLMs to evaluate all\nthese poems. Finally, we analyzed the correlations and differences between\nvarious poem collections to identify literary patterns. Through our\nexperiments, we observed a series of enlightening phenomena that provide\ntechnical support for the future development of high-level literary creation\nbased on LLMs."
                },
                "authors": [
                    {
                        "name": "Cheng Zhao"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Zhen Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhen Wang"
                },
                "author": "Zhen Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00060v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00060v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.06118v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.06118v4",
                "updated": "2024-09-11T07:48:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    7,
                    48,
                    26,
                    2,
                    255,
                    0
                ],
                "published": "2024-01-11T18:54:44Z",
                "published_parsed": [
                    2024,
                    1,
                    11,
                    18,
                    54,
                    44,
                    3,
                    11,
                    0
                ],
                "title": "Extreme Compression of Large Language Models via Additive Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extreme Compression of Large Language Models via Additive Quantization"
                },
                "summary": "The emergence of accurate open large language models (LLMs) has led to a race\ntowards performant quantization techniques which can enable their execution on\nend-user devices. In this paper, we revisit the problem of \"extreme\" LLM\ncompression-defined as targeting extremely low bit counts, such as 2 to 3 bits\nper parameter-from the point of view of classic methods in Multi-Codebook\nQuantization (MCQ). Our algorithm, called AQLM, generalizes the classic\nAdditive Quantization (AQ) approach for information retrieval to advance the\nstate-of-the-art in LLM compression, via two innovations: 1) learned additive\nquantization of weight matrices in input-adaptive fashion, and 2) joint\noptimization of codebook parameters across each transformer blocks. Broadly,\nAQLM is the first scheme that is Pareto optimal in terms of\naccuracy-vs-model-size when compressing to less than 3 bits per parameter, and\nsignificantly improves upon all known schemes in the extreme compression (2bit)\nregime. In addition, AQLM is practical: we provide fast GPU and CPU\nimplementations of AQLM for token generation, which enable us to match or\noutperform optimized FP16 implementations for speed, while executing in a much\nsmaller memory footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of accurate open large language models (LLMs) has led to a race\ntowards performant quantization techniques which can enable their execution on\nend-user devices. In this paper, we revisit the problem of \"extreme\" LLM\ncompression-defined as targeting extremely low bit counts, such as 2 to 3 bits\nper parameter-from the point of view of classic methods in Multi-Codebook\nQuantization (MCQ). Our algorithm, called AQLM, generalizes the classic\nAdditive Quantization (AQ) approach for information retrieval to advance the\nstate-of-the-art in LLM compression, via two innovations: 1) learned additive\nquantization of weight matrices in input-adaptive fashion, and 2) joint\noptimization of codebook parameters across each transformer blocks. Broadly,\nAQLM is the first scheme that is Pareto optimal in terms of\naccuracy-vs-model-size when compressing to less than 3 bits per parameter, and\nsignificantly improves upon all known schemes in the extreme compression (2bit)\nregime. In addition, AQLM is practical: we provide fast GPU and CPU\nimplementations of AQLM for token generation, which enable us to match or\noutperform optimized FP16 implementations for speed, while executing in a much\nsmaller memory footprint."
                },
                "authors": [
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Andrei Panferov"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Elias Frantar"
                    },
                    {
                        "name": "Artem Babenko"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "ICML, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.06118v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.06118v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07072v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07072v1",
                "updated": "2024-09-11T07:48:06Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    7,
                    48,
                    6,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T07:48:06Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    7,
                    48,
                    6,
                    2,
                    255,
                    0
                ],
                "title": "Latent Space Interpretation for Stylistic Analysis and Explainable\n  Authorship Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent Space Interpretation for Stylistic Analysis and Explainable\n  Authorship Attribution"
                },
                "summary": "Recent state-of-the-art authorship attribution methods learn authorship\nrepresentations of texts in a latent, non-interpretable space, hindering their\nusability in real-world applications. Our work proposes a novel approach to\ninterpreting these learned embeddings by identifying representative points in\nthe latent space and utilizing LLMs to generate informative natural language\ndescriptions of the writing style of each point. We evaluate the alignment of\nour interpretable space with the latent one and find that it achieves the best\nprediction agreement compared to other baselines. Additionally, we conduct a\nhuman evaluation to assess the quality of these style descriptions, validating\ntheir utility as explanations for the latent space. Finally, we investigate\nwhether human performance on the challenging AA task improves when aided by our\nsystem's explanations, finding an average improvement of around +20% in\naccuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent state-of-the-art authorship attribution methods learn authorship\nrepresentations of texts in a latent, non-interpretable space, hindering their\nusability in real-world applications. Our work proposes a novel approach to\ninterpreting these learned embeddings by identifying representative points in\nthe latent space and utilizing LLMs to generate informative natural language\ndescriptions of the writing style of each point. We evaluate the alignment of\nour interpretable space with the latent one and find that it achieves the best\nprediction agreement compared to other baselines. Additionally, we conduct a\nhuman evaluation to assess the quality of these style descriptions, validating\ntheir utility as explanations for the latent space. Finally, we investigate\nwhether human performance on the challenging AA task improves when aided by our\nsystem's explanations, finding an average improvement of around +20% in\naccuracy."
                },
                "authors": [
                    {
                        "name": "Milad Alshomary"
                    },
                    {
                        "name": "Narutatsu Ri"
                    },
                    {
                        "name": "Marianna Apidianaki"
                    },
                    {
                        "name": "Ajay Patel"
                    },
                    {
                        "name": "Smaranda Muresan"
                    },
                    {
                        "name": "Kathleen McKeown"
                    }
                ],
                "author_detail": {
                    "name": "Kathleen McKeown"
                },
                "author": "Kathleen McKeown",
                "arxiv_comment": "8 pages, 8 figures, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07072v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07072v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13257v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13257v2",
                "updated": "2024-09-11T07:42:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    7,
                    42,
                    11,
                    2,
                    255,
                    0
                ],
                "published": "2024-08-23T17:59:51Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    17,
                    59,
                    51,
                    4,
                    236,
                    0
                ],
                "title": "MME-RealWorld: Could Your Multimodal LLM Challenge High-Resolution\n  Real-World Scenarios that are Difficult for Humans?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MME-RealWorld: Could Your Multimodal LLM Challenge High-Resolution\n  Real-World Scenarios that are Difficult for Humans?"
                },
                "summary": "Comprehensive evaluation of Multimodal Large Language Models (MLLMs) has\nrecently garnered widespread attention in the research community. However, we\nobserve that existing benchmarks present several common barriers that make it\ndifficult to measure the significant challenges that models face in the real\nworld, including: 1) small data scale leads to a large performance variance; 2)\nreliance on model-based annotations results in restricted data quality; 3)\ninsufficient task difficulty, especially caused by the limited image\nresolution. To tackle these issues, we introduce MME-RealWorld. Specifically,\nwe collect more than $300$K images from public datasets and the Internet,\nfiltering $13,366$ high-quality images for annotation. This involves the\nefforts of professional $25$ annotators and $7$ experts in MLLMs, contributing\nto $29,429$ question-answer pairs that cover $43$ subtasks across $5$\nreal-world scenarios, extremely challenging even for humans. As far as we know,\nMME-RealWorld is the largest manually annotated benchmark to date, featuring\nthe highest resolution and a targeted focus on real-world applications. We\nfurther conduct a thorough evaluation involving $28$ prominent MLLMs, such as\nGPT-4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet. Our results show that even the\nmost advanced models struggle with our benchmarks, where none of them reach\n$60\\%$ accuracy. The challenges of perceiving high-resolution images and\nunderstanding complex real-world scenarios remain urgent issues to be\naddressed. The data and evaluation code are released at\nhttps://mme-realworld.github.io/ .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comprehensive evaluation of Multimodal Large Language Models (MLLMs) has\nrecently garnered widespread attention in the research community. However, we\nobserve that existing benchmarks present several common barriers that make it\ndifficult to measure the significant challenges that models face in the real\nworld, including: 1) small data scale leads to a large performance variance; 2)\nreliance on model-based annotations results in restricted data quality; 3)\ninsufficient task difficulty, especially caused by the limited image\nresolution. To tackle these issues, we introduce MME-RealWorld. Specifically,\nwe collect more than $300$K images from public datasets and the Internet,\nfiltering $13,366$ high-quality images for annotation. This involves the\nefforts of professional $25$ annotators and $7$ experts in MLLMs, contributing\nto $29,429$ question-answer pairs that cover $43$ subtasks across $5$\nreal-world scenarios, extremely challenging even for humans. As far as we know,\nMME-RealWorld is the largest manually annotated benchmark to date, featuring\nthe highest resolution and a targeted focus on real-world applications. We\nfurther conduct a thorough evaluation involving $28$ prominent MLLMs, such as\nGPT-4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet. Our results show that even the\nmost advanced models struggle with our benchmarks, where none of them reach\n$60\\%$ accuracy. The challenges of perceiving high-resolution images and\nunderstanding complex real-world scenarios remain urgent issues to be\naddressed. The data and evaluation code are released at\nhttps://mme-realworld.github.io/ ."
                },
                "authors": [
                    {
                        "name": "Yi-Fan Zhang"
                    },
                    {
                        "name": "Huanyu Zhang"
                    },
                    {
                        "name": "Haochen Tian"
                    },
                    {
                        "name": "Chaoyou Fu"
                    },
                    {
                        "name": "Shuangqing Zhang"
                    },
                    {
                        "name": "Junfei Wu"
                    },
                    {
                        "name": "Feng Li"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Qingsong Wen"
                    },
                    {
                        "name": "Zhang Zhang"
                    },
                    {
                        "name": "Liang Wang"
                    },
                    {
                        "name": "Rong Jin"
                    },
                    {
                        "name": "Tieniu Tan"
                    }
                ],
                "author_detail": {
                    "name": "Tieniu Tan"
                },
                "author": "Tieniu Tan",
                "arxiv_comment": "Project Page: https://mme-realworld.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13257v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13257v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.08011v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.08011v3",
                "updated": "2024-09-11T07:31:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    7,
                    31,
                    29,
                    2,
                    255,
                    0
                ],
                "published": "2024-05-10T18:05:37Z",
                "published_parsed": [
                    2024,
                    5,
                    10,
                    18,
                    5,
                    37,
                    4,
                    131,
                    0
                ],
                "title": "A Survey of Large Language Models for Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Large Language Models for Graphs"
                },
                "summary": "Graphs are an essential data structure utilized to represent relationships in\nreal-world scenarios. Prior research has established that Graph Neural Networks\n(GNNs) deliver impressive outcomes in graph-centric tasks, such as link\nprediction and node classification. Despite these advancements, challenges like\ndata sparsity and limited generalization capabilities continue to persist.\nRecently, Large Language Models (LLMs) have gained attention in natural\nlanguage processing. They excel in language comprehension and summarization.\nIntegrating LLMs with graph learning techniques has attracted interest as a way\nto enhance performance in graph learning tasks. In this survey, we conduct an\nin-depth review of the latest state-of-the-art LLMs applied in graph learning\nand introduce a novel taxonomy to categorize existing methods based on their\nframework design. We detail four unique designs: i) GNNs as Prefix, ii) LLMs as\nPrefix, iii) LLMs-Graphs Integration, and iv) LLMs-Only, highlighting key\nmethodologies within each category. We explore the strengths and limitations of\neach framework, and emphasize potential avenues for future research, including\novercoming current integration challenges between LLMs and graph learning\ntechniques, and venturing into new application areas. This survey aims to serve\nas a valuable resource for researchers and practitioners eager to leverage\nlarge language models in graph learning, and to inspire continued progress in\nthis dynamic field. We consistently maintain the related open-source materials\nat \\url{https://github.com/HKUDS/Awesome-LLM4Graph-Papers}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphs are an essential data structure utilized to represent relationships in\nreal-world scenarios. Prior research has established that Graph Neural Networks\n(GNNs) deliver impressive outcomes in graph-centric tasks, such as link\nprediction and node classification. Despite these advancements, challenges like\ndata sparsity and limited generalization capabilities continue to persist.\nRecently, Large Language Models (LLMs) have gained attention in natural\nlanguage processing. They excel in language comprehension and summarization.\nIntegrating LLMs with graph learning techniques has attracted interest as a way\nto enhance performance in graph learning tasks. In this survey, we conduct an\nin-depth review of the latest state-of-the-art LLMs applied in graph learning\nand introduce a novel taxonomy to categorize existing methods based on their\nframework design. We detail four unique designs: i) GNNs as Prefix, ii) LLMs as\nPrefix, iii) LLMs-Graphs Integration, and iv) LLMs-Only, highlighting key\nmethodologies within each category. We explore the strengths and limitations of\neach framework, and emphasize potential avenues for future research, including\novercoming current integration challenges between LLMs and graph learning\ntechniques, and venturing into new application areas. This survey aims to serve\nas a valuable resource for researchers and practitioners eager to leverage\nlarge language models in graph learning, and to inspire continued progress in\nthis dynamic field. We consistently maintain the related open-source materials\nat \\url{https://github.com/HKUDS/Awesome-LLM4Graph-Papers}."
                },
                "authors": [
                    {
                        "name": "Xubin Ren"
                    },
                    {
                        "name": "Jiabin Tang"
                    },
                    {
                        "name": "Dawei Yin"
                    },
                    {
                        "name": "Nitesh Chawla"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_doi": "10.1145/3637528.3671460",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3637528.3671460",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.08011v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.08011v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published as a KDD'24 survey paper",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.07212v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.07212v2",
                "updated": "2024-09-11T07:29:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    7,
                    29,
                    28,
                    2,
                    255,
                    0
                ],
                "published": "2023-09-13T18:00:00Z",
                "published_parsed": [
                    2023,
                    9,
                    13,
                    18,
                    0,
                    0,
                    2,
                    256,
                    0
                ],
                "title": "Probing New Physics with High-Redshift Quasars: Axions and Non-standard\n  Cosmology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing New Physics with High-Redshift Quasars: Axions and Non-standard\n  Cosmology"
                },
                "summary": "The Hubble diagram of quasars, as candidates to ``standardizable\" candles,\nhas been used to measure the expansion history of the Universe at late times,\nup to very high redshifts ($z \\sim 7$). It has been shown that this history, as\ninferred from the quasar dataset, deviates at $\\gtrsim 3 \\sigma$ level from the\nconcordance ($\\Lambda$CDM) cosmology model preferred by the cosmic microwave\nbackground (CMB) and other datasets. In this article, we investigate whether\nnew physics beyond $\\Lambda$CDM (B$\\Lambda$CDM) or beyond the Standard Model\n(BSM) could make the quasar data consistent with the concordance model. We\nfirst show that an effective redshift-dependent relation between the quasar UV\nand X-ray luminosities, complementing previous phenomenological work in the\nliterature, can potentially remedy the discrepancy. Such a redshift dependence\ncan be realized in a BSM model with axion-photon conversion in the\nintergalactic medium (IGM), although the preferred parameter space is {in\ntension with various other astrophysical constraints on axions, at a level}\ndepending on the specific assumptions made regarding the IGM magnetic field. We\nbriefly discuss a variation of the axion model that could evade these\nastrophysical constraints. On the other hand, we show that models beyond\n$\\Lambda$CDM such as one with a varying dark energy equation of state ($w$CDM)\nor the phenomenological cosmographic model with a polynomial expansion of the\nluminosity distance, cannot alleviate the tension. The code for our analysis,\nbased on \\texttt{emcee}~\\cite{Foreman_Mackey_2013} and\n\\texttt{corner.py}~\\cite{corner}, is publicly available at\n\\href{https://github.com/ChenSun-Phys/high\\_z\\_candles.git}{{\\tt\ngithub.com/ChenSun-Phys/high\\_z\\_candles}}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Hubble diagram of quasars, as candidates to ``standardizable\" candles,\nhas been used to measure the expansion history of the Universe at late times,\nup to very high redshifts ($z \\sim 7$). It has been shown that this history, as\ninferred from the quasar dataset, deviates at $\\gtrsim 3 \\sigma$ level from the\nconcordance ($\\Lambda$CDM) cosmology model preferred by the cosmic microwave\nbackground (CMB) and other datasets. In this article, we investigate whether\nnew physics beyond $\\Lambda$CDM (B$\\Lambda$CDM) or beyond the Standard Model\n(BSM) could make the quasar data consistent with the concordance model. We\nfirst show that an effective redshift-dependent relation between the quasar UV\nand X-ray luminosities, complementing previous phenomenological work in the\nliterature, can potentially remedy the discrepancy. Such a redshift dependence\ncan be realized in a BSM model with axion-photon conversion in the\nintergalactic medium (IGM), although the preferred parameter space is {in\ntension with various other astrophysical constraints on axions, at a level}\ndepending on the specific assumptions made regarding the IGM magnetic field. We\nbriefly discuss a variation of the axion model that could evade these\nastrophysical constraints. On the other hand, we show that models beyond\n$\\Lambda$CDM such as one with a varying dark energy equation of state ($w$CDM)\nor the phenomenological cosmographic model with a polynomial expansion of the\nluminosity distance, cannot alleviate the tension. The code for our analysis,\nbased on \\texttt{emcee}~\\cite{Foreman_Mackey_2013} and\n\\texttt{corner.py}~\\cite{corner}, is publicly available at\n\\href{https://github.com/ChenSun-Phys/high\\_z\\_candles.git}{{\\tt\ngithub.com/ChenSun-Phys/high\\_z\\_candles}}."
                },
                "authors": [
                    {
                        "name": "Chen Sun"
                    },
                    {
                        "name": "Manuel A. Buen-Abad"
                    },
                    {
                        "name": "JiJi Fan"
                    }
                ],
                "author_detail": {
                    "name": "JiJi Fan"
                },
                "author": "JiJi Fan",
                "arxiv_comment": "10+3 pages, 4 figures, 4 tables, 3 appendices; reference updated;\n  matched to the published version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.07212v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.07212v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.02219v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.02219v2",
                "updated": "2024-09-11T07:27:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    7,
                    27,
                    51,
                    2,
                    255,
                    0
                ],
                "published": "2024-05-03T16:25:27Z",
                "published_parsed": [
                    2024,
                    5,
                    3,
                    16,
                    25,
                    27,
                    4,
                    124,
                    0
                ],
                "title": "A Normative Framework for Benchmarking Consumer Fairness in Large\n  Language Model Recommender System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Normative Framework for Benchmarking Consumer Fairness in Large\n  Language Model Recommender System"
                },
                "summary": "The rapid adoption of large language models (LLMs) in recommender systems\n(RS) presents new challenges in understanding and evaluating their biases,\nwhich can result in unfairness or the amplification of stereotypes. Traditional\nfairness evaluations in RS primarily focus on collaborative filtering (CF)\nsettings, which may not fully capture the complexities of LLMs, as these models\noften inherit biases from large, unregulated data. This paper proposes a\nnormative framework to benchmark consumer fairness in LLM-powered recommender\nsystems (RecLLMs).\n  We critically examine how fairness norms in classical RS fall short in\naddressing the challenges posed by LLMs. We argue that this gap can lead to\narbitrary conclusions about fairness, and we propose a more structured, formal\napproach to evaluate fairness in such systems. Our experiments on the MovieLens\ndataset on consumer fairness, using in-context learning (zero-shot vs.\nfew-shot) reveal fairness deviations in age-based recommendations, particularly\nwhen additional contextual examples are introduced (ICL-2). Statistical\nsignificance tests confirm that these deviations are not random, highlighting\nthe need for robust evaluation methods. While this work offers a preliminary\ndiscussion on a proposed normative framework, our hope is that it could provide\na formal, principled approach for auditing and mitigating bias in RecLLMs. The\ncode and dataset used for this work will be shared at \"gihub-anonymized\".",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid adoption of large language models (LLMs) in recommender systems\n(RS) presents new challenges in understanding and evaluating their biases,\nwhich can result in unfairness or the amplification of stereotypes. Traditional\nfairness evaluations in RS primarily focus on collaborative filtering (CF)\nsettings, which may not fully capture the complexities of LLMs, as these models\noften inherit biases from large, unregulated data. This paper proposes a\nnormative framework to benchmark consumer fairness in LLM-powered recommender\nsystems (RecLLMs).\n  We critically examine how fairness norms in classical RS fall short in\naddressing the challenges posed by LLMs. We argue that this gap can lead to\narbitrary conclusions about fairness, and we propose a more structured, formal\napproach to evaluate fairness in such systems. Our experiments on the MovieLens\ndataset on consumer fairness, using in-context learning (zero-shot vs.\nfew-shot) reveal fairness deviations in age-based recommendations, particularly\nwhen additional contextual examples are introduced (ICL-2). Statistical\nsignificance tests confirm that these deviations are not random, highlighting\nthe need for robust evaluation methods. While this work offers a preliminary\ndiscussion on a proposed normative framework, our hope is that it could provide\na formal, principled approach for auditing and mitigating bias in RecLLMs. The\ncode and dataset used for this work will be shared at \"gihub-anonymized\"."
                },
                "authors": [
                    {
                        "name": "Yashar Deldjoo"
                    },
                    {
                        "name": "Fatemeh Nazary"
                    }
                ],
                "author_detail": {
                    "name": "Fatemeh Nazary"
                },
                "author": "Fatemeh Nazary",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.02219v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.02219v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13705v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13705v2",
                "updated": "2024-09-11T07:22:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    7,
                    22,
                    58,
                    2,
                    255,
                    0
                ],
                "published": "2024-08-15T02:42:05Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    2,
                    42,
                    5,
                    3,
                    228,
                    0
                ],
                "title": "Cross-Modal Denoising: A Novel Training Paradigm for Enhancing\n  Speech-Image Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Modal Denoising: A Novel Training Paradigm for Enhancing\n  Speech-Image Retrieval"
                },
                "summary": "The success of speech-image retrieval relies on establishing an effective\nalignment between speech and image. Existing methods often model cross-modal\ninteraction through simple cosine similarity of the global feature of each\nmodality, which fall short in capturing fine-grained details within modalities.\nTo address this issue, we introduce an effective framework and a novel learning\ntask named cross-modal denoising (CMD) to enhance cross-modal interaction to\nachieve finer-level cross-modal alignment. Specifically, CMD is a denoising\ntask designed to reconstruct semantic features from noisy features within one\nmodality by interacting features from another modality. Notably, CMD operates\nexclusively during model training and can be removed during inference without\nadding extra inference time. The experimental results demonstrate that our\nframework outperforms the state-of-the-art method by 2.0% in mean R@1 on the\nFlickr8k dataset and by 1.7% in mean R@1 on the SpokenCOCO dataset for the\nspeech-image retrieval tasks, respectively. These experimental results validate\nthe efficiency and effectiveness of our framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The success of speech-image retrieval relies on establishing an effective\nalignment between speech and image. Existing methods often model cross-modal\ninteraction through simple cosine similarity of the global feature of each\nmodality, which fall short in capturing fine-grained details within modalities.\nTo address this issue, we introduce an effective framework and a novel learning\ntask named cross-modal denoising (CMD) to enhance cross-modal interaction to\nachieve finer-level cross-modal alignment. Specifically, CMD is a denoising\ntask designed to reconstruct semantic features from noisy features within one\nmodality by interacting features from another modality. Notably, CMD operates\nexclusively during model training and can be removed during inference without\nadding extra inference time. The experimental results demonstrate that our\nframework outperforms the state-of-the-art method by 2.0% in mean R@1 on the\nFlickr8k dataset and by 1.7% in mean R@1 on the SpokenCOCO dataset for the\nspeech-image retrieval tasks, respectively. These experimental results validate\nthe efficiency and effectiveness of our framework."
                },
                "authors": [
                    {
                        "name": "Lifeng Zhou"
                    },
                    {
                        "name": "Yuke Li"
                    },
                    {
                        "name": "Rui Deng"
                    },
                    {
                        "name": "Yuting Yang"
                    },
                    {
                        "name": "Haoqi Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Haoqi Zhu"
                },
                "author": "Haoqi Zhu",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2408.13119",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13705v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13705v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07054v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07054v1",
                "updated": "2024-09-11T06:59:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    6,
                    59,
                    37,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T06:59:37Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    6,
                    59,
                    37,
                    2,
                    255,
                    0
                ],
                "title": "Native vs Non-Native Language Prompting: A Comparative Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Native vs Non-Native Language Prompting: A Comparative Analysis"
                },
                "summary": "Large language models (LLMs) have shown remarkable abilities in different\nfields, including standard Natural Language Processing (NLP) tasks. To elicit\nknowledge from LLMs, prompts play a key role, consisting of natural language\ninstructions. Most open and closed source LLMs are trained on available labeled\nand unlabeled resources--digital content such as text, images, audio, and\nvideos. Hence, these models have better knowledge for high-resourced languages\nbut struggle with low-resourced languages. Since prompts play a crucial role in\nunderstanding their capabilities, the language used for prompts remains an\nimportant research question. Although there has been significant research in\nthis area, it is still limited, and less has been explored for medium to\nlow-resourced languages. In this study, we investigate different prompting\nstrategies (native vs. non-native) on 11 different NLP tasks associated with 12\ndifferent Arabic datasets (9.7K data points). In total, we conducted 197\nexperiments involving 3 LLMs, 12 datasets, and 3 prompting strategies. Our\nfindings suggest that, on average, the non-native prompt performs the best,\nfollowed by mixed and native prompts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable abilities in different\nfields, including standard Natural Language Processing (NLP) tasks. To elicit\nknowledge from LLMs, prompts play a key role, consisting of natural language\ninstructions. Most open and closed source LLMs are trained on available labeled\nand unlabeled resources--digital content such as text, images, audio, and\nvideos. Hence, these models have better knowledge for high-resourced languages\nbut struggle with low-resourced languages. Since prompts play a crucial role in\nunderstanding their capabilities, the language used for prompts remains an\nimportant research question. Although there has been significant research in\nthis area, it is still limited, and less has been explored for medium to\nlow-resourced languages. In this study, we investigate different prompting\nstrategies (native vs. non-native) on 11 different NLP tasks associated with 12\ndifferent Arabic datasets (9.7K data points). In total, we conducted 197\nexperiments involving 3 LLMs, 12 datasets, and 3 prompting strategies. Our\nfindings suggest that, on average, the non-native prompt performs the best,\nfollowed by mixed and native prompts."
                },
                "authors": [
                    {
                        "name": "Mohamed Bayan Kmainasi"
                    },
                    {
                        "name": "Rakif Khan"
                    },
                    {
                        "name": "Ali Ezzat Shahroor"
                    },
                    {
                        "name": "Boushra Bendou"
                    },
                    {
                        "name": "Maram Hasanain"
                    },
                    {
                        "name": "Firoj Alam"
                    }
                ],
                "author_detail": {
                    "name": "Firoj Alam"
                },
                "author": "Firoj Alam",
                "arxiv_comment": "Foundation Models, Large Language Models, Arabic NLP, LLMs, Native,\n  Contextual Understanding, Arabic LLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07054v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07054v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07045v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07045v1",
                "updated": "2024-09-11T06:27:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    6,
                    27,
                    50,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T06:27:50Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    6,
                    27,
                    50,
                    2,
                    255,
                    0
                ],
                "title": "Beyond IID: Optimizing Instruction Learning from the Perspective of\n  Instruction Interaction and Dependency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond IID: Optimizing Instruction Learning from the Perspective of\n  Instruction Interaction and Dependency"
                },
                "summary": "With the availability of various instruction datasets, a pivotal challenge is\nhow to effectively select and integrate these instructions to fine-tune large\nlanguage models (LLMs). Previous research mainly focuses on selecting\nindividual high-quality instructions. However, these works overlooked the joint\ninteractions and dependencies between different categories of instructions,\nleading to suboptimal selection strategies. Moreover, the nature of these\ninteraction patterns remains largely unexplored, let alone optimize the\ninstruction set with regard to them. To fill these gaps, in this paper, we: (1)\nsystemically investigate interaction and dependency patterns between different\ncategories of instructions, (2) manage to optimize the instruction set\nconcerning the interaction patterns using a linear programming-based method,\nand optimize the learning schema of SFT using an instruction dependency\ntaxonomy guided curriculum learning. Experimental results across different LLMs\ndemonstrate improved performance over strong baselines on widely adopted\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the availability of various instruction datasets, a pivotal challenge is\nhow to effectively select and integrate these instructions to fine-tune large\nlanguage models (LLMs). Previous research mainly focuses on selecting\nindividual high-quality instructions. However, these works overlooked the joint\ninteractions and dependencies between different categories of instructions,\nleading to suboptimal selection strategies. Moreover, the nature of these\ninteraction patterns remains largely unexplored, let alone optimize the\ninstruction set with regard to them. To fill these gaps, in this paper, we: (1)\nsystemically investigate interaction and dependency patterns between different\ncategories of instructions, (2) manage to optimize the instruction set\nconcerning the interaction patterns using a linear programming-based method,\nand optimize the learning schema of SFT using an instruction dependency\ntaxonomy guided curriculum learning. Experimental results across different LLMs\ndemonstrate improved performance over strong baselines on widely adopted\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Hanyu Zhao"
                    },
                    {
                        "name": "Li Du"
                    },
                    {
                        "name": "Yiming Ju"
                    },
                    {
                        "name": "Chengwei Wu"
                    },
                    {
                        "name": "Tengfei Pan"
                    }
                ],
                "author_detail": {
                    "name": "Tengfei Pan"
                },
                "author": "Tengfei Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07045v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07045v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07025v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07025v1",
                "updated": "2024-09-11T05:42:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    5,
                    42,
                    1,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T05:42:01Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    5,
                    42,
                    1,
                    2,
                    255,
                    0
                ],
                "title": "CPSample: Classifier Protected Sampling for Guarding Training Data\n  During Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CPSample: Classifier Protected Sampling for Guarding Training Data\n  During Diffusion"
                },
                "summary": "Diffusion models have a tendency to exactly replicate their training data,\nespecially when trained on small datasets. Most prior work has sought to\nmitigate this problem by imposing differential privacy constraints or masking\nparts of the training data, resulting in a notable substantial decrease in\nimage quality. We present CPSample, a method that modifies the sampling process\nto prevent training data replication while preserving image quality. CPSample\nutilizes a classifier that is trained to overfit on random binary labels\nattached to the training data. CPSample then uses classifier guidance to steer\nthe generation process away from the set of points that can be classified with\nhigh certainty, a set that includes the training data. CPSample achieves FID\nscores of 4.97 and 2.97 on CIFAR-10 and CelebA-64, respectively, without\nproducing exact replicates of the training data. Unlike prior methods intended\nto guard the training images, CPSample only requires training a classifier\nrather than retraining a diffusion model, which is computationally cheaper.\nMoreover, our technique provides diffusion models with greater robustness\nagainst membership inference attacks, wherein an adversary attempts to discern\nwhich images were in the model's training dataset. We show that CPSample\nbehaves like a built-in rejection sampler, and we demonstrate its capabilities\nto prevent mode collapse in Stable Diffusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have a tendency to exactly replicate their training data,\nespecially when trained on small datasets. Most prior work has sought to\nmitigate this problem by imposing differential privacy constraints or masking\nparts of the training data, resulting in a notable substantial decrease in\nimage quality. We present CPSample, a method that modifies the sampling process\nto prevent training data replication while preserving image quality. CPSample\nutilizes a classifier that is trained to overfit on random binary labels\nattached to the training data. CPSample then uses classifier guidance to steer\nthe generation process away from the set of points that can be classified with\nhigh certainty, a set that includes the training data. CPSample achieves FID\nscores of 4.97 and 2.97 on CIFAR-10 and CelebA-64, respectively, without\nproducing exact replicates of the training data. Unlike prior methods intended\nto guard the training images, CPSample only requires training a classifier\nrather than retraining a diffusion model, which is computationally cheaper.\nMoreover, our technique provides diffusion models with greater robustness\nagainst membership inference attacks, wherein an adversary attempts to discern\nwhich images were in the model's training dataset. We show that CPSample\nbehaves like a built-in rejection sampler, and we demonstrate its capabilities\nto prevent mode collapse in Stable Diffusion."
                },
                "authors": [
                    {
                        "name": "Joshua Kazdan"
                    },
                    {
                        "name": "Hao Sun"
                    },
                    {
                        "name": "Jiaqi Han"
                    },
                    {
                        "name": "Felix Petersen"
                    },
                    {
                        "name": "Stefano Ermon"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Ermon"
                },
                "author": "Stefano Ermon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07025v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07025v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07020v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07020v1",
                "updated": "2024-09-11T05:26:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    5,
                    26,
                    23,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T05:26:23Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    5,
                    26,
                    23,
                    2,
                    255,
                    0
                ],
                "title": "EVENet: Evidence-based Ensemble Learning for Uncertainty-aware Brain\n  Parcellation Using Diffusion MRI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EVENet: Evidence-based Ensemble Learning for Uncertainty-aware Brain\n  Parcellation Using Diffusion MRI"
                },
                "summary": "In this study, we developed an Evidence-based Ensemble Neural Network, namely\nEVENet, for anatomical brain parcellation using diffusion MRI. The key\ninnovation of EVENet is the design of an evidential deep learning framework to\nquantify predictive uncertainty at each voxel during a single inference. Using\nEVENet, we obtained accurate parcellation and uncertainty estimates across\ndifferent datasets from healthy and clinical populations and with different\nimaging acquisitions. The overall network includes five parallel subnetworks,\nwhere each is dedicated to learning the FreeSurfer parcellation for a certain\ndiffusion MRI parameter. An evidence-based ensemble methodology is then\nproposed to fuse the individual outputs. We perform experimental evaluations on\nlarge-scale datasets from multiple imaging sources, including high-quality\ndiffusion MRI data from healthy adults and clinically diffusion MRI data from\nparticipants with various brain diseases (schizophrenia, bipolar disorder,\nattention-deficit/hyperactivity disorder, Parkinson's disease, cerebral small\nvessel disease, and neurosurgical patients with brain tumors). Compared to\nseveral state-of-the-art methods, our experimental results demonstrate highly\nimproved parcellation accuracy across the multiple testing datasets despite the\ndifferences in dMRI acquisition protocols and health conditions. Furthermore,\nthanks to the uncertainty estimation, our EVENet approach demonstrates a good\nability to detect abnormal brain regions in patients with lesions, enhancing\nthe interpretability and reliability of the segmentation results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we developed an Evidence-based Ensemble Neural Network, namely\nEVENet, for anatomical brain parcellation using diffusion MRI. The key\ninnovation of EVENet is the design of an evidential deep learning framework to\nquantify predictive uncertainty at each voxel during a single inference. Using\nEVENet, we obtained accurate parcellation and uncertainty estimates across\ndifferent datasets from healthy and clinical populations and with different\nimaging acquisitions. The overall network includes five parallel subnetworks,\nwhere each is dedicated to learning the FreeSurfer parcellation for a certain\ndiffusion MRI parameter. An evidence-based ensemble methodology is then\nproposed to fuse the individual outputs. We perform experimental evaluations on\nlarge-scale datasets from multiple imaging sources, including high-quality\ndiffusion MRI data from healthy adults and clinically diffusion MRI data from\nparticipants with various brain diseases (schizophrenia, bipolar disorder,\nattention-deficit/hyperactivity disorder, Parkinson's disease, cerebral small\nvessel disease, and neurosurgical patients with brain tumors). Compared to\nseveral state-of-the-art methods, our experimental results demonstrate highly\nimproved parcellation accuracy across the multiple testing datasets despite the\ndifferences in dMRI acquisition protocols and health conditions. Furthermore,\nthanks to the uncertainty estimation, our EVENet approach demonstrates a good\nability to detect abnormal brain regions in patients with lesions, enhancing\nthe interpretability and reliability of the segmentation results."
                },
                "authors": [
                    {
                        "name": "Chenjun Li"
                    },
                    {
                        "name": "Dian Yang"
                    },
                    {
                        "name": "Shun Yao"
                    },
                    {
                        "name": "Shuyue Wang"
                    },
                    {
                        "name": "Ye Wu"
                    },
                    {
                        "name": "Le Zhang"
                    },
                    {
                        "name": "Qiannuo Li"
                    },
                    {
                        "name": "Kang Ik Kevin Cho"
                    },
                    {
                        "name": "Johanna Seitz-Holland"
                    },
                    {
                        "name": "Lipeng Ning"
                    },
                    {
                        "name": "Jon Haitz Legarreta"
                    },
                    {
                        "name": "Yogesh Rathi"
                    },
                    {
                        "name": "Carl-Fredrik Westin"
                    },
                    {
                        "name": "Lauren J. O'Donnell"
                    },
                    {
                        "name": "Nir A. Sochen"
                    },
                    {
                        "name": "Ofer Pasternak"
                    },
                    {
                        "name": "Fan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Fan Zhang"
                },
                "author": "Fan Zhang",
                "arxiv_comment": "15 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07020v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07020v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08252v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08252v2",
                "updated": "2024-09-11T05:19:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    5,
                    19,
                    32,
                    2,
                    255,
                    0
                ],
                "published": "2024-08-15T16:47:59Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    16,
                    47,
                    59,
                    3,
                    228,
                    0
                ],
                "title": "Derivative-Free Guidance in Continuous and Discrete Diffusion Models\n  with Soft Value-Based Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Derivative-Free Guidance in Continuous and Discrete Diffusion Models\n  with Soft Value-Based Decoding"
                },
                "summary": "Diffusion models excel at capturing the natural design spaces of images,\nmolecules, DNA, RNA, and protein sequences. However, rather than merely\ngenerating designs that are natural, we often aim to optimize downstream reward\nfunctions while preserving the naturalness of these design spaces. Existing\nmethods for achieving this goal often require ``differentiable'' proxy models\n(\\textit{e.g.}, classifier guidance or DPS) or involve computationally\nexpensive fine-tuning of diffusion models (\\textit{e.g.}, classifier-free\nguidance, RL-based fine-tuning). In our work, we propose a new method to\naddress these challenges. Our algorithm is an iterative sampling method that\nintegrates soft value functions, which looks ahead to how intermediate noisy\nstates lead to high rewards in the future, into the standard inference\nprocedure of pre-trained diffusion models. Notably, our approach avoids\nfine-tuning generative models and eliminates the need to construct\ndifferentiable models. This enables us to (1) directly utilize\nnon-differentiable features/reward feedback, commonly used in many scientific\ndomains, and (2) apply our method to recent discrete diffusion models in a\nprincipled way. Finally, we demonstrate the effectiveness of our algorithm\nacross several domains, including image generation, molecule generation, and\nDNA/RNA sequence generation. The code is available at\n\\href{https://github.com/masa-ue/SVDD}{https://github.com/masa-ue/SVDD}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models excel at capturing the natural design spaces of images,\nmolecules, DNA, RNA, and protein sequences. However, rather than merely\ngenerating designs that are natural, we often aim to optimize downstream reward\nfunctions while preserving the naturalness of these design spaces. Existing\nmethods for achieving this goal often require ``differentiable'' proxy models\n(\\textit{e.g.}, classifier guidance or DPS) or involve computationally\nexpensive fine-tuning of diffusion models (\\textit{e.g.}, classifier-free\nguidance, RL-based fine-tuning). In our work, we propose a new method to\naddress these challenges. Our algorithm is an iterative sampling method that\nintegrates soft value functions, which looks ahead to how intermediate noisy\nstates lead to high rewards in the future, into the standard inference\nprocedure of pre-trained diffusion models. Notably, our approach avoids\nfine-tuning generative models and eliminates the need to construct\ndifferentiable models. This enables us to (1) directly utilize\nnon-differentiable features/reward feedback, commonly used in many scientific\ndomains, and (2) apply our method to recent discrete diffusion models in a\nprincipled way. Finally, we demonstrate the effectiveness of our algorithm\nacross several domains, including image generation, molecule generation, and\nDNA/RNA sequence generation. The code is available at\n\\href{https://github.com/masa-ue/SVDD}{https://github.com/masa-ue/SVDD}."
                },
                "authors": [
                    {
                        "name": "Xiner Li"
                    },
                    {
                        "name": "Yulai Zhao"
                    },
                    {
                        "name": "Chenyu Wang"
                    },
                    {
                        "name": "Gabriele Scalia"
                    },
                    {
                        "name": "Gokcen Eraslan"
                    },
                    {
                        "name": "Surag Nair"
                    },
                    {
                        "name": "Tommaso Biancalani"
                    },
                    {
                        "name": "Aviv Regev"
                    },
                    {
                        "name": "Sergey Levine"
                    },
                    {
                        "name": "Masatoshi Uehara"
                    }
                ],
                "author_detail": {
                    "name": "Masatoshi Uehara"
                },
                "author": "Masatoshi Uehara",
                "arxiv_comment": "The code is available at https://github.com/masa-ue/SVDD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08252v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08252v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.17602v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.17602v2",
                "updated": "2024-09-11T04:44:53Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    4,
                    44,
                    53,
                    2,
                    255,
                    0
                ],
                "published": "2023-10-26T17:26:22Z",
                "published_parsed": [
                    2023,
                    10,
                    26,
                    17,
                    26,
                    22,
                    3,
                    299,
                    0
                ],
                "title": "Simulation-based Inference of Reionization Parameters from 3D\n  Tomographic 21 cm Light-cone Images -- II: Application of Solid Harmonic\n  Wavelet Scattering Transform",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulation-based Inference of Reionization Parameters from 3D\n  Tomographic 21 cm Light-cone Images -- II: Application of Solid Harmonic\n  Wavelet Scattering Transform"
                },
                "summary": "The information regarding how the intergalactic medium is reionized by\nastrophysical sources is contained in the tomographic three-dimensional 21 cm\nimages from the epoch of reionization. In Zhao et al. (2022a) (\"Paper I\"), we\ndemonstrated for the first time that density estimation likelihood-free\ninference (DELFI) can be applied efficiently to perform a Bayesian inference of\nthe reionization parameters from the 21 cm images. Nevertheless, the 3D image\ndata needs to be compressed into informative summaries as the input of DELFI\nby, e.g., a trained 3D convolutional neural network (CNN) as in Paper I\n(DELFI-3D CNN). Here in this paper, we introduce an alternative data\ncompressor, the solid harmonic wavelet scattering transform (WST), which has a\nsimilar, yet fixed (i.e. no training), architecture to CNN, but we show that\nthis approach (i.e. solid harmonic WST with DELFI) outperforms earlier analyses\nbased on 3D 21 cm images using DELFI-3D CNN in terms of credible regions of\nparameters. Realistic effects, including thermal noise and residual foreground\nafter removal, are also applied to the mock observations from the Square\nKilometre Array (SKA). We show that under the same inference strategy using\nDELFI, the 21 cm image analysis with solid harmonic WST outperforms the 21 cm\npower spectrum analysis. This research serves as a proof of concept,\ndemonstrating the potential to harness the strengths of WST and\nsimulation-based inference to derive insights from future 21 cm light-cone\nimage data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The information regarding how the intergalactic medium is reionized by\nastrophysical sources is contained in the tomographic three-dimensional 21 cm\nimages from the epoch of reionization. In Zhao et al. (2022a) (\"Paper I\"), we\ndemonstrated for the first time that density estimation likelihood-free\ninference (DELFI) can be applied efficiently to perform a Bayesian inference of\nthe reionization parameters from the 21 cm images. Nevertheless, the 3D image\ndata needs to be compressed into informative summaries as the input of DELFI\nby, e.g., a trained 3D convolutional neural network (CNN) as in Paper I\n(DELFI-3D CNN). Here in this paper, we introduce an alternative data\ncompressor, the solid harmonic wavelet scattering transform (WST), which has a\nsimilar, yet fixed (i.e. no training), architecture to CNN, but we show that\nthis approach (i.e. solid harmonic WST with DELFI) outperforms earlier analyses\nbased on 3D 21 cm images using DELFI-3D CNN in terms of credible regions of\nparameters. Realistic effects, including thermal noise and residual foreground\nafter removal, are also applied to the mock observations from the Square\nKilometre Array (SKA). We show that under the same inference strategy using\nDELFI, the 21 cm image analysis with solid harmonic WST outperforms the 21 cm\npower spectrum analysis. This research serves as a proof of concept,\ndemonstrating the potential to harness the strengths of WST and\nsimulation-based inference to derive insights from future 21 cm light-cone\nimage data."
                },
                "authors": [
                    {
                        "name": "Xiaosheng Zhao"
                    },
                    {
                        "name": "Yi Mao"
                    },
                    {
                        "name": "Shifan Zuo"
                    },
                    {
                        "name": "Benjamin D. Wandelt"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin D. Wandelt"
                },
                "author": "Benjamin D. Wandelt",
                "arxiv_comment": "21 pages, 11 figures, 7 tables. Accepted for publication in ApJ.\n  Comments welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.17602v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.17602v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06998v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06998v1",
                "updated": "2024-09-11T04:13:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    4,
                    13,
                    39,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T04:13:39Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    4,
                    13,
                    39,
                    2,
                    255,
                    0
                ],
                "title": "Learning Personalized Scoping for Graph Neural Networks under\n  Heterophily",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Personalized Scoping for Graph Neural Networks under\n  Heterophily"
                },
                "summary": "Heterophilous graphs, where dissimilar nodes tend to connect, pose a\nchallenge for graph neural networks (GNNs) as their superior performance\ntypically comes from aggregating homophilous information. Increasing the GNN\ndepth can expand the scope (i.e., receptive field), potentially finding\nhomophily from the higher-order neighborhoods. However, uniformly expanding the\nscope results in subpar performance since real-world graphs often exhibit\nhomophily disparity between nodes. An ideal way is personalized scopes,\nallowing nodes to have varying scope sizes. Existing methods typically add\nnode-adaptive weights for each hop. Although expressive, they inevitably suffer\nfrom severe overfitting. To address this issue, we formalize personalized\nscoping as a separate scope classification problem that overcomes GNN\noverfitting in node classification. Specifically, we predict the optimal GNN\ndepth for each node. Our theoretical and empirical analysis suggests that\naccurately predicting the depth can significantly enhance generalization. We\nfurther propose Adaptive Scope (AS), a lightweight MLP-based approach that only\nparticipates in GNN inference. AS encodes structural patterns and predicts the\ndepth to select the best model for each node's prediction. Experimental results\nshow that AS is highly flexible with various GNN architectures across a wide\nrange of datasets while significantly improving accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterophilous graphs, where dissimilar nodes tend to connect, pose a\nchallenge for graph neural networks (GNNs) as their superior performance\ntypically comes from aggregating homophilous information. Increasing the GNN\ndepth can expand the scope (i.e., receptive field), potentially finding\nhomophily from the higher-order neighborhoods. However, uniformly expanding the\nscope results in subpar performance since real-world graphs often exhibit\nhomophily disparity between nodes. An ideal way is personalized scopes,\nallowing nodes to have varying scope sizes. Existing methods typically add\nnode-adaptive weights for each hop. Although expressive, they inevitably suffer\nfrom severe overfitting. To address this issue, we formalize personalized\nscoping as a separate scope classification problem that overcomes GNN\noverfitting in node classification. Specifically, we predict the optimal GNN\ndepth for each node. Our theoretical and empirical analysis suggests that\naccurately predicting the depth can significantly enhance generalization. We\nfurther propose Adaptive Scope (AS), a lightweight MLP-based approach that only\nparticipates in GNN inference. AS encodes structural patterns and predicts the\ndepth to select the best model for each node's prediction. Experimental results\nshow that AS is highly flexible with various GNN architectures across a wide\nrange of datasets while significantly improving accuracy."
                },
                "authors": [
                    {
                        "name": "Gangda Deng"
                    },
                    {
                        "name": "Hongkuan Zhou"
                    },
                    {
                        "name": "Rajgopal Kannan"
                    },
                    {
                        "name": "Viktor Prasanna"
                    }
                ],
                "author_detail": {
                    "name": "Viktor Prasanna"
                },
                "author": "Viktor Prasanna",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06998v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06998v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06992v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06992v1",
                "updated": "2024-09-11T03:51:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    3,
                    51,
                    34,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T03:51:34Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    3,
                    51,
                    34,
                    2,
                    255,
                    0
                ],
                "title": "Quantum-Train with Tensor Network Mapping Model and Distributed Circuit\n  Ansatz",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum-Train with Tensor Network Mapping Model and Distributed Circuit\n  Ansatz"
                },
                "summary": "In the Quantum-Train (QT) framework, mapping quantum state measurements to\nclassical neural network weights is a critical challenge that affects the\nscalability and efficiency of hybrid quantum-classical models. The traditional\nQT framework employs a multi-layer perceptron (MLP) for this task, but it\nstruggles with scalability and interpretability. To address these issues, we\npropose replacing the MLP with a tensor network-based model and introducing a\ndistributed circuit ansatz designed for large-scale quantum machine learning\nwith multiple small quantum processing unit nodes. This approach enhances\nscalability, efficiently represents high-dimensional data, and maintains a\ncompact model structure. Our enhanced QT framework retains the benefits of\nreduced parameter count and independence from quantum resources during\ninference. Experimental results on benchmark datasets demonstrate that the\ntensor network-based QT framework achieves competitive performance with\nimproved efficiency and generalization, offering a practical solution for\nscalable hybrid quantum-classical machine learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the Quantum-Train (QT) framework, mapping quantum state measurements to\nclassical neural network weights is a critical challenge that affects the\nscalability and efficiency of hybrid quantum-classical models. The traditional\nQT framework employs a multi-layer perceptron (MLP) for this task, but it\nstruggles with scalability and interpretability. To address these issues, we\npropose replacing the MLP with a tensor network-based model and introducing a\ndistributed circuit ansatz designed for large-scale quantum machine learning\nwith multiple small quantum processing unit nodes. This approach enhances\nscalability, efficiently represents high-dimensional data, and maintains a\ncompact model structure. Our enhanced QT framework retains the benefits of\nreduced parameter count and independence from quantum resources during\ninference. Experimental results on benchmark datasets demonstrate that the\ntensor network-based QT framework achieves competitive performance with\nimproved efficiency and generalization, offering a practical solution for\nscalable hybrid quantum-classical machine learning."
                },
                "authors": [
                    {
                        "name": "Chen-Yu Liu"
                    },
                    {
                        "name": "Chu-Hsuan Abraham Lin"
                    },
                    {
                        "name": "Kuan-Cheng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kuan-Cheng Chen"
                },
                "author": "Kuan-Cheng Chen",
                "arxiv_comment": "4 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06992v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06992v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06991v1",
                "updated": "2024-09-11T03:43:53Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    3,
                    43,
                    53,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T03:43:53Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    3,
                    43,
                    53,
                    2,
                    255,
                    0
                ],
                "title": "1M-Deepfakes Detection Challenge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "1M-Deepfakes Detection Challenge"
                },
                "summary": "The detection and localization of deepfake content, particularly when small\nfake segments are seamlessly mixed with real videos, remains a significant\nchallenge in the field of digital media security. Based on the recently\nreleased AV-Deepfake1M dataset, which contains more than 1 million manipulated\nvideos across more than 2,000 subjects, we introduce the 1M-Deepfakes Detection\nChallenge. This challenge is designed to engage the research community in\ndeveloping advanced methods for detecting and localizing deepfake manipulations\nwithin the large-scale high-realistic audio-visual dataset. The participants\ncan access the AV-Deepfake1M dataset and are required to submit their inference\nresults for evaluation across the metrics for detection or localization tasks.\nThe methodologies developed through the challenge will contribute to the\ndevelopment of next-generation deepfake detection and localization systems.\nEvaluation scripts, baseline models, and accompanying code will be available on\nhttps://github.com/ControlNet/AV-Deepfake1M.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The detection and localization of deepfake content, particularly when small\nfake segments are seamlessly mixed with real videos, remains a significant\nchallenge in the field of digital media security. Based on the recently\nreleased AV-Deepfake1M dataset, which contains more than 1 million manipulated\nvideos across more than 2,000 subjects, we introduce the 1M-Deepfakes Detection\nChallenge. This challenge is designed to engage the research community in\ndeveloping advanced methods for detecting and localizing deepfake manipulations\nwithin the large-scale high-realistic audio-visual dataset. The participants\ncan access the AV-Deepfake1M dataset and are required to submit their inference\nresults for evaluation across the metrics for detection or localization tasks.\nThe methodologies developed through the challenge will contribute to the\ndevelopment of next-generation deepfake detection and localization systems.\nEvaluation scripts, baseline models, and accompanying code will be available on\nhttps://github.com/ControlNet/AV-Deepfake1M."
                },
                "authors": [
                    {
                        "name": "Zhixi Cai"
                    },
                    {
                        "name": "Abhinav Dhall"
                    },
                    {
                        "name": "Shreya Ghosh"
                    },
                    {
                        "name": "Munawar Hayat"
                    },
                    {
                        "name": "Dimitrios Kollias"
                    },
                    {
                        "name": "Kalin Stefanov"
                    },
                    {
                        "name": "Usman Tariq"
                    }
                ],
                "author_detail": {
                    "name": "Usman Tariq"
                },
                "author": "Usman Tariq",
                "arxiv_comment": "ACM MM 2024. Challenge webpage: https://deepfakes1m.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06979v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06979v1",
                "updated": "2024-09-11T03:12:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    3,
                    12,
                    18,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T03:12:18Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    3,
                    12,
                    18,
                    2,
                    255,
                    0
                ],
                "title": "A High-Performance List Decoding Algorithm for Surface Codes with\n  Erroneous Syndrome",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A High-Performance List Decoding Algorithm for Surface Codes with\n  Erroneous Syndrome"
                },
                "summary": "Quantum error-correcting codes (QECCs) are necessary for fault-tolerant\nquantum computation. Surface codes are a class of topological QECCs that have\nattracted significant attention due to their exceptional error-correcting\ncapabilities and easy implementation. In the decoding process of surface codes,\nthe syndromes are crucial for error correction, though they are not always\ncorrectly measured. Most of the existing decoding algorithms for surface codes\nare not equipped to handle erroneous syndrome information or need additional\nmeasurements to correct syndromes with errors, which implies a potential\nincrease in inference complexity and decoding latency. In this paper, we\npropose a high-performance list decoding algorithm for surface codes with\nerroneous syndromes. More specifically, to cope with erroneous syndrome\ninformation, we incorporate syndrome soft information, allowing the syndrome to\nbe listed as well. To enhance the efficiency of the list decoding algorithm, we\nuse LCOSD, which can significantly reduce the average list size in classical\nerror correction compared with the conventional ordered statistics decoding\n(OSD). Numerical results demonstrate that our proposed algorithm significantly\nimproves the decoding performance of surface codes with erroneous syndromes\ncompared to minimum-weight perfect matching (MWPM) and BP decoders.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum error-correcting codes (QECCs) are necessary for fault-tolerant\nquantum computation. Surface codes are a class of topological QECCs that have\nattracted significant attention due to their exceptional error-correcting\ncapabilities and easy implementation. In the decoding process of surface codes,\nthe syndromes are crucial for error correction, though they are not always\ncorrectly measured. Most of the existing decoding algorithms for surface codes\nare not equipped to handle erroneous syndrome information or need additional\nmeasurements to correct syndromes with errors, which implies a potential\nincrease in inference complexity and decoding latency. In this paper, we\npropose a high-performance list decoding algorithm for surface codes with\nerroneous syndromes. More specifically, to cope with erroneous syndrome\ninformation, we incorporate syndrome soft information, allowing the syndrome to\nbe listed as well. To enhance the efficiency of the list decoding algorithm, we\nuse LCOSD, which can significantly reduce the average list size in classical\nerror correction compared with the conventional ordered statistics decoding\n(OSD). Numerical results demonstrate that our proposed algorithm significantly\nimproves the decoding performance of surface codes with erroneous syndromes\ncompared to minimum-weight perfect matching (MWPM) and BP decoders."
                },
                "authors": [
                    {
                        "name": "Jifan Liang"
                    },
                    {
                        "name": "Qianfan Wang"
                    },
                    {
                        "name": "Lvzhou Li"
                    },
                    {
                        "name": "Xiao Ma"
                    }
                ],
                "author_detail": {
                    "name": "Xiao Ma"
                },
                "author": "Xiao Ma",
                "arxiv_comment": "17 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06979v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06979v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06978v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06978v1",
                "updated": "2024-09-11T03:09:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    3,
                    9,
                    55,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T03:09:55Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    3,
                    9,
                    55,
                    2,
                    255,
                    0
                ],
                "title": "Large Language Models and the Extended Church-Turing Thesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models and the Extended Church-Turing Thesis"
                },
                "summary": "The Extended Church-Turing Thesis (ECTT) posits that all effective\ninformation processing, including unbounded and non-uniform interactive\ncomputations, can be described in terms of interactive Turing machines with\nadvice. Does this assertion also apply to the abilities of contemporary large\nlanguage models (LLMs)? From a broader perspective, this question calls for an\ninvestigation of the computational power of LLMs by the classical means of\ncomputability and computational complexity theory, especially the theory of\nautomata. Along these lines, we establish a number of fundamental results.\nFirstly, we argue that any fixed (non-adaptive) LLM is computationally\nequivalent to a, possibly very large, deterministic finite-state transducer.\nThis characterizes the base level of LLMs. We extend this to a key result\nconcerning the simulation of space-bounded Turing machines by LLMs. Secondly,\nwe show that lineages of evolving LLMs are computationally equivalent to\ninteractive Turing machines with advice. The latter finding confirms the\nvalidity of the ECTT for lineages of LLMs. From a computability viewpoint, it\nalso suggests that lineages of LLMs possess super-Turing computational power.\nConsequently, in our computational model knowledge generation is in general a\nnon-algorithmic process realized by lineages of LLMs. Finally, we discuss the\nmerits of our findings in the broader context of several related disciplines\nand philosophies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Extended Church-Turing Thesis (ECTT) posits that all effective\ninformation processing, including unbounded and non-uniform interactive\ncomputations, can be described in terms of interactive Turing machines with\nadvice. Does this assertion also apply to the abilities of contemporary large\nlanguage models (LLMs)? From a broader perspective, this question calls for an\ninvestigation of the computational power of LLMs by the classical means of\ncomputability and computational complexity theory, especially the theory of\nautomata. Along these lines, we establish a number of fundamental results.\nFirstly, we argue that any fixed (non-adaptive) LLM is computationally\nequivalent to a, possibly very large, deterministic finite-state transducer.\nThis characterizes the base level of LLMs. We extend this to a key result\nconcerning the simulation of space-bounded Turing machines by LLMs. Secondly,\nwe show that lineages of evolving LLMs are computationally equivalent to\ninteractive Turing machines with advice. The latter finding confirms the\nvalidity of the ECTT for lineages of LLMs. From a computability viewpoint, it\nalso suggests that lineages of LLMs possess super-Turing computational power.\nConsequently, in our computational model knowledge generation is in general a\nnon-algorithmic process realized by lineages of LLMs. Finally, we discuss the\nmerits of our findings in the broader context of several related disciplines\nand philosophies."
                },
                "authors": [
                    {
                        "name": "Jiří Wiedermann"
                    },
                    {
                        "name": "Jan van Leeuwen"
                    }
                ],
                "author_detail": {
                    "name": "Jan van Leeuwen"
                },
                "author": "Jan van Leeuwen",
                "arxiv_doi": "10.4204/EPTCS.407.14",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.4204/EPTCS.407.14",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.06978v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06978v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings NCMA 2024, arXiv:2409.06120",
                "arxiv_journal_ref": "EPTCS 407, 2024, pp. 198-213",
                "arxiv_primary_category": {
                    "term": "cs.FL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06957v1",
                "updated": "2024-09-11T02:40:38Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    2,
                    40,
                    38,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T02:40:38Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    2,
                    40,
                    38,
                    2,
                    255,
                    0
                ],
                "title": "Policy Filtration in RLHF to Fine-Tune LLM for Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Policy Filtration in RLHF to Fine-Tune LLM for Code Generation"
                },
                "summary": "Reinforcement learning from human feedback (RLHF) is one of the key\ntechniques that helps large language models (LLMs) to follow instructions and\nprovide helpful and harmless responses. While direct policy optimization\nmethods exist, state-of-the-art LLMs adopt RL-based methods (usually PPO) in\nRLHF to train the policy to generate good responses guided by a reward model\nlearned from preference data. The main challenge of these methods is the\ninaccuracy of the intermediate reward model, especially in code generation\ntasks that require long and complex reasoning to score a response. We find that\nthe reliability of the reward model varies across responses assigned with\ndifferent rewards. This motivates us to filter the samples whose rewards may be\nunreliable to improve signal-to-noise ratio during policy learning, resulting\nin Policy Filtration for Proximal Policy Optimization (PF-PPO). To choose a\nproper policy filtration strategy for a given reward model, the coefficient of\ndetermination ($R^2$) between rewards and actual scores on filtered samples\nserves as a good metrics and helps us find several promising strategies. We\nprovide extensive experiments to validate the effectiveness of PF-PPO in code\ngeneration tasks, and find that some variants of PF-PPO are highly effective\nand achieve new state-of-the-art performance across 7-billion-parameter models\non HumanEval, MBPP, and a new and more challenging LeetCode Contest benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning from human feedback (RLHF) is one of the key\ntechniques that helps large language models (LLMs) to follow instructions and\nprovide helpful and harmless responses. While direct policy optimization\nmethods exist, state-of-the-art LLMs adopt RL-based methods (usually PPO) in\nRLHF to train the policy to generate good responses guided by a reward model\nlearned from preference data. The main challenge of these methods is the\ninaccuracy of the intermediate reward model, especially in code generation\ntasks that require long and complex reasoning to score a response. We find that\nthe reliability of the reward model varies across responses assigned with\ndifferent rewards. This motivates us to filter the samples whose rewards may be\nunreliable to improve signal-to-noise ratio during policy learning, resulting\nin Policy Filtration for Proximal Policy Optimization (PF-PPO). To choose a\nproper policy filtration strategy for a given reward model, the coefficient of\ndetermination ($R^2$) between rewards and actual scores on filtered samples\nserves as a good metrics and helps us find several promising strategies. We\nprovide extensive experiments to validate the effectiveness of PF-PPO in code\ngeneration tasks, and find that some variants of PF-PPO are highly effective\nand achieve new state-of-the-art performance across 7-billion-parameter models\non HumanEval, MBPP, and a new and more challenging LeetCode Contest benchmark."
                },
                "authors": [
                    {
                        "name": "Wei Shen"
                    },
                    {
                        "name": "Chuheng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chuheng Zhang"
                },
                "author": "Chuheng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06955v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06955v1",
                "updated": "2024-09-11T02:36:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    2,
                    36,
                    36,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T02:36:36Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    2,
                    36,
                    36,
                    2,
                    255,
                    0
                ],
                "title": "Privacy-Preserving Federated Learning with Consistency via Knowledge\n  Distillation Using Conditional Generator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy-Preserving Federated Learning with Consistency via Knowledge\n  Distillation Using Conditional Generator"
                },
                "summary": "Federated Learning (FL) is gaining popularity as a distributed learning\nframework that only shares model parameters or gradient updates and keeps\nprivate data locally. However, FL is at risk of privacy leakage caused by\nprivacy inference attacks. And most existing privacy-preserving mechanisms in\nFL conflict with achieving high performance and efficiency. Therefore, we\npropose FedMD-CG, a novel FL method with highly competitive performance and\nhigh-level privacy preservation, which decouples each client's local model into\na feature extractor and a classifier, and utilizes a conditional generator\ninstead of the feature extractor to perform server-side model aggregation. To\nensure the consistency of local generators and classifiers, FedMD-CG leverages\nknowledge distillation to train local models and generators at both the latent\nfeature level and the logit level. Also, we construct additional classification\nlosses and design new diversity losses to enhance client-side training.\nFedMD-CG is robust to data heterogeneity and does not require training extra\ndiscriminators (like cGAN). We conduct extensive experiments on various image\nclassification tasks to validate the superiority of FedMD-CG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) is gaining popularity as a distributed learning\nframework that only shares model parameters or gradient updates and keeps\nprivate data locally. However, FL is at risk of privacy leakage caused by\nprivacy inference attacks. And most existing privacy-preserving mechanisms in\nFL conflict with achieving high performance and efficiency. Therefore, we\npropose FedMD-CG, a novel FL method with highly competitive performance and\nhigh-level privacy preservation, which decouples each client's local model into\na feature extractor and a classifier, and utilizes a conditional generator\ninstead of the feature extractor to perform server-side model aggregation. To\nensure the consistency of local generators and classifiers, FedMD-CG leverages\nknowledge distillation to train local models and generators at both the latent\nfeature level and the logit level. Also, we construct additional classification\nlosses and design new diversity losses to enhance client-side training.\nFedMD-CG is robust to data heterogeneity and does not require training extra\ndiscriminators (like cGAN). We conduct extensive experiments on various image\nclassification tasks to validate the superiority of FedMD-CG."
                },
                "authors": [
                    {
                        "name": "Kangyang Luo"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Yunshi Lan"
                    },
                    {
                        "name": "Ming Gao"
                    },
                    {
                        "name": "Jinlong Shu"
                    }
                ],
                "author_detail": {
                    "name": "Jinlong Shu"
                },
                "author": "Jinlong Shu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06955v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06955v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.11504v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.11504v3",
                "updated": "2024-09-11T02:22:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    2,
                    22,
                    58,
                    2,
                    255,
                    0
                ],
                "published": "2024-01-21T14:28:41Z",
                "published_parsed": [
                    2024,
                    1,
                    21,
                    14,
                    28,
                    41,
                    6,
                    21,
                    0
                ],
                "title": "With Greater Text Comes Greater Necessity: Inference-Time Training Helps\n  Long Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With Greater Text Comes Greater Necessity: Inference-Time Training Helps\n  Long Text Generation"
                },
                "summary": "Long text generation, such as novel writing and discourse-level translation\nwith extremely long contexts, presents significant challenges to current\nlanguage models. Existing methods mainly focus on extending the model's context\nwindow through strategies like length extrapolation. However, these approaches\ndemand substantial hardware resources during the training and/or inference\nphases. Our proposed method, Temp-Lora, introduces an alternative concept.\nInstead of relying on the KV cache to store all context information, we embeds\nthis information directly into a temporary Lora module. In the process of long\ntext generation, this module is progressively trained with text generated\npreviously. This approach not only efficiently preserves contextual knowledge\nbut also prevents any permanent alteration to the model's parameters given that\nthe module is discarded post-generation. Extensive experiments on the PG19\nlanguage modeling benchmark and the GuoFeng discourse-level translation\nbenchmark validate the effectiveness of Temp-Lora. Our results show that: 1)\nTemp-Lora substantially enhances generation quality for long text, as indicated\nby a 13.2% decrease in perplexity (PPL) on a subset of PG19, and a 29.3%\ndecrease in PPL along with a 113.2% increase in BLEU score on a subset of\nGuoFeng, 2) Temp-Lora is compatible with and enhances most existing long text\ngeneration methods, and 3) Temp-Lora can greatly reduce computational costs by\nshortening the context window. For example, we can ensure a moderate\nimprovement in generation quality (a decrease of 3.8% in PPL) while enabling a\n51.5% memory usage reduction and a 60.0% decrease in latency for inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long text generation, such as novel writing and discourse-level translation\nwith extremely long contexts, presents significant challenges to current\nlanguage models. Existing methods mainly focus on extending the model's context\nwindow through strategies like length extrapolation. However, these approaches\ndemand substantial hardware resources during the training and/or inference\nphases. Our proposed method, Temp-Lora, introduces an alternative concept.\nInstead of relying on the KV cache to store all context information, we embeds\nthis information directly into a temporary Lora module. In the process of long\ntext generation, this module is progressively trained with text generated\npreviously. This approach not only efficiently preserves contextual knowledge\nbut also prevents any permanent alteration to the model's parameters given that\nthe module is discarded post-generation. Extensive experiments on the PG19\nlanguage modeling benchmark and the GuoFeng discourse-level translation\nbenchmark validate the effectiveness of Temp-Lora. Our results show that: 1)\nTemp-Lora substantially enhances generation quality for long text, as indicated\nby a 13.2% decrease in perplexity (PPL) on a subset of PG19, and a 29.3%\ndecrease in PPL along with a 113.2% increase in BLEU score on a subset of\nGuoFeng, 2) Temp-Lora is compatible with and enhances most existing long text\ngeneration methods, and 3) Temp-Lora can greatly reduce computational costs by\nshortening the context window. For example, we can ensure a moderate\nimprovement in generation quality (a decrease of 3.8% in PPL) while enabling a\n51.5% memory usage reduction and a 60.0% decrease in latency for inference."
                },
                "authors": [
                    {
                        "name": "Y. Wang"
                    },
                    {
                        "name": "D. Ma"
                    },
                    {
                        "name": "D. Cai"
                    }
                ],
                "author_detail": {
                    "name": "D. Cai"
                },
                "author": "D. Cai",
                "arxiv_comment": "COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.11504v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.11504v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06949v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06949v1",
                "updated": "2024-09-11T02:03:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    2,
                    3,
                    51,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T02:03:51Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    2,
                    3,
                    51,
                    2,
                    255,
                    0
                ],
                "title": "You Have Thirteen Hours in Which to Solve the Labyrinth: Enhancing AI\n  Game Masters with Function Calling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "You Have Thirteen Hours in Which to Solve the Labyrinth: Enhancing AI\n  Game Masters with Function Calling"
                },
                "summary": "Developing a consistent and reliable AI game master for text-based games is a\nchallenging task due to the limitations of large language models (LLMs) and the\ncomplexity of the game master's role. This paper presents a novel approach to\nenhance AI game masters by leveraging function calling in the context of the\ntable-top role-playing game \"Jim Henson's Labyrinth: The Adventure Game.\" Our\nmethodology involves integrating game-specific controls through functions,\nwhich we show improves the narrative quality and state update consistency of\nthe AI game master. The experimental results, based on human evaluations and\nunit tests, demonstrate the effectiveness of our approach in enhancing gameplay\nexperience and maintaining coherence with the game state. This work contributes\nto the advancement of game AI and interactive storytelling, offering insights\ninto the design of more engaging and consistent AI-driven game masters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing a consistent and reliable AI game master for text-based games is a\nchallenging task due to the limitations of large language models (LLMs) and the\ncomplexity of the game master's role. This paper presents a novel approach to\nenhance AI game masters by leveraging function calling in the context of the\ntable-top role-playing game \"Jim Henson's Labyrinth: The Adventure Game.\" Our\nmethodology involves integrating game-specific controls through functions,\nwhich we show improves the narrative quality and state update consistency of\nthe AI game master. The experimental results, based on human evaluations and\nunit tests, demonstrate the effectiveness of our approach in enhancing gameplay\nexperience and maintaining coherence with the game state. This work contributes\nto the advancement of game AI and interactive storytelling, offering insights\ninto the design of more engaging and consistent AI-driven game masters."
                },
                "authors": [
                    {
                        "name": "Jaewoo Song"
                    },
                    {
                        "name": "Andrew Zhu"
                    },
                    {
                        "name": "Chris Callison-Burch"
                    }
                ],
                "author_detail": {
                    "name": "Chris Callison-Burch"
                },
                "author": "Chris Callison-Burch",
                "arxiv_comment": "Wordplay Workshop @ ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06949v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06949v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06945v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06945v1",
                "updated": "2024-09-11T01:55:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    1,
                    55,
                    45,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T01:55:45Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    1,
                    55,
                    45,
                    2,
                    255,
                    0
                ],
                "title": "FSMDet: Vision-guided feature diffusion for fully sparse 3D detector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FSMDet: Vision-guided feature diffusion for fully sparse 3D detector"
                },
                "summary": "Fully sparse 3D detection has attracted an increasing interest in the recent\nyears. However, the sparsity of the features in these frameworks challenges the\ngeneration of proposals because of the limited diffusion process. In addition,\nthe quest for efficiency has led to only few work on vision-assisted fully\nsparse models. In this paper, we propose FSMDet (Fully Sparse Multi-modal\nDetection), which use visual information to guide the LiDAR feature diffusion\nprocess while still maintaining the efficiency of the pipeline. Specifically,\nmost of fully sparse works focus on complex customized center fusion\ndiffusion/regression operators. However, we observed that if the adequate\nobject completion is performed, even the simplest interpolation operator leads\nto satisfactory results. Inspired by this observation, we split the\nvision-guided diffusion process into two modules: a Shape Recover Layer\n(SRLayer) and a Self Diffusion Layer (SDLayer). The former uses RGB information\nto recover the shape of the visible part of an object, and the latter uses a\nvisual prior to further spread the features to the center region. Experiments\ndemonstrate that our approach successfully improves the performance of previous\nfully sparse models that use LiDAR only and reaches SOTA performance in\nmultimodal models. At the same time, thanks to the sparse architecture, our\nmethod can be up to 5 times more efficient than previous SOTA methods in the\ninference process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fully sparse 3D detection has attracted an increasing interest in the recent\nyears. However, the sparsity of the features in these frameworks challenges the\ngeneration of proposals because of the limited diffusion process. In addition,\nthe quest for efficiency has led to only few work on vision-assisted fully\nsparse models. In this paper, we propose FSMDet (Fully Sparse Multi-modal\nDetection), which use visual information to guide the LiDAR feature diffusion\nprocess while still maintaining the efficiency of the pipeline. Specifically,\nmost of fully sparse works focus on complex customized center fusion\ndiffusion/regression operators. However, we observed that if the adequate\nobject completion is performed, even the simplest interpolation operator leads\nto satisfactory results. Inspired by this observation, we split the\nvision-guided diffusion process into two modules: a Shape Recover Layer\n(SRLayer) and a Self Diffusion Layer (SDLayer). The former uses RGB information\nto recover the shape of the visible part of an object, and the latter uses a\nvisual prior to further spread the features to the center region. Experiments\ndemonstrate that our approach successfully improves the performance of previous\nfully sparse models that use LiDAR only and reaches SOTA performance in\nmultimodal models. At the same time, thanks to the sparse architecture, our\nmethod can be up to 5 times more efficient than previous SOTA methods in the\ninference process."
                },
                "authors": [
                    {
                        "name": "Tianran Liu"
                    },
                    {
                        "name": "Morteza Mousa Pasandi"
                    },
                    {
                        "name": "Robert Laganiere"
                    }
                ],
                "author_detail": {
                    "name": "Robert Laganiere"
                },
                "author": "Robert Laganiere",
                "arxiv_comment": "Accepted by European Conference on Computer Vision (ECCV) 2024\n  workshop on VCAD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06945v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06945v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09894v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09894v2",
                "updated": "2024-09-11T01:55:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    1,
                    55,
                    42,
                    2,
                    255,
                    0
                ],
                "published": "2024-06-14T10:06:55Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    10,
                    6,
                    55,
                    4,
                    166,
                    0
                ],
                "title": "Period Singer: Integrating Periodic and Aperiodic Variational\n  Autoencoders for Natural-Sounding End-to-End Singing Voice Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Period Singer: Integrating Periodic and Aperiodic Variational\n  Autoencoders for Natural-Sounding End-to-End Singing Voice Synthesis"
                },
                "summary": "In this paper, we present Period Singer, a novel end-to-end singing voice\nsynthesis (SVS) model that utilizes variational inference for periodic and\naperiodic components, aimed at producing natural-sounding waveforms. Recent\nend-to-end SVS models have demonstrated the capability of synthesizing\nhigh-fidelity singing voices. However, owing to deterministic pitch\nconditioning, they do not fully address the one-to-many problem. To address\nthis problem, we present the Period Singer architecture, which integrates\nvariational autoencoders for the periodic and aperiodic components.\nAdditionally, our methodology eliminates the dependency on an external aligner\nby estimating the phoneme alignment through a monotonic alignment search within\nnote boundaries. Our empirical evaluations show that Period Singer outperforms\nexisting end-to-end SVS models on Mandarin and Korean datasets. The efficacy of\nthe proposed method was further corroborated by ablation studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present Period Singer, a novel end-to-end singing voice\nsynthesis (SVS) model that utilizes variational inference for periodic and\naperiodic components, aimed at producing natural-sounding waveforms. Recent\nend-to-end SVS models have demonstrated the capability of synthesizing\nhigh-fidelity singing voices. However, owing to deterministic pitch\nconditioning, they do not fully address the one-to-many problem. To address\nthis problem, we present the Period Singer architecture, which integrates\nvariational autoencoders for the periodic and aperiodic components.\nAdditionally, our methodology eliminates the dependency on an external aligner\nby estimating the phoneme alignment through a monotonic alignment search within\nnote boundaries. Our empirical evaluations show that Period Singer outperforms\nexisting end-to-end SVS models on Mandarin and Korean datasets. The efficacy of\nthe proposed method was further corroborated by ablation studies."
                },
                "authors": [
                    {
                        "name": "Taewoo Kim"
                    },
                    {
                        "name": "Choongsang Cho"
                    },
                    {
                        "name": "Young Han Lee"
                    }
                ],
                "author_detail": {
                    "name": "Young Han Lee"
                },
                "author": "Young Han Lee",
                "arxiv_comment": "Accepted by Interspeech 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09894v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09894v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06277v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06277v2",
                "updated": "2024-09-11T01:47:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    1,
                    47,
                    48,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-10T07:28:13Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    7,
                    28,
                    13,
                    1,
                    254,
                    0
                ],
                "title": "Ferret: Federated Full-Parameter Tuning at Scale for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ferret: Federated Full-Parameter Tuning at Scale for Large Language\n  Models"
                },
                "summary": "Large Language Models (LLMs) have become indispensable in numerous real-world\napplications. Unfortunately, fine-tuning these models at scale, especially in\nfederated settings where data privacy and communication efficiency are\ncritical, presents significant challenges. Existing methods often resort to\nparameter-efficient fine-tuning (PEFT) to mitigate communication overhead, but\nthis typically comes at the cost of model accuracy. To address these\nlimitations, we propose federated full-parameter tuning at scale for LLMs\n(Ferret), the first first-order method with shared randomness to enable\nscalable full-parameter tuning of LLMs across decentralized data sources while\nmaintaining competitive model accuracy. Ferret accomplishes this through three\naspects: (1) it employs widely applied first-order methods for efficient local\nupdates; (2) it projects these updates into a low-dimensional space to\nconsiderably reduce communication overhead; and (3) it reconstructs local\nupdates from this low-dimensional space with shared randomness to facilitate\neffective full-parameter global aggregation, ensuring fast convergence and\ncompetitive final performance. Our rigorous theoretical analyses and insights\nalong with extensive experiments, show that Ferret significantly enhances the\nscalability of existing federated full-parameter tuning approaches by achieving\nhigh computational efficiency, reduced communication overhead, and fast\nconvergence, all while maintaining competitive model accuracy. Our\nimplementation is available at https://github.com/allen4747/Ferret.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become indispensable in numerous real-world\napplications. Unfortunately, fine-tuning these models at scale, especially in\nfederated settings where data privacy and communication efficiency are\ncritical, presents significant challenges. Existing methods often resort to\nparameter-efficient fine-tuning (PEFT) to mitigate communication overhead, but\nthis typically comes at the cost of model accuracy. To address these\nlimitations, we propose federated full-parameter tuning at scale for LLMs\n(Ferret), the first first-order method with shared randomness to enable\nscalable full-parameter tuning of LLMs across decentralized data sources while\nmaintaining competitive model accuracy. Ferret accomplishes this through three\naspects: (1) it employs widely applied first-order methods for efficient local\nupdates; (2) it projects these updates into a low-dimensional space to\nconsiderably reduce communication overhead; and (3) it reconstructs local\nupdates from this low-dimensional space with shared randomness to facilitate\neffective full-parameter global aggregation, ensuring fast convergence and\ncompetitive final performance. Our rigorous theoretical analyses and insights\nalong with extensive experiments, show that Ferret significantly enhances the\nscalability of existing federated full-parameter tuning approaches by achieving\nhigh computational efficiency, reduced communication overhead, and fast\nconvergence, all while maintaining competitive model accuracy. Our\nimplementation is available at https://github.com/allen4747/Ferret."
                },
                "authors": [
                    {
                        "name": "Yao Shu"
                    },
                    {
                        "name": "Wenyang Hu"
                    },
                    {
                        "name": "See-Kiong Ng"
                    },
                    {
                        "name": "Bryan Kian Hsiang Low"
                    },
                    {
                        "name": "Fei Richard Yu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Richard Yu"
                },
                "author": "Fei Richard Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06277v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06277v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06941v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06941v1",
                "updated": "2024-09-11T01:46:49Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    1,
                    46,
                    49,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T01:46:49Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    1,
                    46,
                    49,
                    2,
                    255,
                    0
                ],
                "title": "FreeRide: Harvesting Bubbles in Pipeline Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreeRide: Harvesting Bubbles in Pipeline Parallelism"
                },
                "summary": "The occurrence of bubbles in pipeline parallelism is an inherent limitation\nthat can account for more than 40% of the large language model (LLM) training\ntime and is one of the main reasons for the underutilization of GPU resources\nin LLM training. Harvesting these bubbles for GPU side tasks can increase\nresource utilization and reduce training costs but comes with challenges.\nFirst, because bubbles are discontinuous with various shapes, programming side\ntasks becomes difficult while requiring excessive engineering effort. Second, a\nside task can compete with pipeline training for GPU resources and incur\nsignificant overhead. To address these challenges, we propose FreeRide, a\nsystem designed to harvest bubbles in pipeline parallelism for side tasks.\nFreeRide provides programmers with interfaces to implement side tasks easily,\nmanages bubbles and side tasks during pipeline training, and controls access to\nGPU resources by side tasks to reduce overhead. We demonstrate that FreeRide\nachieves 7.8% average cost savings with a negligible overhead of about 1% in\ntraining LLMs while serving model training, graph analytics, and image\nprocessing side tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The occurrence of bubbles in pipeline parallelism is an inherent limitation\nthat can account for more than 40% of the large language model (LLM) training\ntime and is one of the main reasons for the underutilization of GPU resources\nin LLM training. Harvesting these bubbles for GPU side tasks can increase\nresource utilization and reduce training costs but comes with challenges.\nFirst, because bubbles are discontinuous with various shapes, programming side\ntasks becomes difficult while requiring excessive engineering effort. Second, a\nside task can compete with pipeline training for GPU resources and incur\nsignificant overhead. To address these challenges, we propose FreeRide, a\nsystem designed to harvest bubbles in pipeline parallelism for side tasks.\nFreeRide provides programmers with interfaces to implement side tasks easily,\nmanages bubbles and side tasks during pipeline training, and controls access to\nGPU resources by side tasks to reduce overhead. We demonstrate that FreeRide\nachieves 7.8% average cost savings with a negligible overhead of about 1% in\ntraining LLMs while serving model training, graph analytics, and image\nprocessing side tasks."
                },
                "authors": [
                    {
                        "name": "Jiashu Zhang"
                    },
                    {
                        "name": "Zihan Pan"
                    },
                    {
                        "name": "Molly"
                    },
                    {
                        "name": "Xu"
                    },
                    {
                        "name": "Khuzaima Daudjee"
                    },
                    {
                        "name": "Sihang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Sihang Liu"
                },
                "arxiv_affiliation": "Yiming",
                "author": "Sihang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06941v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06941v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2302.11024v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2302.11024v7",
                "updated": "2024-09-11T01:45:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    1,
                    45,
                    58,
                    2,
                    255,
                    0
                ],
                "published": "2023-02-21T21:44:08Z",
                "published_parsed": [
                    2023,
                    2,
                    21,
                    21,
                    44,
                    8,
                    1,
                    52,
                    0
                ],
                "title": "Gradient Flows for Sampling: Mean-Field Models, Gaussian Approximations\n  and Affine Invariance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gradient Flows for Sampling: Mean-Field Models, Gaussian Approximations\n  and Affine Invariance"
                },
                "summary": "Sampling a probability distribution with an unknown normalization constant is\na fundamental problem in computational science and engineering. This task may\nbe cast as an optimization problem over all probability measures, and an\ninitial distribution can be evolved to the desired minimizer dynamically via\ngradient flows. Mean-field models, whose law is governed by the gradient flow\nin the space of probability measures, may also be identified; particle\napproximations of these mean-field models form the basis of algorithms. The\ngradient flow approach is also the basis of algorithms for variational\ninference, in which the optimization is performed over a parameterized family\nof probability distributions such as Gaussians, and the underlying gradient\nflow is restricted to the parameterized family.\n  By choosing different energy functionals and metrics for the gradient flow,\ndifferent algorithms with different convergence properties arise. In this\npaper, we concentrate on the Kullback-Leibler divergence after showing that, up\nto scaling, it has the unique property that the gradient flows resulting from\nthis choice of energy do not depend on the normalization constant. For the\nmetrics, we focus on variants of the Fisher-Rao, Wasserstein, and Stein\nmetrics; we introduce the affine invariance property for gradient flows, and\ntheir corresponding mean-field models, determine whether a given metric leads\nto affine invariance, and modify it to make it affine invariant if it does not.\nWe study the resulting gradient flows in both probability density space and\nGaussian space. The flow in the Gaussian space may be understood as a Gaussian\napproximation of the flow. We demonstrate that the Gaussian approximation based\non the metric and through moment closure coincide, establish connections\nbetween them, and study their long-time convergence properties showing the\nadvantages of affine invariance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sampling a probability distribution with an unknown normalization constant is\na fundamental problem in computational science and engineering. This task may\nbe cast as an optimization problem over all probability measures, and an\ninitial distribution can be evolved to the desired minimizer dynamically via\ngradient flows. Mean-field models, whose law is governed by the gradient flow\nin the space of probability measures, may also be identified; particle\napproximations of these mean-field models form the basis of algorithms. The\ngradient flow approach is also the basis of algorithms for variational\ninference, in which the optimization is performed over a parameterized family\nof probability distributions such as Gaussians, and the underlying gradient\nflow is restricted to the parameterized family.\n  By choosing different energy functionals and metrics for the gradient flow,\ndifferent algorithms with different convergence properties arise. In this\npaper, we concentrate on the Kullback-Leibler divergence after showing that, up\nto scaling, it has the unique property that the gradient flows resulting from\nthis choice of energy do not depend on the normalization constant. For the\nmetrics, we focus on variants of the Fisher-Rao, Wasserstein, and Stein\nmetrics; we introduce the affine invariance property for gradient flows, and\ntheir corresponding mean-field models, determine whether a given metric leads\nto affine invariance, and modify it to make it affine invariant if it does not.\nWe study the resulting gradient flows in both probability density space and\nGaussian space. The flow in the Gaussian space may be understood as a Gaussian\napproximation of the flow. We demonstrate that the Gaussian approximation based\non the metric and through moment closure coincide, establish connections\nbetween them, and study their long-time convergence properties showing the\nadvantages of affine invariance."
                },
                "authors": [
                    {
                        "name": "Yifan Chen"
                    },
                    {
                        "name": "Daniel Zhengyu Huang"
                    },
                    {
                        "name": "Jiaoyang Huang"
                    },
                    {
                        "name": "Sebastian Reich"
                    },
                    {
                        "name": "Andrew M. Stuart"
                    }
                ],
                "author_detail": {
                    "name": "Andrew M. Stuart"
                },
                "author": "Andrew M. Stuart",
                "arxiv_comment": "82 pages, 8 figures (Welcome any feedback!)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2302.11024v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2302.11024v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16989v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16989v2",
                "updated": "2024-09-11T01:05:08Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    1,
                    5,
                    8,
                    2,
                    255,
                    0
                ],
                "published": "2024-05-27T09:35:22Z",
                "published_parsed": [
                    2024,
                    5,
                    27,
                    9,
                    35,
                    22,
                    0,
                    148,
                    0
                ],
                "title": "Uncertainty Learning for High-dimensional Mean-variance Portfolio",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty Learning for High-dimensional Mean-variance Portfolio"
                },
                "summary": "Robust estimation for modern portfolio selection on a large set of assets\nbecomes more important due to large deviation of empirical inference on big\ndata. We propose a distributionally robust methodology for high-dimensional\nmean-variance portfolio problem, aiming to select an optimal conservative\nportfolio allocation by taking distribution uncertainty into account. With the\nhelp of factor structure, we extend the distributionally robust mean-variance\nproblem investigated by Blanchet et al. (2022, Management Science) to the\nhigh-dimensional scenario and transform it to a new penalized risk minimization\nproblem. Furthermore, we propose a data-adaptive method to estimate the\nquantified uncertainty size, which is the radius around the empirical\nprobability measured by the Wasserstein distance. Asymptotic consistency is\nderived for the estimation of the population parameters involved in selecting\nthe uncertainty size and the selected portfolio return. Our Monte-Carlo\nsimulation results show that the chosen uncertainty size and target return from\nthe proposed procedure are very close to the corresponding oracle version, and\nthe new portfolio strategy is of low risk. Finally, we conduct empirical\nstudies based on S&P index components to show the robust performance of our\nproposal in terms of risk controlling and return-risk balancing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust estimation for modern portfolio selection on a large set of assets\nbecomes more important due to large deviation of empirical inference on big\ndata. We propose a distributionally robust methodology for high-dimensional\nmean-variance portfolio problem, aiming to select an optimal conservative\nportfolio allocation by taking distribution uncertainty into account. With the\nhelp of factor structure, we extend the distributionally robust mean-variance\nproblem investigated by Blanchet et al. (2022, Management Science) to the\nhigh-dimensional scenario and transform it to a new penalized risk minimization\nproblem. Furthermore, we propose a data-adaptive method to estimate the\nquantified uncertainty size, which is the radius around the empirical\nprobability measured by the Wasserstein distance. Asymptotic consistency is\nderived for the estimation of the population parameters involved in selecting\nthe uncertainty size and the selected portfolio return. Our Monte-Carlo\nsimulation results show that the chosen uncertainty size and target return from\nthe proposed procedure are very close to the corresponding oracle version, and\nthe new portfolio strategy is of low risk. Finally, we conduct empirical\nstudies based on S&P index components to show the robust performance of our\nproposal in terms of risk controlling and return-risk balancing."
                },
                "authors": [
                    {
                        "name": "Ruike Wu"
                    },
                    {
                        "name": "Yanrong Yang"
                    },
                    {
                        "name": "Han Lin Shang"
                    },
                    {
                        "name": "Huanjun Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Huanjun Zhu"
                },
                "author": "Huanjun Zhu",
                "arxiv_comment": "41 pages, 3 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16989v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16989v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "91G10, 62P05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06927v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06927v1",
                "updated": "2024-09-11T00:56:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    0,
                    56,
                    2,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T00:56:02Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    0,
                    56,
                    2,
                    2,
                    255,
                    0
                ],
                "title": "Representation Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Representation Tuning"
                },
                "summary": "Activation engineering is becoming increasingly popular as a means of online\ncontrol of large language models (LLMs). In this work, I extend the idea of\nactive steering with vectors that represent a behavioral direction of interest\nto tuning those vectors directly into the model, obviating the need for online\ncontrol. First, I identify activation vectors related to honesty in an\nopen-source LLM (Llama- 2-13b-chat). Next, I demonstrate that model output can\nbe made more or less honest by adding positive or negative multiples of these\nvectors to residual stream activations during generation. Then, I show that a\nsimilar effect can be achieved by fine-tuning the vectors directly into the\nmodel, by use of a dual loss function based on the cosine similarity of\nresidual stream activations to the vectors combined with a standard token-based\nloss (\"representation tuning\"). Finally, I compare the generations in response\nto honesty-probing prompts from the resulting models to those from models\nfine-tuned with a token-based loss alone, and to those from the untuned model\nsubjected to online steering. Overall, fine-tuning the vectors into the models\nusing the cosine similarity plus token loss showed a stronger effect than\nonline steering, and generalized better than using the standard loss,\nsuggesting the potential utility of this approach as a safety measure. Code and\ndata are available at https://github.com/cma1114/representation_tuning; tuned\nmodels are available at https://huggingface.co/collections/cackerman/\nrepresentation-tuning-66da1e5ab41cd1b824687d9f.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation engineering is becoming increasingly popular as a means of online\ncontrol of large language models (LLMs). In this work, I extend the idea of\nactive steering with vectors that represent a behavioral direction of interest\nto tuning those vectors directly into the model, obviating the need for online\ncontrol. First, I identify activation vectors related to honesty in an\nopen-source LLM (Llama- 2-13b-chat). Next, I demonstrate that model output can\nbe made more or less honest by adding positive or negative multiples of these\nvectors to residual stream activations during generation. Then, I show that a\nsimilar effect can be achieved by fine-tuning the vectors directly into the\nmodel, by use of a dual loss function based on the cosine similarity of\nresidual stream activations to the vectors combined with a standard token-based\nloss (\"representation tuning\"). Finally, I compare the generations in response\nto honesty-probing prompts from the resulting models to those from models\nfine-tuned with a token-based loss alone, and to those from the untuned model\nsubjected to online steering. Overall, fine-tuning the vectors into the models\nusing the cosine similarity plus token loss showed a stronger effect than\nonline steering, and generalized better than using the standard loss,\nsuggesting the potential utility of this approach as a safety measure. Code and\ndata are available at https://github.com/cma1114/representation_tuning; tuned\nmodels are available at https://huggingface.co/collections/cackerman/\nrepresentation-tuning-66da1e5ab41cd1b824687d9f."
                },
                "authors": [
                    {
                        "name": "Christopher M. Ackerman"
                    }
                ],
                "author_detail": {
                    "name": "Christopher M. Ackerman"
                },
                "author": "Christopher M. Ackerman",
                "arxiv_comment": "9 pages, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06927v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.16515v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.16515v2",
                "updated": "2024-09-10T23:55:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    23,
                    55,
                    57,
                    1,
                    254,
                    0
                ],
                "published": "2024-01-29T19:37:50Z",
                "published_parsed": [
                    2024,
                    1,
                    29,
                    19,
                    37,
                    50,
                    0,
                    29,
                    0
                ],
                "title": "Dynamic Electro-Optic Analog Memory for Neuromorphic Photonic Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Electro-Optic Analog Memory for Neuromorphic Photonic Computing"
                },
                "summary": "Artificial intelligence (AI) has seen remarkable advancements across various\ndomains, including natural language processing, computer vision, autonomous\nvehicles, and biology. However, the rapid expansion of AI technologies has\nescalated the demand for more powerful computing resources. As digital\ncomputing approaches fundamental limits, neuromorphic photonics emerges as a\npromising platform to complement existing digital systems. In neuromorphic\nphotonic computing, photonic devices are controlled using analog signals. This\nnecessitates the use of digital-to-analog converters (DAC) and\nanalog-to-digital converters (ADC) for interfacing with these devices during\ninference and training. However, data movement between memory and these\nconverters in conventional von Neumann computing architectures consumes energy.\nTo address this, analog memory co-located with photonic computing devices is\nproposed. This approach aims to reduce the reliance on DACs and ADCs and\nminimize data movement to enhance compute efficiency. This paper demonstrates a\nmonolithically integrated neuromorphic photonic circuit with co-located\ncapacitive analog memory and compares various analog memory technologies for\nneuromorphic photonic computing using the MNIST dataset as a benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence (AI) has seen remarkable advancements across various\ndomains, including natural language processing, computer vision, autonomous\nvehicles, and biology. However, the rapid expansion of AI technologies has\nescalated the demand for more powerful computing resources. As digital\ncomputing approaches fundamental limits, neuromorphic photonics emerges as a\npromising platform to complement existing digital systems. In neuromorphic\nphotonic computing, photonic devices are controlled using analog signals. This\nnecessitates the use of digital-to-analog converters (DAC) and\nanalog-to-digital converters (ADC) for interfacing with these devices during\ninference and training. However, data movement between memory and these\nconverters in conventional von Neumann computing architectures consumes energy.\nTo address this, analog memory co-located with photonic computing devices is\nproposed. This approach aims to reduce the reliance on DACs and ADCs and\nminimize data movement to enhance compute efficiency. This paper demonstrates a\nmonolithically integrated neuromorphic photonic circuit with co-located\ncapacitive analog memory and compares various analog memory technologies for\nneuromorphic photonic computing using the MNIST dataset as a benchmark."
                },
                "authors": [
                    {
                        "name": "Sean Lam"
                    },
                    {
                        "name": "Ahmed Khaled"
                    },
                    {
                        "name": "Simon Bilodeau"
                    },
                    {
                        "name": "Bicky A. Marquez"
                    },
                    {
                        "name": "Paul R. Prucnal"
                    },
                    {
                        "name": "Lukas Chrostowski"
                    },
                    {
                        "name": "Bhavin J. Shastri"
                    },
                    {
                        "name": "Sudip Shekhar"
                    }
                ],
                "author_detail": {
                    "name": "Sudip Shekhar"
                },
                "author": "Sudip Shekhar",
                "arxiv_comment": "23 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.16515v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.16515v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06909v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06909v1",
                "updated": "2024-09-10T23:25:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    23,
                    25,
                    13,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T23:25:13Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    23,
                    25,
                    13,
                    1,
                    254,
                    0
                ],
                "title": "Realization of giant elastocaloric cooling at cryogenic temperatures in\n  TmVO$_4$ via a strain load/unload technique",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Realization of giant elastocaloric cooling at cryogenic temperatures in\n  TmVO$_4$ via a strain load/unload technique"
                },
                "summary": "The adiabatic elastocaloric effect relates changes in the strain that a\nmaterial experiences to resulting changes in its temperature. While\nelastocaloric materials have been utilized for cooling in room temperature\napplications, the use of such materials for cryogenic cooling remains\nrelatively unexplored. Here, we use a strain load/unload technique at low\ntemperatures, similar to those employed at room-temperature, to demonstrate a\nlarge cooling effect in TmVO$_4$. For strain changes of $1.8 \\cdot 10^{-3}$,\nthe inferred cooling reaches approximately 50% of the material's starting\ntemperature at 5 K, justifying the moniker \"giant\". Beyond establishing the\nsuitability of this class of material for cryogenic elastocaloric cooling,\nthese measurements also provide additional insight to the entropy landscape in\nthe material as a function of strain and temperature, including the behavior\nproximate to the quadrupolar phase transition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The adiabatic elastocaloric effect relates changes in the strain that a\nmaterial experiences to resulting changes in its temperature. While\nelastocaloric materials have been utilized for cooling in room temperature\napplications, the use of such materials for cryogenic cooling remains\nrelatively unexplored. Here, we use a strain load/unload technique at low\ntemperatures, similar to those employed at room-temperature, to demonstrate a\nlarge cooling effect in TmVO$_4$. For strain changes of $1.8 \\cdot 10^{-3}$,\nthe inferred cooling reaches approximately 50% of the material's starting\ntemperature at 5 K, justifying the moniker \"giant\". Beyond establishing the\nsuitability of this class of material for cryogenic elastocaloric cooling,\nthese measurements also provide additional insight to the entropy landscape in\nthe material as a function of strain and temperature, including the behavior\nproximate to the quadrupolar phase transition."
                },
                "authors": [
                    {
                        "name": "Mark P. Zic"
                    },
                    {
                        "name": "Linda Ye"
                    },
                    {
                        "name": "Maya H. Martinez"
                    },
                    {
                        "name": "Ian R. Fisher"
                    }
                ],
                "author_detail": {
                    "name": "Ian R. Fisher"
                },
                "author": "Ian R. Fisher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06909v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06909v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2409.07453v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07453v1",
                "updated": "2024-09-11T17:59:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    17,
                    59,
                    1,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T17:59:01Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    17,
                    59,
                    1,
                    2,
                    255,
                    0
                ],
                "title": "\"My Grade is Wrong!\": A Contestable AI Framework for Interactive\n  Feedback in Evaluating Student Essays",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"My Grade is Wrong!\": A Contestable AI Framework for Interactive\n  Feedback in Evaluating Student Essays"
                },
                "summary": "Interactive feedback, where feedback flows in both directions between teacher\nand student, is more effective than traditional one-way feedback. However, it\nis often too time-consuming for widespread use in educational practice. While\nLarge Language Models (LLMs) have potential for automating feedback, they\nstruggle with reasoning and interaction in an interactive setting. This paper\nintroduces CAELF, a Contestable AI Empowered LLM Framework for automating\ninteractive feedback. CAELF allows students to query, challenge, and clarify\ntheir feedback by integrating a multi-agent system with computational\nargumentation. Essays are first assessed by multiple Teaching-Assistant Agents\n(TA Agents), and then a Teacher Agent aggregates the evaluations through formal\nreasoning to generate feedback and grades. Students can further engage with the\nfeedback to refine their understanding. A case study on 500 critical thinking\nessays with user studies demonstrates that CAELF significantly improves\ninteractive feedback, enhancing the reasoning and interaction capabilities of\nLLMs. This approach offers a promising solution to overcoming the time and\nresource barriers that have limited the adoption of interactive feedback in\neducational settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interactive feedback, where feedback flows in both directions between teacher\nand student, is more effective than traditional one-way feedback. However, it\nis often too time-consuming for widespread use in educational practice. While\nLarge Language Models (LLMs) have potential for automating feedback, they\nstruggle with reasoning and interaction in an interactive setting. This paper\nintroduces CAELF, a Contestable AI Empowered LLM Framework for automating\ninteractive feedback. CAELF allows students to query, challenge, and clarify\ntheir feedback by integrating a multi-agent system with computational\nargumentation. Essays are first assessed by multiple Teaching-Assistant Agents\n(TA Agents), and then a Teacher Agent aggregates the evaluations through formal\nreasoning to generate feedback and grades. Students can further engage with the\nfeedback to refine their understanding. A case study on 500 critical thinking\nessays with user studies demonstrates that CAELF significantly improves\ninteractive feedback, enhancing the reasoning and interaction capabilities of\nLLMs. This approach offers a promising solution to overcoming the time and\nresource barriers that have limited the adoption of interactive feedback in\neducational settings."
                },
                "authors": [
                    {
                        "name": "Shengxin Hong"
                    },
                    {
                        "name": "Chang Cai"
                    },
                    {
                        "name": "Sixuan Du"
                    },
                    {
                        "name": "Haiyue Feng"
                    },
                    {
                        "name": "Siyuan Liu"
                    },
                    {
                        "name": "Xiuyi Fan"
                    }
                ],
                "author_detail": {
                    "name": "Xiuyi Fan"
                },
                "author": "Xiuyi Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07453v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07453v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07440v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07440v1",
                "updated": "2024-09-11T17:37:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    17,
                    37,
                    48,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T17:37:48Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    17,
                    37,
                    48,
                    2,
                    255,
                    0
                ],
                "title": "SUPER: Evaluating Agents on Setting Up and Executing Tasks from Research\n  Repositories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SUPER: Evaluating Agents on Setting Up and Executing Tasks from Research\n  Repositories"
                },
                "summary": "Given that Large Language Models (LLMs) have made significant progress in\nwriting code, can they now be used to autonomously reproduce results from\nresearch repositories? Such a capability would be a boon to the research\ncommunity, helping researchers validate, understand, and extend prior work. To\nadvance towards this goal, we introduce SUPER, the first benchmark designed to\nevaluate the capability of LLMs in setting up and executing tasks from research\nrepositories. SUPERaims to capture the realistic challenges faced by\nresearchers working with Machine Learning (ML) and Natural Language Processing\n(NLP) research repositories. Our benchmark comprises three distinct problem\nsets: 45 end-to-end problems with annotated expert solutions, 152 sub problems\nderived from the expert set that focus on specific challenges (e.g.,\nconfiguring a trainer), and 602 automatically generated problems for\nlarger-scale development. We introduce various evaluation measures to assess\nboth task success and progress, utilizing gold solutions when available or\napproximations otherwise. We show that state-of-the-art approaches struggle to\nsolve these problems with the best model (GPT-4o) solving only 16.3% of the\nend-to-end set, and 46.1% of the scenarios. This illustrates the challenge of\nthis task, and suggests that SUPER can serve as a valuable resource for the\ncommunity to make and measure progress.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given that Large Language Models (LLMs) have made significant progress in\nwriting code, can they now be used to autonomously reproduce results from\nresearch repositories? Such a capability would be a boon to the research\ncommunity, helping researchers validate, understand, and extend prior work. To\nadvance towards this goal, we introduce SUPER, the first benchmark designed to\nevaluate the capability of LLMs in setting up and executing tasks from research\nrepositories. SUPERaims to capture the realistic challenges faced by\nresearchers working with Machine Learning (ML) and Natural Language Processing\n(NLP) research repositories. Our benchmark comprises three distinct problem\nsets: 45 end-to-end problems with annotated expert solutions, 152 sub problems\nderived from the expert set that focus on specific challenges (e.g.,\nconfiguring a trainer), and 602 automatically generated problems for\nlarger-scale development. We introduce various evaluation measures to assess\nboth task success and progress, utilizing gold solutions when available or\napproximations otherwise. We show that state-of-the-art approaches struggle to\nsolve these problems with the best model (GPT-4o) solving only 16.3% of the\nend-to-end set, and 46.1% of the scenarios. This illustrates the challenge of\nthis task, and suggests that SUPER can serve as a valuable resource for the\ncommunity to make and measure progress."
                },
                "authors": [
                    {
                        "name": "Ben Bogin"
                    },
                    {
                        "name": "Kejuan Yang"
                    },
                    {
                        "name": "Shashank Gupta"
                    },
                    {
                        "name": "Kyle Richardson"
                    },
                    {
                        "name": "Erin Bransom"
                    },
                    {
                        "name": "Peter Clark"
                    },
                    {
                        "name": "Ashish Sabharwal"
                    },
                    {
                        "name": "Tushar Khot"
                    }
                ],
                "author_detail": {
                    "name": "Tushar Khot"
                },
                "author": "Tushar Khot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07440v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07440v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07424v1",
                "updated": "2024-09-11T17:10:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    17,
                    10,
                    20,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T17:10:20Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    17,
                    10,
                    20,
                    2,
                    255,
                    0
                ],
                "title": "Towards Fairer Health Recommendations: finding informative unbiased\n  samples via Word Sense Disambiguation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Fairer Health Recommendations: finding informative unbiased\n  samples via Word Sense Disambiguation"
                },
                "summary": "There have been growing concerns around high-stake applications that rely on\nmodels trained with biased data, which consequently produce biased predictions,\noften harming the most vulnerable. In particular, biased medical data could\ncause health-related applications and recommender systems to create outputs\nthat jeopardize patient care and widen disparities in health outcomes. A recent\nframework titled Fairness via AI posits that, instead of attempting to correct\nmodel biases, researchers must focus on their root causes by using AI to debias\ndata. Inspired by this framework, we tackle bias detection in medical curricula\nusing NLP models, including LLMs, and evaluate them on a gold standard dataset\ncontaining 4,105 excerpts annotated by medical experts for bias from a large\ncorpus. We build on previous work by coauthors which augments the set of\nnegative samples with non-annotated text containing social identifier terms.\nHowever, some of these terms, especially those related to race and ethnicity,\ncan carry different meanings (e.g., \"white matter of spinal cord\"). To address\nthis issue, we propose the use of Word Sense Disambiguation models to refine\ndataset quality by removing irrelevant sentences. We then evaluate fine-tuned\nvariations of BERT models as well as GPT models with zero- and few-shot\nprompting. We found LLMs, considered SOTA on many NLP tasks, unsuitable for\nbias detection, while fine-tuned BERT models generally perform well across all\nevaluated metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There have been growing concerns around high-stake applications that rely on\nmodels trained with biased data, which consequently produce biased predictions,\noften harming the most vulnerable. In particular, biased medical data could\ncause health-related applications and recommender systems to create outputs\nthat jeopardize patient care and widen disparities in health outcomes. A recent\nframework titled Fairness via AI posits that, instead of attempting to correct\nmodel biases, researchers must focus on their root causes by using AI to debias\ndata. Inspired by this framework, we tackle bias detection in medical curricula\nusing NLP models, including LLMs, and evaluate them on a gold standard dataset\ncontaining 4,105 excerpts annotated by medical experts for bias from a large\ncorpus. We build on previous work by coauthors which augments the set of\nnegative samples with non-annotated text containing social identifier terms.\nHowever, some of these terms, especially those related to race and ethnicity,\ncan carry different meanings (e.g., \"white matter of spinal cord\"). To address\nthis issue, we propose the use of Word Sense Disambiguation models to refine\ndataset quality by removing irrelevant sentences. We then evaluate fine-tuned\nvariations of BERT models as well as GPT models with zero- and few-shot\nprompting. We found LLMs, considered SOTA on many NLP tasks, unsuitable for\nbias detection, while fine-tuned BERT models generally perform well across all\nevaluated metrics."
                },
                "authors": [
                    {
                        "name": "Gavin Butts"
                    },
                    {
                        "name": "Pegah Emdad"
                    },
                    {
                        "name": "Jethro Lee"
                    },
                    {
                        "name": "Shannon Song"
                    },
                    {
                        "name": "Chiman Salavati"
                    },
                    {
                        "name": "Willmar Sosa Diaz"
                    },
                    {
                        "name": "Shiri Dori-Hacohen"
                    },
                    {
                        "name": "Fabricio Murai"
                    }
                ],
                "author_detail": {
                    "name": "Fabricio Murai"
                },
                "author": "Fabricio Murai",
                "arxiv_comment": "Accepted for long presentation at the FAcctRec @ Recsys 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; J.3; K.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.12573v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.12573v3",
                "updated": "2024-09-11T16:52:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    16,
                    52,
                    44,
                    2,
                    255,
                    0
                ],
                "published": "2023-11-21T12:38:05Z",
                "published_parsed": [
                    2023,
                    11,
                    21,
                    12,
                    38,
                    5,
                    1,
                    325,
                    0
                ],
                "title": "Moderating Model Marketplaces: Platform Governance Puzzles for AI\n  Intermediaries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Moderating Model Marketplaces: Platform Governance Puzzles for AI\n  Intermediaries"
                },
                "summary": "The AI development community is increasingly making use of hosting\nintermediaries such as Hugging Face provide easy access to user-uploaded models\nand training data. These model marketplaces lower technical deployment barriers\nfor hundreds of thousands of users, yet can be used in numerous potentially\nharmful and illegal ways. In this article, we explain ways in which AI systems,\nwhich can both `contain' content and be open-ended tools, present one of the\ntrickiest platform governance challenges seen to date. We provide case studies\nof several incidents across three illustrative platforms -- Hugging Face,\nGitHub and Civitai -- to examine how model marketplaces moderate models.\nBuilding on this analysis, we outline important (and yet nevertheless limited)\npractices that industry has been developing to respond to moderation demands:\nlicensing, access and use restrictions, automated content moderation, and open\npolicy development. While the policy challenge at hand is a considerable one,\nwe conclude with some ideas as to how platforms could better mobilize resources\nto act as a careful, fair, and proportionate regulatory access point.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The AI development community is increasingly making use of hosting\nintermediaries such as Hugging Face provide easy access to user-uploaded models\nand training data. These model marketplaces lower technical deployment barriers\nfor hundreds of thousands of users, yet can be used in numerous potentially\nharmful and illegal ways. In this article, we explain ways in which AI systems,\nwhich can both `contain' content and be open-ended tools, present one of the\ntrickiest platform governance challenges seen to date. We provide case studies\nof several incidents across three illustrative platforms -- Hugging Face,\nGitHub and Civitai -- to examine how model marketplaces moderate models.\nBuilding on this analysis, we outline important (and yet nevertheless limited)\npractices that industry has been developing to respond to moderation demands:\nlicensing, access and use restrictions, automated content moderation, and open\npolicy development. While the policy challenge at hand is a considerable one,\nwe conclude with some ideas as to how platforms could better mobilize resources\nto act as a careful, fair, and proportionate regulatory access point."
                },
                "authors": [
                    {
                        "name": "Robert Gorwa"
                    },
                    {
                        "name": "Michael Veale"
                    }
                ],
                "author_detail": {
                    "name": "Michael Veale"
                },
                "author": "Michael Veale",
                "arxiv_doi": "10.1080/17579961.2024.2388914",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1080/17579961.2024.2388914",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2311.12573v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.12573v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "(2024) 16(2) Law Innovation and Technology",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07409v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07409v1",
                "updated": "2024-09-11T16:50:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    16,
                    50,
                    29,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T16:50:29Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    16,
                    50,
                    29,
                    2,
                    255,
                    0
                ],
                "title": "Robust Robot Walker: Learning Agile Locomotion over Tiny Traps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Robot Walker: Learning Agile Locomotion over Tiny Traps"
                },
                "summary": "Quadruped robots must exhibit robust walking capabilities in practical\napplications. In this work, we propose a novel approach that enables quadruped\nrobots to pass various small obstacles, or \"tiny traps\". Existing methods often\nrely on exteroceptive sensors, which can be unreliable for detecting such tiny\ntraps. To overcome this limitation, our approach focuses solely on\nproprioceptive inputs. We introduce a two-stage training framework\nincorporating a contact encoder and a classification head to learn implicit\nrepresentations of different traps. Additionally, we design a set of tailored\nreward functions to improve both the stability of training and the ease of\ndeployment for goal-tracking tasks. To benefit further research, we design a\nnew benchmark for tiny trap task. Extensive experiments in both simulation and\nreal-world settings demonstrate the effectiveness and robustness of our method.\nProject Page: https://robust-robot-walker.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quadruped robots must exhibit robust walking capabilities in practical\napplications. In this work, we propose a novel approach that enables quadruped\nrobots to pass various small obstacles, or \"tiny traps\". Existing methods often\nrely on exteroceptive sensors, which can be unreliable for detecting such tiny\ntraps. To overcome this limitation, our approach focuses solely on\nproprioceptive inputs. We introduce a two-stage training framework\nincorporating a contact encoder and a classification head to learn implicit\nrepresentations of different traps. Additionally, we design a set of tailored\nreward functions to improve both the stability of training and the ease of\ndeployment for goal-tracking tasks. To benefit further research, we design a\nnew benchmark for tiny trap task. Extensive experiments in both simulation and\nreal-world settings demonstrate the effectiveness and robustness of our method.\nProject Page: https://robust-robot-walker.github.io/"
                },
                "authors": [
                    {
                        "name": "Shaoting Zhu"
                    },
                    {
                        "name": "Runhan Huang"
                    },
                    {
                        "name": "Linzhan Mou"
                    },
                    {
                        "name": "Hang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hang Zhao"
                },
                "author": "Hang Zhao",
                "arxiv_comment": "10 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07409v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07409v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07407v1",
                "updated": "2024-09-11T16:49:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    16,
                    49,
                    46,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T16:49:46Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    16,
                    49,
                    46,
                    2,
                    255,
                    0
                ],
                "title": "CLNX: Bridging Code and Natural Language for C/C++\n  Vulnerability-Contributing Commits Identification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLNX: Bridging Code and Natural Language for C/C++\n  Vulnerability-Contributing Commits Identification"
                },
                "summary": "Large Language Models (LLMs) have shown great promise in vulnerability\nidentification. As C/C++ comprises half of the Open-Source Software (OSS)\nvulnerabilities over the past decade and updates in OSS mainly occur through\ncommits, enhancing LLMs' ability to identify C/C++ Vulnerability-Contributing\nCommits (VCCs) is essential. However, current studies primarily focus on\nfurther pre-training LLMs on massive code datasets, which is resource-intensive\nand poses efficiency challenges. In this paper, we enhance the ability of\nBERT-based LLMs to identify C/C++ VCCs in a lightweight manner. We propose\nCodeLinguaNexus (CLNX) as a bridge facilitating communication between C/C++\nprograms and LLMs. Based on commits, CLNX efficiently converts the source code\ninto a more natural representation while preserving key details. Specifically,\nCLNX first applies structure-level naturalization to decompose complex\nprograms, followed by token-level naturalization to interpret complex symbols.\nWe evaluate CLNX on public datasets of 25,872 C/C++ functions with their\ncommits. The results show that CLNX significantly enhances the performance of\nLLMs on identifying C/C++ VCCs. Moreover, CLNX-equipped CodeBERT achieves new\nstate-of-the-art and identifies 38 OSS vulnerabilities in the real world.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown great promise in vulnerability\nidentification. As C/C++ comprises half of the Open-Source Software (OSS)\nvulnerabilities over the past decade and updates in OSS mainly occur through\ncommits, enhancing LLMs' ability to identify C/C++ Vulnerability-Contributing\nCommits (VCCs) is essential. However, current studies primarily focus on\nfurther pre-training LLMs on massive code datasets, which is resource-intensive\nand poses efficiency challenges. In this paper, we enhance the ability of\nBERT-based LLMs to identify C/C++ VCCs in a lightweight manner. We propose\nCodeLinguaNexus (CLNX) as a bridge facilitating communication between C/C++\nprograms and LLMs. Based on commits, CLNX efficiently converts the source code\ninto a more natural representation while preserving key details. Specifically,\nCLNX first applies structure-level naturalization to decompose complex\nprograms, followed by token-level naturalization to interpret complex symbols.\nWe evaluate CLNX on public datasets of 25,872 C/C++ functions with their\ncommits. The results show that CLNX significantly enhances the performance of\nLLMs on identifying C/C++ VCCs. Moreover, CLNX-equipped CodeBERT achieves new\nstate-of-the-art and identifies 38 OSS vulnerabilities in the real world."
                },
                "authors": [
                    {
                        "name": "Zeqing Qin"
                    },
                    {
                        "name": "Yiwei Wu"
                    },
                    {
                        "name": "Lansheng Han"
                    }
                ],
                "author_detail": {
                    "name": "Lansheng Han"
                },
                "author": "Lansheng Han",
                "arxiv_comment": "8 pages, 2 figures, conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M25",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07394v1",
                "updated": "2024-09-11T16:35:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    16,
                    35,
                    18,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T16:35:18Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    16,
                    35,
                    18,
                    2,
                    255,
                    0
                ],
                "title": "AdaCAD: Adaptively Decoding to Balance Conflicts between Contextual and\n  Parametric Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaCAD: Adaptively Decoding to Balance Conflicts between Contextual and\n  Parametric Knowledge"
                },
                "summary": "Knowledge conflict arises from discrepancies between information in the\ncontext of a large language model (LLM) and the knowledge stored in its\nparameters. This can hurt performance when using standard decoding techniques,\nwhich tend to ignore the context. Existing test-time contrastive methods seek\nto address this by comparing the LLM's output distribution with and without the\ncontext and adjust the model according to the contrast between them. However,\nwe find that these methods frequently misjudge the degree of conflict and\nstruggle to handle instances that vary in their amount of conflict, with static\nmethods over-adjusting when conflict is absent. We propose a fine-grained,\ninstance-level approach called AdaCAD, which dynamically infers the weight of\nadjustment based on the degree of conflict, as measured by the Jensen-Shannon\ndivergence between distributions representing contextual and parametric\nknowledge. Our experiments across four models on six diverse question-answering\n(QA) datasets and three summarization tasks demonstrate that our training-free\nadaptive method consistently outperforms other decoding methods on QA, with\naverage accuracy gains of 14.21% (absolute) over a static contrastive baseline,\nand improves the factuality of summaries by 5.59 (AlignScore). Furthermore, our\nanalysis shows that while decoding with contrastive baselines hurts performance\nwhen conflict is absent, AdaCAD mitigates these losses, making it more\napplicable to real-world datasets in which some examples have conflict and\nothers do not.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge conflict arises from discrepancies between information in the\ncontext of a large language model (LLM) and the knowledge stored in its\nparameters. This can hurt performance when using standard decoding techniques,\nwhich tend to ignore the context. Existing test-time contrastive methods seek\nto address this by comparing the LLM's output distribution with and without the\ncontext and adjust the model according to the contrast between them. However,\nwe find that these methods frequently misjudge the degree of conflict and\nstruggle to handle instances that vary in their amount of conflict, with static\nmethods over-adjusting when conflict is absent. We propose a fine-grained,\ninstance-level approach called AdaCAD, which dynamically infers the weight of\nadjustment based on the degree of conflict, as measured by the Jensen-Shannon\ndivergence between distributions representing contextual and parametric\nknowledge. Our experiments across four models on six diverse question-answering\n(QA) datasets and three summarization tasks demonstrate that our training-free\nadaptive method consistently outperforms other decoding methods on QA, with\naverage accuracy gains of 14.21% (absolute) over a static contrastive baseline,\nand improves the factuality of summaries by 5.59 (AlignScore). Furthermore, our\nanalysis shows that while decoding with contrastive baselines hurts performance\nwhen conflict is absent, AdaCAD mitigates these losses, making it more\napplicable to real-world datasets in which some examples have conflict and\nothers do not."
                },
                "authors": [
                    {
                        "name": "Han Wang"
                    },
                    {
                        "name": "Archiki Prasad"
                    },
                    {
                        "name": "Elias Stengel-Eskin"
                    },
                    {
                        "name": "Mohit Bansal"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Bansal"
                },
                "author": "Mohit Bansal",
                "arxiv_comment": "16 pages, Code: https://github.com/HanNight/AdaCAD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02076v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02076v3",
                "updated": "2024-09-11T16:35:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    16,
                    35,
                    0,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-03T17:25:54Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    17,
                    25,
                    54,
                    1,
                    247,
                    0
                ],
                "title": "LongGenbench: Benchmarking Long-Form Generation in Long Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongGenbench: Benchmarking Long-Form Generation in Long Context LLMs"
                },
                "summary": "The abilities of long-context language models (LMs) are often evaluated using\nthe \"Needle-in-a-Haystack\" (NIAH) test, which comprises tasks designed to\nassess a model's ability to identify specific information (\"needle\") within\nlarge text sequences (\"haystack\"). While these benchmarks measure how well\nmodels understand long-context input sequences, they do not effectively gauge\nthe quality of long-form text generation--a critical aspect for applications\nsuch as design proposals and creative writing. To address this gap, we have\nintroduced a new long-form text evaluation benchmark, LongGenbench, which tests\nmodels' ability to identify specific events within generated long text\nsequences. In this benchmark, we prompt long-context LMs to create long-form\ntext that must include particular events or constraints and evaluate their\nability to incorporate these elements. We evaluated ten long-context LMs across\nfour distinct scenarios, three types of prompt instructions, and two different\ngeneration-length settings (16K and 32K). Although these models perform well on\nNIAH benchmarks, none demonstrated satisfactory performance on the\nLongGenbench, raising concerns about their ability to generate coherent\nlong-form text that follows instructions. Additionally, as the length of the\ngenerated text increases, all models exhibit a significant drop in performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The abilities of long-context language models (LMs) are often evaluated using\nthe \"Needle-in-a-Haystack\" (NIAH) test, which comprises tasks designed to\nassess a model's ability to identify specific information (\"needle\") within\nlarge text sequences (\"haystack\"). While these benchmarks measure how well\nmodels understand long-context input sequences, they do not effectively gauge\nthe quality of long-form text generation--a critical aspect for applications\nsuch as design proposals and creative writing. To address this gap, we have\nintroduced a new long-form text evaluation benchmark, LongGenbench, which tests\nmodels' ability to identify specific events within generated long text\nsequences. In this benchmark, we prompt long-context LMs to create long-form\ntext that must include particular events or constraints and evaluate their\nability to incorporate these elements. We evaluated ten long-context LMs across\nfour distinct scenarios, three types of prompt instructions, and two different\ngeneration-length settings (16K and 32K). Although these models perform well on\nNIAH benchmarks, none demonstrated satisfactory performance on the\nLongGenbench, raising concerns about their ability to generate coherent\nlong-form text that follows instructions. Additionally, as the length of the\ngenerated text increases, all models exhibit a significant drop in performance."
                },
                "authors": [
                    {
                        "name": "Yuhao Wu"
                    },
                    {
                        "name": "Ming Shan Hee"
                    },
                    {
                        "name": "Zhiqing Hu"
                    },
                    {
                        "name": "Roy Ka-Wei Lee"
                    }
                ],
                "author_detail": {
                    "name": "Roy Ka-Wei Lee"
                },
                "author": "Roy Ka-Wei Lee",
                "arxiv_comment": "work in progress. arXiv admin note: text overlap with\n  arXiv:2404.06654 by other authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02076v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02076v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07372v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07372v1",
                "updated": "2024-09-11T16:03:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    16,
                    3,
                    9,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T16:03:09Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    16,
                    3,
                    9,
                    2,
                    255,
                    0
                ],
                "title": "Awaking the Slides: A Tuning-free and Knowledge-regulated AI Tutoring\n  System via Language Model Coordination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Awaking the Slides: A Tuning-free and Knowledge-regulated AI Tutoring\n  System via Language Model Coordination"
                },
                "summary": "The vast pre-existing slides serve as rich and important materials to carry\nlecture knowledge. However, effectively leveraging lecture slides to serve\nstudents is difficult due to the multi-modal nature of slide content and the\nheterogeneous teaching actions. We study the problem of discovering effective\ndesigns that convert a slide into an interactive lecture. We develop\nSlide2Lecture, a tuning-free and knowledge-regulated intelligent tutoring\nsystem that can (1) effectively convert an input lecture slide into a\nstructured teaching agenda consisting of a set of heterogeneous teaching\nactions; (2) create and manage an interactive lecture that generates responsive\ninteractions catering to student learning demands while regulating the\ninteractions to follow teaching actions. Slide2Lecture contains a complete\npipeline for learners to obtain an interactive classroom experience to learn\nthe slide. For teachers and developers, Slide2Lecture enables customization to\ncater to personalized demands. The evaluation rated by annotators and students\nshows that Slide2Lecture is effective in outperforming the remaining\nimplementation. Slide2Lecture's online deployment has made more than 200K\ninteraction with students in the 3K lecture sessions. We open source\nSlide2Lecture's implementation in\nhttps://anonymous.4open.science/r/slide2lecture-4210/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The vast pre-existing slides serve as rich and important materials to carry\nlecture knowledge. However, effectively leveraging lecture slides to serve\nstudents is difficult due to the multi-modal nature of slide content and the\nheterogeneous teaching actions. We study the problem of discovering effective\ndesigns that convert a slide into an interactive lecture. We develop\nSlide2Lecture, a tuning-free and knowledge-regulated intelligent tutoring\nsystem that can (1) effectively convert an input lecture slide into a\nstructured teaching agenda consisting of a set of heterogeneous teaching\nactions; (2) create and manage an interactive lecture that generates responsive\ninteractions catering to student learning demands while regulating the\ninteractions to follow teaching actions. Slide2Lecture contains a complete\npipeline for learners to obtain an interactive classroom experience to learn\nthe slide. For teachers and developers, Slide2Lecture enables customization to\ncater to personalized demands. The evaluation rated by annotators and students\nshows that Slide2Lecture is effective in outperforming the remaining\nimplementation. Slide2Lecture's online deployment has made more than 200K\ninteraction with students in the 3K lecture sessions. We open source\nSlide2Lecture's implementation in\nhttps://anonymous.4open.science/r/slide2lecture-4210/."
                },
                "authors": [
                    {
                        "name": "Daniel Zhang-Li"
                    },
                    {
                        "name": "Zheyuan Zhang"
                    },
                    {
                        "name": "Jifan Yu"
                    },
                    {
                        "name": "Joy Lim Jia Yin"
                    },
                    {
                        "name": "Shangqing Tu"
                    },
                    {
                        "name": "Linlu Gong"
                    },
                    {
                        "name": "Haohua Wang"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Huiqin Liu"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07372v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07372v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07368v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07368v1",
                "updated": "2024-09-11T15:56:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    56,
                    15,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T15:56:15Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    56,
                    15,
                    2,
                    255,
                    0
                ],
                "title": "Demo: SGCode: A Flexible Prompt-Optimizing System for Secure Generation\n  of Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demo: SGCode: A Flexible Prompt-Optimizing System for Secure Generation\n  of Code"
                },
                "summary": "This paper introduces SGCode, a flexible prompt-optimizing system to generate\nsecure code with large language models (LLMs). SGCode integrates recent\nprompt-optimization approaches with LLMs in a unified system accessible through\nfront-end and back-end APIs, enabling users to 1) generate secure code, which\nis free of vulnerabilities, 2) review and share security analysis, and 3)\neasily switch from one prompt optimization approach to another, while providing\ninsights on model and system performance. We populated SGCode on an AWS server\nwith PromSec, an approach that optimizes prompts by combining an LLM and\nsecurity tools with a lightweight generative adversarial graph neural network\nto detect and fix security vulnerabilities in the generated code. Extensive\nexperiments show that SGCode is practical as a public tool to gain insights\ninto the trade-offs between model utility, secure code generation, and system\ncost. SGCode has only a marginal cost compared with prompting LLMs. SGCode is\navailable at: http://3.131.141.63:8501/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces SGCode, a flexible prompt-optimizing system to generate\nsecure code with large language models (LLMs). SGCode integrates recent\nprompt-optimization approaches with LLMs in a unified system accessible through\nfront-end and back-end APIs, enabling users to 1) generate secure code, which\nis free of vulnerabilities, 2) review and share security analysis, and 3)\neasily switch from one prompt optimization approach to another, while providing\ninsights on model and system performance. We populated SGCode on an AWS server\nwith PromSec, an approach that optimizes prompts by combining an LLM and\nsecurity tools with a lightweight generative adversarial graph neural network\nto detect and fix security vulnerabilities in the generated code. Extensive\nexperiments show that SGCode is practical as a public tool to gain insights\ninto the trade-offs between model utility, secure code generation, and system\ncost. SGCode has only a marginal cost compared with prompting LLMs. SGCode is\navailable at: http://3.131.141.63:8501/."
                },
                "authors": [
                    {
                        "name": "Khiem Ton"
                    },
                    {
                        "name": "Nhi Nguyen"
                    },
                    {
                        "name": "Mahmoud Nazzal"
                    },
                    {
                        "name": "Abdallah Khreishah"
                    },
                    {
                        "name": "Cristian Borcea"
                    },
                    {
                        "name": "NhatHai Phan"
                    },
                    {
                        "name": "Ruoming Jin"
                    },
                    {
                        "name": "Issa Khalil"
                    },
                    {
                        "name": "Yelong Shen"
                    }
                ],
                "author_detail": {
                    "name": "Yelong Shen"
                },
                "author": "Yelong Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07368v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07368v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.13764v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.13764v4",
                "updated": "2024-09-11T15:47:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    47,
                    11,
                    2,
                    255,
                    0
                ],
                "published": "2024-02-21T12:38:59Z",
                "published_parsed": [
                    2024,
                    2,
                    21,
                    12,
                    38,
                    59,
                    2,
                    52,
                    0
                ],
                "title": "CriticEval: Evaluating Large Language Model as Critic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CriticEval: Evaluating Large Language Model as Critic"
                },
                "summary": "Critique ability, i.e., the capability of Large Language Models (LLMs) to\nidentify and rectify flaws in responses, is crucial for their applications in\nself-improvement and scalable oversight. While numerous studies have been\nproposed to evaluate critique ability of LLMs, their comprehensiveness and\nreliability are still limited. To overcome this problem, we introduce\nCriticEval, a novel benchmark designed to comprehensively and reliably evaluate\ncritique ability of LLMs. Specifically, to ensure the comprehensiveness,\nCriticEval evaluates critique ability from four dimensions across nine diverse\ntask scenarios. It evaluates both scalar-valued and textual critiques,\ntargeting responses of varying quality. To ensure the reliability, a large\nnumber of critiques are annotated to serve as references, enabling GPT-4 to\nevaluate textual critiques reliably. Extensive evaluations of open-source and\nclosed-source LLMs first validate the reliability of evaluation in CriticEval.\nThen, experimental results demonstrate the promising potential of open-source\nLLMs, the effectiveness of critique datasets and several intriguing\nrelationships between the critique ability and some critical factors, including\ntask types, response qualities and critique dimensions. Datasets and evaluation\ntoolkit for CriticEval will be publicly released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Critique ability, i.e., the capability of Large Language Models (LLMs) to\nidentify and rectify flaws in responses, is crucial for their applications in\nself-improvement and scalable oversight. While numerous studies have been\nproposed to evaluate critique ability of LLMs, their comprehensiveness and\nreliability are still limited. To overcome this problem, we introduce\nCriticEval, a novel benchmark designed to comprehensively and reliably evaluate\ncritique ability of LLMs. Specifically, to ensure the comprehensiveness,\nCriticEval evaluates critique ability from four dimensions across nine diverse\ntask scenarios. It evaluates both scalar-valued and textual critiques,\ntargeting responses of varying quality. To ensure the reliability, a large\nnumber of critiques are annotated to serve as references, enabling GPT-4 to\nevaluate textual critiques reliably. Extensive evaluations of open-source and\nclosed-source LLMs first validate the reliability of evaluation in CriticEval.\nThen, experimental results demonstrate the promising potential of open-source\nLLMs, the effectiveness of critique datasets and several intriguing\nrelationships between the critique ability and some critical factors, including\ntask types, response qualities and critique dimensions. Datasets and evaluation\ntoolkit for CriticEval will be publicly released."
                },
                "authors": [
                    {
                        "name": "Tian Lan"
                    },
                    {
                        "name": "Wenwei Zhang"
                    },
                    {
                        "name": "Chen Xu"
                    },
                    {
                        "name": "Heyan Huang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Xian-ling Mao"
                    }
                ],
                "author_detail": {
                    "name": "Xian-ling Mao"
                },
                "author": "Xian-ling Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.13764v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.13764v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07355v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07355v1",
                "updated": "2024-09-11T15:40:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    40,
                    7,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T15:40:07Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    40,
                    7,
                    2,
                    255,
                    0
                ],
                "title": "Think Together and Work Better: Combining Humans' and LLMs' Think-Aloud\n  Outcomes for Effective Text Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think Together and Work Better: Combining Humans' and LLMs' Think-Aloud\n  Outcomes for Effective Text Evaluation"
                },
                "summary": "This study introduces \\textbf{InteractEval}, a framework that integrates\nhuman expertise and Large Language Models (LLMs) using the Think-Aloud (TA)\nmethod to generate attributes for checklist-based text evaluation. By combining\nhuman flexibility and reasoning with LLM consistency, InteractEval outperforms\ntraditional non-LLM-based and LLM-based baselines across four distinct\ndimensions, consisting of Coherence, Fluency, Consistency, and Relevance. The\nexperiment also investigates the effectiveness of the TA method, showing that\nit promotes divergent thinking in both humans and LLMs, leading to the\ngeneration of a wider range of relevant attributes and enhance text evaluation\nperformance. Comparative analysis reveals that humans excel at identifying\nattributes related to internal quality (Coherence and Fluency), but LLMs\nperform better at those attributes related to external alignment (Consistency\nand Relevance). Consequently, leveraging both humans and LLMs together produces\nthe best evaluation outcomes. In other words, this study emphasizes the\nnecessity of effectively combining humans and LLMs in an automated\nchecklist-based text evaluation framework. The code is available at\n\\textbf{\\url{https://github.com/BBeeChu/InteractEval.git}}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study introduces \\textbf{InteractEval}, a framework that integrates\nhuman expertise and Large Language Models (LLMs) using the Think-Aloud (TA)\nmethod to generate attributes for checklist-based text evaluation. By combining\nhuman flexibility and reasoning with LLM consistency, InteractEval outperforms\ntraditional non-LLM-based and LLM-based baselines across four distinct\ndimensions, consisting of Coherence, Fluency, Consistency, and Relevance. The\nexperiment also investigates the effectiveness of the TA method, showing that\nit promotes divergent thinking in both humans and LLMs, leading to the\ngeneration of a wider range of relevant attributes and enhance text evaluation\nperformance. Comparative analysis reveals that humans excel at identifying\nattributes related to internal quality (Coherence and Fluency), but LLMs\nperform better at those attributes related to external alignment (Consistency\nand Relevance). Consequently, leveraging both humans and LLMs together produces\nthe best evaluation outcomes. In other words, this study emphasizes the\nnecessity of effectively combining humans and LLMs in an automated\nchecklist-based text evaluation framework. The code is available at\n\\textbf{\\url{https://github.com/BBeeChu/InteractEval.git}}."
                },
                "authors": [
                    {
                        "name": "SeongYeub Chu"
                    },
                    {
                        "name": "JongWoo Kim"
                    },
                    {
                        "name": "MunYong Yi"
                    }
                ],
                "author_detail": {
                    "name": "MunYong Yi"
                },
                "author": "MunYong Yi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07355v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07355v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07353v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07353v1",
                "updated": "2024-09-11T15:39:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    39,
                    42,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T15:39:42Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    39,
                    42,
                    2,
                    255,
                    0
                ],
                "title": "Securing Vision-Language Models with a Robust Encoder Against Jailbreak\n  and Adversarial Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Securing Vision-Language Models with a Robust Encoder Against Jailbreak\n  and Adversarial Attacks"
                },
                "summary": "Large Vision-Language Models (LVLMs), trained on multimodal big datasets,\nhave significantly advanced AI by excelling in vision-language tasks. However,\nthese models remain vulnerable to adversarial attacks, particularly jailbreak\nattacks, which bypass safety protocols and cause the model to generate\nmisleading or harmful responses. This vulnerability stems from both the\ninherent susceptibilities of LLMs and the expanded attack surface introduced by\nthe visual modality. We propose Sim-CLIP+, a novel defense mechanism that\nadversarially fine-tunes the CLIP vision encoder by leveraging a Siamese\narchitecture. This approach maximizes cosine similarity between perturbed and\nclean samples, facilitating resilience against adversarial manipulations.\nSim-CLIP+ offers a plug-and-play solution, allowing seamless integration into\nexisting LVLM architectures as a robust vision encoder. Unlike previous\ndefenses, our method requires no structural modifications to the LVLM and\nincurs minimal computational overhead. Sim-CLIP+ demonstrates effectiveness\nagainst both gradient-based adversarial attacks and various jailbreak\ntechniques. We evaluate Sim-CLIP+ against three distinct jailbreak attack\nstrategies and perform clean evaluations using standard downstream datasets,\nincluding COCO for image captioning and OKVQA for visual question answering.\nExtensive experiments demonstrate that Sim-CLIP+ maintains high clean accuracy\nwhile substantially improving robustness against both gradient-based\nadversarial attacks and jailbreak techniques. Our code and robust vision\nencoders are available at\nhttps://github.com/speedlab-git/Robust-Encoder-against-Jailbreak-attack.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision-Language Models (LVLMs), trained on multimodal big datasets,\nhave significantly advanced AI by excelling in vision-language tasks. However,\nthese models remain vulnerable to adversarial attacks, particularly jailbreak\nattacks, which bypass safety protocols and cause the model to generate\nmisleading or harmful responses. This vulnerability stems from both the\ninherent susceptibilities of LLMs and the expanded attack surface introduced by\nthe visual modality. We propose Sim-CLIP+, a novel defense mechanism that\nadversarially fine-tunes the CLIP vision encoder by leveraging a Siamese\narchitecture. This approach maximizes cosine similarity between perturbed and\nclean samples, facilitating resilience against adversarial manipulations.\nSim-CLIP+ offers a plug-and-play solution, allowing seamless integration into\nexisting LVLM architectures as a robust vision encoder. Unlike previous\ndefenses, our method requires no structural modifications to the LVLM and\nincurs minimal computational overhead. Sim-CLIP+ demonstrates effectiveness\nagainst both gradient-based adversarial attacks and various jailbreak\ntechniques. We evaluate Sim-CLIP+ against three distinct jailbreak attack\nstrategies and perform clean evaluations using standard downstream datasets,\nincluding COCO for image captioning and OKVQA for visual question answering.\nExtensive experiments demonstrate that Sim-CLIP+ maintains high clean accuracy\nwhile substantially improving robustness against both gradient-based\nadversarial attacks and jailbreak techniques. Our code and robust vision\nencoders are available at\nhttps://github.com/speedlab-git/Robust-Encoder-against-Jailbreak-attack.git."
                },
                "authors": [
                    {
                        "name": "Md Zarif Hossain"
                    },
                    {
                        "name": "Ahmed Imteaj"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed Imteaj"
                },
                "author": "Ahmed Imteaj",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07353v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07353v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07314v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07314v1",
                "updated": "2024-09-11T14:44:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    14,
                    44,
                    51,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T14:44:51Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    14,
                    44,
                    51,
                    2,
                    255,
                    0
                ],
                "title": "MEDIC: Towards a Comprehensive Framework for Evaluating LLMs in Clinical\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEDIC: Towards a Comprehensive Framework for Evaluating LLMs in Clinical\n  Applications"
                },
                "summary": "The rapid development of Large Language Models (LLMs) for healthcare\napplications has spurred calls for holistic evaluation beyond frequently-cited\nbenchmarks like USMLE, to better reflect real-world performance. While\nreal-world assessments are valuable indicators of utility, they often lag\nbehind the pace of LLM evolution, likely rendering findings obsolete upon\ndeployment. This temporal disconnect necessitates a comprehensive upfront\nevaluation that can guide model selection for specific clinical applications.\nWe introduce MEDIC, a framework assessing LLMs across five critical dimensions\nof clinical competence: medical reasoning, ethics and bias, data and language\nunderstanding, in-context learning, and clinical safety. MEDIC features a novel\ncross-examination framework quantifying LLM performance across areas like\ncoverage and hallucination detection, without requiring reference outputs. We\napply MEDIC to evaluate LLMs on medical question-answering, safety,\nsummarization, note generation, and other tasks. Our results show performance\ndisparities across model sizes, baseline vs medically finetuned models, and\nhave implications on model selection for applications requiring specific model\nstrengths, such as low hallucination or lower cost of inference. MEDIC's\nmultifaceted evaluation reveals these performance trade-offs, bridging the gap\nbetween theoretical capabilities and practical implementation in healthcare\nsettings, ensuring that the most promising models are identified and adapted\nfor diverse healthcare applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of Large Language Models (LLMs) for healthcare\napplications has spurred calls for holistic evaluation beyond frequently-cited\nbenchmarks like USMLE, to better reflect real-world performance. While\nreal-world assessments are valuable indicators of utility, they often lag\nbehind the pace of LLM evolution, likely rendering findings obsolete upon\ndeployment. This temporal disconnect necessitates a comprehensive upfront\nevaluation that can guide model selection for specific clinical applications.\nWe introduce MEDIC, a framework assessing LLMs across five critical dimensions\nof clinical competence: medical reasoning, ethics and bias, data and language\nunderstanding, in-context learning, and clinical safety. MEDIC features a novel\ncross-examination framework quantifying LLM performance across areas like\ncoverage and hallucination detection, without requiring reference outputs. We\napply MEDIC to evaluate LLMs on medical question-answering, safety,\nsummarization, note generation, and other tasks. Our results show performance\ndisparities across model sizes, baseline vs medically finetuned models, and\nhave implications on model selection for applications requiring specific model\nstrengths, such as low hallucination or lower cost of inference. MEDIC's\nmultifaceted evaluation reveals these performance trade-offs, bridging the gap\nbetween theoretical capabilities and practical implementation in healthcare\nsettings, ensuring that the most promising models are identified and adapted\nfor diverse healthcare applications."
                },
                "authors": [
                    {
                        "name": "Praveen K Kanithi"
                    },
                    {
                        "name": "Clément Christophe"
                    },
                    {
                        "name": "Marco AF Pimentel"
                    },
                    {
                        "name": "Tathagata Raha"
                    },
                    {
                        "name": "Nada Saadi"
                    },
                    {
                        "name": "Hamza Javed"
                    },
                    {
                        "name": "Svetlana Maslenkova"
                    },
                    {
                        "name": "Nasir Hayat"
                    },
                    {
                        "name": "Ronnie Rajan"
                    },
                    {
                        "name": "Shadab Khan"
                    }
                ],
                "author_detail": {
                    "name": "Shadab Khan"
                },
                "author": "Shadab Khan",
                "arxiv_comment": "Technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07314v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07314v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.05993v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.05993v2",
                "updated": "2024-09-11T14:42:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    14,
                    42,
                    29,
                    2,
                    255,
                    0
                ],
                "published": "2024-04-09T03:54:28Z",
                "published_parsed": [
                    2024,
                    4,
                    9,
                    3,
                    54,
                    28,
                    1,
                    100,
                    0
                ],
                "title": "AEGIS: Online Adaptive AI Content Safety Moderation with Ensemble of LLM\n  Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AEGIS: Online Adaptive AI Content Safety Moderation with Ensemble of LLM\n  Experts"
                },
                "summary": "As Large Language Models (LLMs) and generative AI become more widespread, the\ncontent safety risks associated with their use also increase. We find a notable\ndeficiency in high-quality content safety datasets and benchmarks that\ncomprehensively cover a wide range of critical safety areas. To address this,\nwe define a broad content safety risk taxonomy, comprising 13 critical risk and\n9 sparse risk categories. Additionally, we curate AEGISSAFETYDATASET, a new\ndataset of approximately 26, 000 human-LLM interaction instances, complete with\nhuman annotations adhering to the taxonomy. We plan to release this dataset to\nthe community to further research and to help benchmark LLM models for safety.\nTo demonstrate the effectiveness of the dataset, we instruction-tune multiple\nLLM-based safety models. We show that our models (named AEGISSAFETYEXPERTS),\nnot only surpass or perform competitively with the state-of-the-art LLM-based\nsafety models and general purpose LLMs, but also exhibit robustness across\nmultiple jail-break attack categories. We also show how using\nAEGISSAFETYDATASET during the LLM alignment phase does not negatively impact\nthe performance of the aligned models on MT Bench scores. Furthermore, we\npropose AEGIS, a novel application of a no-regret online adaptation framework\nwith strong theoretical guarantees, to perform content moderation with an\nensemble of LLM content safety experts in deployment",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) and generative AI become more widespread, the\ncontent safety risks associated with their use also increase. We find a notable\ndeficiency in high-quality content safety datasets and benchmarks that\ncomprehensively cover a wide range of critical safety areas. To address this,\nwe define a broad content safety risk taxonomy, comprising 13 critical risk and\n9 sparse risk categories. Additionally, we curate AEGISSAFETYDATASET, a new\ndataset of approximately 26, 000 human-LLM interaction instances, complete with\nhuman annotations adhering to the taxonomy. We plan to release this dataset to\nthe community to further research and to help benchmark LLM models for safety.\nTo demonstrate the effectiveness of the dataset, we instruction-tune multiple\nLLM-based safety models. We show that our models (named AEGISSAFETYEXPERTS),\nnot only surpass or perform competitively with the state-of-the-art LLM-based\nsafety models and general purpose LLMs, but also exhibit robustness across\nmultiple jail-break attack categories. We also show how using\nAEGISSAFETYDATASET during the LLM alignment phase does not negatively impact\nthe performance of the aligned models on MT Bench scores. Furthermore, we\npropose AEGIS, a novel application of a no-regret online adaptation framework\nwith strong theoretical guarantees, to perform content moderation with an\nensemble of LLM content safety experts in deployment"
                },
                "authors": [
                    {
                        "name": "Shaona Ghosh"
                    },
                    {
                        "name": "Prasoon Varshney"
                    },
                    {
                        "name": "Erick Galinkin"
                    },
                    {
                        "name": "Christopher Parisien"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Parisien"
                },
                "author": "Christopher Parisien",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.05993v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.05993v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07284v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07284v1",
                "updated": "2024-09-11T14:12:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    14,
                    12,
                    44,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T14:12:44Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    14,
                    12,
                    44,
                    2,
                    255,
                    0
                ],
                "title": "TLD-READY: Traffic Light Detection -- Relevance Estimation and\n  Deployment Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TLD-READY: Traffic Light Detection -- Relevance Estimation and\n  Deployment Analysis"
                },
                "summary": "Effective traffic light detection is a critical component of the perception\nstack in autonomous vehicles. This work introduces a novel deep-learning\ndetection system while addressing the challenges of previous work. Utilizing a\ncomprehensive dataset amalgamation, including the Bosch Small Traffic Lights\nDataset, LISA, the DriveU Traffic Light Dataset, and a proprietary dataset from\nKarlsruhe, we ensure a robust evaluation across varied scenarios. Furthermore,\nwe propose a relevance estimation system that innovatively uses directional\narrow markings on the road, eliminating the need for prior map creation. On the\nDriveU dataset, this approach results in 96% accuracy in relevance estimation.\nFinally, a real-world evaluation is performed to evaluate the deployment and\ngeneralizing abilities of these models. For reproducibility and to facilitate\nfurther research, we provide the model weights and code:\nhttps://github.com/KASTEL-MobilityLab/traffic-light-detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective traffic light detection is a critical component of the perception\nstack in autonomous vehicles. This work introduces a novel deep-learning\ndetection system while addressing the challenges of previous work. Utilizing a\ncomprehensive dataset amalgamation, including the Bosch Small Traffic Lights\nDataset, LISA, the DriveU Traffic Light Dataset, and a proprietary dataset from\nKarlsruhe, we ensure a robust evaluation across varied scenarios. Furthermore,\nwe propose a relevance estimation system that innovatively uses directional\narrow markings on the road, eliminating the need for prior map creation. On the\nDriveU dataset, this approach results in 96% accuracy in relevance estimation.\nFinally, a real-world evaluation is performed to evaluate the deployment and\ngeneralizing abilities of these models. For reproducibility and to facilitate\nfurther research, we provide the model weights and code:\nhttps://github.com/KASTEL-MobilityLab/traffic-light-detection."
                },
                "authors": [
                    {
                        "name": "Nikolai Polley"
                    },
                    {
                        "name": "Svetlana Pavlitska"
                    },
                    {
                        "name": "Yacin Boualili"
                    },
                    {
                        "name": "Patrick Rohrbeck"
                    },
                    {
                        "name": "Paul Stiller"
                    },
                    {
                        "name": "Ashok Kumar Bangaru"
                    },
                    {
                        "name": "J. Marius Zöllner"
                    }
                ],
                "author_detail": {
                    "name": "J. Marius Zöllner"
                },
                "author": "J. Marius Zöllner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07284v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07284v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07276v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07276v1",
                "updated": "2024-09-11T13:49:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    13,
                    49,
                    48,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T13:49:48Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    13,
                    49,
                    48,
                    2,
                    255,
                    0
                ],
                "title": "STORE: Streamlining Semantic Tokenization and Generative Recommendation\n  with A Single LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STORE: Streamlining Semantic Tokenization and Generative Recommendation\n  with A Single LLM"
                },
                "summary": "Traditional recommendation models often rely on unique item identifiers (IDs)\nto distinguish between items, which can hinder their ability to effectively\nleverage item content information and generalize to long-tail or cold-start\nitems. Recently, semantic tokenization has been proposed as a promising\nsolution that aims to tokenize each item's semantic representation into a\nsequence of discrete tokens. In this way, it preserves the item's semantics\nwithin these tokens and ensures that semantically similar items are represented\nby similar tokens. These semantic tokens have become fundamental in training\ngenerative recommendation models. However, existing generative recommendation\nmethods typically involve multiple sub-models for embedding, quantization, and\nrecommendation, leading to an overly complex system. In this paper, we propose\nto streamline the semantic tokenization and generative recommendation process\nwith a unified framework, dubbed STORE, which leverages a single large language\nmodel (LLM) for both tasks. Specifically, we formulate semantic tokenization as\na text-to-token task and generative recommendation as a token-to-token task,\nsupplemented by a token-to-text reconstruction task and a text-to-token\nauxiliary task. All these tasks are framed in a generative manner and trained\nusing a single LLM backbone. Extensive experiments have been conducted to\nvalidate the effectiveness of our STORE framework across various recommendation\ntasks and datasets. We will release the source code and configurations for\nreproducible research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional recommendation models often rely on unique item identifiers (IDs)\nto distinguish between items, which can hinder their ability to effectively\nleverage item content information and generalize to long-tail or cold-start\nitems. Recently, semantic tokenization has been proposed as a promising\nsolution that aims to tokenize each item's semantic representation into a\nsequence of discrete tokens. In this way, it preserves the item's semantics\nwithin these tokens and ensures that semantically similar items are represented\nby similar tokens. These semantic tokens have become fundamental in training\ngenerative recommendation models. However, existing generative recommendation\nmethods typically involve multiple sub-models for embedding, quantization, and\nrecommendation, leading to an overly complex system. In this paper, we propose\nto streamline the semantic tokenization and generative recommendation process\nwith a unified framework, dubbed STORE, which leverages a single large language\nmodel (LLM) for both tasks. Specifically, we formulate semantic tokenization as\na text-to-token task and generative recommendation as a token-to-token task,\nsupplemented by a token-to-text reconstruction task and a text-to-token\nauxiliary task. All these tasks are framed in a generative manner and trained\nusing a single LLM backbone. Extensive experiments have been conducted to\nvalidate the effectiveness of our STORE framework across various recommendation\ntasks and datasets. We will release the source code and configurations for\nreproducible research."
                },
                "authors": [
                    {
                        "name": "Qijiong Liu"
                    },
                    {
                        "name": "Jieming Zhu"
                    },
                    {
                        "name": "Lu Fan"
                    },
                    {
                        "name": "Zhou Zhao"
                    },
                    {
                        "name": "Xiao-Ming Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xiao-Ming Wu"
                },
                "author": "Xiao-Ming Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07276v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07267v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07267v1",
                "updated": "2024-09-11T13:43:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    13,
                    43,
                    1,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T13:43:01Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    13,
                    43,
                    1,
                    2,
                    255,
                    0
                ],
                "title": "MiniDrive: More Efficient Vision-Language Models with Multi-Level 2D\n  Features as Text Tokens for Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiniDrive: More Efficient Vision-Language Models with Multi-Level 2D\n  Features as Text Tokens for Autonomous Driving"
                },
                "summary": "Vision-language models (VLMs) serve as general-purpose end-to-end models in\nautonomous driving, performing subtasks such as prediction, planning, and\nperception through question-and-answer interactions. However, most existing\nmethods rely on computationally expensive visual encoders and large language\nmodels (LLMs), making them difficult to deploy in real-world scenarios and\nreal-time applications. Meanwhile, most existing VLMs lack the ability to\nprocess multiple images, making it difficult to adapt to multi-camera\nperception in autonomous driving. To address these issues, we propose a novel\nframework called MiniDrive, which incorporates our proposed Feature Engineering\nMixture of Experts (FE-MoE) module and Dynamic Instruction Adapter\n(DI-Adapter). The FE-MoE effectively maps 2D features into visual token\nembeddings before being input into the language model. The DI-Adapter enables\nthe visual token embeddings to dynamically change with the instruction text\nembeddings, resolving the issue of static visual token embeddings for the same\nimage in previous approaches. Compared to previous works, MiniDrive achieves\nstate-of-the-art performance in terms of parameter size, floating point\noperations, and response efficiency, with the smallest version containing only\n83M parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) serve as general-purpose end-to-end models in\nautonomous driving, performing subtasks such as prediction, planning, and\nperception through question-and-answer interactions. However, most existing\nmethods rely on computationally expensive visual encoders and large language\nmodels (LLMs), making them difficult to deploy in real-world scenarios and\nreal-time applications. Meanwhile, most existing VLMs lack the ability to\nprocess multiple images, making it difficult to adapt to multi-camera\nperception in autonomous driving. To address these issues, we propose a novel\nframework called MiniDrive, which incorporates our proposed Feature Engineering\nMixture of Experts (FE-MoE) module and Dynamic Instruction Adapter\n(DI-Adapter). The FE-MoE effectively maps 2D features into visual token\nembeddings before being input into the language model. The DI-Adapter enables\nthe visual token embeddings to dynamically change with the instruction text\nembeddings, resolving the issue of static visual token embeddings for the same\nimage in previous approaches. Compared to previous works, MiniDrive achieves\nstate-of-the-art performance in terms of parameter size, floating point\noperations, and response efficiency, with the smallest version containing only\n83M parameters."
                },
                "authors": [
                    {
                        "name": "Enming Zhang"
                    },
                    {
                        "name": "Xingyuan Dai"
                    },
                    {
                        "name": "Yisheng Lv"
                    },
                    {
                        "name": "Qianghai Miao"
                    }
                ],
                "author_detail": {
                    "name": "Qianghai Miao"
                },
                "author": "Qianghai Miao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07267v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07267v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20441v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20441v2",
                "updated": "2024-09-11T13:11:16Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    13,
                    11,
                    16,
                    2,
                    255,
                    0
                ],
                "published": "2024-05-30T19:35:06Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    19,
                    35,
                    6,
                    3,
                    151,
                    0
                ],
                "title": "SECURE: Benchmarking Large Language Models for Cybersecurity Advisory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SECURE: Benchmarking Large Language Models for Cybersecurity Advisory"
                },
                "summary": "Large Language Models (LLMs) have demonstrated potential in cybersecurity\napplications but have also caused lower confidence due to problems like\nhallucinations and a lack of truthfulness. Existing benchmarks provide general\nevaluations but do not sufficiently address the practical and applied aspects\nof LLM performance in cybersecurity-specific tasks. To address this gap, we\nintroduce the SECURE (Security Extraction, Understanding \\& Reasoning\nEvaluation), a benchmark designed to assess LLMs performance in realistic\ncybersecurity scenarios. SECURE includes six datasets focussed on the\nIndustrial Control System sector to evaluate knowledge extraction,\nunderstanding, and reasoning based on industry-standard sources. Our study\nevaluates seven state-of-the-art models on these tasks, providing insights into\ntheir strengths and weaknesses in cybersecurity contexts, and offer\nrecommendations for improving LLMs reliability as cyber advisory tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated potential in cybersecurity\napplications but have also caused lower confidence due to problems like\nhallucinations and a lack of truthfulness. Existing benchmarks provide general\nevaluations but do not sufficiently address the practical and applied aspects\nof LLM performance in cybersecurity-specific tasks. To address this gap, we\nintroduce the SECURE (Security Extraction, Understanding \\& Reasoning\nEvaluation), a benchmark designed to assess LLMs performance in realistic\ncybersecurity scenarios. SECURE includes six datasets focussed on the\nIndustrial Control System sector to evaluate knowledge extraction,\nunderstanding, and reasoning based on industry-standard sources. Our study\nevaluates seven state-of-the-art models on these tasks, providing insights into\ntheir strengths and weaknesses in cybersecurity contexts, and offer\nrecommendations for improving LLMs reliability as cyber advisory tools."
                },
                "authors": [
                    {
                        "name": "Dipkamal Bhusal"
                    },
                    {
                        "name": "Md Tanvirul Alam"
                    },
                    {
                        "name": "Le Nguyen"
                    },
                    {
                        "name": "Ashim Mahara"
                    },
                    {
                        "name": "Zachary Lightcap"
                    },
                    {
                        "name": "Rodney Frazier"
                    },
                    {
                        "name": "Romy Fieblinger"
                    },
                    {
                        "name": "Grace Long Torales"
                    },
                    {
                        "name": "Nidhi Rastogi"
                    }
                ],
                "author_detail": {
                    "name": "Nidhi Rastogi"
                },
                "author": "Nidhi Rastogi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20441v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20441v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07246v1",
                "updated": "2024-09-11T13:04:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    13,
                    4,
                    34,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T13:04:34Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    13,
                    4,
                    34,
                    2,
                    255,
                    0
                ],
                "title": "Propaganda to Hate: A Multimodal Analysis of Arabic Memes with\n  Multi-Agent LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Propaganda to Hate: A Multimodal Analysis of Arabic Memes with\n  Multi-Agent LLMs"
                },
                "summary": "In the past decade, social media platforms have been used for information\ndissemination and consumption. While a major portion of the content is posted\nto promote citizen journalism and public awareness, some content is posted to\nmislead users. Among different content types such as text, images, and videos,\nmemes (text overlaid on images) are particularly prevalent and can serve as\npowerful vehicles for propaganda, hate, and humor. In the current literature,\nthere have been efforts to individually detect such content in memes. However,\nthe study of their intersection is very limited. In this study, we explore the\nintersection between propaganda and hate in memes using a multi-agent LLM-based\napproach. We extend the propagandistic meme dataset with coarse and\nfine-grained hate labels. Our finding suggests that there is an association\nbetween propaganda and hate in memes. We provide detailed experimental results\nthat can serve as a baseline for future studies. We will make the experimental\nresources publicly available to the community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the past decade, social media platforms have been used for information\ndissemination and consumption. While a major portion of the content is posted\nto promote citizen journalism and public awareness, some content is posted to\nmislead users. Among different content types such as text, images, and videos,\nmemes (text overlaid on images) are particularly prevalent and can serve as\npowerful vehicles for propaganda, hate, and humor. In the current literature,\nthere have been efforts to individually detect such content in memes. However,\nthe study of their intersection is very limited. In this study, we explore the\nintersection between propaganda and hate in memes using a multi-agent LLM-based\napproach. We extend the propagandistic meme dataset with coarse and\nfine-grained hate labels. Our finding suggests that there is an association\nbetween propaganda and hate in memes. We provide detailed experimental results\nthat can serve as a baseline for future studies. We will make the experimental\nresources publicly available to the community."
                },
                "authors": [
                    {
                        "name": "Firoj Alam"
                    },
                    {
                        "name": "Md. Rafiul Biswas"
                    },
                    {
                        "name": "Uzair Shah"
                    },
                    {
                        "name": "Wajdi Zaghouani"
                    },
                    {
                        "name": "Georgios Mikros"
                    }
                ],
                "author_detail": {
                    "name": "Georgios Mikros"
                },
                "author": "Georgios Mikros",
                "arxiv_comment": "propaganda, hate-speech, disinformation, misinformation, fake news,\n  LLMs, GPT-4, multimodality, multimodal LLMs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07239v1",
                "updated": "2024-09-11T12:53:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    12,
                    53,
                    7,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T12:53:07Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    12,
                    53,
                    7,
                    2,
                    255,
                    0
                ],
                "title": "PiTe: Pixel-Temporal Alignment for Large Video-Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PiTe: Pixel-Temporal Alignment for Large Video-Language Model"
                },
                "summary": "Fueled by the Large Language Models (LLMs) wave, Large Visual-Language Models\n(LVLMs) have emerged as a pivotal advancement, bridging the gap between image\nand text. However, video making it challenging for LVLMs to perform adequately\ndue to the complexity of the relationship between language and spatial-temporal\ndata structure. Recent Large Video-Language Models (LVidLMs) align feature of\nstatic visual data like image into latent space of language feature, by general\nmulti-modal tasks to leverage abilities of LLMs sufficiently. In this paper, we\nexplore fine-grained alignment approach via object trajectory for different\nmodalities across both spatial and temporal dimensions simultaneously. Thus, we\npropose a novel LVidLM by trajectory-guided Pixel-Temporal Alignment, dubbed\nPiTe, that exhibits promising applicable model property. To achieve\nfine-grained video-language alignment, we curate a multi-modal pre-training\ndataset PiTe-143k, the dataset provision of moving trajectories in pixel level\nfor all individual objects, that appear and mention in the video and caption\nboth, by our automatic annotation pipeline. Meanwhile, PiTe demonstrates\nastounding capabilities on myriad video-related multi-modal tasks through beat\nthe state-of-the-art methods by a large margin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fueled by the Large Language Models (LLMs) wave, Large Visual-Language Models\n(LVLMs) have emerged as a pivotal advancement, bridging the gap between image\nand text. However, video making it challenging for LVLMs to perform adequately\ndue to the complexity of the relationship between language and spatial-temporal\ndata structure. Recent Large Video-Language Models (LVidLMs) align feature of\nstatic visual data like image into latent space of language feature, by general\nmulti-modal tasks to leverage abilities of LLMs sufficiently. In this paper, we\nexplore fine-grained alignment approach via object trajectory for different\nmodalities across both spatial and temporal dimensions simultaneously. Thus, we\npropose a novel LVidLM by trajectory-guided Pixel-Temporal Alignment, dubbed\nPiTe, that exhibits promising applicable model property. To achieve\nfine-grained video-language alignment, we curate a multi-modal pre-training\ndataset PiTe-143k, the dataset provision of moving trajectories in pixel level\nfor all individual objects, that appear and mention in the video and caption\nboth, by our automatic annotation pipeline. Meanwhile, PiTe demonstrates\nastounding capabilities on myriad video-related multi-modal tasks through beat\nthe state-of-the-art methods by a large margin."
                },
                "authors": [
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Pengxiang Ding"
                    },
                    {
                        "name": "Siteng Huang"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Han Zhao"
                    },
                    {
                        "name": "Donglin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Donglin Wang"
                },
                "author": "Donglin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.00374v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.00374v3",
                "updated": "2024-09-11T12:48:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    12,
                    48,
                    42,
                    2,
                    255,
                    0
                ],
                "published": "2023-12-01T06:36:17Z",
                "published_parsed": [
                    2023,
                    12,
                    1,
                    6,
                    36,
                    17,
                    4,
                    335,
                    0
                ],
                "title": "The Philosopher's Stone: Trojaning Plugins of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Philosopher's Stone: Trojaning Plugins of Large Language Models"
                },
                "summary": "Open-source Large Language Models (LLMs) have recently gained popularity\nbecause of their comparable performance to proprietary LLMs. To efficiently\nfulfill domain-specialized tasks, open-source LLMs can be refined, without\nexpensive accelerators, using low-rank adapters. However, it is still unknown\nwhether low-rank adapters can be exploited to control LLMs. To address this\ngap, we demonstrate that an infected adapter can induce, on specific\ntriggers,an LLM to output content defined by an adversary and to even\nmaliciously use tools. To train a Trojan adapter, we propose two novel attacks,\nPOLISHED and FUSION, that improve over prior approaches. POLISHED uses a\nsuperior LLM to align na\\\"ively poisoned data based on our insight that it can\nbetter inject poisoning knowledge during training. In contrast, FUSION\nleverages a novel over-poisoning procedure to transform a benign adapter into a\nmalicious one by magnifying the attention between trigger and target in model\nweights. In our experiments, we first conduct two case studies to demonstrate\nthat a compromised LLM agent can use malware to control the system (e.g., a\nLLM-driven robot) or to launch a spear-phishing attack. Then, in terms of\ntargeted misinformation, we show that our attacks provide higher attack\neffectiveness than the existing baseline and, for the purpose of attracting\ndownloads, preserve or improve the adapter's utility. Finally, we designed and\nevaluated three potential defenses. However, none proved entirely effective in\nsafeguarding against our attacks, highlighting the need for more robust\ndefenses supporting a secure LLM supply chain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-source Large Language Models (LLMs) have recently gained popularity\nbecause of their comparable performance to proprietary LLMs. To efficiently\nfulfill domain-specialized tasks, open-source LLMs can be refined, without\nexpensive accelerators, using low-rank adapters. However, it is still unknown\nwhether low-rank adapters can be exploited to control LLMs. To address this\ngap, we demonstrate that an infected adapter can induce, on specific\ntriggers,an LLM to output content defined by an adversary and to even\nmaliciously use tools. To train a Trojan adapter, we propose two novel attacks,\nPOLISHED and FUSION, that improve over prior approaches. POLISHED uses a\nsuperior LLM to align na\\\"ively poisoned data based on our insight that it can\nbetter inject poisoning knowledge during training. In contrast, FUSION\nleverages a novel over-poisoning procedure to transform a benign adapter into a\nmalicious one by magnifying the attention between trigger and target in model\nweights. In our experiments, we first conduct two case studies to demonstrate\nthat a compromised LLM agent can use malware to control the system (e.g., a\nLLM-driven robot) or to launch a spear-phishing attack. Then, in terms of\ntargeted misinformation, we show that our attacks provide higher attack\neffectiveness than the existing baseline and, for the purpose of attracting\ndownloads, preserve or improve the adapter's utility. Finally, we designed and\nevaluated three potential defenses. However, none proved entirely effective in\nsafeguarding against our attacks, highlighting the need for more robust\ndefenses supporting a secure LLM supply chain."
                },
                "authors": [
                    {
                        "name": "Tian Dong"
                    },
                    {
                        "name": "Minhui Xue"
                    },
                    {
                        "name": "Guoxing Chen"
                    },
                    {
                        "name": "Rayne Holland"
                    },
                    {
                        "name": "Yan Meng"
                    },
                    {
                        "name": "Shaofeng Li"
                    },
                    {
                        "name": "Zhen Liu"
                    },
                    {
                        "name": "Haojin Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Haojin Zhu"
                },
                "author": "Haojin Zhu",
                "arxiv_comment": "Accepted by NDSS Symposium 2025. Please cite this paper as \"Tian\n  Dong, Minhui Xue, Guoxing Chen, Rayne Holland, Yan Meng, Shaofeng Li, Zhen\n  Liu, Haojin Zhu. The Philosopher's Stone: Trojaning Plugins of Large Language\n  Models. In the 32nd Annual Network and Distributed System Security Symposium\n  (NDSS 2025).\"",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.00374v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.00374v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13555v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13555v2",
                "updated": "2024-09-11T12:19:14Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    12,
                    19,
                    14,
                    2,
                    255,
                    0
                ],
                "published": "2024-06-19T13:44:56Z",
                "published_parsed": [
                    2024,
                    6,
                    19,
                    13,
                    44,
                    56,
                    2,
                    171,
                    0
                ],
                "title": "BiLD: Bi-directional Logits Difference Loss for Large Language Model\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BiLD: Bi-directional Logits Difference Loss for Large Language Model\n  Distillation"
                },
                "summary": "In recent years, large language models (LLMs) have shown exceptional\ncapabilities across various natural language processing (NLP) tasks. However,\nsuch impressive performance often comes with the trade-off of an increased\nparameter size, posing significant challenges for widespread deployment.\nKnowledge distillation (KD) provides a solution by transferring knowledge from\na large teacher model to a smaller student model. In this paper, we explore the\ntask-specific distillation of LLMs at the logit level. Our investigation\nreveals that the logits of fine-tuned LLMs exhibit a more extreme long-tail\ndistribution than those from vision models, with hidden \"noise\" in the long\ntail affecting distillation performance. Furthermore, existing logits\ndistillation methods often struggle to effectively utilize the internal ranking\ninformation from the logits. To address these, we propose the Bi-directional\nLogits Difference (BiLD) loss. The BiLD loss filters out the long-tail noise by\nutilizing only top-$k$ teacher and student logits, and leverages the internal\nlogits ranking information by constructing logits differences. To evaluate BiLD\nloss, we conduct comprehensive experiments on 13 datasets using two types of\nLLMs. Our results show that the BiLD loss, with only the top-8 logits,\noutperforms supervised fine-tuning (SFT), vanilla KL loss, and five other\ndistillation methods from both NLP and CV fields.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, large language models (LLMs) have shown exceptional\ncapabilities across various natural language processing (NLP) tasks. However,\nsuch impressive performance often comes with the trade-off of an increased\nparameter size, posing significant challenges for widespread deployment.\nKnowledge distillation (KD) provides a solution by transferring knowledge from\na large teacher model to a smaller student model. In this paper, we explore the\ntask-specific distillation of LLMs at the logit level. Our investigation\nreveals that the logits of fine-tuned LLMs exhibit a more extreme long-tail\ndistribution than those from vision models, with hidden \"noise\" in the long\ntail affecting distillation performance. Furthermore, existing logits\ndistillation methods often struggle to effectively utilize the internal ranking\ninformation from the logits. To address these, we propose the Bi-directional\nLogits Difference (BiLD) loss. The BiLD loss filters out the long-tail noise by\nutilizing only top-$k$ teacher and student logits, and leverages the internal\nlogits ranking information by constructing logits differences. To evaluate BiLD\nloss, we conduct comprehensive experiments on 13 datasets using two types of\nLLMs. Our results show that the BiLD loss, with only the top-8 logits,\noutperforms supervised fine-tuning (SFT), vanilla KL loss, and five other\ndistillation methods from both NLP and CV fields."
                },
                "authors": [
                    {
                        "name": "Minchong Li"
                    },
                    {
                        "name": "Feng Zhou"
                    },
                    {
                        "name": "Xiaohui Song"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohui Song"
                },
                "author": "Xiaohui Song",
                "arxiv_comment": "Submitted to COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13555v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13555v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02616v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02616v5",
                "updated": "2024-09-11T11:59:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    11,
                    59,
                    25,
                    2,
                    255,
                    0
                ],
                "published": "2024-06-03T09:41:42Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    9,
                    41,
                    42,
                    0,
                    155,
                    0
                ],
                "title": "Adaptive Layer Splitting for Wireless LLM Inference in Edge Computing: A\n  Model-Based Reinforcement Learning Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Layer Splitting for Wireless LLM Inference in Edge Computing: A\n  Model-Based Reinforcement Learning Approach"
                },
                "summary": "Optimizing the deployment of large language models (LLMs) in edge computing\nenvironments is critical for enhancing privacy and computational efficiency.\nToward efficient wireless LLM inference in edge computing, this study\ncomprehensively analyzes the impact of different splitting points in mainstream\nopen-source LLMs. On this basis, this study introduces a framework taking\ninspiration from model-based reinforcement learning (MBRL) to determine the\noptimal splitting point across the edge and user equipment (UE). By\nincorporating a reward surrogate model, our approach significantly reduces the\ncomputational cost of frequent performance evaluations. Extensive simulations\ndemonstrate that this method effectively balances inference performance and\ncomputational load under varying network conditions, providing a robust\nsolution for LLM deployment in decentralized settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing the deployment of large language models (LLMs) in edge computing\nenvironments is critical for enhancing privacy and computational efficiency.\nToward efficient wireless LLM inference in edge computing, this study\ncomprehensively analyzes the impact of different splitting points in mainstream\nopen-source LLMs. On this basis, this study introduces a framework taking\ninspiration from model-based reinforcement learning (MBRL) to determine the\noptimal splitting point across the edge and user equipment (UE). By\nincorporating a reward surrogate model, our approach significantly reduces the\ncomputational cost of frequent performance evaluations. Extensive simulations\ndemonstrate that this method effectively balances inference performance and\ncomputational load under varying network conditions, providing a robust\nsolution for LLM deployment in decentralized settings."
                },
                "authors": [
                    {
                        "name": "Yuxuan Chen"
                    },
                    {
                        "name": "Rongpeng Li"
                    },
                    {
                        "name": "Xiaoxue Yu"
                    },
                    {
                        "name": "Zhifeng Zhao"
                    },
                    {
                        "name": "Honggang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Honggang Zhang"
                },
                "author": "Honggang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02616v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02616v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07178v1",
                "updated": "2024-09-11T10:41:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    10,
                    41,
                    5,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T10:41:05Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    10,
                    41,
                    5,
                    2,
                    255,
                    0
                ],
                "title": "Identify Design Problems Through Questioning: Exploring Role-playing\n  Interactions with Large Language Models to Foster Design Questioning Skills",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identify Design Problems Through Questioning: Exploring Role-playing\n  Interactions with Large Language Models to Foster Design Questioning Skills"
                },
                "summary": "Identifying design problems is a crucial step for creating plausible\nsolutions, but it is challenging for design novices due to their limited\nknowledge and experience. Questioning is a promising skill that enables\nstudents to independently identify design problems without being passive or\nrelying on instructors. This study explores role-playing interactions with\nLarge Language Model (LLM)-powered Conversational Agents (CAs) to foster the\nquestioning skills of novice design students. We proposed an LLM-powered CA\nprototype and conducted a preliminary study with 16 novice design students\nengaged in a real-world design class to observe the interactions between\nstudents and the LLM-powered CAs. Our findings indicate that while the CAs\nstimulated questioning and reduced pressure to ask questions, it also\ninadvertently led to over-reliance on LLM responses. We proposed design\nconsiderations and future works for LLM-powered CA to foster questioning\nskills.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying design problems is a crucial step for creating plausible\nsolutions, but it is challenging for design novices due to their limited\nknowledge and experience. Questioning is a promising skill that enables\nstudents to independently identify design problems without being passive or\nrelying on instructors. This study explores role-playing interactions with\nLarge Language Model (LLM)-powered Conversational Agents (CAs) to foster the\nquestioning skills of novice design students. We proposed an LLM-powered CA\nprototype and conducted a preliminary study with 16 novice design students\nengaged in a real-world design class to observe the interactions between\nstudents and the LLM-powered CAs. Our findings indicate that while the CAs\nstimulated questioning and reduced pressure to ask questions, it also\ninadvertently led to over-reliance on LLM responses. We proposed design\nconsiderations and future works for LLM-powered CA to foster questioning\nskills."
                },
                "authors": [
                    {
                        "name": "Hyunseung Lim"
                    },
                    {
                        "name": "Dasom Choi"
                    },
                    {
                        "name": "Hwajung Hong"
                    }
                ],
                "author_detail": {
                    "name": "Hwajung Hong"
                },
                "author": "Hwajung Hong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07165v1",
                "updated": "2024-09-11T10:24:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    10,
                    24,
                    43,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T10:24:43Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    10,
                    24,
                    43,
                    2,
                    255,
                    0
                ],
                "title": "Linear Time Complexity Conformers with SummaryMixing for Streaming\n  Speech Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear Time Complexity Conformers with SummaryMixing for Streaming\n  Speech Recognition"
                },
                "summary": "Automatic speech recognition (ASR) with an encoder equipped with\nself-attention, whether streaming or non-streaming, takes quadratic time in the\nlength of the speech utterance. This slows down training and decoding, increase\ntheir cost, and limit the deployment of the ASR in constrained devices.\nSummaryMixing is a promising linear-time complexity alternative to\nself-attention for non-streaming speech recognition that, for the first time,\npreserves or outperforms the accuracy of self-attention models. Unfortunately,\nthe original definition of SummaryMixing is not suited to streaming speech\nrecognition. Hence, this work extends SummaryMixing to a Conformer Transducer\nthat works in both a streaming and an offline mode. It shows that this new\nlinear-time complexity speech encoder outperforms self-attention in both\nscenarios while requiring less compute and memory during training and decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic speech recognition (ASR) with an encoder equipped with\nself-attention, whether streaming or non-streaming, takes quadratic time in the\nlength of the speech utterance. This slows down training and decoding, increase\ntheir cost, and limit the deployment of the ASR in constrained devices.\nSummaryMixing is a promising linear-time complexity alternative to\nself-attention for non-streaming speech recognition that, for the first time,\npreserves or outperforms the accuracy of self-attention models. Unfortunately,\nthe original definition of SummaryMixing is not suited to streaming speech\nrecognition. Hence, this work extends SummaryMixing to a Conformer Transducer\nthat works in both a streaming and an offline mode. It shows that this new\nlinear-time complexity speech encoder outperforms self-attention in both\nscenarios while requiring less compute and memory during training and decoding."
                },
                "authors": [
                    {
                        "name": "Titouan Parcollet"
                    },
                    {
                        "name": "Rogier van Dalen"
                    },
                    {
                        "name": "Shucong Zhang"
                    },
                    {
                        "name": "Sourav Batthacharya"
                    }
                ],
                "author_detail": {
                    "name": "Sourav Batthacharya"
                },
                "author": "Sourav Batthacharya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07162v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07162v1",
                "updated": "2024-09-11T10:21:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    10,
                    21,
                    13,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T10:21:13Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    10,
                    21,
                    13,
                    2,
                    255,
                    0
                ],
                "title": "A Fine-grained Sentiment Analysis of App Reviews using Large Language\n  Models: An Evaluation Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Fine-grained Sentiment Analysis of App Reviews using Large Language\n  Models: An Evaluation Study"
                },
                "summary": "Analyzing user reviews for sentiment towards app features can provide\nvaluable insights into users' perceptions of app functionality and their\nevolving needs. Given the volume of user reviews received daily, an automated\nmechanism to generate feature-level sentiment summaries of user reviews is\nneeded. Recent advances in Large Language Models (LLMs) such as ChatGPT have\nshown impressive performance on several new tasks without updating the model's\nparameters i.e. using zero or a few labeled examples. Despite these\nadvancements, LLMs' capabilities to perform feature-specific sentiment analysis\nof user reviews remain unexplored. This study compares the performance of\nstate-of-the-art LLMs, including GPT-4, ChatGPT, and LLama-2-chat variants, for\nextracting app features and associated sentiments under 0-shot, 1-shot, and\n5-shot scenarios. Results indicate the best-performing GPT-4 model outperforms\nrule-based approaches by 23.6% in f1-score with zero-shot feature extraction;\n5-shot further improving it by 6%. GPT-4 achieves a 74% f1-score for predicting\npositive sentiment towards correctly predicted app features, with 5-shot\nenhancing it by 7%. Our study suggests that LLM models are promising for\ngenerating feature-specific sentiment summaries of user reviews.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing user reviews for sentiment towards app features can provide\nvaluable insights into users' perceptions of app functionality and their\nevolving needs. Given the volume of user reviews received daily, an automated\nmechanism to generate feature-level sentiment summaries of user reviews is\nneeded. Recent advances in Large Language Models (LLMs) such as ChatGPT have\nshown impressive performance on several new tasks without updating the model's\nparameters i.e. using zero or a few labeled examples. Despite these\nadvancements, LLMs' capabilities to perform feature-specific sentiment analysis\nof user reviews remain unexplored. This study compares the performance of\nstate-of-the-art LLMs, including GPT-4, ChatGPT, and LLama-2-chat variants, for\nextracting app features and associated sentiments under 0-shot, 1-shot, and\n5-shot scenarios. Results indicate the best-performing GPT-4 model outperforms\nrule-based approaches by 23.6% in f1-score with zero-shot feature extraction;\n5-shot further improving it by 6%. GPT-4 achieves a 74% f1-score for predicting\npositive sentiment towards correctly predicted app features, with 5-shot\nenhancing it by 7%. Our study suggests that LLM models are promising for\ngenerating feature-specific sentiment summaries of user reviews."
                },
                "authors": [
                    {
                        "name": "Faiz Ali Shah"
                    },
                    {
                        "name": "Ahmed Sabir"
                    },
                    {
                        "name": "Rajesh Sharma"
                    }
                ],
                "author_detail": {
                    "name": "Rajesh Sharma"
                },
                "author": "Rajesh Sharma",
                "arxiv_comment": "The summary of the project is available at\n  https://ahmed.jp/project_page/App_LLMs_2024/app_llms.html",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07162v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07162v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2109.08783v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2109.08783v2",
                "updated": "2024-09-11T10:10:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    10,
                    10,
                    13,
                    2,
                    255,
                    0
                ],
                "published": "2021-09-17T23:38:02Z",
                "published_parsed": [
                    2021,
                    9,
                    17,
                    23,
                    38,
                    2,
                    4,
                    260,
                    0
                ],
                "title": "From the Beginning: Key Transitions in the First 15 Years of DNSSEC",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From the Beginning: Key Transitions in the First 15 Years of DNSSEC"
                },
                "summary": "When the global rollout of the DNS Security Extensions (DNSSEC) began in\n2005, a first-of-its-kind trial started: The complexity of a core Internet\nprotocol was magnified in favor of better security for the overall Internet.\nThereby, the scale of the loosely-federated delegation in DNS became an\nunprecedented cryptographic key management challenge. Though fundamental for\ncurrent and future operational success, our community lacks a clear notion of\nhow to empirically evaluate the process of securely transitioning keys.\n  In this paper, we propose two building blocks to formally characterize and\nassess key transitions. First, the anatomy of key transitions, i.e., measurable\nand well-defined properties of key changes; and second, a novel classification\nmodel based on this anatomy for describing key transition practices in abstract\nterms. This abstraction allows for classifying operational behavior. We apply\nour proposed transition anatomy and transition classes to describe the global\nDNSSEC deployment. Specifically, we use measurements from the first 15 years of\nthe DNSSEC rollout to detect and understand which key transitions have been\nused to what degree and which rates of errors and warnings occurred. In\ncontrast to prior work, we consider all possible transitions and not only 1:1\nkey rollovers. Our results show measurable gaps between prescribed key\nmanagement processes and key transitions in the wild. We also find evidence\nthat such noncompliant transitions are needed in operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When the global rollout of the DNS Security Extensions (DNSSEC) began in\n2005, a first-of-its-kind trial started: The complexity of a core Internet\nprotocol was magnified in favor of better security for the overall Internet.\nThereby, the scale of the loosely-federated delegation in DNS became an\nunprecedented cryptographic key management challenge. Though fundamental for\ncurrent and future operational success, our community lacks a clear notion of\nhow to empirically evaluate the process of securely transitioning keys.\n  In this paper, we propose two building blocks to formally characterize and\nassess key transitions. First, the anatomy of key transitions, i.e., measurable\nand well-defined properties of key changes; and second, a novel classification\nmodel based on this anatomy for describing key transition practices in abstract\nterms. This abstraction allows for classifying operational behavior. We apply\nour proposed transition anatomy and transition classes to describe the global\nDNSSEC deployment. Specifically, we use measurements from the first 15 years of\nthe DNSSEC rollout to detect and understand which key transitions have been\nused to what degree and which rates of errors and warnings occurred. In\ncontrast to prior work, we consider all possible transitions and not only 1:1\nkey rollovers. Our results show measurable gaps between prescribed key\nmanagement processes and key transitions in the wild. We also find evidence\nthat such noncompliant transitions are needed in operations."
                },
                "authors": [
                    {
                        "name": "Eric Osterweil"
                    },
                    {
                        "name": "Pouyan Fotouhi Tehrani"
                    },
                    {
                        "name": "Thomas C. Schmidt"
                    },
                    {
                        "name": "Matthias Wählisch"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Wählisch"
                },
                "author": "Matthias Wählisch",
                "arxiv_doi": "10.1109/TNSM.2022.3195406",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TNSM.2022.3195406",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2109.08783v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2109.08783v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Transactions on Network and Service Management, Vol. 19, No.\n  4, pp. 5265-5283, Dec. 2022",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11346v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11346v3",
                "updated": "2024-09-11T10:05:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    10,
                    5,
                    37,
                    2,
                    255,
                    0
                ],
                "published": "2024-06-17T09:08:30Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    9,
                    8,
                    30,
                    0,
                    169,
                    0
                ],
                "title": "WaDec: Decompiling WebAssembly Using Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WaDec: Decompiling WebAssembly Using Large Language Model"
                },
                "summary": "WebAssembly (abbreviated Wasm) has emerged as a cornerstone of web\ndevelopment, offering a compact binary format that allows high-performance\napplications to run at near-native speeds in web browsers. Despite its\nadvantages, Wasm's binary nature presents significant challenges for developers\nand researchers, particularly regarding readability when debugging or analyzing\nweb applications. Therefore, effective decompilation becomes crucial.\nUnfortunately, traditional decompilers often struggle with producing readable\noutputs. While some large language model (LLM)-based decompilers have shown\ngood compatibility with general binary files, they still face specific\nchallenges when dealing with Wasm. In this paper, we introduce a novel\napproach, WaDec, which is the first use of a fine-tuned LLM to interpret and\ndecompile Wasm binary code into a higher-level, more comprehensible source code\nrepresentation. The LLM was meticulously fine-tuned using a specialized dataset\nof wat-c code snippets, employing self-supervised learning techniques. This\nenables WaDec to effectively decompile not only complete wat functions but also\nfiner-grained wat code snippets. Our experiments demonstrate that WaDec\nmarkedly outperforms current state-of-the-art tools, offering substantial\nimprovements across several metrics. It achieves a code inflation rate of only\n3.34%, a dramatic 97% reduction compared to the state-of-the-art's 116.94%.\nUnlike baselines' output that cannot be directly compiled or executed, WaDec\nmaintains a recompilability rate of 52.11%, a re-execution rate of 43.55%, and\nan output consistency of 27.15%. Additionally, it significantly exceeds\nstate-of-the-art performance in AST edit distance similarity by 185%,\ncyclomatic complexity by 8%, and cosine similarity by 41%, achieving an average\ncode similarity above 50%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WebAssembly (abbreviated Wasm) has emerged as a cornerstone of web\ndevelopment, offering a compact binary format that allows high-performance\napplications to run at near-native speeds in web browsers. Despite its\nadvantages, Wasm's binary nature presents significant challenges for developers\nand researchers, particularly regarding readability when debugging or analyzing\nweb applications. Therefore, effective decompilation becomes crucial.\nUnfortunately, traditional decompilers often struggle with producing readable\noutputs. While some large language model (LLM)-based decompilers have shown\ngood compatibility with general binary files, they still face specific\nchallenges when dealing with Wasm. In this paper, we introduce a novel\napproach, WaDec, which is the first use of a fine-tuned LLM to interpret and\ndecompile Wasm binary code into a higher-level, more comprehensible source code\nrepresentation. The LLM was meticulously fine-tuned using a specialized dataset\nof wat-c code snippets, employing self-supervised learning techniques. This\nenables WaDec to effectively decompile not only complete wat functions but also\nfiner-grained wat code snippets. Our experiments demonstrate that WaDec\nmarkedly outperforms current state-of-the-art tools, offering substantial\nimprovements across several metrics. It achieves a code inflation rate of only\n3.34%, a dramatic 97% reduction compared to the state-of-the-art's 116.94%.\nUnlike baselines' output that cannot be directly compiled or executed, WaDec\nmaintains a recompilability rate of 52.11%, a re-execution rate of 43.55%, and\nan output consistency of 27.15%. Additionally, it significantly exceeds\nstate-of-the-art performance in AST edit distance similarity by 185%,\ncyclomatic complexity by 8%, and cosine similarity by 41%, achieving an average\ncode similarity above 50%."
                },
                "authors": [
                    {
                        "name": "Xinyu She"
                    },
                    {
                        "name": "Yanjie Zhao"
                    },
                    {
                        "name": "Haoyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoyu Wang"
                },
                "author": "Haoyu Wang",
                "arxiv_comment": "This paper was accepted by ASE 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11346v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11346v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07150v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07150v1",
                "updated": "2024-09-11T09:54:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    9,
                    54,
                    45,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T09:54:45Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    9,
                    54,
                    45,
                    2,
                    255,
                    0
                ],
                "title": "ZKFault: Fault attack analysis on zero-knowledge based post-quantum\n  digital signature schemes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZKFault: Fault attack analysis on zero-knowledge based post-quantum\n  digital signature schemes"
                },
                "summary": "Computationally hard problems based on coding theory, such as the syndrome\ndecoding problem, have been used for constructing secure cryptographic schemes\nfor a long time. Schemes based on these problems are also assumed to be secure\nagainst quantum computers. However, these schemes are often considered\nimpractical for real-world deployment due to large key sizes and inefficient\ncomputation time. In the recent call for standardization of additional\npost-quantum digital signatures by the National Institute of Standards and\nTechnology, several code-based candidates have been proposed, including LESS,\nCROSS, and MEDS. These schemes are designed on the relatively new\nzero-knowledge framework. Although several works analyze the hardness of these\nschemes, there is hardly any work that examines the security of these schemes\nin the presence of physical attacks.\n  In this work, we analyze these signature schemes from the perspective of\nfault attacks. All these schemes use a similar tree-based construction to\ncompress the signature size. We attack this component of these schemes.\nTherefore, our attack is applicable to all of these schemes. In this work, we\nfirst analyze the LESS signature scheme and devise our attack. Furthermore, we\nshowed how this attack can be extended to the CROSS signature scheme. Our\nattacks are built on very simple fault assumptions. Our results show that we\ncan recover the entire secret key of LESS and CROSS using as little as a single\nfault. Finally, we propose various countermeasures to prevent these kinds of\nattacks and discuss their efficiency and shortcomings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computationally hard problems based on coding theory, such as the syndrome\ndecoding problem, have been used for constructing secure cryptographic schemes\nfor a long time. Schemes based on these problems are also assumed to be secure\nagainst quantum computers. However, these schemes are often considered\nimpractical for real-world deployment due to large key sizes and inefficient\ncomputation time. In the recent call for standardization of additional\npost-quantum digital signatures by the National Institute of Standards and\nTechnology, several code-based candidates have been proposed, including LESS,\nCROSS, and MEDS. These schemes are designed on the relatively new\nzero-knowledge framework. Although several works analyze the hardness of these\nschemes, there is hardly any work that examines the security of these schemes\nin the presence of physical attacks.\n  In this work, we analyze these signature schemes from the perspective of\nfault attacks. All these schemes use a similar tree-based construction to\ncompress the signature size. We attack this component of these schemes.\nTherefore, our attack is applicable to all of these schemes. In this work, we\nfirst analyze the LESS signature scheme and devise our attack. Furthermore, we\nshowed how this attack can be extended to the CROSS signature scheme. Our\nattacks are built on very simple fault assumptions. Our results show that we\ncan recover the entire secret key of LESS and CROSS using as little as a single\nfault. Finally, we propose various countermeasures to prevent these kinds of\nattacks and discuss their efficiency and shortcomings."
                },
                "authors": [
                    {
                        "name": "Puja Mondal"
                    },
                    {
                        "name": "Supriya Adhikary"
                    },
                    {
                        "name": "Suparna Kundu"
                    },
                    {
                        "name": "Angshuman Karmakar"
                    }
                ],
                "author_detail": {
                    "name": "Angshuman Karmakar"
                },
                "author": "Angshuman Karmakar",
                "arxiv_comment": "35 pages including appendix and bibliography",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07150v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07150v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "E.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07136v1",
                "updated": "2024-09-11T09:31:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    9,
                    31,
                    44,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T09:31:44Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    9,
                    31,
                    44,
                    2,
                    255,
                    0
                ],
                "title": "Leveraging Unstructured Text Data for Federated Instruction Tuning of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Unstructured Text Data for Federated Instruction Tuning of\n  Large Language Models"
                },
                "summary": "Federated instruction tuning enables multiple clients to collaboratively\nfine-tune a shared large language model (LLM) that can follow humans'\ninstructions without directly sharing raw data. However, existing literature\nimpractically requires that all the clients readily hold instruction-tuning\ndata (i.e., structured instruction-response pairs), which necessitates massive\nhuman annotations since clients' data is usually unstructured text instead.\nAddressing this, we propose a novel and flexible framework FedIT-U2S, which can\nautomatically transform unstructured corpus into structured data for federated\ninstruction tuning. FedIT-U2S consists two key steps: (1) few-shot\ninstruction-tuning data generation, where each unstructured data piece together\nwith several examples is combined to prompt an LLM in generating an\ninstruction-response pair. To further enhance the flexibility, a\nretrieval-based example selection technique is proposed, where the examples are\nautomatically selected based on the relatedness between the client's data piece\nand example pool, bypassing the need of determining examples in advance. (2) A\ntypical federated instruction tuning process based on the generated data.\nOverall, FedIT-U2S can be applied to diverse scenarios as long as the client\nholds valuable text corpus, broadening the application scope of federated\ninstruction tuning. We conduct a series of experiments on three domains\n(medicine, knowledge, and math), showing that our proposed FedIT-U2S can\nconsistently and significantly brings improvement over the base LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated instruction tuning enables multiple clients to collaboratively\nfine-tune a shared large language model (LLM) that can follow humans'\ninstructions without directly sharing raw data. However, existing literature\nimpractically requires that all the clients readily hold instruction-tuning\ndata (i.e., structured instruction-response pairs), which necessitates massive\nhuman annotations since clients' data is usually unstructured text instead.\nAddressing this, we propose a novel and flexible framework FedIT-U2S, which can\nautomatically transform unstructured corpus into structured data for federated\ninstruction tuning. FedIT-U2S consists two key steps: (1) few-shot\ninstruction-tuning data generation, where each unstructured data piece together\nwith several examples is combined to prompt an LLM in generating an\ninstruction-response pair. To further enhance the flexibility, a\nretrieval-based example selection technique is proposed, where the examples are\nautomatically selected based on the relatedness between the client's data piece\nand example pool, bypassing the need of determining examples in advance. (2) A\ntypical federated instruction tuning process based on the generated data.\nOverall, FedIT-U2S can be applied to diverse scenarios as long as the client\nholds valuable text corpus, broadening the application scope of federated\ninstruction tuning. We conduct a series of experiments on three domains\n(medicine, knowledge, and math), showing that our proposed FedIT-U2S can\nconsistently and significantly brings improvement over the base LLM."
                },
                "authors": [
                    {
                        "name": "Rui Ye"
                    },
                    {
                        "name": "Rui Ge"
                    },
                    {
                        "name": "Yuchi Fengting"
                    },
                    {
                        "name": "Jingyi Chai"
                    },
                    {
                        "name": "Yanfeng Wang"
                    },
                    {
                        "name": "Siheng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Siheng Chen"
                },
                "author": "Siheng Chen",
                "arxiv_comment": "11 pages, work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07132v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07132v1",
                "updated": "2024-09-11T09:29:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    9,
                    29,
                    28,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T09:29:28Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    9,
                    29,
                    28,
                    2,
                    255,
                    0
                ],
                "title": "LLM-based feature generation from text for interpretable machine\n  learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based feature generation from text for interpretable machine\n  learning"
                },
                "summary": "Existing text representations such as embeddings and bag-of-words are not\nsuitable for rule learning due to their high dimensionality and absent or\nquestionable feature-level interpretability. This article explores whether\nlarge language models (LLMs) could address this by extracting a small number of\ninterpretable features from text. We demonstrate this process on two datasets\n(CORD-19 and M17+) containing several thousand scientific articles from\nmultiple disciplines and a target being a proxy for research impact. An\nevaluation based on testing for the statistically significant correlation with\nresearch impact has shown that LLama 2-generated features are semantically\nmeaningful. We consequently used these generated features in text\nclassification to predict the binary target variable representing the citation\nrate for the CORD-19 dataset and the ordinal 5-class target representing an\nexpert-awarded grade in the M17+ dataset. Machine-learning models trained on\nthe LLM-generated features provided similar predictive performance to the\nstate-of-the-art embedding model SciBERT for scientific text. The LLM used only\n62 features compared to 768 features in SciBERT embeddings, and these features\nwere directly interpretable, corresponding to notions such as article\nmethodological rigor, novelty, or grammatical correctness. As the final step,\nwe extract a small number of well-interpretable action rules. Consistently\ncompetitive results obtained with the same LLM feature set across both\nthematically diverse datasets show that this approach generalizes across\ndomains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing text representations such as embeddings and bag-of-words are not\nsuitable for rule learning due to their high dimensionality and absent or\nquestionable feature-level interpretability. This article explores whether\nlarge language models (LLMs) could address this by extracting a small number of\ninterpretable features from text. We demonstrate this process on two datasets\n(CORD-19 and M17+) containing several thousand scientific articles from\nmultiple disciplines and a target being a proxy for research impact. An\nevaluation based on testing for the statistically significant correlation with\nresearch impact has shown that LLama 2-generated features are semantically\nmeaningful. We consequently used these generated features in text\nclassification to predict the binary target variable representing the citation\nrate for the CORD-19 dataset and the ordinal 5-class target representing an\nexpert-awarded grade in the M17+ dataset. Machine-learning models trained on\nthe LLM-generated features provided similar predictive performance to the\nstate-of-the-art embedding model SciBERT for scientific text. The LLM used only\n62 features compared to 768 features in SciBERT embeddings, and these features\nwere directly interpretable, corresponding to notions such as article\nmethodological rigor, novelty, or grammatical correctness. As the final step,\nwe extract a small number of well-interpretable action rules. Consistently\ncompetitive results obtained with the same LLM feature set across both\nthematically diverse datasets show that this approach generalizes across\ndomains."
                },
                "authors": [
                    {
                        "name": "Vojtěch Balek"
                    },
                    {
                        "name": "Lukáš Sýkora"
                    },
                    {
                        "name": "Vilém Sklenák"
                    },
                    {
                        "name": "Tomáš Kliegr"
                    }
                ],
                "author_detail": {
                    "name": "Tomáš Kliegr"
                },
                "author": "Tomáš Kliegr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07132v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07132v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07131v1",
                "updated": "2024-09-11T09:27:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    9,
                    27,
                    50,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T09:27:50Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    9,
                    27,
                    50,
                    2,
                    255,
                    0
                ],
                "title": "Reranking Laws for Language Generation: A Communication-Theoretic\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reranking Laws for Language Generation: A Communication-Theoretic\n  Perspective"
                },
                "summary": "To ensure large language models (LLMs) are used safely, one must reduce their\npropensity to hallucinate or to generate unacceptable answers. A simple and\noften used strategy is to first let the LLM generate multiple hypotheses and\nthen employ a reranker to choose the best one. In this paper, we draw a\nparallel between this strategy and the use of redundancy to decrease the error\nrate in noisy communication channels. We conceptualize the generator as a\nsender transmitting multiple descriptions of a message through parallel noisy\nchannels. The receiver decodes the message by ranking the (potentially\ncorrupted) descriptions and selecting the one found to be most reliable. We\nprovide conditions under which this protocol is asymptotically error-free\n(i.e., yields an acceptable answer almost surely) even in scenarios where the\nreranker is imperfect (governed by Mallows or Zipf-Mandelbrot models) and the\nchannel distributions are statistically dependent. We use our framework to\nobtain reranking laws which we validate empirically on two real-world tasks\nusing LLMs: text-to-code generation with DeepSeek-Coder 7B and machine\ntranslation of medical data with TowerInstruct 13B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To ensure large language models (LLMs) are used safely, one must reduce their\npropensity to hallucinate or to generate unacceptable answers. A simple and\noften used strategy is to first let the LLM generate multiple hypotheses and\nthen employ a reranker to choose the best one. In this paper, we draw a\nparallel between this strategy and the use of redundancy to decrease the error\nrate in noisy communication channels. We conceptualize the generator as a\nsender transmitting multiple descriptions of a message through parallel noisy\nchannels. The receiver decodes the message by ranking the (potentially\ncorrupted) descriptions and selecting the one found to be most reliable. We\nprovide conditions under which this protocol is asymptotically error-free\n(i.e., yields an acceptable answer almost surely) even in scenarios where the\nreranker is imperfect (governed by Mallows or Zipf-Mandelbrot models) and the\nchannel distributions are statistically dependent. We use our framework to\nobtain reranking laws which we validate empirically on two real-world tasks\nusing LLMs: text-to-code generation with DeepSeek-Coder 7B and machine\ntranslation of medical data with TowerInstruct 13B."
                },
                "authors": [
                    {
                        "name": "António Farinhas"
                    },
                    {
                        "name": "Haau-Sing Li"
                    },
                    {
                        "name": "André F. T. Martins"
                    }
                ],
                "author_detail": {
                    "name": "André F. T. Martins"
                },
                "author": "André F. T. Martins",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07123v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07123v1",
                "updated": "2024-09-11T09:21:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    9,
                    21,
                    20,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T09:21:20Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    9,
                    21,
                    20,
                    2,
                    255,
                    0
                ],
                "title": "Cross-Refine: Improving Natural Language Explanation Generation by\n  Learning in Tandem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Refine: Improving Natural Language Explanation Generation by\n  Learning in Tandem"
                },
                "summary": "Natural language explanations (NLEs) are vital for elucidating the reasoning\nbehind large language model (LLM) decisions. Many techniques have been\ndeveloped to generate NLEs using LLMs. However, like humans, LLMs might not\nalways produce optimal NLEs on first attempt. Inspired by human learning\nprocesses, we introduce Cross-Refine, which employs role modeling by deploying\ntwo LLMs as generator and critic, respectively. The generator outputs a first\nNLE and then refines this initial explanation using feedback and suggestions\nprovided by the critic. Cross-Refine does not require any supervised training\ndata or additional training. We validate Cross-Refine across three NLP tasks\nusing three state-of-the-art open-source LLMs through automatic and human\nevaluation. We select Self-Refine (Madaan et al., 2023) as the baseline, which\nonly utilizes self-feedback to refine the explanations. Our findings from\nautomatic evaluation and a user study indicate that Cross-Refine outperforms\nSelf-Refine. Meanwhile, Cross-Refine can perform effectively with less powerful\nLLMs, whereas Self-Refine only yields strong results with ChatGPT.\nAdditionally, we conduct an ablation study to assess the importance of feedback\nand suggestions. Both of them play an important role in refining explanations.\nWe further evaluate Cross-Refine on a bilingual dataset in English and German.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural language explanations (NLEs) are vital for elucidating the reasoning\nbehind large language model (LLM) decisions. Many techniques have been\ndeveloped to generate NLEs using LLMs. However, like humans, LLMs might not\nalways produce optimal NLEs on first attempt. Inspired by human learning\nprocesses, we introduce Cross-Refine, which employs role modeling by deploying\ntwo LLMs as generator and critic, respectively. The generator outputs a first\nNLE and then refines this initial explanation using feedback and suggestions\nprovided by the critic. Cross-Refine does not require any supervised training\ndata or additional training. We validate Cross-Refine across three NLP tasks\nusing three state-of-the-art open-source LLMs through automatic and human\nevaluation. We select Self-Refine (Madaan et al., 2023) as the baseline, which\nonly utilizes self-feedback to refine the explanations. Our findings from\nautomatic evaluation and a user study indicate that Cross-Refine outperforms\nSelf-Refine. Meanwhile, Cross-Refine can perform effectively with less powerful\nLLMs, whereas Self-Refine only yields strong results with ChatGPT.\nAdditionally, we conduct an ablation study to assess the importance of feedback\nand suggestions. Both of them play an important role in refining explanations.\nWe further evaluate Cross-Refine on a bilingual dataset in English and German."
                },
                "authors": [
                    {
                        "name": "Qianli Wang"
                    },
                    {
                        "name": "Tatiana Anikina"
                    },
                    {
                        "name": "Nils Feldhus"
                    },
                    {
                        "name": "Simon Ostermann"
                    },
                    {
                        "name": "Sebastian Möller"
                    },
                    {
                        "name": "Vera Schmitt"
                    }
                ],
                "author_detail": {
                    "name": "Vera Schmitt"
                },
                "author": "Vera Schmitt",
                "arxiv_comment": "17 pages; under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07123v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07123v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07110v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07110v1",
                "updated": "2024-09-11T08:56:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    8,
                    56,
                    27,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T08:56:27Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    8,
                    56,
                    27,
                    2,
                    255,
                    0
                ],
                "title": "Bio-Eng-LMM AI Assist chatbot: A Comprehensive Tool for Research and\n  Education",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bio-Eng-LMM AI Assist chatbot: A Comprehensive Tool for Research and\n  Education"
                },
                "summary": "This article introduces Bio-Eng-LMM AI chatbot, a versatile platform designed\nto enhance user interaction for educational and research purposes. Leveraging\ncutting-edge open-source Large Language Models (LLMs), Bio-Eng-LMM operates as\na sophisticated AI assistant, exploiting the capabilities of traditional models\nlike ChatGPT. Central to Bio-Eng-LMM is its implementation of Retrieval\nAugmented Generation (RAG) through three primary methods: integration of\npreprocessed documents, real-time processing of user-uploaded files, and\ninformation retrieval from any specified website. Additionally, the chatbot\nincorporates image generation via a Stable Diffusion Model (SDM), image\nunderstanding and response generation through LLAVA, and search functionality\non the internet powered by secure search engine such as DuckDuckGo. To provide\ncomprehensive support, Bio-Eng-LMM offers text summarization, website content\nsummarization, and both text and voice interaction. The chatbot maintains\nsession memory to ensure contextually relevant and coherent responses. This\nintegrated platform builds upon the strengths of RAG-GPT and Web-Based RAG\nQuery (WBRQ) where the system fetches relevant information directly from the\nweb to enhance the LLMs response generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This article introduces Bio-Eng-LMM AI chatbot, a versatile platform designed\nto enhance user interaction for educational and research purposes. Leveraging\ncutting-edge open-source Large Language Models (LLMs), Bio-Eng-LMM operates as\na sophisticated AI assistant, exploiting the capabilities of traditional models\nlike ChatGPT. Central to Bio-Eng-LMM is its implementation of Retrieval\nAugmented Generation (RAG) through three primary methods: integration of\npreprocessed documents, real-time processing of user-uploaded files, and\ninformation retrieval from any specified website. Additionally, the chatbot\nincorporates image generation via a Stable Diffusion Model (SDM), image\nunderstanding and response generation through LLAVA, and search functionality\non the internet powered by secure search engine such as DuckDuckGo. To provide\ncomprehensive support, Bio-Eng-LMM offers text summarization, website content\nsummarization, and both text and voice interaction. The chatbot maintains\nsession memory to ensure contextually relevant and coherent responses. This\nintegrated platform builds upon the strengths of RAG-GPT and Web-Based RAG\nQuery (WBRQ) where the system fetches relevant information directly from the\nweb to enhance the LLMs response generation."
                },
                "authors": [
                    {
                        "name": "Ali Forootani"
                    },
                    {
                        "name": "Danial Esmaeili Aliabadi"
                    },
                    {
                        "name": "Daniela Thraen"
                    }
                ],
                "author_detail": {
                    "name": "Daniela Thraen"
                },
                "author": "Daniela Thraen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07110v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07110v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.16352v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.16352v2",
                "updated": "2024-09-11T08:23:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    8,
                    23,
                    58,
                    2,
                    255,
                    0
                ],
                "published": "2024-02-26T07:17:25Z",
                "published_parsed": [
                    2024,
                    2,
                    26,
                    7,
                    17,
                    25,
                    0,
                    57,
                    0
                ],
                "title": "MathGenie: Generating Synthetic Data with Question Back-translation for\n  Enhancing Mathematical Reasoning of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MathGenie: Generating Synthetic Data with Question Back-translation for\n  Enhancing Mathematical Reasoning of LLMs"
                },
                "summary": "Large language models (LLMs) have exhibited great potential in mathematical\nreasoning. However, there remains a performance gap in this area between\nexisting open-source models and closed-source models such as GPT-4. In this\npaper, we introduce MathGenie, a novel method for generating diverse and\nreliable math problems from a small-scale problem-solution dataset (denoted as\nseed data). We augment the ground-truth solutions of our seed data and train a\nback-translation model to translate the augmented solutions back into new\nquestions. Subsequently, we generate code-integrated solutions for the new\nquestions. To ensure the correctness of the code-integrated solutions, we\nemploy rationale-based strategy for solution verification. Various pretrained\nmodels, ranging from 7B to 70B, are trained on the newly curated data to test\nthe effectiveness of the proposed augmentation technique, resulting in a family\nof models known as MathGenieLM. These models consistently outperform previous\nopen-source models across five representative mathematical reasoning datasets,\nachieving state-of-the-art performance. In particular, MathGenieLM-InternLM2\nachieves an accuracy of 87.7% on GSM8K and 55.7% on MATH, securing the best\noverall score among open-source language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have exhibited great potential in mathematical\nreasoning. However, there remains a performance gap in this area between\nexisting open-source models and closed-source models such as GPT-4. In this\npaper, we introduce MathGenie, a novel method for generating diverse and\nreliable math problems from a small-scale problem-solution dataset (denoted as\nseed data). We augment the ground-truth solutions of our seed data and train a\nback-translation model to translate the augmented solutions back into new\nquestions. Subsequently, we generate code-integrated solutions for the new\nquestions. To ensure the correctness of the code-integrated solutions, we\nemploy rationale-based strategy for solution verification. Various pretrained\nmodels, ranging from 7B to 70B, are trained on the newly curated data to test\nthe effectiveness of the proposed augmentation technique, resulting in a family\nof models known as MathGenieLM. These models consistently outperform previous\nopen-source models across five representative mathematical reasoning datasets,\nachieving state-of-the-art performance. In particular, MathGenieLM-InternLM2\nachieves an accuracy of 87.7% on GSM8K and 55.7% on MATH, securing the best\noverall score among open-source language models."
                },
                "authors": [
                    {
                        "name": "Zimu Lu"
                    },
                    {
                        "name": "Aojun Zhou"
                    },
                    {
                        "name": "Houxing Ren"
                    },
                    {
                        "name": "Ke Wang"
                    },
                    {
                        "name": "Weikang Shi"
                    },
                    {
                        "name": "Junting Pan"
                    },
                    {
                        "name": "Mingjie Zhan"
                    },
                    {
                        "name": "Hongsheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongsheng Li"
                },
                "author": "Hongsheng Li",
                "arxiv_comment": "ACL 2024 camera ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.16352v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.16352v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07088v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07088v1",
                "updated": "2024-09-11T08:16:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    8,
                    16,
                    20,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T08:16:20Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    8,
                    16,
                    20,
                    2,
                    255,
                    0
                ],
                "title": "Ontology-Free General-Domain Knowledge Graph-to-Text Generation Dataset\n  Synthesis using Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ontology-Free General-Domain Knowledge Graph-to-Text Generation Dataset\n  Synthesis using Large Language Model"
                },
                "summary": "Knowledge Graph-to-Text (G2T) generation involves verbalizing structured\nknowledge graphs into natural language text. Recent advancements in Pretrained\nLanguage Models (PLMs) have improved G2T performance, but their effectiveness\ndepends on datasets with precise graph-text alignment. However, the scarcity of\nhigh-quality, general-domain G2T generation datasets restricts progress in the\ngeneral-domain G2T generation research. To address this issue, we introduce\nWikipedia Ontology-Free Graph-text dataset (WikiOFGraph), a new large-scale G2T\ndataset generated using a novel method that leverages Large Language Model\n(LLM) and Data-QuestEval. Our new dataset, which contains 5.85M general-domain\ngraph-text pairs, offers high graph-text consistency without relying on\nexternal ontologies. Experimental results demonstrate that PLM fine-tuned on\nWikiOFGraph outperforms those trained on other datasets across various\nevaluation metrics. Our method proves to be a scalable and effective solution\nfor generating high-quality G2T data, significantly advancing the field of G2T\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Graph-to-Text (G2T) generation involves verbalizing structured\nknowledge graphs into natural language text. Recent advancements in Pretrained\nLanguage Models (PLMs) have improved G2T performance, but their effectiveness\ndepends on datasets with precise graph-text alignment. However, the scarcity of\nhigh-quality, general-domain G2T generation datasets restricts progress in the\ngeneral-domain G2T generation research. To address this issue, we introduce\nWikipedia Ontology-Free Graph-text dataset (WikiOFGraph), a new large-scale G2T\ndataset generated using a novel method that leverages Large Language Model\n(LLM) and Data-QuestEval. Our new dataset, which contains 5.85M general-domain\ngraph-text pairs, offers high graph-text consistency without relying on\nexternal ontologies. Experimental results demonstrate that PLM fine-tuned on\nWikiOFGraph outperforms those trained on other datasets across various\nevaluation metrics. Our method proves to be a scalable and effective solution\nfor generating high-quality G2T data, significantly advancing the field of G2T\ngeneration."
                },
                "authors": [
                    {
                        "name": "Daehee Kim"
                    },
                    {
                        "name": "Deokhyung Kang"
                    },
                    {
                        "name": "Sangwon Ryu"
                    },
                    {
                        "name": "Gary Geunbae Lee"
                    }
                ],
                "author_detail": {
                    "name": "Gary Geunbae Lee"
                },
                "author": "Gary Geunbae Lee",
                "arxiv_comment": "18 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07088v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07088v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07085v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07085v1",
                "updated": "2024-09-11T08:11:16Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    8,
                    11,
                    16,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T08:11:16Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    8,
                    11,
                    16,
                    2,
                    255,
                    0
                ],
                "title": "Understanding Knowledge Drift in LLMs through Misinformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Knowledge Drift in LLMs through Misinformation"
                },
                "summary": "Large Language Models (LLMs) have revolutionized numerous applications,\nmaking them an integral part of our digital ecosystem. However, their\nreliability becomes critical, especially when these models are exposed to\nmisinformation. We primarily analyze the susceptibility of state-of-the-art\nLLMs to factual inaccuracies when they encounter false information in a QnA\nscenario, an issue that can lead to a phenomenon we refer to as *knowledge\ndrift*, which significantly undermines the trustworthiness of these models. We\nevaluate the factuality and the uncertainty of the models' responses relying on\nEntropy, Perplexity, and Token Probability metrics. Our experiments reveal that\nan LLM's uncertainty can increase up to 56.6% when the question is answered\nincorrectly due to the exposure to false information. At the same time,\nrepeated exposure to the same false information can decrease the models\nuncertainty again (-52.8% w.r.t. the answers on the untainted prompts),\npotentially manipulating the underlying model's beliefs and introducing a drift\nfrom its original knowledge. These findings provide insights into LLMs'\nrobustness and vulnerability to adversarial inputs, paving the way for\ndeveloping more reliable LLM applications across various domains. The code is\navailable at https://github.com/afastowski/knowledge_drift.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized numerous applications,\nmaking them an integral part of our digital ecosystem. However, their\nreliability becomes critical, especially when these models are exposed to\nmisinformation. We primarily analyze the susceptibility of state-of-the-art\nLLMs to factual inaccuracies when they encounter false information in a QnA\nscenario, an issue that can lead to a phenomenon we refer to as *knowledge\ndrift*, which significantly undermines the trustworthiness of these models. We\nevaluate the factuality and the uncertainty of the models' responses relying on\nEntropy, Perplexity, and Token Probability metrics. Our experiments reveal that\nan LLM's uncertainty can increase up to 56.6% when the question is answered\nincorrectly due to the exposure to false information. At the same time,\nrepeated exposure to the same false information can decrease the models\nuncertainty again (-52.8% w.r.t. the answers on the untainted prompts),\npotentially manipulating the underlying model's beliefs and introducing a drift\nfrom its original knowledge. These findings provide insights into LLMs'\nrobustness and vulnerability to adversarial inputs, paving the way for\ndeveloping more reliable LLM applications across various domains. The code is\navailable at https://github.com/afastowski/knowledge_drift."
                },
                "authors": [
                    {
                        "name": "Alina Fastowski"
                    },
                    {
                        "name": "Gjergji Kasneci"
                    }
                ],
                "author_detail": {
                    "name": "Gjergji Kasneci"
                },
                "author": "Gjergji Kasneci",
                "arxiv_comment": "13 pages, 3 figures. Accepted at DELTA workshop at KDD 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07085v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07085v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00060v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00060v2",
                "updated": "2024-09-11T07:51:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    7,
                    51,
                    43,
                    2,
                    255,
                    0
                ],
                "published": "2024-08-22T04:25:06Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    4,
                    25,
                    6,
                    3,
                    235,
                    0
                ],
                "title": "Understanding Literary Texts by LLMs: A Case Study of Ancient Chinese\n  Poetry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Literary Texts by LLMs: A Case Study of Ancient Chinese\n  Poetry"
                },
                "summary": "The birth and rapid development of large language models (LLMs) have caused\nquite a stir in the field of literature. Once considered unattainable, AI's\nrole in literary creation is increasingly becoming a reality. In genres such as\npoetry, jokes, and short stories, numerous AI tools have emerged, offering\nrefreshing new perspectives. However, it's difficult to further improve the\nquality of these works. This is primarily because understanding and\nappreciating a good literary work involves a considerable threshold, such as\nknowledge of literary theory, aesthetic sensibility, interdisciplinary\nknowledge. Therefore, authoritative data in this area is quite lacking.\nAdditionally, evaluating literary works is often complex and hard to fully\nquantify, which directly hinders the further development of AI creation.\n  To address this issue, this paper attempts to explore the mysteries of\nliterary texts from the perspective of LLMs, using ancient Chinese poetry as an\nexample for experimentation. First, we collected a variety of ancient poems\nfrom different sources and had experts annotate a small portion of them. Then,\nwe designed a range of comprehension metrics based on LLMs to evaluate all\nthese poems. Finally, we analyzed the correlations and differences between\nvarious poem collections to identify literary patterns. Through our\nexperiments, we observed a series of enlightening phenomena that provide\ntechnical support for the future development of high-level literary creation\nbased on LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The birth and rapid development of large language models (LLMs) have caused\nquite a stir in the field of literature. Once considered unattainable, AI's\nrole in literary creation is increasingly becoming a reality. In genres such as\npoetry, jokes, and short stories, numerous AI tools have emerged, offering\nrefreshing new perspectives. However, it's difficult to further improve the\nquality of these works. This is primarily because understanding and\nappreciating a good literary work involves a considerable threshold, such as\nknowledge of literary theory, aesthetic sensibility, interdisciplinary\nknowledge. Therefore, authoritative data in this area is quite lacking.\nAdditionally, evaluating literary works is often complex and hard to fully\nquantify, which directly hinders the further development of AI creation.\n  To address this issue, this paper attempts to explore the mysteries of\nliterary texts from the perspective of LLMs, using ancient Chinese poetry as an\nexample for experimentation. First, we collected a variety of ancient poems\nfrom different sources and had experts annotate a small portion of them. Then,\nwe designed a range of comprehension metrics based on LLMs to evaluate all\nthese poems. Finally, we analyzed the correlations and differences between\nvarious poem collections to identify literary patterns. Through our\nexperiments, we observed a series of enlightening phenomena that provide\ntechnical support for the future development of high-level literary creation\nbased on LLMs."
                },
                "authors": [
                    {
                        "name": "Cheng Zhao"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Zhen Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhen Wang"
                },
                "author": "Zhen Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00060v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00060v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.06118v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.06118v4",
                "updated": "2024-09-11T07:48:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    7,
                    48,
                    26,
                    2,
                    255,
                    0
                ],
                "published": "2024-01-11T18:54:44Z",
                "published_parsed": [
                    2024,
                    1,
                    11,
                    18,
                    54,
                    44,
                    3,
                    11,
                    0
                ],
                "title": "Extreme Compression of Large Language Models via Additive Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extreme Compression of Large Language Models via Additive Quantization"
                },
                "summary": "The emergence of accurate open large language models (LLMs) has led to a race\ntowards performant quantization techniques which can enable their execution on\nend-user devices. In this paper, we revisit the problem of \"extreme\" LLM\ncompression-defined as targeting extremely low bit counts, such as 2 to 3 bits\nper parameter-from the point of view of classic methods in Multi-Codebook\nQuantization (MCQ). Our algorithm, called AQLM, generalizes the classic\nAdditive Quantization (AQ) approach for information retrieval to advance the\nstate-of-the-art in LLM compression, via two innovations: 1) learned additive\nquantization of weight matrices in input-adaptive fashion, and 2) joint\noptimization of codebook parameters across each transformer blocks. Broadly,\nAQLM is the first scheme that is Pareto optimal in terms of\naccuracy-vs-model-size when compressing to less than 3 bits per parameter, and\nsignificantly improves upon all known schemes in the extreme compression (2bit)\nregime. In addition, AQLM is practical: we provide fast GPU and CPU\nimplementations of AQLM for token generation, which enable us to match or\noutperform optimized FP16 implementations for speed, while executing in a much\nsmaller memory footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of accurate open large language models (LLMs) has led to a race\ntowards performant quantization techniques which can enable their execution on\nend-user devices. In this paper, we revisit the problem of \"extreme\" LLM\ncompression-defined as targeting extremely low bit counts, such as 2 to 3 bits\nper parameter-from the point of view of classic methods in Multi-Codebook\nQuantization (MCQ). Our algorithm, called AQLM, generalizes the classic\nAdditive Quantization (AQ) approach for information retrieval to advance the\nstate-of-the-art in LLM compression, via two innovations: 1) learned additive\nquantization of weight matrices in input-adaptive fashion, and 2) joint\noptimization of codebook parameters across each transformer blocks. Broadly,\nAQLM is the first scheme that is Pareto optimal in terms of\naccuracy-vs-model-size when compressing to less than 3 bits per parameter, and\nsignificantly improves upon all known schemes in the extreme compression (2bit)\nregime. In addition, AQLM is practical: we provide fast GPU and CPU\nimplementations of AQLM for token generation, which enable us to match or\noutperform optimized FP16 implementations for speed, while executing in a much\nsmaller memory footprint."
                },
                "authors": [
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Andrei Panferov"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Elias Frantar"
                    },
                    {
                        "name": "Artem Babenko"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "ICML, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.06118v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.06118v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07072v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07072v1",
                "updated": "2024-09-11T07:48:06Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    7,
                    48,
                    6,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T07:48:06Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    7,
                    48,
                    6,
                    2,
                    255,
                    0
                ],
                "title": "Latent Space Interpretation for Stylistic Analysis and Explainable\n  Authorship Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent Space Interpretation for Stylistic Analysis and Explainable\n  Authorship Attribution"
                },
                "summary": "Recent state-of-the-art authorship attribution methods learn authorship\nrepresentations of texts in a latent, non-interpretable space, hindering their\nusability in real-world applications. Our work proposes a novel approach to\ninterpreting these learned embeddings by identifying representative points in\nthe latent space and utilizing LLMs to generate informative natural language\ndescriptions of the writing style of each point. We evaluate the alignment of\nour interpretable space with the latent one and find that it achieves the best\nprediction agreement compared to other baselines. Additionally, we conduct a\nhuman evaluation to assess the quality of these style descriptions, validating\ntheir utility as explanations for the latent space. Finally, we investigate\nwhether human performance on the challenging AA task improves when aided by our\nsystem's explanations, finding an average improvement of around +20% in\naccuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent state-of-the-art authorship attribution methods learn authorship\nrepresentations of texts in a latent, non-interpretable space, hindering their\nusability in real-world applications. Our work proposes a novel approach to\ninterpreting these learned embeddings by identifying representative points in\nthe latent space and utilizing LLMs to generate informative natural language\ndescriptions of the writing style of each point. We evaluate the alignment of\nour interpretable space with the latent one and find that it achieves the best\nprediction agreement compared to other baselines. Additionally, we conduct a\nhuman evaluation to assess the quality of these style descriptions, validating\ntheir utility as explanations for the latent space. Finally, we investigate\nwhether human performance on the challenging AA task improves when aided by our\nsystem's explanations, finding an average improvement of around +20% in\naccuracy."
                },
                "authors": [
                    {
                        "name": "Milad Alshomary"
                    },
                    {
                        "name": "Narutatsu Ri"
                    },
                    {
                        "name": "Marianna Apidianaki"
                    },
                    {
                        "name": "Ajay Patel"
                    },
                    {
                        "name": "Smaranda Muresan"
                    },
                    {
                        "name": "Kathleen McKeown"
                    }
                ],
                "author_detail": {
                    "name": "Kathleen McKeown"
                },
                "author": "Kathleen McKeown",
                "arxiv_comment": "8 pages, 8 figures, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07072v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07072v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13257v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13257v2",
                "updated": "2024-09-11T07:42:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    7,
                    42,
                    11,
                    2,
                    255,
                    0
                ],
                "published": "2024-08-23T17:59:51Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    17,
                    59,
                    51,
                    4,
                    236,
                    0
                ],
                "title": "MME-RealWorld: Could Your Multimodal LLM Challenge High-Resolution\n  Real-World Scenarios that are Difficult for Humans?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MME-RealWorld: Could Your Multimodal LLM Challenge High-Resolution\n  Real-World Scenarios that are Difficult for Humans?"
                },
                "summary": "Comprehensive evaluation of Multimodal Large Language Models (MLLMs) has\nrecently garnered widespread attention in the research community. However, we\nobserve that existing benchmarks present several common barriers that make it\ndifficult to measure the significant challenges that models face in the real\nworld, including: 1) small data scale leads to a large performance variance; 2)\nreliance on model-based annotations results in restricted data quality; 3)\ninsufficient task difficulty, especially caused by the limited image\nresolution. To tackle these issues, we introduce MME-RealWorld. Specifically,\nwe collect more than $300$K images from public datasets and the Internet,\nfiltering $13,366$ high-quality images for annotation. This involves the\nefforts of professional $25$ annotators and $7$ experts in MLLMs, contributing\nto $29,429$ question-answer pairs that cover $43$ subtasks across $5$\nreal-world scenarios, extremely challenging even for humans. As far as we know,\nMME-RealWorld is the largest manually annotated benchmark to date, featuring\nthe highest resolution and a targeted focus on real-world applications. We\nfurther conduct a thorough evaluation involving $28$ prominent MLLMs, such as\nGPT-4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet. Our results show that even the\nmost advanced models struggle with our benchmarks, where none of them reach\n$60\\%$ accuracy. The challenges of perceiving high-resolution images and\nunderstanding complex real-world scenarios remain urgent issues to be\naddressed. The data and evaluation code are released at\nhttps://mme-realworld.github.io/ .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comprehensive evaluation of Multimodal Large Language Models (MLLMs) has\nrecently garnered widespread attention in the research community. However, we\nobserve that existing benchmarks present several common barriers that make it\ndifficult to measure the significant challenges that models face in the real\nworld, including: 1) small data scale leads to a large performance variance; 2)\nreliance on model-based annotations results in restricted data quality; 3)\ninsufficient task difficulty, especially caused by the limited image\nresolution. To tackle these issues, we introduce MME-RealWorld. Specifically,\nwe collect more than $300$K images from public datasets and the Internet,\nfiltering $13,366$ high-quality images for annotation. This involves the\nefforts of professional $25$ annotators and $7$ experts in MLLMs, contributing\nto $29,429$ question-answer pairs that cover $43$ subtasks across $5$\nreal-world scenarios, extremely challenging even for humans. As far as we know,\nMME-RealWorld is the largest manually annotated benchmark to date, featuring\nthe highest resolution and a targeted focus on real-world applications. We\nfurther conduct a thorough evaluation involving $28$ prominent MLLMs, such as\nGPT-4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet. Our results show that even the\nmost advanced models struggle with our benchmarks, where none of them reach\n$60\\%$ accuracy. The challenges of perceiving high-resolution images and\nunderstanding complex real-world scenarios remain urgent issues to be\naddressed. The data and evaluation code are released at\nhttps://mme-realworld.github.io/ ."
                },
                "authors": [
                    {
                        "name": "Yi-Fan Zhang"
                    },
                    {
                        "name": "Huanyu Zhang"
                    },
                    {
                        "name": "Haochen Tian"
                    },
                    {
                        "name": "Chaoyou Fu"
                    },
                    {
                        "name": "Shuangqing Zhang"
                    },
                    {
                        "name": "Junfei Wu"
                    },
                    {
                        "name": "Feng Li"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Qingsong Wen"
                    },
                    {
                        "name": "Zhang Zhang"
                    },
                    {
                        "name": "Liang Wang"
                    },
                    {
                        "name": "Rong Jin"
                    },
                    {
                        "name": "Tieniu Tan"
                    }
                ],
                "author_detail": {
                    "name": "Tieniu Tan"
                },
                "author": "Tieniu Tan",
                "arxiv_comment": "Project Page: https://mme-realworld.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13257v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13257v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.08011v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.08011v3",
                "updated": "2024-09-11T07:31:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    7,
                    31,
                    29,
                    2,
                    255,
                    0
                ],
                "published": "2024-05-10T18:05:37Z",
                "published_parsed": [
                    2024,
                    5,
                    10,
                    18,
                    5,
                    37,
                    4,
                    131,
                    0
                ],
                "title": "A Survey of Large Language Models for Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Large Language Models for Graphs"
                },
                "summary": "Graphs are an essential data structure utilized to represent relationships in\nreal-world scenarios. Prior research has established that Graph Neural Networks\n(GNNs) deliver impressive outcomes in graph-centric tasks, such as link\nprediction and node classification. Despite these advancements, challenges like\ndata sparsity and limited generalization capabilities continue to persist.\nRecently, Large Language Models (LLMs) have gained attention in natural\nlanguage processing. They excel in language comprehension and summarization.\nIntegrating LLMs with graph learning techniques has attracted interest as a way\nto enhance performance in graph learning tasks. In this survey, we conduct an\nin-depth review of the latest state-of-the-art LLMs applied in graph learning\nand introduce a novel taxonomy to categorize existing methods based on their\nframework design. We detail four unique designs: i) GNNs as Prefix, ii) LLMs as\nPrefix, iii) LLMs-Graphs Integration, and iv) LLMs-Only, highlighting key\nmethodologies within each category. We explore the strengths and limitations of\neach framework, and emphasize potential avenues for future research, including\novercoming current integration challenges between LLMs and graph learning\ntechniques, and venturing into new application areas. This survey aims to serve\nas a valuable resource for researchers and practitioners eager to leverage\nlarge language models in graph learning, and to inspire continued progress in\nthis dynamic field. We consistently maintain the related open-source materials\nat \\url{https://github.com/HKUDS/Awesome-LLM4Graph-Papers}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphs are an essential data structure utilized to represent relationships in\nreal-world scenarios. Prior research has established that Graph Neural Networks\n(GNNs) deliver impressive outcomes in graph-centric tasks, such as link\nprediction and node classification. Despite these advancements, challenges like\ndata sparsity and limited generalization capabilities continue to persist.\nRecently, Large Language Models (LLMs) have gained attention in natural\nlanguage processing. They excel in language comprehension and summarization.\nIntegrating LLMs with graph learning techniques has attracted interest as a way\nto enhance performance in graph learning tasks. In this survey, we conduct an\nin-depth review of the latest state-of-the-art LLMs applied in graph learning\nand introduce a novel taxonomy to categorize existing methods based on their\nframework design. We detail four unique designs: i) GNNs as Prefix, ii) LLMs as\nPrefix, iii) LLMs-Graphs Integration, and iv) LLMs-Only, highlighting key\nmethodologies within each category. We explore the strengths and limitations of\neach framework, and emphasize potential avenues for future research, including\novercoming current integration challenges between LLMs and graph learning\ntechniques, and venturing into new application areas. This survey aims to serve\nas a valuable resource for researchers and practitioners eager to leverage\nlarge language models in graph learning, and to inspire continued progress in\nthis dynamic field. We consistently maintain the related open-source materials\nat \\url{https://github.com/HKUDS/Awesome-LLM4Graph-Papers}."
                },
                "authors": [
                    {
                        "name": "Xubin Ren"
                    },
                    {
                        "name": "Jiabin Tang"
                    },
                    {
                        "name": "Dawei Yin"
                    },
                    {
                        "name": "Nitesh Chawla"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_doi": "10.1145/3637528.3671460",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3637528.3671460",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.08011v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.08011v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published as a KDD'24 survey paper",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.02219v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.02219v2",
                "updated": "2024-09-11T07:27:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    7,
                    27,
                    51,
                    2,
                    255,
                    0
                ],
                "published": "2024-05-03T16:25:27Z",
                "published_parsed": [
                    2024,
                    5,
                    3,
                    16,
                    25,
                    27,
                    4,
                    124,
                    0
                ],
                "title": "A Normative Framework for Benchmarking Consumer Fairness in Large\n  Language Model Recommender System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Normative Framework for Benchmarking Consumer Fairness in Large\n  Language Model Recommender System"
                },
                "summary": "The rapid adoption of large language models (LLMs) in recommender systems\n(RS) presents new challenges in understanding and evaluating their biases,\nwhich can result in unfairness or the amplification of stereotypes. Traditional\nfairness evaluations in RS primarily focus on collaborative filtering (CF)\nsettings, which may not fully capture the complexities of LLMs, as these models\noften inherit biases from large, unregulated data. This paper proposes a\nnormative framework to benchmark consumer fairness in LLM-powered recommender\nsystems (RecLLMs).\n  We critically examine how fairness norms in classical RS fall short in\naddressing the challenges posed by LLMs. We argue that this gap can lead to\narbitrary conclusions about fairness, and we propose a more structured, formal\napproach to evaluate fairness in such systems. Our experiments on the MovieLens\ndataset on consumer fairness, using in-context learning (zero-shot vs.\nfew-shot) reveal fairness deviations in age-based recommendations, particularly\nwhen additional contextual examples are introduced (ICL-2). Statistical\nsignificance tests confirm that these deviations are not random, highlighting\nthe need for robust evaluation methods. While this work offers a preliminary\ndiscussion on a proposed normative framework, our hope is that it could provide\na formal, principled approach for auditing and mitigating bias in RecLLMs. The\ncode and dataset used for this work will be shared at \"gihub-anonymized\".",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid adoption of large language models (LLMs) in recommender systems\n(RS) presents new challenges in understanding and evaluating their biases,\nwhich can result in unfairness or the amplification of stereotypes. Traditional\nfairness evaluations in RS primarily focus on collaborative filtering (CF)\nsettings, which may not fully capture the complexities of LLMs, as these models\noften inherit biases from large, unregulated data. This paper proposes a\nnormative framework to benchmark consumer fairness in LLM-powered recommender\nsystems (RecLLMs).\n  We critically examine how fairness norms in classical RS fall short in\naddressing the challenges posed by LLMs. We argue that this gap can lead to\narbitrary conclusions about fairness, and we propose a more structured, formal\napproach to evaluate fairness in such systems. Our experiments on the MovieLens\ndataset on consumer fairness, using in-context learning (zero-shot vs.\nfew-shot) reveal fairness deviations in age-based recommendations, particularly\nwhen additional contextual examples are introduced (ICL-2). Statistical\nsignificance tests confirm that these deviations are not random, highlighting\nthe need for robust evaluation methods. While this work offers a preliminary\ndiscussion on a proposed normative framework, our hope is that it could provide\na formal, principled approach for auditing and mitigating bias in RecLLMs. The\ncode and dataset used for this work will be shared at \"gihub-anonymized\"."
                },
                "authors": [
                    {
                        "name": "Yashar Deldjoo"
                    },
                    {
                        "name": "Fatemeh Nazary"
                    }
                ],
                "author_detail": {
                    "name": "Fatemeh Nazary"
                },
                "author": "Fatemeh Nazary",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.02219v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.02219v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07054v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07054v1",
                "updated": "2024-09-11T06:59:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    6,
                    59,
                    37,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T06:59:37Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    6,
                    59,
                    37,
                    2,
                    255,
                    0
                ],
                "title": "Native vs Non-Native Language Prompting: A Comparative Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Native vs Non-Native Language Prompting: A Comparative Analysis"
                },
                "summary": "Large language models (LLMs) have shown remarkable abilities in different\nfields, including standard Natural Language Processing (NLP) tasks. To elicit\nknowledge from LLMs, prompts play a key role, consisting of natural language\ninstructions. Most open and closed source LLMs are trained on available labeled\nand unlabeled resources--digital content such as text, images, audio, and\nvideos. Hence, these models have better knowledge for high-resourced languages\nbut struggle with low-resourced languages. Since prompts play a crucial role in\nunderstanding their capabilities, the language used for prompts remains an\nimportant research question. Although there has been significant research in\nthis area, it is still limited, and less has been explored for medium to\nlow-resourced languages. In this study, we investigate different prompting\nstrategies (native vs. non-native) on 11 different NLP tasks associated with 12\ndifferent Arabic datasets (9.7K data points). In total, we conducted 197\nexperiments involving 3 LLMs, 12 datasets, and 3 prompting strategies. Our\nfindings suggest that, on average, the non-native prompt performs the best,\nfollowed by mixed and native prompts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable abilities in different\nfields, including standard Natural Language Processing (NLP) tasks. To elicit\nknowledge from LLMs, prompts play a key role, consisting of natural language\ninstructions. Most open and closed source LLMs are trained on available labeled\nand unlabeled resources--digital content such as text, images, audio, and\nvideos. Hence, these models have better knowledge for high-resourced languages\nbut struggle with low-resourced languages. Since prompts play a crucial role in\nunderstanding their capabilities, the language used for prompts remains an\nimportant research question. Although there has been significant research in\nthis area, it is still limited, and less has been explored for medium to\nlow-resourced languages. In this study, we investigate different prompting\nstrategies (native vs. non-native) on 11 different NLP tasks associated with 12\ndifferent Arabic datasets (9.7K data points). In total, we conducted 197\nexperiments involving 3 LLMs, 12 datasets, and 3 prompting strategies. Our\nfindings suggest that, on average, the non-native prompt performs the best,\nfollowed by mixed and native prompts."
                },
                "authors": [
                    {
                        "name": "Mohamed Bayan Kmainasi"
                    },
                    {
                        "name": "Rakif Khan"
                    },
                    {
                        "name": "Ali Ezzat Shahroor"
                    },
                    {
                        "name": "Boushra Bendou"
                    },
                    {
                        "name": "Maram Hasanain"
                    },
                    {
                        "name": "Firoj Alam"
                    }
                ],
                "author_detail": {
                    "name": "Firoj Alam"
                },
                "author": "Firoj Alam",
                "arxiv_comment": "Foundation Models, Large Language Models, Arabic NLP, LLMs, Native,\n  Contextual Understanding, Arabic LLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07054v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07054v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07045v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07045v1",
                "updated": "2024-09-11T06:27:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    6,
                    27,
                    50,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T06:27:50Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    6,
                    27,
                    50,
                    2,
                    255,
                    0
                ],
                "title": "Beyond IID: Optimizing Instruction Learning from the Perspective of\n  Instruction Interaction and Dependency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond IID: Optimizing Instruction Learning from the Perspective of\n  Instruction Interaction and Dependency"
                },
                "summary": "With the availability of various instruction datasets, a pivotal challenge is\nhow to effectively select and integrate these instructions to fine-tune large\nlanguage models (LLMs). Previous research mainly focuses on selecting\nindividual high-quality instructions. However, these works overlooked the joint\ninteractions and dependencies between different categories of instructions,\nleading to suboptimal selection strategies. Moreover, the nature of these\ninteraction patterns remains largely unexplored, let alone optimize the\ninstruction set with regard to them. To fill these gaps, in this paper, we: (1)\nsystemically investigate interaction and dependency patterns between different\ncategories of instructions, (2) manage to optimize the instruction set\nconcerning the interaction patterns using a linear programming-based method,\nand optimize the learning schema of SFT using an instruction dependency\ntaxonomy guided curriculum learning. Experimental results across different LLMs\ndemonstrate improved performance over strong baselines on widely adopted\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the availability of various instruction datasets, a pivotal challenge is\nhow to effectively select and integrate these instructions to fine-tune large\nlanguage models (LLMs). Previous research mainly focuses on selecting\nindividual high-quality instructions. However, these works overlooked the joint\ninteractions and dependencies between different categories of instructions,\nleading to suboptimal selection strategies. Moreover, the nature of these\ninteraction patterns remains largely unexplored, let alone optimize the\ninstruction set with regard to them. To fill these gaps, in this paper, we: (1)\nsystemically investigate interaction and dependency patterns between different\ncategories of instructions, (2) manage to optimize the instruction set\nconcerning the interaction patterns using a linear programming-based method,\nand optimize the learning schema of SFT using an instruction dependency\ntaxonomy guided curriculum learning. Experimental results across different LLMs\ndemonstrate improved performance over strong baselines on widely adopted\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Hanyu Zhao"
                    },
                    {
                        "name": "Li Du"
                    },
                    {
                        "name": "Yiming Ju"
                    },
                    {
                        "name": "Chengwei Wu"
                    },
                    {
                        "name": "Tengfei Pan"
                    }
                ],
                "author_detail": {
                    "name": "Tengfei Pan"
                },
                "author": "Tengfei Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07045v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07045v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08448v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08448v4",
                "updated": "2024-09-11T06:12:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    6,
                    12,
                    17,
                    2,
                    255,
                    0
                ],
                "published": "2024-08-15T22:57:39Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    22,
                    57,
                    39,
                    3,
                    228,
                    0
                ],
                "title": "Exploring Cross-model Neuronal Correlations in the Context of Predicting\n  Model Performance and Generalizability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Cross-model Neuronal Correlations in the Context of Predicting\n  Model Performance and Generalizability"
                },
                "summary": "As Artificial Intelligence (AI) models are increasingly integrated into\ncritical systems, the need for a robust framework to establish the\ntrustworthiness of AI is increasingly paramount. While collaborative efforts\nhave established conceptual foundations for such a framework, there remains a\nsignificant gap in developing concrete, technically robust methods for\nassessing AI model quality and performance. A critical drawback in the\ntraditional methods for assessing the validity and generalizability of models\nis their dependence on internal developer datasets, rendering it challenging to\nindependently assess and verify their performance claims. This paper introduces\na novel approach for assessing a newly trained model's performance based on\nanother known model by calculating correlation between neural networks. The\nproposed method evaluates correlations by determining if, for each neuron in\none network, there exists a neuron in the other network that produces similar\noutput. This approach has implications for memory efficiency, allowing for the\nuse of smaller networks when high correlation exists between networks of\ndifferent sizes. Additionally, the method provides insights into robustness,\nsuggesting that if two highly correlated networks are compared and one\ndemonstrates robustness when operating in production environments, the other is\nlikely to exhibit similar robustness. This contribution advances the technical\ntoolkit for responsible AI, supporting more comprehensive and nuanced\nevaluations of AI models to ensure their safe and effective deployment. Code is\navailable at https://github.com/aheldis/Cross-model-correlation.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Artificial Intelligence (AI) models are increasingly integrated into\ncritical systems, the need for a robust framework to establish the\ntrustworthiness of AI is increasingly paramount. While collaborative efforts\nhave established conceptual foundations for such a framework, there remains a\nsignificant gap in developing concrete, technically robust methods for\nassessing AI model quality and performance. A critical drawback in the\ntraditional methods for assessing the validity and generalizability of models\nis their dependence on internal developer datasets, rendering it challenging to\nindependently assess and verify their performance claims. This paper introduces\na novel approach for assessing a newly trained model's performance based on\nanother known model by calculating correlation between neural networks. The\nproposed method evaluates correlations by determining if, for each neuron in\none network, there exists a neuron in the other network that produces similar\noutput. This approach has implications for memory efficiency, allowing for the\nuse of smaller networks when high correlation exists between networks of\ndifferent sizes. Additionally, the method provides insights into robustness,\nsuggesting that if two highly correlated networks are compared and one\ndemonstrates robustness when operating in production environments, the other is\nlikely to exhibit similar robustness. This contribution advances the technical\ntoolkit for responsible AI, supporting more comprehensive and nuanced\nevaluations of AI models to ensure their safe and effective deployment. Code is\navailable at https://github.com/aheldis/Cross-model-correlation.git."
                },
                "authors": [
                    {
                        "name": "Haniyeh Ehsani Oskouie"
                    },
                    {
                        "name": "Lionel Levine"
                    },
                    {
                        "name": "Majid Sarrafzadeh"
                    }
                ],
                "author_detail": {
                    "name": "Majid Sarrafzadeh"
                },
                "author": "Majid Sarrafzadeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08448v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08448v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07003v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07003v1",
                "updated": "2024-09-11T04:31:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    4,
                    31,
                    9,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T04:31:09Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    4,
                    31,
                    9,
                    2,
                    255,
                    0
                ],
                "title": "ODYSSEE: Oyster Detection Yielded by Sensor Systems on Edge Electronics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ODYSSEE: Oyster Detection Yielded by Sensor Systems on Edge Electronics"
                },
                "summary": "Oysters are a keystone species in coastal ecosystems, offering significant\neconomic, environmental, and cultural benefits. However, current monitoring\nsystems are often destructive, typically involving dredging to physically\ncollect and count oysters. A nondestructive alternative is manual\nidentification from video footage collected by divers, which is time-consuming\nand labor-intensive with expert input.\n  An alternative to human monitoring is the deployment of a system with trained\nobject detection models that performs real-time, on edge oyster detection in\nthe field. One such platform is the Aqua2 robot. Effective training of these\nmodels requires extensive high-quality data, which is difficult to obtain in\nmarine settings. To address these complications, we introduce a novel method\nthat leverages stable diffusion to generate high-quality synthetic data for the\nmarine domain. We exploit diffusion models to create photorealistic marine\nimagery, using ControlNet inputs to ensure consistency with the segmentation\nground-truth mask, the geometry of the scene, and the target domain of real\nunderwater images for oysters. The resulting dataset is used to train a\nYOLOv10-based vision model, achieving a state-of-the-art 0.657 mAP@50 for\noyster detection on the Aqua2 platform. The system we introduce not only\nimproves oyster habitat monitoring, but also paves the way to autonomous\nsurveillance for various tasks in marine contexts, improving aquaculture and\nconservation efforts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oysters are a keystone species in coastal ecosystems, offering significant\neconomic, environmental, and cultural benefits. However, current monitoring\nsystems are often destructive, typically involving dredging to physically\ncollect and count oysters. A nondestructive alternative is manual\nidentification from video footage collected by divers, which is time-consuming\nand labor-intensive with expert input.\n  An alternative to human monitoring is the deployment of a system with trained\nobject detection models that performs real-time, on edge oyster detection in\nthe field. One such platform is the Aqua2 robot. Effective training of these\nmodels requires extensive high-quality data, which is difficult to obtain in\nmarine settings. To address these complications, we introduce a novel method\nthat leverages stable diffusion to generate high-quality synthetic data for the\nmarine domain. We exploit diffusion models to create photorealistic marine\nimagery, using ControlNet inputs to ensure consistency with the segmentation\nground-truth mask, the geometry of the scene, and the target domain of real\nunderwater images for oysters. The resulting dataset is used to train a\nYOLOv10-based vision model, achieving a state-of-the-art 0.657 mAP@50 for\noyster detection on the Aqua2 platform. The system we introduce not only\nimproves oyster habitat monitoring, but also paves the way to autonomous\nsurveillance for various tasks in marine contexts, improving aquaculture and\nconservation efforts."
                },
                "authors": [
                    {
                        "name": "Xiaomin Lin"
                    },
                    {
                        "name": "Vivek Mange"
                    },
                    {
                        "name": "Arjun Suresh"
                    },
                    {
                        "name": "Bernhard Neuberger"
                    },
                    {
                        "name": "Aadi Palnitkar"
                    },
                    {
                        "name": "Brendan Campbell"
                    },
                    {
                        "name": "Alan Williams"
                    },
                    {
                        "name": "Kleio Baxevani"
                    },
                    {
                        "name": "Jeremy Mallette"
                    },
                    {
                        "name": "Alhim Vera"
                    },
                    {
                        "name": "Markus Vincze"
                    },
                    {
                        "name": "Ioannis Rekleitis"
                    },
                    {
                        "name": "Herbert G. Tanner"
                    },
                    {
                        "name": "Yiannis Aloimonos"
                    }
                ],
                "author_detail": {
                    "name": "Yiannis Aloimonos"
                },
                "author": "Yiannis Aloimonos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07003v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07003v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12067v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12067v2",
                "updated": "2024-09-11T04:06:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    4,
                    6,
                    45,
                    2,
                    255,
                    0
                ],
                "published": "2024-08-22T02:11:14Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    2,
                    11,
                    14,
                    3,
                    235,
                    0
                ],
                "title": "Distributed Noncoherent Joint Transmission Based on Multi-Agent\n  Reinforcement Learning for Dense Small Cell MISO Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed Noncoherent Joint Transmission Based on Multi-Agent\n  Reinforcement Learning for Dense Small Cell MISO Systems"
                },
                "summary": "We consider a dense small cell (DSC) network where multi-antenna small cell\nbase stations (SBSs) transmit data to single-antenna users over a shared\nfrequency band. To enhance capacity, a state-of-the-art technique known as\nnoncoherent joint transmission (JT) is applied, enabling users to receive data\nfrom multiple coordinated SBSs. However, the sum rate maximization problem with\nnoncoherent JT is inherently nonconvex and NP-hard. While existing\noptimization-based noncoherent JT algorithms can provide near-optimal\nperformance, they require global channel state information (CSI) and multiple\niterations, which makes them difficult to be implemeted in DSC networks.To\novercome these challenges, we first prove that the optimal beamforming\nstructure is the same for both the power minimization problem and the sum rate\nmaximization problem, and then mathematically derive the optimal beamforming\nstructure for both problems by solving the power minimization problem.The\noptimal beamforming structure can effectively reduces the variable\ndimensions.By exploiting the optimal beamforming structure, we propose a deep\ndeterministic policy gradient-based distributed noncoherent JT scheme to\nmaximize the system sum rate.In the proposed scheme, each SBS utilizes global\ninformation for training and uses local CSI to determine beamforming vectors.\nSimulation results demonstrate that the proposed scheme achieves comparable\nperformance with considerably lower computational complexity and information\noverhead compared to centralized iterative optimization-based techniques,\nmaking it more attractive for practical deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a dense small cell (DSC) network where multi-antenna small cell\nbase stations (SBSs) transmit data to single-antenna users over a shared\nfrequency band. To enhance capacity, a state-of-the-art technique known as\nnoncoherent joint transmission (JT) is applied, enabling users to receive data\nfrom multiple coordinated SBSs. However, the sum rate maximization problem with\nnoncoherent JT is inherently nonconvex and NP-hard. While existing\noptimization-based noncoherent JT algorithms can provide near-optimal\nperformance, they require global channel state information (CSI) and multiple\niterations, which makes them difficult to be implemeted in DSC networks.To\novercome these challenges, we first prove that the optimal beamforming\nstructure is the same for both the power minimization problem and the sum rate\nmaximization problem, and then mathematically derive the optimal beamforming\nstructure for both problems by solving the power minimization problem.The\noptimal beamforming structure can effectively reduces the variable\ndimensions.By exploiting the optimal beamforming structure, we propose a deep\ndeterministic policy gradient-based distributed noncoherent JT scheme to\nmaximize the system sum rate.In the proposed scheme, each SBS utilizes global\ninformation for training and uses local CSI to determine beamforming vectors.\nSimulation results demonstrate that the proposed scheme achieves comparable\nperformance with considerably lower computational complexity and information\noverhead compared to centralized iterative optimization-based techniques,\nmaking it more attractive for practical deployment."
                },
                "authors": [
                    {
                        "name": "Shaozhuang Bai"
                    },
                    {
                        "name": "Zhenzhen Gao"
                    },
                    {
                        "name": "Xuewen Liao"
                    }
                ],
                "author_detail": {
                    "name": "Xuewen Liao"
                },
                "author": "Xuewen Liao",
                "arxiv_comment": "After thorough discussions with my co-authors, we have identified\n  certain issues with the paper that cannot be resolved through revisions. As a\n  result, we have collectively decided to complete withdraw the paper from\n  arXiv",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12067v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12067v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06978v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06978v1",
                "updated": "2024-09-11T03:09:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    3,
                    9,
                    55,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T03:09:55Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    3,
                    9,
                    55,
                    2,
                    255,
                    0
                ],
                "title": "Large Language Models and the Extended Church-Turing Thesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models and the Extended Church-Turing Thesis"
                },
                "summary": "The Extended Church-Turing Thesis (ECTT) posits that all effective\ninformation processing, including unbounded and non-uniform interactive\ncomputations, can be described in terms of interactive Turing machines with\nadvice. Does this assertion also apply to the abilities of contemporary large\nlanguage models (LLMs)? From a broader perspective, this question calls for an\ninvestigation of the computational power of LLMs by the classical means of\ncomputability and computational complexity theory, especially the theory of\nautomata. Along these lines, we establish a number of fundamental results.\nFirstly, we argue that any fixed (non-adaptive) LLM is computationally\nequivalent to a, possibly very large, deterministic finite-state transducer.\nThis characterizes the base level of LLMs. We extend this to a key result\nconcerning the simulation of space-bounded Turing machines by LLMs. Secondly,\nwe show that lineages of evolving LLMs are computationally equivalent to\ninteractive Turing machines with advice. The latter finding confirms the\nvalidity of the ECTT for lineages of LLMs. From a computability viewpoint, it\nalso suggests that lineages of LLMs possess super-Turing computational power.\nConsequently, in our computational model knowledge generation is in general a\nnon-algorithmic process realized by lineages of LLMs. Finally, we discuss the\nmerits of our findings in the broader context of several related disciplines\nand philosophies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Extended Church-Turing Thesis (ECTT) posits that all effective\ninformation processing, including unbounded and non-uniform interactive\ncomputations, can be described in terms of interactive Turing machines with\nadvice. Does this assertion also apply to the abilities of contemporary large\nlanguage models (LLMs)? From a broader perspective, this question calls for an\ninvestigation of the computational power of LLMs by the classical means of\ncomputability and computational complexity theory, especially the theory of\nautomata. Along these lines, we establish a number of fundamental results.\nFirstly, we argue that any fixed (non-adaptive) LLM is computationally\nequivalent to a, possibly very large, deterministic finite-state transducer.\nThis characterizes the base level of LLMs. We extend this to a key result\nconcerning the simulation of space-bounded Turing machines by LLMs. Secondly,\nwe show that lineages of evolving LLMs are computationally equivalent to\ninteractive Turing machines with advice. The latter finding confirms the\nvalidity of the ECTT for lineages of LLMs. From a computability viewpoint, it\nalso suggests that lineages of LLMs possess super-Turing computational power.\nConsequently, in our computational model knowledge generation is in general a\nnon-algorithmic process realized by lineages of LLMs. Finally, we discuss the\nmerits of our findings in the broader context of several related disciplines\nand philosophies."
                },
                "authors": [
                    {
                        "name": "Jiří Wiedermann"
                    },
                    {
                        "name": "Jan van Leeuwen"
                    }
                ],
                "author_detail": {
                    "name": "Jan van Leeuwen"
                },
                "author": "Jan van Leeuwen",
                "arxiv_doi": "10.4204/EPTCS.407.14",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.4204/EPTCS.407.14",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.06978v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06978v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings NCMA 2024, arXiv:2409.06120",
                "arxiv_journal_ref": "EPTCS 407, 2024, pp. 198-213",
                "arxiv_primary_category": {
                    "term": "cs.FL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06957v1",
                "updated": "2024-09-11T02:40:38Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    2,
                    40,
                    38,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T02:40:38Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    2,
                    40,
                    38,
                    2,
                    255,
                    0
                ],
                "title": "Policy Filtration in RLHF to Fine-Tune LLM for Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Policy Filtration in RLHF to Fine-Tune LLM for Code Generation"
                },
                "summary": "Reinforcement learning from human feedback (RLHF) is one of the key\ntechniques that helps large language models (LLMs) to follow instructions and\nprovide helpful and harmless responses. While direct policy optimization\nmethods exist, state-of-the-art LLMs adopt RL-based methods (usually PPO) in\nRLHF to train the policy to generate good responses guided by a reward model\nlearned from preference data. The main challenge of these methods is the\ninaccuracy of the intermediate reward model, especially in code generation\ntasks that require long and complex reasoning to score a response. We find that\nthe reliability of the reward model varies across responses assigned with\ndifferent rewards. This motivates us to filter the samples whose rewards may be\nunreliable to improve signal-to-noise ratio during policy learning, resulting\nin Policy Filtration for Proximal Policy Optimization (PF-PPO). To choose a\nproper policy filtration strategy for a given reward model, the coefficient of\ndetermination ($R^2$) between rewards and actual scores on filtered samples\nserves as a good metrics and helps us find several promising strategies. We\nprovide extensive experiments to validate the effectiveness of PF-PPO in code\ngeneration tasks, and find that some variants of PF-PPO are highly effective\nand achieve new state-of-the-art performance across 7-billion-parameter models\non HumanEval, MBPP, and a new and more challenging LeetCode Contest benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning from human feedback (RLHF) is one of the key\ntechniques that helps large language models (LLMs) to follow instructions and\nprovide helpful and harmless responses. While direct policy optimization\nmethods exist, state-of-the-art LLMs adopt RL-based methods (usually PPO) in\nRLHF to train the policy to generate good responses guided by a reward model\nlearned from preference data. The main challenge of these methods is the\ninaccuracy of the intermediate reward model, especially in code generation\ntasks that require long and complex reasoning to score a response. We find that\nthe reliability of the reward model varies across responses assigned with\ndifferent rewards. This motivates us to filter the samples whose rewards may be\nunreliable to improve signal-to-noise ratio during policy learning, resulting\nin Policy Filtration for Proximal Policy Optimization (PF-PPO). To choose a\nproper policy filtration strategy for a given reward model, the coefficient of\ndetermination ($R^2$) between rewards and actual scores on filtered samples\nserves as a good metrics and helps us find several promising strategies. We\nprovide extensive experiments to validate the effectiveness of PF-PPO in code\ngeneration tasks, and find that some variants of PF-PPO are highly effective\nand achieve new state-of-the-art performance across 7-billion-parameter models\non HumanEval, MBPP, and a new and more challenging LeetCode Contest benchmark."
                },
                "authors": [
                    {
                        "name": "Wei Shen"
                    },
                    {
                        "name": "Chuheng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chuheng Zhang"
                },
                "author": "Chuheng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00481v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00481v2",
                "updated": "2024-09-11T02:35:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    2,
                    35,
                    51,
                    2,
                    255,
                    0
                ],
                "published": "2024-08-31T15:26:57Z",
                "published_parsed": [
                    2024,
                    8,
                    31,
                    15,
                    26,
                    57,
                    5,
                    244,
                    0
                ],
                "title": "DCIM-AVSR : Efficient Audio-Visual Speech Recognition via Dual Conformer\n  Interaction Module",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DCIM-AVSR : Efficient Audio-Visual Speech Recognition via Dual Conformer\n  Interaction Module"
                },
                "summary": "Speech recognition is the technology that enables machines to interpret and\nprocess human speech, converting spoken language into text or commands. This\ntechnology is essential for applications such as virtual assistants,\ntranscription services, and communication tools. The Audio-Visual Speech\nRecognition (AVSR) model enhances traditional speech recognition, particularly\nin noisy environments, by incorporating visual modalities like lip movements\nand facial expressions. While traditional AVSR models trained on large-scale\ndatasets with numerous parameters can achieve remarkable accuracy, often\nsurpassing human performance, they also come with high training costs and\ndeployment challenges. To address these issues, we introduce an efficient AVSR\nmodel that reduces the number of parameters through the integration of a Dual\nConformer Interaction Module (DCIM). In addition, we propose a pre-training\nmethod that further optimizes model performance by selectively updating\nparameters, leading to significant improvements in efficiency. Unlike\nconventional models that require the system to independently learn the\nhierarchical relationship between audio and visual modalities, our approach\nincorporates this distinction directly into the model architecture. This design\nenhances both efficiency and performance, resulting in a more practical and\neffective solution for AVSR tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speech recognition is the technology that enables machines to interpret and\nprocess human speech, converting spoken language into text or commands. This\ntechnology is essential for applications such as virtual assistants,\ntranscription services, and communication tools. The Audio-Visual Speech\nRecognition (AVSR) model enhances traditional speech recognition, particularly\nin noisy environments, by incorporating visual modalities like lip movements\nand facial expressions. While traditional AVSR models trained on large-scale\ndatasets with numerous parameters can achieve remarkable accuracy, often\nsurpassing human performance, they also come with high training costs and\ndeployment challenges. To address these issues, we introduce an efficient AVSR\nmodel that reduces the number of parameters through the integration of a Dual\nConformer Interaction Module (DCIM). In addition, we propose a pre-training\nmethod that further optimizes model performance by selectively updating\nparameters, leading to significant improvements in efficiency. Unlike\nconventional models that require the system to independently learn the\nhierarchical relationship between audio and visual modalities, our approach\nincorporates this distinction directly into the model architecture. This design\nenhances both efficiency and performance, resulting in a more practical and\neffective solution for AVSR tasks."
                },
                "authors": [
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Qian Wang"
                    }
                ],
                "author_detail": {
                    "name": "Qian Wang"
                },
                "author": "Qian Wang",
                "arxiv_comment": "Submitted to ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00481v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00481v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06949v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06949v1",
                "updated": "2024-09-11T02:03:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    2,
                    3,
                    51,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T02:03:51Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    2,
                    3,
                    51,
                    2,
                    255,
                    0
                ],
                "title": "You Have Thirteen Hours in Which to Solve the Labyrinth: Enhancing AI\n  Game Masters with Function Calling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "You Have Thirteen Hours in Which to Solve the Labyrinth: Enhancing AI\n  Game Masters with Function Calling"
                },
                "summary": "Developing a consistent and reliable AI game master for text-based games is a\nchallenging task due to the limitations of large language models (LLMs) and the\ncomplexity of the game master's role. This paper presents a novel approach to\nenhance AI game masters by leveraging function calling in the context of the\ntable-top role-playing game \"Jim Henson's Labyrinth: The Adventure Game.\" Our\nmethodology involves integrating game-specific controls through functions,\nwhich we show improves the narrative quality and state update consistency of\nthe AI game master. The experimental results, based on human evaluations and\nunit tests, demonstrate the effectiveness of our approach in enhancing gameplay\nexperience and maintaining coherence with the game state. This work contributes\nto the advancement of game AI and interactive storytelling, offering insights\ninto the design of more engaging and consistent AI-driven game masters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing a consistent and reliable AI game master for text-based games is a\nchallenging task due to the limitations of large language models (LLMs) and the\ncomplexity of the game master's role. This paper presents a novel approach to\nenhance AI game masters by leveraging function calling in the context of the\ntable-top role-playing game \"Jim Henson's Labyrinth: The Adventure Game.\" Our\nmethodology involves integrating game-specific controls through functions,\nwhich we show improves the narrative quality and state update consistency of\nthe AI game master. The experimental results, based on human evaluations and\nunit tests, demonstrate the effectiveness of our approach in enhancing gameplay\nexperience and maintaining coherence with the game state. This work contributes\nto the advancement of game AI and interactive storytelling, offering insights\ninto the design of more engaging and consistent AI-driven game masters."
                },
                "authors": [
                    {
                        "name": "Jaewoo Song"
                    },
                    {
                        "name": "Andrew Zhu"
                    },
                    {
                        "name": "Chris Callison-Burch"
                    }
                ],
                "author_detail": {
                    "name": "Chris Callison-Burch"
                },
                "author": "Chris Callison-Burch",
                "arxiv_comment": "Wordplay Workshop @ ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06949v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06949v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06277v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06277v2",
                "updated": "2024-09-11T01:47:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    1,
                    47,
                    48,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-10T07:28:13Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    7,
                    28,
                    13,
                    1,
                    254,
                    0
                ],
                "title": "Ferret: Federated Full-Parameter Tuning at Scale for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ferret: Federated Full-Parameter Tuning at Scale for Large Language\n  Models"
                },
                "summary": "Large Language Models (LLMs) have become indispensable in numerous real-world\napplications. Unfortunately, fine-tuning these models at scale, especially in\nfederated settings where data privacy and communication efficiency are\ncritical, presents significant challenges. Existing methods often resort to\nparameter-efficient fine-tuning (PEFT) to mitigate communication overhead, but\nthis typically comes at the cost of model accuracy. To address these\nlimitations, we propose federated full-parameter tuning at scale for LLMs\n(Ferret), the first first-order method with shared randomness to enable\nscalable full-parameter tuning of LLMs across decentralized data sources while\nmaintaining competitive model accuracy. Ferret accomplishes this through three\naspects: (1) it employs widely applied first-order methods for efficient local\nupdates; (2) it projects these updates into a low-dimensional space to\nconsiderably reduce communication overhead; and (3) it reconstructs local\nupdates from this low-dimensional space with shared randomness to facilitate\neffective full-parameter global aggregation, ensuring fast convergence and\ncompetitive final performance. Our rigorous theoretical analyses and insights\nalong with extensive experiments, show that Ferret significantly enhances the\nscalability of existing federated full-parameter tuning approaches by achieving\nhigh computational efficiency, reduced communication overhead, and fast\nconvergence, all while maintaining competitive model accuracy. Our\nimplementation is available at https://github.com/allen4747/Ferret.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become indispensable in numerous real-world\napplications. Unfortunately, fine-tuning these models at scale, especially in\nfederated settings where data privacy and communication efficiency are\ncritical, presents significant challenges. Existing methods often resort to\nparameter-efficient fine-tuning (PEFT) to mitigate communication overhead, but\nthis typically comes at the cost of model accuracy. To address these\nlimitations, we propose federated full-parameter tuning at scale for LLMs\n(Ferret), the first first-order method with shared randomness to enable\nscalable full-parameter tuning of LLMs across decentralized data sources while\nmaintaining competitive model accuracy. Ferret accomplishes this through three\naspects: (1) it employs widely applied first-order methods for efficient local\nupdates; (2) it projects these updates into a low-dimensional space to\nconsiderably reduce communication overhead; and (3) it reconstructs local\nupdates from this low-dimensional space with shared randomness to facilitate\neffective full-parameter global aggregation, ensuring fast convergence and\ncompetitive final performance. Our rigorous theoretical analyses and insights\nalong with extensive experiments, show that Ferret significantly enhances the\nscalability of existing federated full-parameter tuning approaches by achieving\nhigh computational efficiency, reduced communication overhead, and fast\nconvergence, all while maintaining competitive model accuracy. Our\nimplementation is available at https://github.com/allen4747/Ferret."
                },
                "authors": [
                    {
                        "name": "Yao Shu"
                    },
                    {
                        "name": "Wenyang Hu"
                    },
                    {
                        "name": "See-Kiong Ng"
                    },
                    {
                        "name": "Bryan Kian Hsiang Low"
                    },
                    {
                        "name": "Fei Richard Yu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Richard Yu"
                },
                "author": "Fei Richard Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06277v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06277v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06941v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06941v1",
                "updated": "2024-09-11T01:46:49Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    1,
                    46,
                    49,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T01:46:49Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    1,
                    46,
                    49,
                    2,
                    255,
                    0
                ],
                "title": "FreeRide: Harvesting Bubbles in Pipeline Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreeRide: Harvesting Bubbles in Pipeline Parallelism"
                },
                "summary": "The occurrence of bubbles in pipeline parallelism is an inherent limitation\nthat can account for more than 40% of the large language model (LLM) training\ntime and is one of the main reasons for the underutilization of GPU resources\nin LLM training. Harvesting these bubbles for GPU side tasks can increase\nresource utilization and reduce training costs but comes with challenges.\nFirst, because bubbles are discontinuous with various shapes, programming side\ntasks becomes difficult while requiring excessive engineering effort. Second, a\nside task can compete with pipeline training for GPU resources and incur\nsignificant overhead. To address these challenges, we propose FreeRide, a\nsystem designed to harvest bubbles in pipeline parallelism for side tasks.\nFreeRide provides programmers with interfaces to implement side tasks easily,\nmanages bubbles and side tasks during pipeline training, and controls access to\nGPU resources by side tasks to reduce overhead. We demonstrate that FreeRide\nachieves 7.8% average cost savings with a negligible overhead of about 1% in\ntraining LLMs while serving model training, graph analytics, and image\nprocessing side tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The occurrence of bubbles in pipeline parallelism is an inherent limitation\nthat can account for more than 40% of the large language model (LLM) training\ntime and is one of the main reasons for the underutilization of GPU resources\nin LLM training. Harvesting these bubbles for GPU side tasks can increase\nresource utilization and reduce training costs but comes with challenges.\nFirst, because bubbles are discontinuous with various shapes, programming side\ntasks becomes difficult while requiring excessive engineering effort. Second, a\nside task can compete with pipeline training for GPU resources and incur\nsignificant overhead. To address these challenges, we propose FreeRide, a\nsystem designed to harvest bubbles in pipeline parallelism for side tasks.\nFreeRide provides programmers with interfaces to implement side tasks easily,\nmanages bubbles and side tasks during pipeline training, and controls access to\nGPU resources by side tasks to reduce overhead. We demonstrate that FreeRide\nachieves 7.8% average cost savings with a negligible overhead of about 1% in\ntraining LLMs while serving model training, graph analytics, and image\nprocessing side tasks."
                },
                "authors": [
                    {
                        "name": "Jiashu Zhang"
                    },
                    {
                        "name": "Zihan Pan"
                    },
                    {
                        "name": "Molly"
                    },
                    {
                        "name": "Xu"
                    },
                    {
                        "name": "Khuzaima Daudjee"
                    },
                    {
                        "name": "Sihang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Sihang Liu"
                },
                "arxiv_affiliation": "Yiming",
                "author": "Sihang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06941v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06941v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06927v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06927v1",
                "updated": "2024-09-11T00:56:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    0,
                    56,
                    2,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T00:56:02Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    0,
                    56,
                    2,
                    2,
                    255,
                    0
                ],
                "title": "Representation Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Representation Tuning"
                },
                "summary": "Activation engineering is becoming increasingly popular as a means of online\ncontrol of large language models (LLMs). In this work, I extend the idea of\nactive steering with vectors that represent a behavioral direction of interest\nto tuning those vectors directly into the model, obviating the need for online\ncontrol. First, I identify activation vectors related to honesty in an\nopen-source LLM (Llama- 2-13b-chat). Next, I demonstrate that model output can\nbe made more or less honest by adding positive or negative multiples of these\nvectors to residual stream activations during generation. Then, I show that a\nsimilar effect can be achieved by fine-tuning the vectors directly into the\nmodel, by use of a dual loss function based on the cosine similarity of\nresidual stream activations to the vectors combined with a standard token-based\nloss (\"representation tuning\"). Finally, I compare the generations in response\nto honesty-probing prompts from the resulting models to those from models\nfine-tuned with a token-based loss alone, and to those from the untuned model\nsubjected to online steering. Overall, fine-tuning the vectors into the models\nusing the cosine similarity plus token loss showed a stronger effect than\nonline steering, and generalized better than using the standard loss,\nsuggesting the potential utility of this approach as a safety measure. Code and\ndata are available at https://github.com/cma1114/representation_tuning; tuned\nmodels are available at https://huggingface.co/collections/cackerman/\nrepresentation-tuning-66da1e5ab41cd1b824687d9f.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation engineering is becoming increasingly popular as a means of online\ncontrol of large language models (LLMs). In this work, I extend the idea of\nactive steering with vectors that represent a behavioral direction of interest\nto tuning those vectors directly into the model, obviating the need for online\ncontrol. First, I identify activation vectors related to honesty in an\nopen-source LLM (Llama- 2-13b-chat). Next, I demonstrate that model output can\nbe made more or less honest by adding positive or negative multiples of these\nvectors to residual stream activations during generation. Then, I show that a\nsimilar effect can be achieved by fine-tuning the vectors directly into the\nmodel, by use of a dual loss function based on the cosine similarity of\nresidual stream activations to the vectors combined with a standard token-based\nloss (\"representation tuning\"). Finally, I compare the generations in response\nto honesty-probing prompts from the resulting models to those from models\nfine-tuned with a token-based loss alone, and to those from the untuned model\nsubjected to online steering. Overall, fine-tuning the vectors into the models\nusing the cosine similarity plus token loss showed a stronger effect than\nonline steering, and generalized better than using the standard loss,\nsuggesting the potential utility of this approach as a safety measure. Code and\ndata are available at https://github.com/cma1114/representation_tuning; tuned\nmodels are available at https://huggingface.co/collections/cackerman/\nrepresentation-tuning-66da1e5ab41cd1b824687d9f."
                },
                "authors": [
                    {
                        "name": "Christopher M. Ackerman"
                    }
                ],
                "author_detail": {
                    "name": "Christopher M. Ackerman"
                },
                "author": "Christopher M. Ackerman",
                "arxiv_comment": "9 pages, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06927v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.08988v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.08988v2",
                "updated": "2024-09-10T23:40:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    23,
                    40,
                    52,
                    1,
                    254,
                    0
                ],
                "published": "2024-02-14T07:08:30Z",
                "published_parsed": [
                    2024,
                    2,
                    14,
                    7,
                    8,
                    30,
                    2,
                    45,
                    0
                ],
                "title": "An In-Depth Investigation of LEO Satellite Topology Design Parameters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An In-Depth Investigation of LEO Satellite Topology Design Parameters"
                },
                "summary": "Low Earth Orbit (LEO) satellite networks are rapidly gaining traction today.\nAlthough several real-world deployments exist, our preliminary analysis of LEO\ntopology performance with the soon-to-be operational Inter-Satellite Links\n(ISLs) reveals several interesting characteristics that are difficult to\nexplain based on our current understanding of topologies. For example, a\nreal-world satellite shell with a low density of satellites offers better\nlatency performance than another shell with nearly double the number of\nsatellites. In this work, we conduct an in-depth investigation of LEO satellite\ntopology design parameters and their impact on network performance while using\nthe ISLs. In particular, we focus on three design parameters: the number of\norbits in a shell, the inclination of orbits, and the number of satellites per\norbit. Through an extensive analysis of real-world and synthetic satellite\nconfigurations, we uncover several interesting properties of satellite\ntopologies. Notably, there exist thresholds for the number of satellites per\norbit and the number of orbits below which the latency performance degrades\nsignificantly. Moreover, network delay between a pair of traffic endpoints\ndepends on the alignment of the satellite's orbit (Inclination) with the\ngeographic locations of endpoints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low Earth Orbit (LEO) satellite networks are rapidly gaining traction today.\nAlthough several real-world deployments exist, our preliminary analysis of LEO\ntopology performance with the soon-to-be operational Inter-Satellite Links\n(ISLs) reveals several interesting characteristics that are difficult to\nexplain based on our current understanding of topologies. For example, a\nreal-world satellite shell with a low density of satellites offers better\nlatency performance than another shell with nearly double the number of\nsatellites. In this work, we conduct an in-depth investigation of LEO satellite\ntopology design parameters and their impact on network performance while using\nthe ISLs. In particular, we focus on three design parameters: the number of\norbits in a shell, the inclination of orbits, and the number of satellites per\norbit. Through an extensive analysis of real-world and synthetic satellite\nconfigurations, we uncover several interesting properties of satellite\ntopologies. Notably, there exist thresholds for the number of satellites per\norbit and the number of orbits below which the latency performance degrades\nsignificantly. Moreover, network delay between a pair of traffic endpoints\ndepends on the alignment of the satellite's orbit (Inclination) with the\ngeographic locations of endpoints."
                },
                "authors": [
                    {
                        "name": "Wenyi Zhang"
                    },
                    {
                        "name": "Zihan Xu"
                    },
                    {
                        "name": "Sangeetha Abdu Jyothi"
                    }
                ],
                "author_detail": {
                    "name": "Sangeetha Abdu Jyothi"
                },
                "author": "Sangeetha Abdu Jyothi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.08988v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.08988v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.02466v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.02466v3",
                "updated": "2024-09-10T23:17:16Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    23,
                    17,
                    16,
                    1,
                    254,
                    0
                ],
                "published": "2024-05-03T20:00:40Z",
                "published_parsed": [
                    2024,
                    5,
                    3,
                    20,
                    0,
                    40,
                    4,
                    124,
                    0
                ],
                "title": "ProFLingo: A Fingerprinting-based Intellectual Property Protection\n  Scheme for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProFLingo: A Fingerprinting-based Intellectual Property Protection\n  Scheme for Large Language Models"
                },
                "summary": "Large language models (LLMs) have attracted significant attention in recent\nyears. Due to their \"Large\" nature, training LLMs from scratch consumes immense\ncomputational resources. Since several major players in the artificial\nintelligence (AI) field have open-sourced their original LLMs, an increasing\nnumber of individuals and smaller companies are able to build derivative LLMs\nbased on these open-sourced models at much lower costs. However, this practice\nopens up possibilities for unauthorized use or reproduction that may not comply\nwith licensing agreements, and fine-tuning can change the model's behavior,\nthus complicating the determination of model ownership. Current intellectual\nproperty (IP) protection schemes for LLMs are either designed for white-box\nsettings or require additional modifications to the original model, which\nrestricts their use in real-world settings.\n  In this paper, we propose ProFLingo, a black-box fingerprinting-based IP\nprotection scheme for LLMs. ProFLingo generates queries that elicit specific\nresponses from an original model, thereby establishing unique fingerprints. Our\nscheme assesses the effectiveness of these queries on a suspect model to\ndetermine whether it has been derived from the original model. ProFLingo offers\na non-invasive approach, which neither requires knowledge of the suspect model\nnor modifications to the base model or its training process. To the best of our\nknowledge, our method represents the first black-box fingerprinting technique\nfor IP protection for LLMs. Our source code and generated queries are available\nat: https://github.com/hengvt/ProFLingo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have attracted significant attention in recent\nyears. Due to their \"Large\" nature, training LLMs from scratch consumes immense\ncomputational resources. Since several major players in the artificial\nintelligence (AI) field have open-sourced their original LLMs, an increasing\nnumber of individuals and smaller companies are able to build derivative LLMs\nbased on these open-sourced models at much lower costs. However, this practice\nopens up possibilities for unauthorized use or reproduction that may not comply\nwith licensing agreements, and fine-tuning can change the model's behavior,\nthus complicating the determination of model ownership. Current intellectual\nproperty (IP) protection schemes for LLMs are either designed for white-box\nsettings or require additional modifications to the original model, which\nrestricts their use in real-world settings.\n  In this paper, we propose ProFLingo, a black-box fingerprinting-based IP\nprotection scheme for LLMs. ProFLingo generates queries that elicit specific\nresponses from an original model, thereby establishing unique fingerprints. Our\nscheme assesses the effectiveness of these queries on a suspect model to\ndetermine whether it has been derived from the original model. ProFLingo offers\na non-invasive approach, which neither requires knowledge of the suspect model\nnor modifications to the base model or its training process. To the best of our\nknowledge, our method represents the first black-box fingerprinting technique\nfor IP protection for LLMs. Our source code and generated queries are available\nat: https://github.com/hengvt/ProFLingo."
                },
                "authors": [
                    {
                        "name": "Heng Jin"
                    },
                    {
                        "name": "Chaoyu Zhang"
                    },
                    {
                        "name": "Shanghao Shi"
                    },
                    {
                        "name": "Wenjing Lou"
                    },
                    {
                        "name": "Y. Thomas Hou"
                    }
                ],
                "author_detail": {
                    "name": "Y. Thomas Hou"
                },
                "author": "Y. Thomas Hou",
                "arxiv_comment": "This is the author's pre-print version of the work. It is posted here\n  for your personal use. Not for redistribution",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.02466v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.02466v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06903v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06903v1",
                "updated": "2024-09-10T22:57:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    22,
                    57,
                    58,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T22:57:58Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    22,
                    57,
                    58,
                    1,
                    254,
                    0
                ],
                "title": "Semi-Supervised Reward Modeling via Iterative Self-Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semi-Supervised Reward Modeling via Iterative Self-Training"
                },
                "summary": "Reward models (RM) capture the values and preferences of humans and play a\ncentral role in Reinforcement Learning with Human Feedback (RLHF) to align\npretrained large language models (LLMs). Traditionally, training these models\nrelies on extensive human-annotated preference data, which poses significant\nchallenges in terms of scalability and cost. To overcome these limitations, we\npropose Semi-Supervised Reward Modeling (SSRM), an approach that enhances RM\ntraining using unlabeled data. Given an unlabeled dataset, SSRM involves three\nkey iterative steps: pseudo-labeling unlabeled examples, selecting\nhigh-confidence examples through a confidence threshold, and supervised\nfinetuning on the refined dataset. Across extensive experiments on various\nmodel configurations, we demonstrate that SSRM significantly improves reward\nmodels without incurring additional labeling costs. Notably, SSRM can achieve\nperformance comparable to models trained entirely on labeled data of equivalent\nvolumes. Overall, SSRM substantially reduces the dependency on large volumes of\nhuman-annotated data, thereby decreasing the overall cost and time involved in\ntraining effective reward models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward models (RM) capture the values and preferences of humans and play a\ncentral role in Reinforcement Learning with Human Feedback (RLHF) to align\npretrained large language models (LLMs). Traditionally, training these models\nrelies on extensive human-annotated preference data, which poses significant\nchallenges in terms of scalability and cost. To overcome these limitations, we\npropose Semi-Supervised Reward Modeling (SSRM), an approach that enhances RM\ntraining using unlabeled data. Given an unlabeled dataset, SSRM involves three\nkey iterative steps: pseudo-labeling unlabeled examples, selecting\nhigh-confidence examples through a confidence threshold, and supervised\nfinetuning on the refined dataset. Across extensive experiments on various\nmodel configurations, we demonstrate that SSRM significantly improves reward\nmodels without incurring additional labeling costs. Notably, SSRM can achieve\nperformance comparable to models trained entirely on labeled data of equivalent\nvolumes. Overall, SSRM substantially reduces the dependency on large volumes of\nhuman-annotated data, thereby decreasing the overall cost and time involved in\ntraining effective reward models."
                },
                "authors": [
                    {
                        "name": "Yifei He"
                    },
                    {
                        "name": "Haoxiang Wang"
                    },
                    {
                        "name": "Ziyan Jiang"
                    },
                    {
                        "name": "Alexandros Papangelis"
                    },
                    {
                        "name": "Han Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Han Zhao"
                },
                "author": "Han Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06903v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06903v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00008v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00008v2",
                "updated": "2024-09-10T22:35:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    22,
                    35,
                    13,
                    1,
                    254,
                    0
                ],
                "published": "2024-07-23T23:37:29Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    23,
                    37,
                    29,
                    1,
                    205,
                    0
                ],
                "title": "ScaleLLM: A Resource-Frugal LLM Serving Framework by Optimizing\n  End-to-End Efficiency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ScaleLLM: A Resource-Frugal LLM Serving Framework by Optimizing\n  End-to-End Efficiency"
                },
                "summary": "Large language models (LLMs) have surged in popularity and are extensively\nused in commercial applications, where the efficiency of model serving is\ncrucial for the user experience. Most current research focuses on optimizing\nindividual sub-procedures, e.g. local inference and communication, however,\nthere is no comprehensive framework that provides a holistic system view for\noptimizing LLM serving in an end-to-end manner. In this work, we conduct a\ndetailed analysis to identify major bottlenecks that impact end-to-end latency\nin LLM serving systems. Our analysis reveals that a comprehensive LLM serving\nendpoint must address a series of efficiency bottlenecks that extend beyond LLM\ninference. We then propose ScaleLLM, an optimized system for resource-efficient\nLLM serving. Our extensive experiments reveal that with 64 concurrent requests,\nScaleLLM achieves a 4.3x speed up over vLLM and outperforms state-of-the-arts\nwith 1.5x higher throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have surged in popularity and are extensively\nused in commercial applications, where the efficiency of model serving is\ncrucial for the user experience. Most current research focuses on optimizing\nindividual sub-procedures, e.g. local inference and communication, however,\nthere is no comprehensive framework that provides a holistic system view for\noptimizing LLM serving in an end-to-end manner. In this work, we conduct a\ndetailed analysis to identify major bottlenecks that impact end-to-end latency\nin LLM serving systems. Our analysis reveals that a comprehensive LLM serving\nendpoint must address a series of efficiency bottlenecks that extend beyond LLM\ninference. We then propose ScaleLLM, an optimized system for resource-efficient\nLLM serving. Our extensive experiments reveal that with 64 concurrent requests,\nScaleLLM achieves a 4.3x speed up over vLLM and outperforms state-of-the-arts\nwith 1.5x higher throughput."
                },
                "authors": [
                    {
                        "name": "Yuhang Yao"
                    },
                    {
                        "name": "Han Jin"
                    },
                    {
                        "name": "Alay Dilipbhai Shah"
                    },
                    {
                        "name": "Shanshan Han"
                    },
                    {
                        "name": "Zijian Hu"
                    },
                    {
                        "name": "Yide Ran"
                    },
                    {
                        "name": "Dimitris Stripelis"
                    },
                    {
                        "name": "Zhaozhuo Xu"
                    },
                    {
                        "name": "Salman Avestimehr"
                    },
                    {
                        "name": "Chaoyang He"
                    }
                ],
                "author_detail": {
                    "name": "Chaoyang He"
                },
                "author": "Chaoyang He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00008v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00008v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.05930v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.05930v2",
                "updated": "2024-09-10T22:22:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    22,
                    22,
                    43,
                    1,
                    254,
                    0
                ],
                "published": "2024-02-08T18:58:02Z",
                "published_parsed": [
                    2024,
                    2,
                    8,
                    18,
                    58,
                    2,
                    3,
                    39,
                    0
                ],
                "title": "WebLINX: Real-World Website Navigation with Multi-Turn Dialogue",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WebLINX: Real-World Website Navigation with Multi-Turn Dialogue"
                },
                "summary": "We propose the problem of conversational web navigation, where a digital\nagent controls a web browser and follows user instructions to solve real-world\ntasks in a multi-turn dialogue fashion. To support this problem, we introduce\nWEBLINX - a large-scale benchmark of 100K interactions across 2300 expert\ndemonstrations of conversational web navigation. Our benchmark covers a broad\nrange of patterns on over 150 real-world websites and can be used to train and\nevaluate agents in diverse scenarios. Due to the magnitude of information\npresent, Large Language Models (LLMs) cannot process entire web pages in\nreal-time. To solve this bottleneck, we design a retrieval-inspired model that\nefficiently prunes HTML pages by ranking relevant elements. We use the selected\nelements, along with screenshots and action history, to assess a variety of\nmodels for their ability to replicate human behavior when navigating the web.\nOur experiments span from small text-only to proprietary multimodal LLMs. We\nfind that smaller finetuned decoders surpass the best zero-shot LLMs (including\nGPT-4V), but also larger finetuned multimodal models which were explicitly\npretrained on screenshots. However, all finetuned models struggle to generalize\nto unseen websites. Our findings highlight the need for large multimodal models\nthat can generalize to novel settings. Our code, data and models are available\nfor research: https://mcgill-nlp.github.io/weblinx",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose the problem of conversational web navigation, where a digital\nagent controls a web browser and follows user instructions to solve real-world\ntasks in a multi-turn dialogue fashion. To support this problem, we introduce\nWEBLINX - a large-scale benchmark of 100K interactions across 2300 expert\ndemonstrations of conversational web navigation. Our benchmark covers a broad\nrange of patterns on over 150 real-world websites and can be used to train and\nevaluate agents in diverse scenarios. Due to the magnitude of information\npresent, Large Language Models (LLMs) cannot process entire web pages in\nreal-time. To solve this bottleneck, we design a retrieval-inspired model that\nefficiently prunes HTML pages by ranking relevant elements. We use the selected\nelements, along with screenshots and action history, to assess a variety of\nmodels for their ability to replicate human behavior when navigating the web.\nOur experiments span from small text-only to proprietary multimodal LLMs. We\nfind that smaller finetuned decoders surpass the best zero-shot LLMs (including\nGPT-4V), but also larger finetuned multimodal models which were explicitly\npretrained on screenshots. However, all finetuned models struggle to generalize\nto unseen websites. Our findings highlight the need for large multimodal models\nthat can generalize to novel settings. Our code, data and models are available\nfor research: https://mcgill-nlp.github.io/weblinx"
                },
                "authors": [
                    {
                        "name": "Xing Han Lù"
                    },
                    {
                        "name": "Zdeněk Kasner"
                    },
                    {
                        "name": "Siva Reddy"
                    }
                ],
                "author_detail": {
                    "name": "Siva Reddy"
                },
                "author": "Siva Reddy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.05930v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.05930v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06883v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06883v1",
                "updated": "2024-09-10T21:54:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    21,
                    54,
                    46,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T21:54:46Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    21,
                    54,
                    46,
                    1,
                    254,
                    0
                ],
                "title": "A Dataset for Evaluating LLM-based Evaluation Functions for Research\n  Question Extraction Task",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Dataset for Evaluating LLM-based Evaluation Functions for Research\n  Question Extraction Task"
                },
                "summary": "The progress in text summarization techniques has been remarkable. However\nthe task of accurately extracting and summarizing necessary information from\nhighly specialized documents such as research papers has not been sufficiently\ninvestigated. We are focusing on the task of extracting research questions (RQ)\nfrom research papers and construct a new dataset consisting of machine learning\npapers, RQ extracted from these papers by GPT-4, and human evaluations of the\nextracted RQ from multiple perspectives. Using this dataset, we systematically\ncompared recently proposed LLM-based evaluation functions for summarizations,\nand found that none of the functions showed sufficiently high correlations with\nhuman evaluations. We expect our dataset provides a foundation for further\nresearch on developing better evaluation functions tailored to the RQ\nextraction task, and contribute to enhance the performance of the task. The\ndataset is available at https://github.com/auto-res/PaperRQ-HumanAnno-Dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The progress in text summarization techniques has been remarkable. However\nthe task of accurately extracting and summarizing necessary information from\nhighly specialized documents such as research papers has not been sufficiently\ninvestigated. We are focusing on the task of extracting research questions (RQ)\nfrom research papers and construct a new dataset consisting of machine learning\npapers, RQ extracted from these papers by GPT-4, and human evaluations of the\nextracted RQ from multiple perspectives. Using this dataset, we systematically\ncompared recently proposed LLM-based evaluation functions for summarizations,\nand found that none of the functions showed sufficiently high correlations with\nhuman evaluations. We expect our dataset provides a foundation for further\nresearch on developing better evaluation functions tailored to the RQ\nextraction task, and contribute to enhance the performance of the task. The\ndataset is available at https://github.com/auto-res/PaperRQ-HumanAnno-Dataset."
                },
                "authors": [
                    {
                        "name": "Yuya Fujisaki"
                    },
                    {
                        "name": "Shiro Takagi"
                    },
                    {
                        "name": "Hideki Asoh"
                    },
                    {
                        "name": "Wataru Kumagai"
                    }
                ],
                "author_detail": {
                    "name": "Wataru Kumagai"
                },
                "author": "Wataru Kumagai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06883v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06883v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03131v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03131v2",
                "updated": "2024-09-10T21:53:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    21,
                    53,
                    46,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-04T23:45:10Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    23,
                    45,
                    10,
                    2,
                    248,
                    0
                ],
                "title": "Well, that escalated quickly: The Single-Turn Crescendo Attack (STCA)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Well, that escalated quickly: The Single-Turn Crescendo Attack (STCA)"
                },
                "summary": "This paper introduces a new method for adversarial attacks on large language\nmodels (LLMs) called the Single-Turn Crescendo Attack (STCA). Building on the\nmulti-turn crescendo attack method introduced by Russinovich, Salem, and Eldan\n(2024), which gradually escalates the context to provoke harmful responses, the\nSTCA achieves similar outcomes in a single interaction. By condensing the\nescalation into a single, well-crafted prompt, the STCA bypasses typical\nmoderation filters that LLMs use to prevent inappropriate outputs. This\ntechnique reveals vulnerabilities in current LLMs and emphasizes the importance\nof stronger safeguards in responsible AI (RAI). The STCA offers a novel method\nthat has not been previously explored.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a new method for adversarial attacks on large language\nmodels (LLMs) called the Single-Turn Crescendo Attack (STCA). Building on the\nmulti-turn crescendo attack method introduced by Russinovich, Salem, and Eldan\n(2024), which gradually escalates the context to provoke harmful responses, the\nSTCA achieves similar outcomes in a single interaction. By condensing the\nescalation into a single, well-crafted prompt, the STCA bypasses typical\nmoderation filters that LLMs use to prevent inappropriate outputs. This\ntechnique reveals vulnerabilities in current LLMs and emphasizes the importance\nof stronger safeguards in responsible AI (RAI). The STCA offers a novel method\nthat has not been previously explored."
                },
                "authors": [
                    {
                        "name": "Alan Aqrawi"
                    },
                    {
                        "name": "Arian Abbasi"
                    }
                ],
                "author_detail": {
                    "name": "Arian Abbasi"
                },
                "author": "Arian Abbasi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03131v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03131v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17992v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17992v2",
                "updated": "2024-09-10T21:51:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    21,
                    51,
                    23,
                    1,
                    254,
                    0
                ],
                "published": "2024-07-25T12:38:08Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    12,
                    38,
                    8,
                    3,
                    207,
                    0
                ],
                "title": "Amortized Active Learning for Nonparametric Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amortized Active Learning for Nonparametric Functions"
                },
                "summary": "Active learning (AL) is a sequential learning scheme aiming to select the\nmost informative data. AL reduces data consumption and avoids the cost of\nlabeling large amounts of data. However, AL trains the model and solves an\nacquisition optimization for each selection. It becomes expensive when the\nmodel training or acquisition optimization is challenging. In this paper, we\nfocus on active nonparametric function learning, where the gold standard\nGaussian process (GP) approaches suffer from cubic time complexity. We propose\nan amortized AL method, where new data are suggested by a neural network which\nis trained up-front without any real data (Figure 1). Our method avoids\nrepeated model training and requires no acquisition optimization during the AL\ndeployment. We (i) utilize GPs as function priors to construct an AL simulator,\n(ii) train an AL policy that can zero-shot generalize from simulation to real\nlearning problems of nonparametric functions and (iii) achieve real-time data\nselection and comparable learning performances to time-consuming baseline\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active learning (AL) is a sequential learning scheme aiming to select the\nmost informative data. AL reduces data consumption and avoids the cost of\nlabeling large amounts of data. However, AL trains the model and solves an\nacquisition optimization for each selection. It becomes expensive when the\nmodel training or acquisition optimization is challenging. In this paper, we\nfocus on active nonparametric function learning, where the gold standard\nGaussian process (GP) approaches suffer from cubic time complexity. We propose\nan amortized AL method, where new data are suggested by a neural network which\nis trained up-front without any real data (Figure 1). Our method avoids\nrepeated model training and requires no acquisition optimization during the AL\ndeployment. We (i) utilize GPs as function priors to construct an AL simulator,\n(ii) train an AL policy that can zero-shot generalize from simulation to real\nlearning problems of nonparametric functions and (iii) achieve real-time data\nselection and comparable learning performances to time-consuming baseline\nmethods."
                },
                "authors": [
                    {
                        "name": "Cen-You Li"
                    },
                    {
                        "name": "Marc Toussaint"
                    },
                    {
                        "name": "Barbara Rakitsch"
                    },
                    {
                        "name": "Christoph Zimmer"
                    }
                ],
                "author_detail": {
                    "name": "Christoph Zimmer"
                },
                "author": "Christoph Zimmer",
                "arxiv_comment": "Accepted for publication at ECML-PKDD 2024 Workshop & Tutorial on\n  Interactive Adaptive Learning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17992v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17992v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05735v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05735v2",
                "updated": "2024-09-10T21:46:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    21,
                    46,
                    32,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-09T15:44:39Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    15,
                    44,
                    39,
                    0,
                    253,
                    0
                ],
                "title": "A System and Benchmark for LLM-based Q&A on Heterogeneous Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A System and Benchmark for LLM-based Q&A on Heterogeneous Data"
                },
                "summary": "In many industrial settings, users wish to ask questions whose answers may be\nfound in structured data sources such as a spreadsheets, databases, APIs, or\ncombinations thereof. Often, the user doesn't know how to identify or access\nthe right data source. This problem is compounded even further if multiple (and\npotentially siloed) data sources must be assembled to derive the answer.\nRecently, various Text-to-SQL applications that leverage Large Language Models\n(LLMs) have addressed some of these problems by enabling users to ask questions\nin natural language. However, these applications remain impractical in\nrealistic industrial settings because they fail to cope with the data source\nheterogeneity that typifies such environments. In this paper, we address\nheterogeneity by introducing the siwarex platform, which enables seamless\nnatural language access to both databases and APIs. To demonstrate the\neffectiveness of siwarex, we extend the popular Spider dataset and benchmark by\nreplacing some of its tables by data retrieval APIs. We find that siwarex does\na good job of coping with data source heterogeneity. Our modified Spider\nbenchmark will soon be available to the research community",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In many industrial settings, users wish to ask questions whose answers may be\nfound in structured data sources such as a spreadsheets, databases, APIs, or\ncombinations thereof. Often, the user doesn't know how to identify or access\nthe right data source. This problem is compounded even further if multiple (and\npotentially siloed) data sources must be assembled to derive the answer.\nRecently, various Text-to-SQL applications that leverage Large Language Models\n(LLMs) have addressed some of these problems by enabling users to ask questions\nin natural language. However, these applications remain impractical in\nrealistic industrial settings because they fail to cope with the data source\nheterogeneity that typifies such environments. In this paper, we address\nheterogeneity by introducing the siwarex platform, which enables seamless\nnatural language access to both databases and APIs. To demonstrate the\neffectiveness of siwarex, we extend the popular Spider dataset and benchmark by\nreplacing some of its tables by data retrieval APIs. We find that siwarex does\na good job of coping with data source heterogeneity. Our modified Spider\nbenchmark will soon be available to the research community"
                },
                "authors": [
                    {
                        "name": "Achille Fokoue"
                    },
                    {
                        "name": "Srideepika Jayaraman"
                    },
                    {
                        "name": "Elham Khabiri"
                    },
                    {
                        "name": "Jeffrey O. Kephart"
                    },
                    {
                        "name": "Yingjie Li"
                    },
                    {
                        "name": "Dhruv Shah"
                    },
                    {
                        "name": "Youssef Drissi"
                    },
                    {
                        "name": "Fenno F. Heath III"
                    },
                    {
                        "name": "Anu Bhamidipaty"
                    },
                    {
                        "name": "Fateh A. Tipu"
                    },
                    {
                        "name": "Robert J. Baseman"
                    }
                ],
                "author_detail": {
                    "name": "Robert J. Baseman"
                },
                "author": "Robert J. Baseman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05735v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05735v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06863v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06863v1",
                "updated": "2024-09-10T21:00:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    21,
                    0,
                    33,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T21:00:33Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    21,
                    0,
                    33,
                    1,
                    254,
                    0
                ],
                "title": "Towards Understanding Human Emotional Fluctuations with Sparse Check-In\n  Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Understanding Human Emotional Fluctuations with Sparse Check-In\n  Data"
                },
                "summary": "Data sparsity is a key challenge limiting the power of AI tools across\nvarious domains. The problem is especially pronounced in domains that require\nactive user input rather than measurements derived from automated sensors. It\nis a critical barrier to harnessing the full potential of AI in domains\nrequiring active user engagement, such as self-reported mood check-ins, where\ncapturing a continuous picture of emotional states is essential. In this\ncontext, sparse data can hinder efforts to capture the nuances of individual\nemotional experiences such as causes, triggers, and contributing factors.\nExisting methods for addressing data scarcity often rely on heuristics or large\nestablished datasets, favoring deep learning models that lack adaptability to\nnew domains. This paper proposes a novel probabilistic framework that\nintegrates user-centric feedback-based learning, allowing for personalized\npredictions despite limited data. Achieving 60% accuracy in predicting user\nstates among 64 options (chance of 1/64), this framework effectively mitigates\ndata sparsity. It is versatile across various applications, bridging the gap\nbetween theoretical AI research and practical deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data sparsity is a key challenge limiting the power of AI tools across\nvarious domains. The problem is especially pronounced in domains that require\nactive user input rather than measurements derived from automated sensors. It\nis a critical barrier to harnessing the full potential of AI in domains\nrequiring active user engagement, such as self-reported mood check-ins, where\ncapturing a continuous picture of emotional states is essential. In this\ncontext, sparse data can hinder efforts to capture the nuances of individual\nemotional experiences such as causes, triggers, and contributing factors.\nExisting methods for addressing data scarcity often rely on heuristics or large\nestablished datasets, favoring deep learning models that lack adaptability to\nnew domains. This paper proposes a novel probabilistic framework that\nintegrates user-centric feedback-based learning, allowing for personalized\npredictions despite limited data. Achieving 60% accuracy in predicting user\nstates among 64 options (chance of 1/64), this framework effectively mitigates\ndata sparsity. It is versatile across various applications, bridging the gap\nbetween theoretical AI research and practical deployment."
                },
                "authors": [
                    {
                        "name": "Sagar Paresh Shah"
                    },
                    {
                        "name": "Ga Wu"
                    },
                    {
                        "name": "Sean W. Kortschot"
                    },
                    {
                        "name": "Samuel Daviau"
                    }
                ],
                "author_detail": {
                    "name": "Samuel Daviau"
                },
                "author": "Samuel Daviau",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06863v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06863v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06859v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06859v1",
                "updated": "2024-09-10T20:49:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    20,
                    49,
                    5,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T20:49:05Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    20,
                    49,
                    5,
                    1,
                    254,
                    0
                ],
                "title": "NSP: A Neuro-Symbolic Natural Language Navigational Planner",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NSP: A Neuro-Symbolic Natural Language Navigational Planner"
                },
                "summary": "Path planners that can interpret free-form natural language instructions hold\npromise to automate a wide range of robotics applications. These planners\nsimplify user interactions and enable intuitive control over complex\nsemi-autonomous systems. While existing symbolic approaches offer guarantees on\nthe correctness and efficiency, they struggle to parse free-form natural\nlanguage inputs. Conversely, neural approaches based on pre-trained Large\nLanguage Models (LLMs) can manage natural language inputs but lack performance\nguarantees. In this paper, we propose a neuro-symbolic framework for path\nplanning from natural language inputs called NSP. The framework leverages the\nneural reasoning abilities of LLMs to i) craft symbolic representations of the\nenvironment and ii) a symbolic path planning algorithm. Next, a solution to the\npath planning problem is obtained by executing the algorithm on the environment\nrepresentation. The framework uses a feedback loop from the symbolic execution\nenvironment to the neural generation process to self-correct syntax errors and\nsatisfy execution time constraints. We evaluate our neuro-symbolic approach\nusing a benchmark suite with 1500 path-planning problems. The experimental\nevaluation shows that our neuro-symbolic approach produces 90.1% valid paths\nthat are on average 19-77% shorter than state-of-the-art neural approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Path planners that can interpret free-form natural language instructions hold\npromise to automate a wide range of robotics applications. These planners\nsimplify user interactions and enable intuitive control over complex\nsemi-autonomous systems. While existing symbolic approaches offer guarantees on\nthe correctness and efficiency, they struggle to parse free-form natural\nlanguage inputs. Conversely, neural approaches based on pre-trained Large\nLanguage Models (LLMs) can manage natural language inputs but lack performance\nguarantees. In this paper, we propose a neuro-symbolic framework for path\nplanning from natural language inputs called NSP. The framework leverages the\nneural reasoning abilities of LLMs to i) craft symbolic representations of the\nenvironment and ii) a symbolic path planning algorithm. Next, a solution to the\npath planning problem is obtained by executing the algorithm on the environment\nrepresentation. The framework uses a feedback loop from the symbolic execution\nenvironment to the neural generation process to self-correct syntax errors and\nsatisfy execution time constraints. We evaluate our neuro-symbolic approach\nusing a benchmark suite with 1500 path-planning problems. The experimental\nevaluation shows that our neuro-symbolic approach produces 90.1% valid paths\nthat are on average 19-77% shorter than state-of-the-art neural approaches."
                },
                "authors": [
                    {
                        "name": "William English"
                    },
                    {
                        "name": "Dominic Simon"
                    },
                    {
                        "name": "Rickard Ewetz"
                    },
                    {
                        "name": "Sumit Jha"
                    }
                ],
                "author_detail": {
                    "name": "Sumit Jha"
                },
                "author": "Sumit Jha",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06859v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06859v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06857v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06857v1",
                "updated": "2024-09-10T20:45:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    20,
                    45,
                    43,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T20:45:43Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    20,
                    45,
                    43,
                    1,
                    254,
                    0
                ],
                "title": "What is the Role of Small Models in the LLM Era: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What is the Role of Small Models in the LLM Era: A Survey"
                },
                "summary": "Large Language Models (LLMs) have made significant progress in advancing\nartificial general intelligence (AGI), leading to the development of\nincreasingly large models such as GPT-4 and LLaMA-405B. However, scaling up\nmodel sizes results in exponentially higher computational costs and energy\nconsumption, making these models impractical for academic researchers and\nbusinesses with limited resources. At the same time, Small Models (SMs) are\nfrequently used in practical settings, although their significance is currently\nunderestimated. This raises important questions about the role of small models\nin the era of LLMs, a topic that has received limited attention in prior\nresearch. In this work, we systematically examine the relationship between LLMs\nand SMs from two key perspectives: Collaboration and Competition. We hope this\nsurvey provides valuable insights for practitioners, fostering a deeper\nunderstanding of the contribution of small models and promoting more efficient\nuse of computational resources. The code is available at\nhttps://github.com/tigerchen52/role_of_small_models",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have made significant progress in advancing\nartificial general intelligence (AGI), leading to the development of\nincreasingly large models such as GPT-4 and LLaMA-405B. However, scaling up\nmodel sizes results in exponentially higher computational costs and energy\nconsumption, making these models impractical for academic researchers and\nbusinesses with limited resources. At the same time, Small Models (SMs) are\nfrequently used in practical settings, although their significance is currently\nunderestimated. This raises important questions about the role of small models\nin the era of LLMs, a topic that has received limited attention in prior\nresearch. In this work, we systematically examine the relationship between LLMs\nand SMs from two key perspectives: Collaboration and Competition. We hope this\nsurvey provides valuable insights for practitioners, fostering a deeper\nunderstanding of the contribution of small models and promoting more efficient\nuse of computational resources. The code is available at\nhttps://github.com/tigerchen52/role_of_small_models"
                },
                "authors": [
                    {
                        "name": "Lihu Chen"
                    },
                    {
                        "name": "Gaël Varoquaux"
                    }
                ],
                "author_detail": {
                    "name": "Gaël Varoquaux"
                },
                "author": "Gaël Varoquaux",
                "arxiv_comment": "a survey paper of small models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06857v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.07544v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.07544v3",
                "updated": "2024-09-10T20:35:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    20,
                    35,
                    25,
                    1,
                    254,
                    0
                ],
                "published": "2024-04-11T08:12:43Z",
                "published_parsed": [
                    2024,
                    4,
                    11,
                    8,
                    12,
                    43,
                    3,
                    102,
                    0
                ],
                "title": "From Words to Numbers: Your Large Language Model Is Secretly A Capable\n  Regressor When Given In-Context Examples",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Words to Numbers: Your Large Language Model Is Secretly A Capable\n  Regressor When Given In-Context Examples"
                },
                "summary": "We analyze how well pre-trained large language models (e.g., Llama2, GPT-4,\nClaude 3, etc) can do linear and non-linear regression when given in-context\nexamples, without any additional training or gradient updates. Our findings\nreveal that several large language models (e.g., GPT-4, Claude 3) are able to\nperform regression tasks with a performance rivaling (or even outperforming)\nthat of traditional supervised methods such as Random Forest, Bagging, or\nGradient Boosting. For example, on the challenging Friedman #2 regression\ndataset, Claude 3 outperforms many supervised methods such as AdaBoost, SVM,\nRandom Forest, KNN, or Gradient Boosting. We then investigate how well the\nperformance of large language models scales with the number of in-context\nexemplars. We borrow from the notion of regret from online learning and\nempirically show that LLMs are capable of obtaining a sub-linear regret.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We analyze how well pre-trained large language models (e.g., Llama2, GPT-4,\nClaude 3, etc) can do linear and non-linear regression when given in-context\nexamples, without any additional training or gradient updates. Our findings\nreveal that several large language models (e.g., GPT-4, Claude 3) are able to\nperform regression tasks with a performance rivaling (or even outperforming)\nthat of traditional supervised methods such as Random Forest, Bagging, or\nGradient Boosting. For example, on the challenging Friedman #2 regression\ndataset, Claude 3 outperforms many supervised methods such as AdaBoost, SVM,\nRandom Forest, KNN, or Gradient Boosting. We then investigate how well the\nperformance of large language models scales with the number of in-context\nexemplars. We borrow from the notion of regret from online learning and\nempirically show that LLMs are capable of obtaining a sub-linear regret."
                },
                "authors": [
                    {
                        "name": "Robert Vacareanu"
                    },
                    {
                        "name": "Vlad-Andrei Negru"
                    },
                    {
                        "name": "Vasile Suciu"
                    },
                    {
                        "name": "Mihai Surdeanu"
                    }
                ],
                "author_detail": {
                    "name": "Mihai Surdeanu"
                },
                "author": "Mihai Surdeanu",
                "arxiv_comment": "55 pages, 48 figures COLM camera-ready version; Changes include: (i)\n  added real-world datasets (Appendix I), (ii) fixed typos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.07544v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.07544v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06851v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06851v1",
                "updated": "2024-09-10T20:19:14Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    20,
                    19,
                    14,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T20:19:14Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    20,
                    19,
                    14,
                    1,
                    254,
                    0
                ],
                "title": "LIME-M: Less Is More for Evaluation of MLLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LIME-M: Less Is More for Evaluation of MLLMs"
                },
                "summary": "With the remarkable success achieved by Multimodal Large Language Models\n(MLLMs), numerous benchmarks have been designed to assess MLLMs' ability to\nguide their development in image perception tasks (e.g., image captioning and\nvisual question answering). However, the existence of numerous benchmarks\nresults in a substantial computational burden when evaluating model performance\nacross all of them. Moreover, these benchmarks contain many overly simple\nproblems or challenging samples, which do not effectively differentiate the\ncapabilities among various MLLMs. To address these challenges, we propose a\npipeline to process the existing benchmarks, which consists of two modules: (1)\nSemi-Automated Screening Process and (2) Eliminating Answer Leakage. The\nSemi-Automated Screening Process filters out samples that cannot distinguish\nthe model's capabilities by synthesizing various MLLMs and manually evaluating\nthem. The Eliminate Answer Leakage module filters samples whose answers can be\ninferred without images. Finally, we curate the LIME-M: Less Is More for\nEvaluation of Multimodal LLMs, a lightweight Multimodal benchmark that can more\neffectively evaluate the performance of different models. Our experiments\ndemonstrate that: LIME-M can better distinguish the performance of different\nMLLMs with fewer samples (24% of the original) and reduced time (23% of the\noriginal); LIME-M eliminates answer leakage, focusing mainly on the information\nwithin images; The current automatic metric (i.e., CIDEr) is insufficient for\nevaluating MLLMs' capabilities in captioning. Moreover, removing the caption\ntask score when calculating the overall score provides a more accurate\nreflection of model performance differences. All our codes and data are\nreleased at https://github.com/kangreen0210/LIME-M.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the remarkable success achieved by Multimodal Large Language Models\n(MLLMs), numerous benchmarks have been designed to assess MLLMs' ability to\nguide their development in image perception tasks (e.g., image captioning and\nvisual question answering). However, the existence of numerous benchmarks\nresults in a substantial computational burden when evaluating model performance\nacross all of them. Moreover, these benchmarks contain many overly simple\nproblems or challenging samples, which do not effectively differentiate the\ncapabilities among various MLLMs. To address these challenges, we propose a\npipeline to process the existing benchmarks, which consists of two modules: (1)\nSemi-Automated Screening Process and (2) Eliminating Answer Leakage. The\nSemi-Automated Screening Process filters out samples that cannot distinguish\nthe model's capabilities by synthesizing various MLLMs and manually evaluating\nthem. The Eliminate Answer Leakage module filters samples whose answers can be\ninferred without images. Finally, we curate the LIME-M: Less Is More for\nEvaluation of Multimodal LLMs, a lightweight Multimodal benchmark that can more\neffectively evaluate the performance of different models. Our experiments\ndemonstrate that: LIME-M can better distinguish the performance of different\nMLLMs with fewer samples (24% of the original) and reduced time (23% of the\noriginal); LIME-M eliminates answer leakage, focusing mainly on the information\nwithin images; The current automatic metric (i.e., CIDEr) is insufficient for\nevaluating MLLMs' capabilities in captioning. Moreover, removing the caption\ntask score when calculating the overall score provides a more accurate\nreflection of model performance differences. All our codes and data are\nreleased at https://github.com/kangreen0210/LIME-M."
                },
                "authors": [
                    {
                        "name": "Kang Zhu"
                    },
                    {
                        "name": "Qianbo Zang"
                    },
                    {
                        "name": "Shian Jia"
                    },
                    {
                        "name": "Siwei Wu"
                    },
                    {
                        "name": "Feiteng Fang"
                    },
                    {
                        "name": "Yizhi Li"
                    },
                    {
                        "name": "Shuyue Guo"
                    },
                    {
                        "name": "Tianyu Zheng"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Haoning Wu"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Zachary Liu"
                    },
                    {
                        "name": "Xiang Yue"
                    },
                    {
                        "name": "J. H. Liu"
                    },
                    {
                        "name": "Chenghua Lin"
                    },
                    {
                        "name": "Min Yang"
                    },
                    {
                        "name": "Shiwen Ni"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Ge Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ge Zhang"
                },
                "author": "Ge Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06851v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06851v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06828v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06828v1",
                "updated": "2024-09-10T19:12:06Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    19,
                    12,
                    6,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T19:12:06Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    19,
                    12,
                    6,
                    1,
                    254,
                    0
                ],
                "title": "A Hardened CO$_2$ Sensor for In-Ground Continuous Measurement in a\n  Perennial Grass System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Hardened CO$_2$ Sensor for In-Ground Continuous Measurement in a\n  Perennial Grass System"
                },
                "summary": "Carbon dioxide levels below the soil surface are an important measurement\nrelating to plant health, especially for plants such as perennial grasses in\nnorthern climates where ice encasement can occur over winter. In such cases,\nthe CO$_2$ levels can build up and become toxic. This is likely a significant\ncontributor to turfgrass death over winter; however, there is an insufficient\namount of data regarding this phenomenon in large part due to the lack of\neffective sensors. Many off the shelf CO$_2$ sensors exist, but they are not\nsufficiently hardened for in ground deployment over winter. As a result, the\nonly options currently available are very costly automated gas samplers or\nmanual sampling at intervals with laboratory testing -- a process that results\nin a limited number of data points and is labor intensive. To combat this\nproblem we have taken an established NDIR CO$_2$ sensor and hardened it for use\nin winter and ice encased environments to allow for continuous automated\nsampling of subsurface CO$_2$ levels to better understand ice encasement damage\nin perennial grass systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Carbon dioxide levels below the soil surface are an important measurement\nrelating to plant health, especially for plants such as perennial grasses in\nnorthern climates where ice encasement can occur over winter. In such cases,\nthe CO$_2$ levels can build up and become toxic. This is likely a significant\ncontributor to turfgrass death over winter; however, there is an insufficient\namount of data regarding this phenomenon in large part due to the lack of\neffective sensors. Many off the shelf CO$_2$ sensors exist, but they are not\nsufficiently hardened for in ground deployment over winter. As a result, the\nonly options currently available are very costly automated gas samplers or\nmanual sampling at intervals with laboratory testing -- a process that results\nin a limited number of data points and is labor intensive. To combat this\nproblem we have taken an established NDIR CO$_2$ sensor and hardened it for use\nin winter and ice encased environments to allow for continuous automated\nsampling of subsurface CO$_2$ levels to better understand ice encasement damage\nin perennial grass systems."
                },
                "authors": [
                    {
                        "name": "Bobby Schulz"
                    },
                    {
                        "name": "Bryan Runck"
                    },
                    {
                        "name": "Andrew Hollman"
                    },
                    {
                        "name": "Ann Piotrowski"
                    },
                    {
                        "name": "Eric Watkins"
                    }
                ],
                "author_detail": {
                    "name": "Eric Watkins"
                },
                "author": "Eric Watkins",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06828v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06828v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06816v1",
                "updated": "2024-09-10T18:52:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    18,
                    52,
                    40,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T18:52:40Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    18,
                    52,
                    40,
                    1,
                    254,
                    0
                ],
                "title": "LLM-Enhanced Software Patch Localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Enhanced Software Patch Localization"
                },
                "summary": "Open source software (OSS) is integral to modern product development, and any\nvulnerability within it potentially compromises numerous products. While\ndevelopers strive to apply security patches, pinpointing these patches among\nextensive OSS updates remains a challenge. Security patch localization (SPL)\nrecommendation methods are leading approaches to address this. However,\nexisting SPL models often falter when a commit lacks a clear association with\nits corresponding CVE, and do not consider a scenario that a vulnerability has\nmultiple patches proposed over time before it has been fully resolved. To\naddress these challenges, we introduce LLM-SPL, a recommendation-based SPL\napproach that leverages the capabilities of the Large Language Model (LLM) to\nlocate the security patch commit for a given CVE. More specifically, we propose\na joint learning framework, in which the outputs of LLM serves as additional\nfeatures to aid our recommendation model in prioritizing security patches. Our\nevaluation on a dataset of 1,915 CVEs associated with 2,461 patches\ndemonstrates that LLM-SPL excels in ranking patch commits, surpassing the\nstate-of-the-art method in terms of Recall, while significantly reducing manual\neffort. Notably, for vulnerabilities requiring multiple patches, LLM-SPL\nsignificantly improves Recall by 22.83\\%, NDCG by 19.41\\%, and reduces manual\neffort by over 25\\% when checking up to the top 10 rankings. The dataset and\nsource code are available at\n\\url{https://anonymous.4open.science/r/LLM-SPL-91F8}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open source software (OSS) is integral to modern product development, and any\nvulnerability within it potentially compromises numerous products. While\ndevelopers strive to apply security patches, pinpointing these patches among\nextensive OSS updates remains a challenge. Security patch localization (SPL)\nrecommendation methods are leading approaches to address this. However,\nexisting SPL models often falter when a commit lacks a clear association with\nits corresponding CVE, and do not consider a scenario that a vulnerability has\nmultiple patches proposed over time before it has been fully resolved. To\naddress these challenges, we introduce LLM-SPL, a recommendation-based SPL\napproach that leverages the capabilities of the Large Language Model (LLM) to\nlocate the security patch commit for a given CVE. More specifically, we propose\na joint learning framework, in which the outputs of LLM serves as additional\nfeatures to aid our recommendation model in prioritizing security patches. Our\nevaluation on a dataset of 1,915 CVEs associated with 2,461 patches\ndemonstrates that LLM-SPL excels in ranking patch commits, surpassing the\nstate-of-the-art method in terms of Recall, while significantly reducing manual\neffort. Notably, for vulnerabilities requiring multiple patches, LLM-SPL\nsignificantly improves Recall by 22.83\\%, NDCG by 19.41\\%, and reduces manual\neffort by over 25\\% when checking up to the top 10 rankings. The dataset and\nsource code are available at\n\\url{https://anonymous.4open.science/r/LLM-SPL-91F8}."
                },
                "authors": [
                    {
                        "name": "Jinhong Yu"
                    },
                    {
                        "name": "Yi Chen"
                    },
                    {
                        "name": "Di Tang"
                    },
                    {
                        "name": "Xiaozhong Liu"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Chen Wu"
                    },
                    {
                        "name": "Haixu Tang"
                    }
                ],
                "author_detail": {
                    "name": "Haixu Tang"
                },
                "author": "Haixu Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06814v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06814v1",
                "updated": "2024-09-10T18:43:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    18,
                    43,
                    39,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T18:43:39Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    18,
                    43,
                    39,
                    1,
                    254,
                    0
                ],
                "title": "\"Come to us first\": Centering Community Organizations in Artificial\n  Intelligence for Social Good Partnerships",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Come to us first\": Centering Community Organizations in Artificial\n  Intelligence for Social Good Partnerships"
                },
                "summary": "Artificial Intelligence for Social Good (AI4SG) has emerged as a growing body\nof research and practice exploring the potential of AI technologies to tackle\nsocial issues. This area emphasizes interdisciplinary partnerships with\ncommunity organizations, such as non-profits and government agencies. However,\namidst excitement about new advances in AI and their potential impact, the\nneeds, expectations, and aspirations of these community organizations--and\nwhether they are being met--are not well understood. Understanding these\nfactors is important to ensure that the considerable efforts by AI teams and\ncommunity organizations can actually achieve the positive social impact they\nstrive for. Drawing on the Data Feminism framework, we explored the\nperspectives of community organization members on their partnerships with AI\nteams through 16 semi-structured interviews. Our study highlights the pervasive\ninfluence of funding agendas and the optimism surrounding AI's potential.\nDespite the significant intellectual contributions and labor provided by\ncommunity organization members, their goals were frequently sidelined in favor\nof other stakeholders, including AI teams. While many community organization\nmembers expected tangible project deployment, only two out of 14 projects we\nstudied reached the deployment stage. However, community organization members\nsustained their belief in the potential of the projects, still seeing\ndiminished goals as valuable. To enhance the efficacy of future collaborations,\nour participants shared their aspirations for success, calling for\nco-leadership starting from the early stages of projects. We propose data\nco-liberation as a grounding principle for approaching AI4SG moving forward,\npositing that community organizations' co-leadership is essential for fostering\nmore effective, sustainable, and ethical development of AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Intelligence for Social Good (AI4SG) has emerged as a growing body\nof research and practice exploring the potential of AI technologies to tackle\nsocial issues. This area emphasizes interdisciplinary partnerships with\ncommunity organizations, such as non-profits and government agencies. However,\namidst excitement about new advances in AI and their potential impact, the\nneeds, expectations, and aspirations of these community organizations--and\nwhether they are being met--are not well understood. Understanding these\nfactors is important to ensure that the considerable efforts by AI teams and\ncommunity organizations can actually achieve the positive social impact they\nstrive for. Drawing on the Data Feminism framework, we explored the\nperspectives of community organization members on their partnerships with AI\nteams through 16 semi-structured interviews. Our study highlights the pervasive\ninfluence of funding agendas and the optimism surrounding AI's potential.\nDespite the significant intellectual contributions and labor provided by\ncommunity organization members, their goals were frequently sidelined in favor\nof other stakeholders, including AI teams. While many community organization\nmembers expected tangible project deployment, only two out of 14 projects we\nstudied reached the deployment stage. However, community organization members\nsustained their belief in the potential of the projects, still seeing\ndiminished goals as valuable. To enhance the efficacy of future collaborations,\nour participants shared their aspirations for success, calling for\nco-leadership starting from the early stages of projects. We propose data\nco-liberation as a grounding principle for approaching AI4SG moving forward,\npositing that community organizations' co-leadership is essential for fostering\nmore effective, sustainable, and ethical development of AI."
                },
                "authors": [
                    {
                        "name": "Hongjin Lin"
                    },
                    {
                        "name": "Naveena Karusala"
                    },
                    {
                        "name": "Chinasa T. Okolo"
                    },
                    {
                        "name": "Catherine D'Ignazio"
                    },
                    {
                        "name": "Krzysztof Z. Gajos"
                    }
                ],
                "author_detail": {
                    "name": "Krzysztof Z. Gajos"
                },
                "author": "Krzysztof Z. Gajos",
                "arxiv_doi": "10.1145/3687009",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3687009",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.06814v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06814v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to the Proc. ACM Hum. Comput. Interact. 8, CSCW2",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13918v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13918v3",
                "updated": "2024-09-10T18:34:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    18,
                    34,
                    23,
                    1,
                    254,
                    0
                ],
                "published": "2024-08-25T19:03:46Z",
                "published_parsed": [
                    2024,
                    8,
                    25,
                    19,
                    3,
                    46,
                    6,
                    238,
                    0
                ],
                "title": "Geo-Llama: Leveraging LLMs for Human Mobility Trajectory Generation with\n  Spatiotemporal Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geo-Llama: Leveraging LLMs for Human Mobility Trajectory Generation with\n  Spatiotemporal Constraints"
                },
                "summary": "Simulating human mobility data is essential for various application domains,\nincluding transportation, urban planning, and epidemic control, since real data\nare often inaccessible to researchers due to expensive costs and privacy\nissues. Several existing deep generative solutions propose learning from real\ntrajectories to generate synthetic ones. Despite the progress, most of them\nsuffer from training stability issues and scale poorly with growing data size.\nMore importantly, they generally lack control mechanisms to steer the generated\ntrajectories based on spatiotemporal constraints such as fixing specific\nvisits. To address such limitations, we formally define the controlled\ntrajectory generation problem with spatiotemporal constraints and propose\nGeo-Llama. This novel LLM-inspired framework enforces explicit visit\nconstraints in a contextually coherent way. It fine-tunes pre-trained LLMs on\ntrajectories with a visit-wise permutation strategy where each visit\ncorresponds to a time and location. This enables the model to capture the\nspatiotemporal patterns regardless of visit orders and allows flexible and\nin-context constraint integration through prompts during generation. Extensive\nexperiments on real-world and synthetic datasets validate the effectiveness of\nGeo-Llama, demonstrating its versatility and robustness in handling a broad\nrange of constraints to generate more realistic trajectories compared to\nexisting methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating human mobility data is essential for various application domains,\nincluding transportation, urban planning, and epidemic control, since real data\nare often inaccessible to researchers due to expensive costs and privacy\nissues. Several existing deep generative solutions propose learning from real\ntrajectories to generate synthetic ones. Despite the progress, most of them\nsuffer from training stability issues and scale poorly with growing data size.\nMore importantly, they generally lack control mechanisms to steer the generated\ntrajectories based on spatiotemporal constraints such as fixing specific\nvisits. To address such limitations, we formally define the controlled\ntrajectory generation problem with spatiotemporal constraints and propose\nGeo-Llama. This novel LLM-inspired framework enforces explicit visit\nconstraints in a contextually coherent way. It fine-tunes pre-trained LLMs on\ntrajectories with a visit-wise permutation strategy where each visit\ncorresponds to a time and location. This enables the model to capture the\nspatiotemporal patterns regardless of visit orders and allows flexible and\nin-context constraint integration through prompts during generation. Extensive\nexperiments on real-world and synthetic datasets validate the effectiveness of\nGeo-Llama, demonstrating its versatility and robustness in handling a broad\nrange of constraints to generate more realistic trajectories compared to\nexisting methods."
                },
                "authors": [
                    {
                        "name": "Siyu Li"
                    },
                    {
                        "name": "Toan Tran"
                    },
                    {
                        "name": "Haowen Lin"
                    },
                    {
                        "name": "John Krumm"
                    },
                    {
                        "name": "Cyrus Shahabi"
                    },
                    {
                        "name": "Li Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Li Xiong"
                },
                "author": "Li Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13918v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13918v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.11616v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.11616v3",
                "updated": "2024-09-10T18:17:06Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    18,
                    17,
                    6,
                    1,
                    254,
                    0
                ],
                "published": "2023-10-17T22:42:12Z",
                "published_parsed": [
                    2023,
                    10,
                    17,
                    22,
                    42,
                    12,
                    1,
                    290,
                    0
                ],
                "title": "Evidence of interrelated cognitive-like capabilities in large language\n  models: Indications of artificial general intelligence or achievement?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evidence of interrelated cognitive-like capabilities in large language\n  models: Indications of artificial general intelligence or achievement?"
                },
                "summary": "Large language models (LLMs) are advanced artificial intelligence (AI)\nsystems that can perform a variety of tasks commonly found in human\nintelligence tests, such as defining words, performing calculations, and\nengaging in verbal reasoning. There are also substantial individual differences\nin LLM capacities. Given the consistent observation of a positive manifold and\ngeneral intelligence factor in human samples, along with group-level factors\n(e.g., crystallized intelligence), we hypothesized that LLM test scores may\nalso exhibit positive intercorrelations, which could potentially give rise to\nan artificial general ability (AGA) factor and one or more group-level factors.\nBased on a sample of 591 LLMs and scores from 12 tests aligned with fluid\nreasoning (Gf), domain-specific knowledge (Gkn), reading/writing (Grw), and\nquantitative knowledge (Gq), we found strong empirical evidence for a positive\nmanifold and a general factor of ability. Additionally, we identified a\ncombined Gkn/Grw group-level factor. Finally, the number of LLM parameters\ncorrelated positively with both general factor of ability and Gkn/Grw factor\nscores, although the effects showed diminishing returns. We interpreted our\nresults to suggest that LLMs, like human cognitive abilities, may share a\ncommon underlying efficiency in processing information and solving problems,\nthough whether LLMs manifest primarily achievement/expertise rather than\nintelligence remains to be determined. Finally, while models with greater\nnumbers of parameters exhibit greater general cognitive-like abilities, akin to\nthe connection between greater neuronal density and human general intelligence,\nother characteristics must also be involved.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are advanced artificial intelligence (AI)\nsystems that can perform a variety of tasks commonly found in human\nintelligence tests, such as defining words, performing calculations, and\nengaging in verbal reasoning. There are also substantial individual differences\nin LLM capacities. Given the consistent observation of a positive manifold and\ngeneral intelligence factor in human samples, along with group-level factors\n(e.g., crystallized intelligence), we hypothesized that LLM test scores may\nalso exhibit positive intercorrelations, which could potentially give rise to\nan artificial general ability (AGA) factor and one or more group-level factors.\nBased on a sample of 591 LLMs and scores from 12 tests aligned with fluid\nreasoning (Gf), domain-specific knowledge (Gkn), reading/writing (Grw), and\nquantitative knowledge (Gq), we found strong empirical evidence for a positive\nmanifold and a general factor of ability. Additionally, we identified a\ncombined Gkn/Grw group-level factor. Finally, the number of LLM parameters\ncorrelated positively with both general factor of ability and Gkn/Grw factor\nscores, although the effects showed diminishing returns. We interpreted our\nresults to suggest that LLMs, like human cognitive abilities, may share a\ncommon underlying efficiency in processing information and solving problems,\nthough whether LLMs manifest primarily achievement/expertise rather than\nintelligence remains to be determined. Finally, while models with greater\nnumbers of parameters exhibit greater general cognitive-like abilities, akin to\nthe connection between greater neuronal density and human general intelligence,\nother characteristics must also be involved."
                },
                "authors": [
                    {
                        "name": "David Ilić"
                    },
                    {
                        "name": "Gilles E. Gignac"
                    }
                ],
                "author_detail": {
                    "name": "Gilles E. Gignac"
                },
                "author": "Gilles E. Gignac",
                "arxiv_doi": "10.1016/j.intell.2024.101858",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.intell.2024.101858",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.11616v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.11616v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "9 pages, 2 figures",
                "arxiv_journal_ref": "Intelligence, Volume 106, September/October 2024, 101858",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06805v1",
                "updated": "2024-09-10T18:16:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    18,
                    16,
                    28,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T18:16:28Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    18,
                    16,
                    28,
                    1,
                    254,
                    0
                ],
                "title": "Personalized Federated Learning Techniques: Empirical Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized Federated Learning Techniques: Empirical Analysis"
                },
                "summary": "Personalized Federated Learning (pFL) holds immense promise for tailoring\nmachine learning models to individual users while preserving data privacy.\nHowever, achieving optimal performance in pFL often requires a careful\nbalancing act between memory overhead costs and model accuracy. This paper\ndelves into the trade-offs inherent in pFL, offering valuable insights for\nselecting the right algorithms for diverse real-world scenarios. We empirically\nevaluate ten prominent pFL techniques across various datasets and data splits,\nuncovering significant differences in their performance. Our study reveals\ninteresting insights into how pFL methods that utilize personalized (local)\naggregation exhibit the fastest convergence due to their efficiency in\ncommunication and computation. Conversely, fine-tuning methods face limitations\nin handling data heterogeneity and potential adversarial attacks while\nmulti-objective learning methods achieve higher accuracy at the cost of\nadditional training and resource consumption. Our study emphasizes the critical\nrole of communication efficiency in scaling pFL, demonstrating how it can\nsignificantly affect resource usage in real-world deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized Federated Learning (pFL) holds immense promise for tailoring\nmachine learning models to individual users while preserving data privacy.\nHowever, achieving optimal performance in pFL often requires a careful\nbalancing act between memory overhead costs and model accuracy. This paper\ndelves into the trade-offs inherent in pFL, offering valuable insights for\nselecting the right algorithms for diverse real-world scenarios. We empirically\nevaluate ten prominent pFL techniques across various datasets and data splits,\nuncovering significant differences in their performance. Our study reveals\ninteresting insights into how pFL methods that utilize personalized (local)\naggregation exhibit the fastest convergence due to their efficiency in\ncommunication and computation. Conversely, fine-tuning methods face limitations\nin handling data heterogeneity and potential adversarial attacks while\nmulti-objective learning methods achieve higher accuracy at the cost of\nadditional training and resource consumption. Our study emphasizes the critical\nrole of communication efficiency in scaling pFL, demonstrating how it can\nsignificantly affect resource usage in real-world deployments."
                },
                "authors": [
                    {
                        "name": "Azal Ahmad Khan"
                    },
                    {
                        "name": "Ahmad Faraz Khan"
                    },
                    {
                        "name": "Haider Ali"
                    },
                    {
                        "name": "Ali Anwar"
                    }
                ],
                "author_detail": {
                    "name": "Ali Anwar"
                },
                "author": "Ali Anwar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06801v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06801v1",
                "updated": "2024-09-10T18:11:54Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    18,
                    11,
                    54,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T18:11:54Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    18,
                    11,
                    54,
                    1,
                    254,
                    0
                ],
                "title": "Understanding and Mitigating the Impacts of Differentially Private\n  Census Data on State Level Redistricting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and Mitigating the Impacts of Differentially Private\n  Census Data on State Level Redistricting"
                },
                "summary": "Data from the Decennial Census is published only after applying a disclosure\navoidance system (DAS). Data users were shaken by the adoption of differential\nprivacy in the 2020 DAS, a radical departure from past methods. The change\nraises the question of whether redistricting law permits, forbids, or requires\ntaking account of the effect of disclosure avoidance. Such uncertainty creates\nlegal risks for redistricters, as Alabama argued in a lawsuit seeking to\nprevent the 2020 DAS's deployment. We consider two redistricting settings in\nwhich a data user might be concerned about the impacts of privacy preserving\nnoise: drawing equal population districts and litigating voting rights cases.\nWhat discrepancies arise if the user does nothing to account for disclosure\navoidance? How might the user adapt her analyses to mitigate those\ndiscrepancies? We study these questions by comparing the official 2010\nRedistricting Data to the 2010 Demonstration Data -- created using the 2020 DAS\n-- in an analysis of millions of algorithmically generated state legislative\nredistricting plans. In both settings, we observe that an analyst may come to\nincorrect conclusions if they do not account for noise. With minor adaptations,\nthough, the underlying policy goals remain achievable: tweaking selection\ncriteria enables a redistricter to draw balanced plans, and illustrative plans\ncan still be used as evidence of the maximum number of majority-minority\ndistricts that are possible in a geography. At least for state legislatures,\nAlabama's claim that differential privacy ``inhibits a State's right to draw\nfair lines'' appears unfounded.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data from the Decennial Census is published only after applying a disclosure\navoidance system (DAS). Data users were shaken by the adoption of differential\nprivacy in the 2020 DAS, a radical departure from past methods. The change\nraises the question of whether redistricting law permits, forbids, or requires\ntaking account of the effect of disclosure avoidance. Such uncertainty creates\nlegal risks for redistricters, as Alabama argued in a lawsuit seeking to\nprevent the 2020 DAS's deployment. We consider two redistricting settings in\nwhich a data user might be concerned about the impacts of privacy preserving\nnoise: drawing equal population districts and litigating voting rights cases.\nWhat discrepancies arise if the user does nothing to account for disclosure\navoidance? How might the user adapt her analyses to mitigate those\ndiscrepancies? We study these questions by comparing the official 2010\nRedistricting Data to the 2010 Demonstration Data -- created using the 2020 DAS\n-- in an analysis of millions of algorithmically generated state legislative\nredistricting plans. In both settings, we observe that an analyst may come to\nincorrect conclusions if they do not account for noise. With minor adaptations,\nthough, the underlying policy goals remain achievable: tweaking selection\ncriteria enables a redistricter to draw balanced plans, and illustrative plans\ncan still be used as evidence of the maximum number of majority-minority\ndistricts that are possible in a geography. At least for state legislatures,\nAlabama's claim that differential privacy ``inhibits a State's right to draw\nfair lines'' appears unfounded."
                },
                "authors": [
                    {
                        "name": "Christian Cianfarani"
                    },
                    {
                        "name": "Aloni Cohen"
                    }
                ],
                "author_detail": {
                    "name": "Aloni Cohen"
                },
                "author": "Aloni Cohen",
                "arxiv_comment": "24 pages, 5 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06801v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06801v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06691v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06691v1",
                "updated": "2024-09-10T17:54:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    17,
                    54,
                    28,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T17:54:28Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    17,
                    54,
                    28,
                    1,
                    254,
                    0
                ],
                "title": "Geometric-Averaged Preference Optimization for Soft Preference Labels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geometric-Averaged Preference Optimization for Soft Preference Labels"
                },
                "summary": "Many algorithms for aligning LLMs with human preferences assume that human\npreferences are binary and deterministic. However, it is reasonable to think\nthat they can vary with different individuals, and thus should be\ndistributional to reflect the fine-grained relationship between the responses.\nIn this work, we introduce the distributional soft preference labels and\nimprove Direct Preference Optimization (DPO) with a weighted geometric average\nof the LLM output likelihood in the loss function. In doing so, the scale of\nlearning loss is adjusted based on the soft labels, and the loss with equally\npreferred responses would be close to zero. This simple modification can be\neasily applied to any DPO family and helps the models escape from the\nover-optimization and objective mismatch prior works suffer from. In our\nexperiments, we simulate the soft preference labels with AI feedback from LLMs\nand demonstrate that geometric averaging consistently improves performance on\nstandard benchmarks for alignment research. In particular, we observe more\npreferable responses than binary labels and significant improvements with data\nwhere modestly-confident labels are in the majority.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many algorithms for aligning LLMs with human preferences assume that human\npreferences are binary and deterministic. However, it is reasonable to think\nthat they can vary with different individuals, and thus should be\ndistributional to reflect the fine-grained relationship between the responses.\nIn this work, we introduce the distributional soft preference labels and\nimprove Direct Preference Optimization (DPO) with a weighted geometric average\nof the LLM output likelihood in the loss function. In doing so, the scale of\nlearning loss is adjusted based on the soft labels, and the loss with equally\npreferred responses would be close to zero. This simple modification can be\neasily applied to any DPO family and helps the models escape from the\nover-optimization and objective mismatch prior works suffer from. In our\nexperiments, we simulate the soft preference labels with AI feedback from LLMs\nand demonstrate that geometric averaging consistently improves performance on\nstandard benchmarks for alignment research. In particular, we observe more\npreferable responses than binary labels and significant improvements with data\nwhere modestly-confident labels are in the majority."
                },
                "authors": [
                    {
                        "name": "Hiroki Furuta"
                    },
                    {
                        "name": "Kuang-Huei Lee"
                    },
                    {
                        "name": "Shixiang Shane Gu"
                    },
                    {
                        "name": "Yutaka Matsuo"
                    },
                    {
                        "name": "Aleksandra Faust"
                    },
                    {
                        "name": "Heiga Zen"
                    },
                    {
                        "name": "Izzeddin Gur"
                    }
                ],
                "author_detail": {
                    "name": "Izzeddin Gur"
                },
                "author": "Izzeddin Gur",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06691v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06691v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06679v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06679v1",
                "updated": "2024-09-10T17:44:35Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    17,
                    44,
                    35,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T17:44:35Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    17,
                    44,
                    35,
                    1,
                    254,
                    0
                ],
                "title": "E2LLM: Encoder Elongated Large Language Models for Long-Context\n  Understanding and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "E2LLM: Encoder Elongated Large Language Models for Long-Context\n  Understanding and Reasoning"
                },
                "summary": "In the realm of Large Language Models (LLMs), the ability to process long\ncontexts is increasingly crucial for tasks such as multi-round dialogues, code\ngeneration, and document summarization. This paper addresses the challenges of\nenhancing the long-context performance, reducing computational complexity, and\nleveraging pretrained models collectively termed the \"impossible triangle.\" We\nintroduce E2LLM (Encoder Elongated Large Language Models), a novel approach\nthat effectively navigates this paradox. The method involves splitting long\ncontexts into chunks, compressing each into embedding vectors via a pretrained\ntext encoder, and utilizing an adapter to align these representations with a\ndecoder-only LLM. Two training objectives, focusing on reconstruction of the\nencoder output and long-context instruction fine-tuning, are employed to\nfacilitate the understanding of soft prompts by the LLM. Experimental results\ndemonstrate that E2LLM achieves superior performance in long-context scenarios\nwhile balancing efficiency, performance, and compatibility with pretrained\nmodels. Our framework thus represents a significant advancement in the field,\ncontributing to effective long-text modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the realm of Large Language Models (LLMs), the ability to process long\ncontexts is increasingly crucial for tasks such as multi-round dialogues, code\ngeneration, and document summarization. This paper addresses the challenges of\nenhancing the long-context performance, reducing computational complexity, and\nleveraging pretrained models collectively termed the \"impossible triangle.\" We\nintroduce E2LLM (Encoder Elongated Large Language Models), a novel approach\nthat effectively navigates this paradox. The method involves splitting long\ncontexts into chunks, compressing each into embedding vectors via a pretrained\ntext encoder, and utilizing an adapter to align these representations with a\ndecoder-only LLM. Two training objectives, focusing on reconstruction of the\nencoder output and long-context instruction fine-tuning, are employed to\nfacilitate the understanding of soft prompts by the LLM. Experimental results\ndemonstrate that E2LLM achieves superior performance in long-context scenarios\nwhile balancing efficiency, performance, and compatibility with pretrained\nmodels. Our framework thus represents a significant advancement in the field,\ncontributing to effective long-text modeling."
                },
                "authors": [
                    {
                        "name": "Zihan Liao"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Hang Yu"
                    },
                    {
                        "name": "Lingxiao Wei"
                    },
                    {
                        "name": "Jianguo Li"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Wei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhang"
                },
                "author": "Wei Zhang",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06679v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06679v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06666v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06666v1",
                "updated": "2024-09-10T17:34:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    17,
                    34,
                    34,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T17:34:34Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    17,
                    34,
                    34,
                    1,
                    254,
                    0
                ],
                "title": "LLaMA-Omni: Seamless Speech Interaction with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLaMA-Omni: Seamless Speech Interaction with Large Language Models"
                },
                "summary": "Models like GPT-4o enable real-time interaction with large language models\n(LLMs) through speech, significantly enhancing user experience compared to\ntraditional text-based interaction. However, there is still a lack of\nexploration on how to build speech interaction models based on open-source\nLLMs. To address this, we propose LLaMA-Omni, a novel model architecture\ndesigned for low-latency and high-quality speech interaction with LLMs.\nLLaMA-Omni integrates a pretrained speech encoder, a speech adaptor, an LLM,\nand a streaming speech decoder. It eliminates the need for speech\ntranscription, and can simultaneously generate text and speech responses\ndirectly from speech instructions with extremely low latency. We build our\nmodel based on the latest Llama-3.1-8B-Instruct model. To align the model with\nspeech interaction scenarios, we construct a dataset named InstructS2S-200K,\nwhich includes 200K speech instructions and corresponding speech responses.\nExperimental results show that compared to previous speech-language models,\nLLaMA-Omni provides better responses in both content and style, with a response\nlatency as low as 226ms. Additionally, training LLaMA-Omni takes less than 3\ndays on just 4 GPUs, paving the way for the efficient development of\nspeech-language models in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Models like GPT-4o enable real-time interaction with large language models\n(LLMs) through speech, significantly enhancing user experience compared to\ntraditional text-based interaction. However, there is still a lack of\nexploration on how to build speech interaction models based on open-source\nLLMs. To address this, we propose LLaMA-Omni, a novel model architecture\ndesigned for low-latency and high-quality speech interaction with LLMs.\nLLaMA-Omni integrates a pretrained speech encoder, a speech adaptor, an LLM,\nand a streaming speech decoder. It eliminates the need for speech\ntranscription, and can simultaneously generate text and speech responses\ndirectly from speech instructions with extremely low latency. We build our\nmodel based on the latest Llama-3.1-8B-Instruct model. To align the model with\nspeech interaction scenarios, we construct a dataset named InstructS2S-200K,\nwhich includes 200K speech instructions and corresponding speech responses.\nExperimental results show that compared to previous speech-language models,\nLLaMA-Omni provides better responses in both content and style, with a response\nlatency as low as 226ms. Additionally, training LLaMA-Omni takes less than 3\ndays on just 4 GPUs, paving the way for the efficient development of\nspeech-language models in the future."
                },
                "authors": [
                    {
                        "name": "Qingkai Fang"
                    },
                    {
                        "name": "Shoutao Guo"
                    },
                    {
                        "name": "Yan Zhou"
                    },
                    {
                        "name": "Zhengrui Ma"
                    },
                    {
                        "name": "Shaolei Zhang"
                    },
                    {
                        "name": "Yang Feng"
                    }
                ],
                "author_detail": {
                    "name": "Yang Feng"
                },
                "author": "Yang Feng",
                "arxiv_comment": "Preprint. Project: https://github.com/ictnlp/LLaMA-Omni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06666v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06666v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00055v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00055v2",
                "updated": "2024-09-10T17:26:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    17,
                    26,
                    29,
                    1,
                    254,
                    0
                ],
                "published": "2024-08-21T04:47:26Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    4,
                    47,
                    26,
                    2,
                    234,
                    0
                ],
                "title": "SORSA: Singular Values and Orthonormal Regularized Singular Vectors\n  Adaptation of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SORSA: Singular Values and Orthonormal Regularized Singular Vectors\n  Adaptation of Large Language Models"
                },
                "summary": "The rapid advancement in large language models (LLMs) comes with a\nsignificant increase in their parameter size, presenting challenges for\nadaptation and fine-tuning. Parameter-efficient fine-tuning (PEFT) methods are\nwidely used to adapt LLMs for downstream tasks efficiently. In this paper, we\npropose Singular Values and Orthonormal Regularized Singular Vectors\nAdaptation, or SORSA, a novel PEFT method. We introduce a method to analyze the\nvariation of the parameters by performing singular value decomposition (SVD)\nand discuss and analyze SORSA's superiority in minimizing the alteration in the\nSVD aspect. Each SORSA adapter consists of two main parts: trainable principal\nsingular weights $W_p = U_p \\Sigma_p V^\\top_p$, and frozen residual weights\n$W_r = U_r \\Sigma_r V^\\top_r$. These parts are initialized by performing SVD on\npre-trained weights. Moreover, we implement and analyze an orthonormal\nregularizer, which could effectively transfer the scaling information into\n$\\Sigma_p$ and ultimately allows the training process to be more efficient.\nSORSA adapters could be merged during inference, thus eliminating any inference\nlatency. After all, SORSA shows a faster convergence than PiSSA and LoRA in our\nexperiments. On the MATH benchmark, Llama 2 7B adapted using SORSA achieved\n10.36% accuracy, outperforming LoRA (5.50%), Full FT (7.22%), and PiSSA\n(7.44%). On the GSM-8K benchmark, SORSA achieved 56.03% accuracy, surpassing\nLoRA (42.30%), Full FT (49.05%), and PiSSA (53.07%). We conclude that SORSA\noffers a new perspective on parameter-efficient fine-tuning, demonstrating\nremarkable performance. The code is available at\nhttps://github.com/Gunale0926/SORSA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement in large language models (LLMs) comes with a\nsignificant increase in their parameter size, presenting challenges for\nadaptation and fine-tuning. Parameter-efficient fine-tuning (PEFT) methods are\nwidely used to adapt LLMs for downstream tasks efficiently. In this paper, we\npropose Singular Values and Orthonormal Regularized Singular Vectors\nAdaptation, or SORSA, a novel PEFT method. We introduce a method to analyze the\nvariation of the parameters by performing singular value decomposition (SVD)\nand discuss and analyze SORSA's superiority in minimizing the alteration in the\nSVD aspect. Each SORSA adapter consists of two main parts: trainable principal\nsingular weights $W_p = U_p \\Sigma_p V^\\top_p$, and frozen residual weights\n$W_r = U_r \\Sigma_r V^\\top_r$. These parts are initialized by performing SVD on\npre-trained weights. Moreover, we implement and analyze an orthonormal\nregularizer, which could effectively transfer the scaling information into\n$\\Sigma_p$ and ultimately allows the training process to be more efficient.\nSORSA adapters could be merged during inference, thus eliminating any inference\nlatency. After all, SORSA shows a faster convergence than PiSSA and LoRA in our\nexperiments. On the MATH benchmark, Llama 2 7B adapted using SORSA achieved\n10.36% accuracy, outperforming LoRA (5.50%), Full FT (7.22%), and PiSSA\n(7.44%). On the GSM-8K benchmark, SORSA achieved 56.03% accuracy, surpassing\nLoRA (42.30%), Full FT (49.05%), and PiSSA (53.07%). We conclude that SORSA\noffers a new perspective on parameter-efficient fine-tuning, demonstrating\nremarkable performance. The code is available at\nhttps://github.com/Gunale0926/SORSA."
                },
                "authors": [
                    {
                        "name": "Yang Cao"
                    }
                ],
                "author_detail": {
                    "name": "Yang Cao"
                },
                "author": "Yang Cao",
                "arxiv_comment": "12 pages, 5 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00055v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00055v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06653v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06653v1",
                "updated": "2024-09-10T17:16:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    17,
                    16,
                    42,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T17:16:42Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    17,
                    16,
                    42,
                    1,
                    254,
                    0
                ],
                "title": "Human Perception of LLM-generated Text Content in Social Media\n  Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human Perception of LLM-generated Text Content in Social Media\n  Environments"
                },
                "summary": "Emerging technologies, particularly artificial intelligence (AI), and more\nspecifically Large Language Models (LLMs) have provided malicious actors with\npowerful tools for manipulating digital discourse. LLMs have the potential to\naffect traditional forms of democratic engagements, such as voter choice,\ngovernment surveys, or even online communication with regulators; since bots\nare capable of producing large quantities of credible text. To investigate the\nhuman perception of LLM-generated content, we recruited over 1,000 participants\nwho then tried to differentiate bot from human posts in social media discussion\nthreads. We found that humans perform poorly at identifying the true nature of\nuser posts on social media. We also found patterns in how humans identify\nLLM-generated text content in social media discourse. Finally, we observed the\nUncanny Valley effect in text dialogue in both user perception and\nidentification. This indicates that despite humans being poor at the\nidentification process, they can still sense discomfort when reading\nLLM-generated content.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging technologies, particularly artificial intelligence (AI), and more\nspecifically Large Language Models (LLMs) have provided malicious actors with\npowerful tools for manipulating digital discourse. LLMs have the potential to\naffect traditional forms of democratic engagements, such as voter choice,\ngovernment surveys, or even online communication with regulators; since bots\nare capable of producing large quantities of credible text. To investigate the\nhuman perception of LLM-generated content, we recruited over 1,000 participants\nwho then tried to differentiate bot from human posts in social media discussion\nthreads. We found that humans perform poorly at identifying the true nature of\nuser posts on social media. We also found patterns in how humans identify\nLLM-generated text content in social media discourse. Finally, we observed the\nUncanny Valley effect in text dialogue in both user perception and\nidentification. This indicates that despite humans being poor at the\nidentification process, they can still sense discomfort when reading\nLLM-generated content."
                },
                "authors": [
                    {
                        "name": "Kristina Radivojevic"
                    },
                    {
                        "name": "Matthew Chou"
                    },
                    {
                        "name": "Karla Badillo-Urquiola"
                    },
                    {
                        "name": "Paul Brenner"
                    }
                ],
                "author_detail": {
                    "name": "Paul Brenner"
                },
                "author": "Paul Brenner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06653v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06653v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06646v1",
                "updated": "2024-09-10T17:05:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    17,
                    5,
                    11,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T17:05:11Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    17,
                    5,
                    11,
                    1,
                    254,
                    0
                ],
                "title": "Optimal Workload Placement on Multi-Instance GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Workload Placement on Multi-Instance GPUs"
                },
                "summary": "There is an urgent and pressing need to optimize usage of Graphical\nProcessing Units (GPUs), which have arguably become one of the most expensive\nand sought after IT resources. To help with this goal, several of the current\ngeneration of GPUs support a partitioning feature, called Multi-Instance GPU\n(MIG) to allow multiple workloads to share a GPU, albeit with some constraints.\nIn this paper we investigate how to optimize the placement of Large Language\nModel (LLM)-based AI Inferencing workloads on GPUs. We first identify and\npresent several use cases that are encountered in practice that require\nworkloads to be efficiently placed or migrated to other GPUs to make room for\nincoming workloads. The overarching goal is to use as few GPUs as possible and\nto further minimize memory and compute wastage on GPUs that are utilized. We\nhave developed two approaches to address this problem: an optimization method\nand a heuristic method. We benchmark these with two workload scheduling\nheuristics for multiple use cases. Our results show up to 2.85x improvement in\nthe number of GPUs used and up to 70% reduction in GPU wastage over baseline\nheuristics. We plan to enable the SRE community to leverage our proposed method\nin production environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is an urgent and pressing need to optimize usage of Graphical\nProcessing Units (GPUs), which have arguably become one of the most expensive\nand sought after IT resources. To help with this goal, several of the current\ngeneration of GPUs support a partitioning feature, called Multi-Instance GPU\n(MIG) to allow multiple workloads to share a GPU, albeit with some constraints.\nIn this paper we investigate how to optimize the placement of Large Language\nModel (LLM)-based AI Inferencing workloads on GPUs. We first identify and\npresent several use cases that are encountered in practice that require\nworkloads to be efficiently placed or migrated to other GPUs to make room for\nincoming workloads. The overarching goal is to use as few GPUs as possible and\nto further minimize memory and compute wastage on GPUs that are utilized. We\nhave developed two approaches to address this problem: an optimization method\nand a heuristic method. We benchmark these with two workload scheduling\nheuristics for multiple use cases. Our results show up to 2.85x improvement in\nthe number of GPUs used and up to 70% reduction in GPU wastage over baseline\nheuristics. We plan to enable the SRE community to leverage our proposed method\nin production environments."
                },
                "authors": [
                    {
                        "name": "Bekir Turkkan"
                    },
                    {
                        "name": "Pavankumar Murali"
                    },
                    {
                        "name": "Pavithra Harsha"
                    },
                    {
                        "name": "Rohan Arora"
                    },
                    {
                        "name": "Gerard Vanloo"
                    },
                    {
                        "name": "Chandra Narayanaswami"
                    }
                ],
                "author_detail": {
                    "name": "Chandra Narayanaswami"
                },
                "author": "Chandra Narayanaswami",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06643v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06643v1",
                "updated": "2024-09-10T16:59:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    16,
                    59,
                    33,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T16:59:33Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    16,
                    59,
                    33,
                    1,
                    254,
                    0
                ],
                "title": "Strategic management analysis: from data to strategy diagram by LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strategic management analysis: from data to strategy diagram by LLM"
                },
                "summary": "Strategy management analyses are created by business consultants with common\nanalysis frameworks (i.e. comparative analyses) and associated diagrams. We\nshow these can be largely constructed using LLMs, starting with the extraction\nof insights from data, organization of those insights according to a strategy\nmanagement framework, and then depiction in the typical strategy management\ndiagram for that framework (static textual visualizations). We discuss caveats\nand future directions to generalize for broader uses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strategy management analyses are created by business consultants with common\nanalysis frameworks (i.e. comparative analyses) and associated diagrams. We\nshow these can be largely constructed using LLMs, starting with the extraction\nof insights from data, organization of those insights according to a strategy\nmanagement framework, and then depiction in the typical strategy management\ndiagram for that framework (static textual visualizations). We discuss caveats\nand future directions to generalize for broader uses."
                },
                "authors": [
                    {
                        "name": "Richard Brath"
                    },
                    {
                        "name": "Adam Bradley"
                    },
                    {
                        "name": "David Jonker"
                    }
                ],
                "author_detail": {
                    "name": "David Jonker"
                },
                "author": "David Jonker",
                "arxiv_comment": "NLVIZ Workshop at IEEE VIZ 2024. 7 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06643v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06643v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06635v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06635v1",
                "updated": "2024-09-10T16:46:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    16,
                    46,
                    18,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T16:46:18Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    16,
                    46,
                    18,
                    1,
                    254,
                    0
                ],
                "title": "MoWE-Audio: Multitask AudioLLMs with Mixture of Weak Encoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoWE-Audio: Multitask AudioLLMs with Mixture of Weak Encoders"
                },
                "summary": "The rapid advancements in large language models (LLMs) have significantly\nenhanced natural language processing capabilities, facilitating the development\nof AudioLLMs that process and understand speech and audio inputs alongside\ntext. Existing AudioLLMs typically combine a pre-trained audio encoder with a\npre-trained LLM, which are subsequently finetuned on specific audio tasks.\nHowever, the pre-trained audio encoder has constrained capacity to capture\nfeatures for new tasks and datasets. To address this, we propose to incorporate\nmixtures of `weak' encoders (MoWE) into the AudioLLM framework. MoWE\nsupplements a base encoder with a pool of relatively light weight encoders,\nselectively activated based on the audio input to enhance feature extraction\nwithout significantly increasing model size. Our empirical results demonstrate\nthat MoWE effectively improves multi-task performance, broadening the\napplicability of AudioLLMs to more diverse audio tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancements in large language models (LLMs) have significantly\nenhanced natural language processing capabilities, facilitating the development\nof AudioLLMs that process and understand speech and audio inputs alongside\ntext. Existing AudioLLMs typically combine a pre-trained audio encoder with a\npre-trained LLM, which are subsequently finetuned on specific audio tasks.\nHowever, the pre-trained audio encoder has constrained capacity to capture\nfeatures for new tasks and datasets. To address this, we propose to incorporate\nmixtures of `weak' encoders (MoWE) into the AudioLLM framework. MoWE\nsupplements a base encoder with a pool of relatively light weight encoders,\nselectively activated based on the audio input to enhance feature extraction\nwithout significantly increasing model size. Our empirical results demonstrate\nthat MoWE effectively improves multi-task performance, broadening the\napplicability of AudioLLMs to more diverse audio tasks."
                },
                "authors": [
                    {
                        "name": "Wenyu Zhang"
                    },
                    {
                        "name": "Shuo Sun"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Xunlong Zou"
                    },
                    {
                        "name": "Zhuohan Liu"
                    },
                    {
                        "name": "Yingxu He"
                    },
                    {
                        "name": "Geyu Lin"
                    },
                    {
                        "name": "Nancy F. Chen"
                    },
                    {
                        "name": "Ai Ti Aw"
                    }
                ],
                "author_detail": {
                    "name": "Ai Ti Aw"
                },
                "author": "Ai Ti Aw",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06635v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06756v1",
                "updated": "2024-09-10T16:28:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    16,
                    28,
                    50,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T16:28:50Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    16,
                    28,
                    50,
                    1,
                    254,
                    0
                ],
                "title": "Beyond designer's knowledge: Generating materials design hypotheses via\n  large language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond designer's knowledge: Generating materials design hypotheses via\n  large language models"
                },
                "summary": "Materials design often relies on human-generated hypotheses, a process\ninherently limited by cognitive constraints such as knowledge gaps and limited\nability to integrate and extract knowledge implications, particularly when\nmultidisciplinary expertise is required. This work demonstrates that large\nlanguage models (LLMs), coupled with prompt engineering, can effectively\ngenerate non-trivial materials hypotheses by integrating scientific principles\nfrom diverse sources without explicit design guidance by human experts. These\ninclude design ideas for high-entropy alloys with superior cryogenic properties\nand halide solid electrolytes with enhanced ionic conductivity and formability.\nThese design ideas have been experimentally validated in high-impact\npublications in 2023 not available in the LLM training data, demonstrating the\nLLM's ability to generate highly valuable and realizable innovative ideas not\nestablished in the literature. Our approach primarily leverages materials\nsystem charts encoding processing-structure-property relationships, enabling\nmore effective data integration by condensing key information from numerous\npapers, and evaluation and categorization of numerous hypotheses for human\ncognition, both through the LLM. This LLM-driven approach opens the door to new\navenues of artificial intelligence-driven materials discovery by accelerating\ndesign, democratizing innovation, and expanding capabilities beyond the\ndesigner's direct knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Materials design often relies on human-generated hypotheses, a process\ninherently limited by cognitive constraints such as knowledge gaps and limited\nability to integrate and extract knowledge implications, particularly when\nmultidisciplinary expertise is required. This work demonstrates that large\nlanguage models (LLMs), coupled with prompt engineering, can effectively\ngenerate non-trivial materials hypotheses by integrating scientific principles\nfrom diverse sources without explicit design guidance by human experts. These\ninclude design ideas for high-entropy alloys with superior cryogenic properties\nand halide solid electrolytes with enhanced ionic conductivity and formability.\nThese design ideas have been experimentally validated in high-impact\npublications in 2023 not available in the LLM training data, demonstrating the\nLLM's ability to generate highly valuable and realizable innovative ideas not\nestablished in the literature. Our approach primarily leverages materials\nsystem charts encoding processing-structure-property relationships, enabling\nmore effective data integration by condensing key information from numerous\npapers, and evaluation and categorization of numerous hypotheses for human\ncognition, both through the LLM. This LLM-driven approach opens the door to new\navenues of artificial intelligence-driven materials discovery by accelerating\ndesign, democratizing innovation, and expanding capabilities beyond the\ndesigner's direct knowledge."
                },
                "authors": [
                    {
                        "name": "Quanliang Liu"
                    },
                    {
                        "name": "Maciej P. Polak"
                    },
                    {
                        "name": "So Yeon Kim"
                    },
                    {
                        "name": "MD Al Amin Shuvo"
                    },
                    {
                        "name": "Hrishikesh Shridhar Deodhar"
                    },
                    {
                        "name": "Jeongsoo Han"
                    },
                    {
                        "name": "Dane Morgan"
                    },
                    {
                        "name": "Hyunseok Oh"
                    }
                ],
                "author_detail": {
                    "name": "Hyunseok Oh"
                },
                "author": "Hyunseok Oh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06624v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06624v1",
                "updated": "2024-09-10T16:26:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    16,
                    26,
                    43,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T16:26:43Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    16,
                    26,
                    43,
                    1,
                    254,
                    0
                ],
                "title": "A Practice of Post-Training on Llama-3 70B with Optimal Selection of\n  Additional Language Mixture Ratio",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Practice of Post-Training on Llama-3 70B with Optimal Selection of\n  Additional Language Mixture Ratio"
                },
                "summary": "Large Language Models (LLM) often needs to be Continual Pre-Trained (CPT) to\nobtain the unfamiliar language skill or adapt into new domains. The huge\ntraining cost of CPT often asks for cautious choice of key hyper-parameters\nsuch as the mixture ratio of extra language or domain corpus. However, there is\nno systematic study which bridge the gap between the optimal mixture ratio and\nthe actual model performance, and the gap between experimental scaling law and\nthe actual deployment in the full model size. In this paper, we perform CPT on\nLlama-3 8B and 70B to enhance its Chinese ability. We study the optimal\ncorrelation between the Additional Language Mixture Ratio (ALMR) and the\nLearning Rate (LR) on the 8B size which directly indicate the optimal\nexperimental set up. By thorough choice of hyper-parameter, and subsequent\nfine-tuning, the model capability is improved not only on the Chinese-related\nbenchmark, but also some specific domains including math, coding and emotional\nintelligence. We deploy the final 70B version of LLM on an real-life chat\nsystem which obtain satisfying performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLM) often needs to be Continual Pre-Trained (CPT) to\nobtain the unfamiliar language skill or adapt into new domains. The huge\ntraining cost of CPT often asks for cautious choice of key hyper-parameters\nsuch as the mixture ratio of extra language or domain corpus. However, there is\nno systematic study which bridge the gap between the optimal mixture ratio and\nthe actual model performance, and the gap between experimental scaling law and\nthe actual deployment in the full model size. In this paper, we perform CPT on\nLlama-3 8B and 70B to enhance its Chinese ability. We study the optimal\ncorrelation between the Additional Language Mixture Ratio (ALMR) and the\nLearning Rate (LR) on the 8B size which directly indicate the optimal\nexperimental set up. By thorough choice of hyper-parameter, and subsequent\nfine-tuning, the model capability is improved not only on the Chinese-related\nbenchmark, but also some specific domains including math, coding and emotional\nintelligence. We deploy the final 70B version of LLM on an real-life chat\nsystem which obtain satisfying performance."
                },
                "authors": [
                    {
                        "name": "Ningyuan Xi"
                    },
                    {
                        "name": "Yetao Wu"
                    },
                    {
                        "name": "Kun Fan"
                    },
                    {
                        "name": "Teng Chen"
                    },
                    {
                        "name": "Qingqing Gu"
                    },
                    {
                        "name": "Peng Yu"
                    },
                    {
                        "name": "Jinxian Qu"
                    },
                    {
                        "name": "Chenxi Liu"
                    },
                    {
                        "name": "Zhonglin Jiang"
                    },
                    {
                        "name": "Yong Chen"
                    },
                    {
                        "name": "Luo Ji"
                    }
                ],
                "author_detail": {
                    "name": "Luo Ji"
                },
                "author": "Luo Ji",
                "arxiv_comment": "11 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06624v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06624v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05731v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05731v2",
                "updated": "2024-09-10T16:25:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    16,
                    25,
                    58,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-09T15:41:53Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    15,
                    41,
                    53,
                    0,
                    253,
                    0
                ],
                "title": "What Did My Car Say? Impact of Autonomous Vehicle Explanation Errors and\n  Driving Context On Comfort, Reliance, Satisfaction, and Driving Confidence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Did My Car Say? Impact of Autonomous Vehicle Explanation Errors and\n  Driving Context On Comfort, Reliance, Satisfaction, and Driving Confidence"
                },
                "summary": "Explanations for autonomous vehicle (AV) decisions may build trust, however,\nexplanations can contain errors. In a simulated driving study (n = 232), we\ntested how AV explanation errors, driving context characteristics (perceived\nharm and driving difficulty), and personal traits (prior trust and expertise)\naffected a passenger's comfort in relying on an AV, preference for control,\nconfidence in the AV's ability, and explanation satisfaction. Errors negatively\naffected all outcomes. Surprisingly, despite identical driving, explanation\nerrors reduced ratings of the AV's driving ability. Severity and potential harm\namplified the negative impact of errors. Contextual harm and driving difficulty\ndirectly impacted outcome ratings and influenced the relationship between\nerrors and outcomes. Prior trust and expertise were positively associated with\noutcome ratings. Results emphasize the need for accurate, contextually\nadaptive, and personalized AV explanations to foster trust, reliance,\nsatisfaction, and confidence. We conclude with design, research, and deployment\nrecommendations for trustworthy AV explanation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explanations for autonomous vehicle (AV) decisions may build trust, however,\nexplanations can contain errors. In a simulated driving study (n = 232), we\ntested how AV explanation errors, driving context characteristics (perceived\nharm and driving difficulty), and personal traits (prior trust and expertise)\naffected a passenger's comfort in relying on an AV, preference for control,\nconfidence in the AV's ability, and explanation satisfaction. Errors negatively\naffected all outcomes. Surprisingly, despite identical driving, explanation\nerrors reduced ratings of the AV's driving ability. Severity and potential harm\namplified the negative impact of errors. Contextual harm and driving difficulty\ndirectly impacted outcome ratings and influenced the relationship between\nerrors and outcomes. Prior trust and expertise were positively associated with\noutcome ratings. Results emphasize the need for accurate, contextually\nadaptive, and personalized AV explanations to foster trust, reliance,\nsatisfaction, and confidence. We conclude with design, research, and deployment\nrecommendations for trustworthy AV explanation systems."
                },
                "authors": [
                    {
                        "name": "Robert Kaufman"
                    },
                    {
                        "name": "Aaron Broukhim"
                    },
                    {
                        "name": "David Kirsh"
                    },
                    {
                        "name": "Nadir Weibel"
                    }
                ],
                "author_detail": {
                    "name": "Nadir Weibel"
                },
                "author": "Nadir Weibel",
                "arxiv_comment": "23 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05731v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05731v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06622v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06622v1",
                "updated": "2024-09-10T16:22:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    16,
                    22,
                    18,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T16:22:18Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    16,
                    22,
                    18,
                    1,
                    254,
                    0
                ],
                "title": "Exploring Italian sentence embeddings properties through multi-tasking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Italian sentence embeddings properties through multi-tasking"
                },
                "summary": "We investigate to what degree existing LLMs encode abstract linguistic\ninformation in Italian in a multi-task setting. We exploit curated synthetic\ndata on a large scale -- several Blackbird Language Matrices (BLMs) problems in\nItalian -- and use them to study how sentence representations built using\npre-trained language models encode specific syntactic and semantic information.\nWe use a two-level architecture to model separately a compression of the\nsentence embeddings into a representation that contains relevant information\nfor a task, and a BLM task. We then investigate whether we can obtain\ncompressed sentence representations that encode syntactic and semantic\ninformation relevant to several BLM tasks. While we expected that the sentence\nstructure -- in terms of sequence of phrases/chunks -- and chunk properties\ncould be shared across tasks, performance and error analysis show that the\nclues for the different tasks are encoded in different manners in the sentence\nembeddings, suggesting that abstract linguistic notions such as constituents or\nthematic roles does not seem to be present in the pretrained sentence\nembeddings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate to what degree existing LLMs encode abstract linguistic\ninformation in Italian in a multi-task setting. We exploit curated synthetic\ndata on a large scale -- several Blackbird Language Matrices (BLMs) problems in\nItalian -- and use them to study how sentence representations built using\npre-trained language models encode specific syntactic and semantic information.\nWe use a two-level architecture to model separately a compression of the\nsentence embeddings into a representation that contains relevant information\nfor a task, and a BLM task. We then investigate whether we can obtain\ncompressed sentence representations that encode syntactic and semantic\ninformation relevant to several BLM tasks. While we expected that the sentence\nstructure -- in terms of sequence of phrases/chunks -- and chunk properties\ncould be shared across tasks, performance and error analysis show that the\nclues for the different tasks are encoded in different manners in the sentence\nembeddings, suggesting that abstract linguistic notions such as constituents or\nthematic roles does not seem to be present in the pretrained sentence\nembeddings."
                },
                "authors": [
                    {
                        "name": "Vivi Nastase"
                    },
                    {
                        "name": "Giuseppe Samo"
                    },
                    {
                        "name": "Chunyang Jiang"
                    },
                    {
                        "name": "Paola Merlo"
                    }
                ],
                "author_detail": {
                    "name": "Paola Merlo"
                },
                "author": "Paola Merlo",
                "arxiv_comment": "9 pages, 9 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06622v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06622v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06754v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06754v1",
                "updated": "2024-09-10T16:05:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    16,
                    5,
                    2,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T16:05:02Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    16,
                    5,
                    2,
                    1,
                    254,
                    0
                ],
                "title": "Scaling Law Hypothesis for Multimodal Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Law Hypothesis for Multimodal Model"
                },
                "summary": "We propose a scaling law hypothesis for multimodal models processing text,\naudio, images, and video within a shared token and embedding space. Our\nframework predicts model performance based on modality-specific compression and\ntokenization efficiency, extending established scaling laws from text-based\ndecoder models to mixed-modality systems. We explore whether leveraging more\ntraining data in multiple modalities can reduce the size of the multimodal\nmodel, enabling efficient deployment on resource-constrained devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a scaling law hypothesis for multimodal models processing text,\naudio, images, and video within a shared token and embedding space. Our\nframework predicts model performance based on modality-specific compression and\ntokenization efficiency, extending established scaling laws from text-based\ndecoder models to mixed-modality systems. We explore whether leveraging more\ntraining data in multiple modalities can reduce the size of the multimodal\nmodel, enabling efficient deployment on resource-constrained devices."
                },
                "authors": [
                    {
                        "name": "Qingyun Sun"
                    },
                    {
                        "name": "Zhen Guo"
                    }
                ],
                "author_detail": {
                    "name": "Zhen Guo"
                },
                "author": "Zhen Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06754v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06754v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.04745v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.04745v3",
                "updated": "2024-09-10T15:51:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    15,
                    51,
                    27,
                    1,
                    254,
                    0
                ],
                "published": "2024-03-07T18:49:36Z",
                "published_parsed": [
                    2024,
                    3,
                    7,
                    18,
                    49,
                    36,
                    3,
                    67,
                    0
                ],
                "title": "Not All Errors Are Made Equal: A Regret Metric for Detecting\n  System-level Trajectory Prediction Failures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Errors Are Made Equal: A Regret Metric for Detecting\n  System-level Trajectory Prediction Failures"
                },
                "summary": "Robot decision-making increasingly relies on data-driven human prediction\nmodels when operating around people. While these models are known to mispredict\nin out-of-distribution interactions, only a subset of prediction errors impact\ndownstream robot performance. We propose characterizing such \"system-level\"\nprediction failures via the mathematical notion of regret: high-regret\ninteractions are precisely those in which mispredictions degraded closed-loop\nrobot performance. We further introduce a probabilistic generalization of\nregret that calibrates failure detection across disparate deployment contexts\nand renders regret compatible with reward-based and reward-free (e.g.,\ngenerative) planners. In simulated autonomous driving interactions and social\nnavigation interactions deployed on hardware, we showcase that our system-level\nfailure metric can be used offline to automatically extract closed-loop\nhuman-robot interactions that state-of-the-art generative human predictors and\nrobot planners previously struggled with. We further find that the very\npresence of high-regret data during human predictor fine-tuning is highly\npredictive of robot re-deployment performance improvements. Fine-tuning with\nthe informative but significantly smaller high-regret data (23% of deployment\ndata) is competitive with fine-tuning on the full deployment dataset,\nindicating a promising avenue for efficiently mitigating system-level\nhuman-robot interaction failures. Project website:\nhttps://cmu-intentlab.github.io/not-all-errors/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robot decision-making increasingly relies on data-driven human prediction\nmodels when operating around people. While these models are known to mispredict\nin out-of-distribution interactions, only a subset of prediction errors impact\ndownstream robot performance. We propose characterizing such \"system-level\"\nprediction failures via the mathematical notion of regret: high-regret\ninteractions are precisely those in which mispredictions degraded closed-loop\nrobot performance. We further introduce a probabilistic generalization of\nregret that calibrates failure detection across disparate deployment contexts\nand renders regret compatible with reward-based and reward-free (e.g.,\ngenerative) planners. In simulated autonomous driving interactions and social\nnavigation interactions deployed on hardware, we showcase that our system-level\nfailure metric can be used offline to automatically extract closed-loop\nhuman-robot interactions that state-of-the-art generative human predictors and\nrobot planners previously struggled with. We further find that the very\npresence of high-regret data during human predictor fine-tuning is highly\npredictive of robot re-deployment performance improvements. Fine-tuning with\nthe informative but significantly smaller high-regret data (23% of deployment\ndata) is competitive with fine-tuning on the full deployment dataset,\nindicating a promising avenue for efficiently mitigating system-level\nhuman-robot interaction failures. Project website:\nhttps://cmu-intentlab.github.io/not-all-errors/"
                },
                "authors": [
                    {
                        "name": "Kensuke Nakamura"
                    },
                    {
                        "name": "Ran Tian"
                    },
                    {
                        "name": "Andrea Bajcsy"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Bajcsy"
                },
                "author": "Andrea Bajcsy",
                "arxiv_comment": "6 figures, 3 tables, Accepted to CoRL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.04745v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.04745v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06601v1",
                "updated": "2024-09-10T15:51:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    15,
                    51,
                    15,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T15:51:15Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    15,
                    51,
                    15,
                    1,
                    254,
                    0
                ],
                "title": "Alleviating Hallucinations in Large Language Models with Scepticism\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alleviating Hallucinations in Large Language Models with Scepticism\n  Modeling"
                },
                "summary": "Hallucinations is a major challenge for large language models (LLMs),\nprevents adoption in diverse fields. Uncertainty estimation could be used for\nalleviating the damages of hallucinations. The skeptical emotion of human could\nbe useful for enhancing the ability of self estimation. Inspirited by this\nobservation, we proposed a new approach called Skepticism Modeling (SM). This\napproach is formalized by combining the information of token and logits for\nself estimation. We construct the doubt emotion aware data, perform continual\npre-training, and then fine-tune the LLMs, improve their ability of self\nestimation. Experimental results demonstrate this new approach effectively\nenhances a model's ability to estimate their uncertainty, and validate its\ngeneralization ability of other tasks by out-of-domain experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucinations is a major challenge for large language models (LLMs),\nprevents adoption in diverse fields. Uncertainty estimation could be used for\nalleviating the damages of hallucinations. The skeptical emotion of human could\nbe useful for enhancing the ability of self estimation. Inspirited by this\nobservation, we proposed a new approach called Skepticism Modeling (SM). This\napproach is formalized by combining the information of token and logits for\nself estimation. We construct the doubt emotion aware data, perform continual\npre-training, and then fine-tune the LLMs, improve their ability of self\nestimation. Experimental results demonstrate this new approach effectively\nenhances a model's ability to estimate their uncertainty, and validate its\ngeneralization ability of other tasks by out-of-domain experiments."
                },
                "authors": [
                    {
                        "name": "Yetao Wu"
                    },
                    {
                        "name": "Yihong Wang"
                    },
                    {
                        "name": "Teng Chen"
                    },
                    {
                        "name": "Chenxi Liu"
                    },
                    {
                        "name": "Ningyuan Xi"
                    },
                    {
                        "name": "Qingqing Gu"
                    },
                    {
                        "name": "Hongyang Lei"
                    },
                    {
                        "name": "Zhonglin Jiang"
                    },
                    {
                        "name": "Yong Chen"
                    },
                    {
                        "name": "Luo Ji"
                    }
                ],
                "author_detail": {
                    "name": "Luo Ji"
                },
                "author": "Luo Ji",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06595v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06595v1",
                "updated": "2024-09-10T15:39:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    15,
                    39,
                    32,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T15:39:32Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    15,
                    39,
                    32,
                    1,
                    254,
                    0
                ],
                "title": "GroUSE: A Benchmark to Evaluate Evaluators in Grounded Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GroUSE: A Benchmark to Evaluate Evaluators in Grounded Question\n  Answering"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has emerged as a common paradigm to use\nLarge Language Models (LLMs) alongside private and up-to-date knowledge bases.\nIn this work, we address the challenges of using LLM-as-a-Judge when evaluating\ngrounded answers generated by RAG systems. To assess the calibration and\ndiscrimination capabilities of judge models, we identify 7 generator failure\nmodes and introduce GroUSE (Grounded QA Unitary Scoring of Evaluators), a\nmeta-evaluation benchmark of 144 unit tests. This benchmark reveals that\nexisting automated RAG evaluation frameworks often overlook important failure\nmodes, even when using GPT-4 as a judge.\n  To improve on the current design of automated RAG evaluation frameworks, we\npropose a novel pipeline and find that while closed models perform well on\nGroUSE, state-of-the-art open-source judges do not generalize to our proposed\ncriteria, despite strong correlation with GPT-4's judgement. Our findings\nsuggest that correlation with GPT-4 is an incomplete proxy for the practical\nperformance of judge models and should be supplemented with evaluations on unit\ntests for precise failure mode detection.\n  We further show that finetuning Llama-3 on GPT-4's reasoning traces\nsignificantly boosts its evaluation capabilities, improving upon both\ncorrelation with GPT-4's evaluations and calibration on reference situations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has emerged as a common paradigm to use\nLarge Language Models (LLMs) alongside private and up-to-date knowledge bases.\nIn this work, we address the challenges of using LLM-as-a-Judge when evaluating\ngrounded answers generated by RAG systems. To assess the calibration and\ndiscrimination capabilities of judge models, we identify 7 generator failure\nmodes and introduce GroUSE (Grounded QA Unitary Scoring of Evaluators), a\nmeta-evaluation benchmark of 144 unit tests. This benchmark reveals that\nexisting automated RAG evaluation frameworks often overlook important failure\nmodes, even when using GPT-4 as a judge.\n  To improve on the current design of automated RAG evaluation frameworks, we\npropose a novel pipeline and find that while closed models perform well on\nGroUSE, state-of-the-art open-source judges do not generalize to our proposed\ncriteria, despite strong correlation with GPT-4's judgement. Our findings\nsuggest that correlation with GPT-4 is an incomplete proxy for the practical\nperformance of judge models and should be supplemented with evaluations on unit\ntests for precise failure mode detection.\n  We further show that finetuning Llama-3 on GPT-4's reasoning traces\nsignificantly boosts its evaluation capabilities, improving upon both\ncorrelation with GPT-4's evaluations and calibration on reference situations."
                },
                "authors": [
                    {
                        "name": "Sacha Muller"
                    },
                    {
                        "name": "António Loison"
                    },
                    {
                        "name": "Bilel Omrani"
                    },
                    {
                        "name": "Gautier Viaud"
                    }
                ],
                "author_detail": {
                    "name": "Gautier Viaud"
                },
                "author": "Gautier Viaud",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06595v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06595v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08921v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08921v2",
                "updated": "2024-09-10T15:38:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    15,
                    38,
                    56,
                    1,
                    254,
                    0
                ],
                "published": "2024-08-15T12:20:24Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    12,
                    20,
                    24,
                    3,
                    228,
                    0
                ],
                "title": "Graph Retrieval-Augmented Generation: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Retrieval-Augmented Generation: A Survey"
                },
                "summary": "Recently, Retrieval-Augmented Generation (RAG) has achieved remarkable\nsuccess in addressing the challenges of Large Language Models (LLMs) without\nnecessitating retraining. By referencing an external knowledge base, RAG\nrefines LLM outputs, effectively mitigating issues such as ``hallucination'',\nlack of domain-specific knowledge, and outdated information. However, the\ncomplex structure of relationships among different entities in databases\npresents challenges for RAG systems. In response, GraphRAG leverages structural\ninformation across entities to enable more precise and comprehensive retrieval,\ncapturing relational knowledge and facilitating more accurate, context-aware\nresponses. Given the novelty and potential of GraphRAG, a systematic review of\ncurrent technologies is imperative. This paper provides the first comprehensive\noverview of GraphRAG methodologies. We formalize the GraphRAG workflow,\nencompassing Graph-Based Indexing, Graph-Guided Retrieval, and Graph-Enhanced\nGeneration. We then outline the core technologies and training methods at each\nstage. Additionally, we examine downstream tasks, application domains,\nevaluation methodologies, and industrial use cases of GraphRAG. Finally, we\nexplore future research directions to inspire further inquiries and advance\nprogress in the field. In order to track recent progress in this field, we set\nup a repository at \\url{https://github.com/pengboci/GraphRAG-Survey}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Retrieval-Augmented Generation (RAG) has achieved remarkable\nsuccess in addressing the challenges of Large Language Models (LLMs) without\nnecessitating retraining. By referencing an external knowledge base, RAG\nrefines LLM outputs, effectively mitigating issues such as ``hallucination'',\nlack of domain-specific knowledge, and outdated information. However, the\ncomplex structure of relationships among different entities in databases\npresents challenges for RAG systems. In response, GraphRAG leverages structural\ninformation across entities to enable more precise and comprehensive retrieval,\ncapturing relational knowledge and facilitating more accurate, context-aware\nresponses. Given the novelty and potential of GraphRAG, a systematic review of\ncurrent technologies is imperative. This paper provides the first comprehensive\noverview of GraphRAG methodologies. We formalize the GraphRAG workflow,\nencompassing Graph-Based Indexing, Graph-Guided Retrieval, and Graph-Enhanced\nGeneration. We then outline the core technologies and training methods at each\nstage. Additionally, we examine downstream tasks, application domains,\nevaluation methodologies, and industrial use cases of GraphRAG. Finally, we\nexplore future research directions to inspire further inquiries and advance\nprogress in the field. In order to track recent progress in this field, we set\nup a repository at \\url{https://github.com/pengboci/GraphRAG-Survey}."
                },
                "authors": [
                    {
                        "name": "Boci Peng"
                    },
                    {
                        "name": "Yun Zhu"
                    },
                    {
                        "name": "Yongchao Liu"
                    },
                    {
                        "name": "Xiaohe Bo"
                    },
                    {
                        "name": "Haizhou Shi"
                    },
                    {
                        "name": "Chuntao Hong"
                    },
                    {
                        "name": "Yan Zhang"
                    },
                    {
                        "name": "Siliang Tang"
                    }
                ],
                "author_detail": {
                    "name": "Siliang Tang"
                },
                "author": "Siliang Tang",
                "arxiv_comment": "Ongoing work. Compared to the first version, several references have\n  been added and a GitHub repository link has been provided",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08921v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08921v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06568v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06568v1",
                "updated": "2024-09-10T15:02:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    15,
                    2,
                    34,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T15:02:34Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    15,
                    2,
                    34,
                    1,
                    254,
                    0
                ],
                "title": "Think-on-Process: Dynamic Process Generation for Collaborative\n  Development of Multi-Agent System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think-on-Process: Dynamic Process Generation for Collaborative\n  Development of Multi-Agent System"
                },
                "summary": "Software development is a collaborative endeavor that requires individuals\nfrom different departments to work together in order to collectively develop a\nhigh-quality software system. In this context, people have begun to explore a\nmethod that leverages multi-agent systems based on LLMs to carry out software\ndevelopment. However, existing research tends to rigidly fix the software\ndevelopment process in a framework in code form, thus failing to dynamically\nadjust the software development process in real-time to meet the more flexible\nand variable software environment. In this paper, we propose a dynamic process\ngeneration framework, named ToP (Think-on-Process). The core idea of ToP is to\nleverage experiential knowledge (i.e., process models) to guide LLMs in\ngenerating software development processes (i.e., instances). These instances\nwill guide multi-agent in software development and employ a compiler to provide\nfeedback on the development outcomes. Subsequently, we utilize heuristic\nalgorithms to filter the instances and apply process mining algorithms to\nderive process model. Finally, the process model will be converted into text,\nformatted as prompts, to enhance the ability of LLMs to generate other\ninstances. Experiments demonstrate that our framework ToP significantly\nenhances the dynamic process generation capability of the GPT-3.5 and GPT-4 for\nfive categories of software development tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software development is a collaborative endeavor that requires individuals\nfrom different departments to work together in order to collectively develop a\nhigh-quality software system. In this context, people have begun to explore a\nmethod that leverages multi-agent systems based on LLMs to carry out software\ndevelopment. However, existing research tends to rigidly fix the software\ndevelopment process in a framework in code form, thus failing to dynamically\nadjust the software development process in real-time to meet the more flexible\nand variable software environment. In this paper, we propose a dynamic process\ngeneration framework, named ToP (Think-on-Process). The core idea of ToP is to\nleverage experiential knowledge (i.e., process models) to guide LLMs in\ngenerating software development processes (i.e., instances). These instances\nwill guide multi-agent in software development and employ a compiler to provide\nfeedback on the development outcomes. Subsequently, we utilize heuristic\nalgorithms to filter the instances and apply process mining algorithms to\nderive process model. Finally, the process model will be converted into text,\nformatted as prompts, to enhance the ability of LLMs to generate other\ninstances. Experiments demonstrate that our framework ToP significantly\nenhances the dynamic process generation capability of the GPT-3.5 and GPT-4 for\nfive categories of software development tasks."
                },
                "authors": [
                    {
                        "name": "Leilei Lin"
                    },
                    {
                        "name": "Yingming Zhou"
                    },
                    {
                        "name": "Wenlong Chen"
                    },
                    {
                        "name": "Chen Qian"
                    }
                ],
                "author_detail": {
                    "name": "Chen Qian"
                },
                "author": "Chen Qian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06568v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06568v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2210.14999v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2210.14999v2",
                "updated": "2024-09-10T14:53:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    14,
                    53,
                    46,
                    1,
                    254,
                    0
                ],
                "published": "2022-10-26T19:40:10Z",
                "published_parsed": [
                    2022,
                    10,
                    26,
                    19,
                    40,
                    10,
                    2,
                    299,
                    0
                ],
                "title": "Secure IP Address Allocation at Cloud Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secure IP Address Allocation at Cloud Scale"
                },
                "summary": "Public clouds necessitate dynamic resource allocation and sharing. However,\nthe dynamic allocation of IP addresses can be abused by adversaries to source\nmalicious traffic, bypass rate limiting systems, and even capture traffic\nintended for other cloud tenants. As a result, both the cloud provider and\ntheir customers are put at risk, and defending against these threats requires a\nrigorous analysis of tenant behavior, adversarial strategies, and cloud\nprovider policies. In this paper, we develop a practical defense for IP address\nallocation through such an analysis. We first develop a statistical model of\ncloud tenant deployment behavior based on literature and measurement of\ndeployed systems. Through this, we analyze IP allocation policies under\nexisting and novel threat models. In response to our stronger proposed threat\nmodel, we design IP scan segmentation, an IP allocation policy that protects\nthe address pool against adversarial scanning even when an adversary is not\nlimited by number of cloud tenants. Through empirical evaluation on both\nsynthetic and real-world allocation traces, we show that IP scan segmentation\nreduces adversaries' ability to rapidly allocate addresses, protecting both\naddress space reputation and cloud tenant data. In this way, we show that\nprincipled analysis and implementation of cloud IP address allocation can lead\nto substantial security gains for tenants and their users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Public clouds necessitate dynamic resource allocation and sharing. However,\nthe dynamic allocation of IP addresses can be abused by adversaries to source\nmalicious traffic, bypass rate limiting systems, and even capture traffic\nintended for other cloud tenants. As a result, both the cloud provider and\ntheir customers are put at risk, and defending against these threats requires a\nrigorous analysis of tenant behavior, adversarial strategies, and cloud\nprovider policies. In this paper, we develop a practical defense for IP address\nallocation through such an analysis. We first develop a statistical model of\ncloud tenant deployment behavior based on literature and measurement of\ndeployed systems. Through this, we analyze IP allocation policies under\nexisting and novel threat models. In response to our stronger proposed threat\nmodel, we design IP scan segmentation, an IP allocation policy that protects\nthe address pool against adversarial scanning even when an adversary is not\nlimited by number of cloud tenants. Through empirical evaluation on both\nsynthetic and real-world allocation traces, we show that IP scan segmentation\nreduces adversaries' ability to rapidly allocate addresses, protecting both\naddress space reputation and cloud tenant data. In this way, we show that\nprincipled analysis and implementation of cloud IP address allocation can lead\nto substantial security gains for tenants and their users."
                },
                "authors": [
                    {
                        "name": "Eric Pauley"
                    },
                    {
                        "name": "Kyle Domico"
                    },
                    {
                        "name": "Blaine Hoak"
                    },
                    {
                        "name": "Ryan Sheatsley"
                    },
                    {
                        "name": "Quinn Burke"
                    },
                    {
                        "name": "Yohan Beugin"
                    },
                    {
                        "name": "Engin Kirda"
                    },
                    {
                        "name": "Patrick McDaniel"
                    }
                ],
                "author_detail": {
                    "name": "Patrick McDaniel"
                },
                "arxiv_affiliation": "University of Wisconsin-Madison",
                "author": "Patrick McDaniel",
                "arxiv_comment": "Replaced with version to appear in 2025 Network and Distributed\n  Systems Security (NDSS) Symposium",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2210.14999v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2210.14999v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06558v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06558v1",
                "updated": "2024-09-10T14:39:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    14,
                    39,
                    4,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T14:39:04Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    14,
                    39,
                    4,
                    1,
                    254,
                    0
                ],
                "title": "MAPS: Energy-Reliability Tradeoff Management in Autonomous Vehicles\n  Through LLMs Penetrated Science",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAPS: Energy-Reliability Tradeoff Management in Autonomous Vehicles\n  Through LLMs Penetrated Science"
                },
                "summary": "As autonomous vehicles become more prevalent, highly accurate and efficient\nsystems are increasingly critical to improve safety, performance, and energy\nconsumption. Efficient management of energy-reliability tradeoffs in these\nsystems demands the ability to predict various conditions during vehicle\noperations. With the promising improvement of Large Language Models (LLMs) and\nthe emergence of well-known models like ChatGPT, unique opportunities for\nautonomous vehicle-related predictions have been provided in recent years. This\npaper proposed MAPS using LLMs as map reader co-drivers to predict the vital\nparameters to set during the autonomous vehicle operation to balance the\nenergy-reliability tradeoff. The MAPS method demonstrates a 20% improvement in\nnavigation accuracy compared to the best baseline method. MAPS also shows 11%\nenergy savings in computational units and up to 54% in both mechanical and\ncomputational units.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As autonomous vehicles become more prevalent, highly accurate and efficient\nsystems are increasingly critical to improve safety, performance, and energy\nconsumption. Efficient management of energy-reliability tradeoffs in these\nsystems demands the ability to predict various conditions during vehicle\noperations. With the promising improvement of Large Language Models (LLMs) and\nthe emergence of well-known models like ChatGPT, unique opportunities for\nautonomous vehicle-related predictions have been provided in recent years. This\npaper proposed MAPS using LLMs as map reader co-drivers to predict the vital\nparameters to set during the autonomous vehicle operation to balance the\nenergy-reliability tradeoff. The MAPS method demonstrates a 20% improvement in\nnavigation accuracy compared to the best baseline method. MAPS also shows 11%\nenergy savings in computational units and up to 54% in both mechanical and\ncomputational units."
                },
                "authors": [
                    {
                        "name": "Mahdieh Aliazam"
                    },
                    {
                        "name": "Ali Javadi"
                    },
                    {
                        "name": "Amir Mahdi Hosseini Monazzah"
                    },
                    {
                        "name": "Ahmad Akbari Azirani"
                    }
                ],
                "author_detail": {
                    "name": "Ahmad Akbari Azirani"
                },
                "author": "Ahmad Akbari Azirani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06558v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06558v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06540v1",
                "updated": "2024-09-10T14:15:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    14,
                    15,
                    30,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T14:15:30Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    14,
                    15,
                    30,
                    1,
                    254,
                    0
                ],
                "title": "Mapping News Narratives Using LLMs and Narrative-Structured Text\n  Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping News Narratives Using LLMs and Narrative-Structured Text\n  Embeddings"
                },
                "summary": "Given the profound impact of narratives across various societal levels, from\npersonal identities to international politics, it is crucial to understand\ntheir distribution and development over time. This is particularly important in\nonline spaces. On the Web, narratives can spread rapidly and intensify societal\ndivides and conflicts. While many qualitative approaches exist, quantifying\nnarratives remains a significant challenge. Computational narrative analysis\nlacks frameworks that are both comprehensive and generalizable. To address this\ngap, we introduce a numerical narrative representation grounded in\nstructuralist linguistic theory. Chiefly, Greimas' Actantial Model represents a\nnarrative through a constellation of six functional character roles. These\nso-called actants are genre-agnostic, making the model highly generalizable. We\nextract the actants using an open-source LLM and integrate them into a\nNarrative-Structured Text Embedding that captures both the semantics and\nnarrative structure of a text. We demonstrate the analytical insights of the\nmethod on the example of 5000 full-text news articles from Al Jazeera and The\nWashington Post on the Israel-Palestine conflict. Our method successfully\ndistinguishes articles that cover the same topics but differ in narrative\nstructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given the profound impact of narratives across various societal levels, from\npersonal identities to international politics, it is crucial to understand\ntheir distribution and development over time. This is particularly important in\nonline spaces. On the Web, narratives can spread rapidly and intensify societal\ndivides and conflicts. While many qualitative approaches exist, quantifying\nnarratives remains a significant challenge. Computational narrative analysis\nlacks frameworks that are both comprehensive and generalizable. To address this\ngap, we introduce a numerical narrative representation grounded in\nstructuralist linguistic theory. Chiefly, Greimas' Actantial Model represents a\nnarrative through a constellation of six functional character roles. These\nso-called actants are genre-agnostic, making the model highly generalizable. We\nextract the actants using an open-source LLM and integrate them into a\nNarrative-Structured Text Embedding that captures both the semantics and\nnarrative structure of a text. We demonstrate the analytical insights of the\nmethod on the example of 5000 full-text news articles from Al Jazeera and The\nWashington Post on the Israel-Palestine conflict. Our method successfully\ndistinguishes articles that cover the same topics but differ in narrative\nstructure."
                },
                "authors": [
                    {
                        "name": "Jan Elfes"
                    }
                ],
                "author_detail": {
                    "name": "Jan Elfes"
                },
                "author": "Jan Elfes",
                "arxiv_comment": "19 pages, 13 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14467v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14467v2",
                "updated": "2024-09-10T14:08:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    14,
                    8,
                    29,
                    1,
                    254,
                    0
                ],
                "published": "2024-07-19T17:14:16Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    17,
                    14,
                    16,
                    4,
                    201,
                    0
                ],
                "title": "Check-Eval: A Checklist-based Approach for Evaluating Text Quality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Check-Eval: A Checklist-based Approach for Evaluating Text Quality"
                },
                "summary": "Evaluating the quality of text generated by large language models (LLMs)\nremains a significant challenge. Traditional metrics often fail to align well\nwith human judgments, particularly in tasks requiring creativity and nuance. In\nthis paper, we propose \\textsc{Check-Eval}, a novel evaluation framework\nleveraging LLMs to assess the quality of generated text through a\nchecklist-based approach. \\textsc{Check-Eval} can be employed as both a\nreference-free and reference-dependent evaluation method, providing a\nstructured and interpretable assessment of text quality. The framework consists\nof two main stages: checklist generation and checklist evaluation. We validate\n\\textsc{Check-Eval} on two benchmark datasets: Portuguese Legal Semantic\nTextual Similarity and \\textsc{SummEval}. Our results demonstrate that\n\\textsc{Check-Eval} achieves higher correlations with human judgments compared\nto existing metrics, such as \\textsc{G-Eval} and \\textsc{GPTScore},\nunderscoring its potential as a more reliable and effective evaluation\nframework for natural language generation tasks. The code for our experiments\nis available at \\url{https://anonymous.4open.science/r/check-eval-0DB4}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the quality of text generated by large language models (LLMs)\nremains a significant challenge. Traditional metrics often fail to align well\nwith human judgments, particularly in tasks requiring creativity and nuance. In\nthis paper, we propose \\textsc{Check-Eval}, a novel evaluation framework\nleveraging LLMs to assess the quality of generated text through a\nchecklist-based approach. \\textsc{Check-Eval} can be employed as both a\nreference-free and reference-dependent evaluation method, providing a\nstructured and interpretable assessment of text quality. The framework consists\nof two main stages: checklist generation and checklist evaluation. We validate\n\\textsc{Check-Eval} on two benchmark datasets: Portuguese Legal Semantic\nTextual Similarity and \\textsc{SummEval}. Our results demonstrate that\n\\textsc{Check-Eval} achieves higher correlations with human judgments compared\nto existing metrics, such as \\textsc{G-Eval} and \\textsc{GPTScore},\nunderscoring its potential as a more reliable and effective evaluation\nframework for natural language generation tasks. The code for our experiments\nis available at \\url{https://anonymous.4open.science/r/check-eval-0DB4}"
                },
                "authors": [
                    {
                        "name": "Jayr Pereira"
                    },
                    {
                        "name": "Andre Assumpcao"
                    },
                    {
                        "name": "Roberto Lotufo"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Lotufo"
                },
                "author": "Roberto Lotufo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14467v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14467v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06518v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06518v1",
                "updated": "2024-09-10T13:54:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    13,
                    54,
                    4,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T13:54:04Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    13,
                    54,
                    4,
                    1,
                    254,
                    0
                ],
                "title": "Questioning Internal Knowledge Structure of Large Language Models\n  Through the Lens of the Olympic Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Questioning Internal Knowledge Structure of Large Language Models\n  Through the Lens of the Olympic Games"
                },
                "summary": "Large language models (LLMs) have become a dominant approach in natural\nlanguage processing, yet their internal knowledge structures remain largely\nunexplored. In this paper, we analyze the internal knowledge structures of LLMs\nusing historical medal tallies from the Olympic Games. We task the models with\nproviding the medal counts for each team and identifying which teams achieved\nspecific rankings. Our results reveal that while state-of-the-art LLMs perform\nremarkably well in reporting medal counts for individual teams, they struggle\nsignificantly with questions about specific rankings. This suggests that the\ninternal knowledge structures of LLMs are fundamentally different from those of\nhumans, who can easily infer rankings from known medal counts. To support\nfurther research, we publicly release our code, dataset, and model outputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have become a dominant approach in natural\nlanguage processing, yet their internal knowledge structures remain largely\nunexplored. In this paper, we analyze the internal knowledge structures of LLMs\nusing historical medal tallies from the Olympic Games. We task the models with\nproviding the medal counts for each team and identifying which teams achieved\nspecific rankings. Our results reveal that while state-of-the-art LLMs perform\nremarkably well in reporting medal counts for individual teams, they struggle\nsignificantly with questions about specific rankings. This suggests that the\ninternal knowledge structures of LLMs are fundamentally different from those of\nhumans, who can easily infer rankings from known medal counts. To support\nfurther research, we publicly release our code, dataset, and model outputs."
                },
                "authors": [
                    {
                        "name": "Juhwan Choi"
                    },
                    {
                        "name": "YoungBin Kim"
                    }
                ],
                "author_detail": {
                    "name": "YoungBin Kim"
                },
                "author": "YoungBin Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06518v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06518v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]