[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2602.16564v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16564v1",
                "title": "A Scalable Approach to Solving Simulation-Based Network Security Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Scalable Approach to Solving Simulation-Based Network Security Games"
                },
                "updated": "2026-02-18T16:07:01Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    16,
                    7,
                    1,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16564v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16564v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce MetaDOAR, a lightweight meta-controller that augments the Double Oracle / PSRO paradigm with a learned, partition-aware filtering layer and Q-value caching to enable scalable multi-agent reinforcement learning on very large cyber-network environments. MetaDOAR learns a compact state projection from per node structural embeddings to rapidly score and select a small subset of devices (a top-k partition) on which a conventional low-level actor performs focused beam search utilizing a critic agent. Selected candidate actions are evaluated with batched critic forwards and stored in an LRU cache keyed by a quantized state projection and local action identifiers, dramatically reducing redundant critic computation while preserving decision quality via conservative k-hop cache invalidation. Empirically, MetaDOAR attains higher player payoffs than SOTA baselines on large network topologies, without significant scaling issues in terms of memory usage or training time. This contribution provide a practical, theoretically motivated path to efficient hierarchical policy learning for large-scale networked decision problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce MetaDOAR, a lightweight meta-controller that augments the Double Oracle / PSRO paradigm with a learned, partition-aware filtering layer and Q-value caching to enable scalable multi-agent reinforcement learning on very large cyber-network environments. MetaDOAR learns a compact state projection from per node structural embeddings to rapidly score and select a small subset of devices (a top-k partition) on which a conventional low-level actor performs focused beam search utilizing a critic agent. Selected candidate actions are evaluated with batched critic forwards and stored in an LRU cache keyed by a quantized state projection and local action identifiers, dramatically reducing redundant critic computation while preserving decision quality via conservative k-hop cache invalidation. Empirically, MetaDOAR attains higher player payoffs than SOTA baselines on large network topologies, without significant scaling issues in terms of memory usage or training time. This contribution provide a practical, theoretically motivated path to efficient hierarchical policy learning for large-scale networked decision problems."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T16:07:01Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    16,
                    7,
                    1,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Michael Lanier"
                    },
                    {
                        "name": "Yevgeniy Vorobeychik"
                    }
                ],
                "author_detail": {
                    "name": "Yevgeniy Vorobeychik"
                },
                "author": "Yevgeniy Vorobeychik"
            },
            {
                "id": "http://arxiv.org/abs/2602.16512v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16512v1",
                "title": "Framework of Thoughts: A Foundation Framework for Dynamic and Optimized Reasoning based on Chains, Trees, and Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Framework of Thoughts: A Foundation Framework for Dynamic and Optimized Reasoning based on Chains, Trees, and Graphs"
                },
                "updated": "2026-02-18T14:58:25Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    14,
                    58,
                    25,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16512v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16512v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Prompting schemes such as Chain of Thought, Tree of Thoughts, and Graph of Thoughts can significantly enhance the reasoning capabilities of large language models. However, most existing schemes require users to define static, problem-specific reasoning structures that lack adaptability to dynamic or unseen problem types. Additionally, these schemes are often under-optimized in terms of hyperparameters, prompts, runtime, and prompting cost. To address these limitations, we introduce Framework of Thoughts (FoT)--a general-purpose foundation framework for building and optimizing dynamic reasoning schemes. FoT comes with built-in features for hyperparameter tuning, prompt optimization, parallel execution, and intelligent caching, unlocking the latent performance potential of reasoning schemes. We demonstrate FoT's capabilities by implementing three popular schemes--Tree of Thoughts, Graph of Thoughts, and ProbTree--within FoT. We empirically show that FoT enables significantly faster execution, reduces costs, and achieves better task scores through optimization. We release our codebase to facilitate the development of future dynamic and efficient reasoning schemes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompting schemes such as Chain of Thought, Tree of Thoughts, and Graph of Thoughts can significantly enhance the reasoning capabilities of large language models. However, most existing schemes require users to define static, problem-specific reasoning structures that lack adaptability to dynamic or unseen problem types. Additionally, these schemes are often under-optimized in terms of hyperparameters, prompts, runtime, and prompting cost. To address these limitations, we introduce Framework of Thoughts (FoT)--a general-purpose foundation framework for building and optimizing dynamic reasoning schemes. FoT comes with built-in features for hyperparameter tuning, prompt optimization, parallel execution, and intelligent caching, unlocking the latent performance potential of reasoning schemes. We demonstrate FoT's capabilities by implementing three popular schemes--Tree of Thoughts, Graph of Thoughts, and ProbTree--within FoT. We empirically show that FoT enables significantly faster execution, reduces costs, and achieves better task scores through optimization. We release our codebase to facilitate the development of future dynamic and efficient reasoning schemes."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T14:58:25Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    14,
                    58,
                    25,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Felix Fricke"
                    },
                    {
                        "name": "Simon Malberg"
                    },
                    {
                        "name": "Georg Groh"
                    }
                ],
                "author_detail": {
                    "name": "Georg Groh"
                },
                "author": "Georg Groh"
            },
            {
                "id": "http://arxiv.org/abs/2602.16284v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16284v1",
                "title": "Fast KV Compaction via Attention Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast KV Compaction via Attention Matching"
                },
                "updated": "2026-02-18T09:06:53Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    9,
                    6,
                    53,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16284v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16284v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Scaling language models to long contexts is often bottlenecked by the size of the key-value (KV) cache. In deployed settings, long contexts are typically managed through compaction in token space via summarization. However, summarization can be highly lossy, substantially harming downstream performance. Recent work on Cartridges has shown that it is possible to train highly compact KV caches in latent space that closely match full-context performance, but at the cost of slow and expensive end-to-end optimization. This work describes an approach for fast context compaction in latent space through Attention Matching, which constructs compact keys and values to reproduce attention outputs and preserve attention mass at a per-KV-head level. We show that this formulation naturally decomposes into simple subproblems, some of which admit efficient closed-form solutions. Within this framework, we develop a family of methods that significantly push the Pareto frontier of compaction time versus quality, achieving up to 50x compaction in seconds on some datasets with little quality loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to long contexts is often bottlenecked by the size of the key-value (KV) cache. In deployed settings, long contexts are typically managed through compaction in token space via summarization. However, summarization can be highly lossy, substantially harming downstream performance. Recent work on Cartridges has shown that it is possible to train highly compact KV caches in latent space that closely match full-context performance, but at the cost of slow and expensive end-to-end optimization. This work describes an approach for fast context compaction in latent space through Attention Matching, which constructs compact keys and values to reproduce attention outputs and preserve attention mass at a per-KV-head level. We show that this formulation naturally decomposes into simple subproblems, some of which admit efficient closed-form solutions. Within this framework, we develop a family of methods that significantly push the Pareto frontier of compaction time versus quality, achieving up to 50x compaction in seconds on some datasets with little quality loss."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T09:06:53Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    9,
                    6,
                    53,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Adam Zweiger"
                    },
                    {
                        "name": "Xinghong Fu"
                    },
                    {
                        "name": "Han Guo"
                    },
                    {
                        "name": "Yoon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Yoon Kim"
                },
                "author": "Yoon Kim"
            },
            {
                "id": "http://arxiv.org/abs/2502.01258v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2502.01258v4",
                "title": "Magnetizing altermagnets by ultrafast asymmetric spin dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Magnetizing altermagnets by ultrafast asymmetric spin dynamics"
                },
                "updated": "2026-02-18T07:41:23Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    7,
                    41,
                    23,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2502.01258v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2502.01258v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Laser pulses are known to induce symmetric demagnetization: equal loss of magnetic moments in the identical sublattices of antiferromagnets and ferromagnets at ultrashort timescales. Using time-dependent density functional theory, we show that linearly polarized laser pulses can drive asymmetric demagnetization between otherwise identical sublattices in the $d$-wave compensated altermagnet (AM) RuO$_2$, resulting in a \\textit{photo-induced ferrimagnetic state} with a strong net magnetization of $\\sim$0.2 $μ_B$ per unit cell. The sign and magnitude of this metastable magnetization are highly controllable by laser polarization. We identify polarization-selective asymmetric optical intersite spin transfer (a-OISTR) as the primary mechanism generating the net moment, followed by asymmetric spin flips (a-SF) that further amplifies it. Both effects originate from the characteristic nodal spin band topology of \\textit{d}-wave AMs. Moreover, we demonstrate that this laser-induced magnetization is universal across various $d$-wave AMs, including experimentally confirmed KV$_2$Se$_2$O and RbV$_2$Te$_2$O. We uncover a robust route to light-controlled magnetization in AMs on ultrafast timescales.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Laser pulses are known to induce symmetric demagnetization: equal loss of magnetic moments in the identical sublattices of antiferromagnets and ferromagnets at ultrashort timescales. Using time-dependent density functional theory, we show that linearly polarized laser pulses can drive asymmetric demagnetization between otherwise identical sublattices in the $d$-wave compensated altermagnet (AM) RuO$_2$, resulting in a \\textit{photo-induced ferrimagnetic state} with a strong net magnetization of $\\sim$0.2 $μ_B$ per unit cell. The sign and magnitude of this metastable magnetization are highly controllable by laser polarization. We identify polarization-selective asymmetric optical intersite spin transfer (a-OISTR) as the primary mechanism generating the net moment, followed by asymmetric spin flips (a-SF) that further amplifies it. Both effects originate from the characteristic nodal spin band topology of \\textit{d}-wave AMs. Moreover, we demonstrate that this laser-induced magnetization is universal across various $d$-wave AMs, including experimentally confirmed KV$_2$Se$_2$O and RbV$_2$Te$_2$O. We uncover a robust route to light-controlled magnetization in AMs on ultrafast timescales."
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-02-03T11:31:12Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    11,
                    31,
                    12,
                    0,
                    34,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci"
                },
                "authors": [
                    {
                        "name": "Zhaobo Zhou"
                    },
                    {
                        "name": "Sangeeta Sharma"
                    },
                    {
                        "name": "John Kay Dewhurst"
                    },
                    {
                        "name": "Junjie He"
                    }
                ],
                "author_detail": {
                    "name": "Junjie He"
                },
                "author": "Junjie He"
            },
            {
                "id": "http://arxiv.org/abs/2602.08242v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.08242v4",
                "title": "Software Testing at the Network Layer: Automated HTTP API Quality Assessment and Security Analysis of Production Web Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software Testing at the Network Layer: Automated HTTP API Quality Assessment and Security Analysis of Production Web Applications"
                },
                "updated": "2026-02-18T04:00:37Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    4,
                    0,
                    37,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.08242v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.08242v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Modern web applications rely heavily on client-side API calls to fetch data, render content, and communicate with backend services. However, the quality of these network interactions (redundant requests, missing cache headers, oversized payloads, and excessive third-party dependencies) is rarely tested in a systematic way. Moreover, many of these quality deficiencies carry security implications: missing cache headers enable cache poisoning, excessive third-party dependencies expand the supply-chain attack surface, and error responses risk leaking server internals. In this study, we present an automated software testing framework that captures and analyzes the complete HTTP traffic of 18 production websites spanning 11 categories (e-commerce, news, government, developer tools, travel, and more). Using automated browser instrumentation via Playwright, we record 108 HAR (HTTP Archive) files across 3 independent runs per page, then apply 8 heuristic-based anti-pattern detectors to produce a composite quality score (0-100) for each site. Our results reveal a wide quality spectrum: minimalist server-rendered sites achieve perfect scores of 100, while content-heavy commercial sites score as low as 56.8. We identify redundant API calls and missing cache headers as the two most pervasive anti-patterns, each affecting 67% of sites, while third-party overhead exceeds 20% on 72% of sites. One utility site makes 2,684 requests per page load, which is 447x more than the most minimal site. To protect site reputations, all identities are anonymized using category-based pseudonyms. We provide all analysis scripts, anonymized results, and reproducibility instructions as an open artifact. This work establishes an empirical baseline for HTTP API call quality across the modern web and offers a reproducible testing framework that researchers and practitioners can apply to their own applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern web applications rely heavily on client-side API calls to fetch data, render content, and communicate with backend services. However, the quality of these network interactions (redundant requests, missing cache headers, oversized payloads, and excessive third-party dependencies) is rarely tested in a systematic way. Moreover, many of these quality deficiencies carry security implications: missing cache headers enable cache poisoning, excessive third-party dependencies expand the supply-chain attack surface, and error responses risk leaking server internals. In this study, we present an automated software testing framework that captures and analyzes the complete HTTP traffic of 18 production websites spanning 11 categories (e-commerce, news, government, developer tools, travel, and more). Using automated browser instrumentation via Playwright, we record 108 HAR (HTTP Archive) files across 3 independent runs per page, then apply 8 heuristic-based anti-pattern detectors to produce a composite quality score (0-100) for each site. Our results reveal a wide quality spectrum: minimalist server-rendered sites achieve perfect scores of 100, while content-heavy commercial sites score as low as 56.8. We identify redundant API calls and missing cache headers as the two most pervasive anti-patterns, each affecting 67% of sites, while third-party overhead exceeds 20% on 72% of sites. One utility site makes 2,684 requests per page load, which is 447x more than the most minimal site. To protect site reputations, all identities are anonymized using category-based pseudonyms. We provide all analysis scripts, anonymized results, and reproducibility instructions as an open artifact. This work establishes an empirical baseline for HTTP API call quality across the modern web and offers a reproducible testing framework that researchers and practitioners can apply to their own applications."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-09T03:39:45Z",
                "published_parsed": [
                    2026,
                    2,
                    9,
                    3,
                    39,
                    45,
                    0,
                    40,
                    0
                ],
                "arxiv_comment": "18+ pages, 5 figures, 3 tables. Code and data: https://github.com/amughalbscs16/network-layer-quality-testing",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Ali Hassaan Mughal"
                    },
                    {
                        "name": "Muhammad Bilal"
                    },
                    {
                        "name": "Noor Fatima"
                    }
                ],
                "author_detail": {
                    "name": "Noor Fatima"
                },
                "author": "Noor Fatima"
            },
            {
                "id": "http://arxiv.org/abs/2601.11837v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.11837v2",
                "title": "High-Voltage Performance Testing in LAr of the PMMA Cathode Connection for the DarkSide-20k Experiment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Voltage Performance Testing in LAr of the PMMA Cathode Connection for the DarkSide-20k Experiment"
                },
                "updated": "2026-02-18T02:02:11Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    2,
                    2,
                    11,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.11837v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.11837v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "DarkSide-20k (DS-20k) is a next-generation dual-phase liquid argon (LAr) time projection chamber (TPC) devoted to the direct-detection of dark matter. The detector is currently under construction in Hall-C at the Laboratori Nazionali del Gran Sasso, Italy, at a depth of approximately 3500 m water equivalent. The detector will instrument 49.7 t of low-radioactivity underground LAr contained within an acrylic TPC and is designed to reach a WIMP-nucleon spin-independent cross-section sensitivity down to $10^{-48}\\,\\mathrm{cm}^{2}$ for a WIMP mass of $0.1\\,\\mathrm{TeV}/c^{2}$ in a 200 tonne-year run. In DS-20k a uniform electric drift field is established in the active volume to transport ionization electrons toward the electroluminescence region, with the required high voltage delivered to the TPC cathode through a custom cable and stress-cone assembly. At the University of California, Davis, a dedicated test setup was developed to reproduce the DS-20k cathode high-voltage connection in LAr, matching the local electric-field conditions. This work summarizes the results of a comprehensive test campaign validating the operation of the DS-20k cathode HV system in LAr up to $-100$ kV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DarkSide-20k (DS-20k) is a next-generation dual-phase liquid argon (LAr) time projection chamber (TPC) devoted to the direct-detection of dark matter. The detector is currently under construction in Hall-C at the Laboratori Nazionali del Gran Sasso, Italy, at a depth of approximately 3500 m water equivalent. The detector will instrument 49.7 t of low-radioactivity underground LAr contained within an acrylic TPC and is designed to reach a WIMP-nucleon spin-independent cross-section sensitivity down to $10^{-48}\\,\\mathrm{cm}^{2}$ for a WIMP mass of $0.1\\,\\mathrm{TeV}/c^{2}$ in a 200 tonne-year run. In DS-20k a uniform electric drift field is established in the active volume to transport ionization electrons toward the electroluminescence region, with the required high voltage delivered to the TPC cathode through a custom cable and stress-cone assembly. At the University of California, Davis, a dedicated test setup was developed to reproduce the DS-20k cathode high-voltage connection in LAr, matching the local electric-field conditions. This work summarizes the results of a comprehensive test campaign validating the operation of the DS-20k cathode HV system in LAr up to $-100$ kV."
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-17T00:03:03Z",
                "published_parsed": [
                    2026,
                    1,
                    17,
                    0,
                    3,
                    3,
                    5,
                    17,
                    0
                ],
                "arxiv_comment": "Proceedings of LIght Detection In Noble Elements - LIDINE 2025",
                "arxiv_primary_category": {
                    "term": "physics.ins-det"
                },
                "authors": [
                    {
                        "name": "Ludovico Luzzi"
                    }
                ],
                "author_detail": {
                    "name": "Ludovico Luzzi"
                },
                "author": "Ludovico Luzzi"
            },
            {
                "id": "http://arxiv.org/abs/2602.16132v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16132v1",
                "title": "CHAI: CacHe Attention Inference for text2video",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CHAI: CacHe Attention Inference for text2video"
                },
                "updated": "2026-02-18T01:53:29Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    1,
                    53,
                    29,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16132v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16132v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Text-to-video diffusion models deliver impressive results but remain slow because of the sequential denoising of 3D latents. Existing approaches to speed up inference either require expensive model retraining or use heuristic-based step skipping, which struggles to maintain video quality as the number of denoising steps decreases. Our work, CHAI, aims to use cross-inference caching to reduce latency while maintaining video quality. We introduce Cache Attention as an effective method for attending to shared objects/scenes across cross-inference latents. This selective attention mechanism enables effective reuse of cached latents across semantically related prompts, yielding high cache hit rates. We show that it is possible to generate high-quality videos using Cache Attention with as few as 8 denoising steps. When integrated into the overall system, CHAI is 1.65x - 3.35x faster than baseline OpenSora 1.2 while maintaining video quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-video diffusion models deliver impressive results but remain slow because of the sequential denoising of 3D latents. Existing approaches to speed up inference either require expensive model retraining or use heuristic-based step skipping, which struggles to maintain video quality as the number of denoising steps decreases. Our work, CHAI, aims to use cross-inference caching to reduce latency while maintaining video quality. We introduce Cache Attention as an effective method for attending to shared objects/scenes across cross-inference latents. This selective attention mechanism enables effective reuse of cached latents across semantically related prompts, yielding high cache hit rates. We show that it is possible to generate high-quality videos using Cache Attention with as few as 8 denoising steps. When integrated into the overall system, CHAI is 1.65x - 3.35x faster than baseline OpenSora 1.2 while maintaining video quality."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T01:53:29Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    1,
                    53,
                    29,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Joel Mathew Cherian"
                    },
                    {
                        "name": "Ashutosh Muralidhara Bharadwaj"
                    },
                    {
                        "name": "Vima Gupta"
                    },
                    {
                        "name": "Anand Padmanabha Iyer"
                    }
                ],
                "author_detail": {
                    "name": "Anand Padmanabha Iyer"
                },
                "author": "Anand Padmanabha Iyer"
            },
            {
                "id": "http://arxiv.org/abs/2602.02958v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.02958v2",
                "title": "Quant VideoGen: Auto-Regressive Long Video Generation via 2-Bit KV-Cache Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quant VideoGen: Auto-Regressive Long Video Generation via 2-Bit KV-Cache Quantization"
                },
                "updated": "2026-02-17T23:49:23Z",
                "updated_parsed": [
                    2026,
                    2,
                    17,
                    23,
                    49,
                    23,
                    1,
                    48,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.02958v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.02958v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Despite rapid progress in autoregressive video diffusion, an emerging system algorithm bottleneck limits both deployability and generation capability: KV cache memory. In autoregressive video generation models, the KV cache grows with generation history and quickly dominates GPU memory, often exceeding 30 GB, preventing deployment on widely available hardware. More critically, constrained KV cache budgets restrict the effective working memory, directly degrading long horizon consistency in identity, layout, and motion. To address this challenge, we present Quant VideoGen (QVG), a training free KV cache quantization framework for autoregressive video diffusion models. QVG leverages video spatiotemporal redundancy through Semantic Aware Smoothing, producing low magnitude, quantization friendly residuals. It further introduces Progressive Residual Quantization, a coarse to fine multi stage scheme that reduces quantization error while enabling a smooth quality memory trade off. Across LongCat Video, HY WorldPlay, and Self Forcing benchmarks, QVG establishes a new Pareto frontier between quality and memory efficiency, reducing KV cache memory by up to 7.0 times with less than 4% end to end latency overhead while consistently outperforming existing baselines in generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite rapid progress in autoregressive video diffusion, an emerging system algorithm bottleneck limits both deployability and generation capability: KV cache memory. In autoregressive video generation models, the KV cache grows with generation history and quickly dominates GPU memory, often exceeding 30 GB, preventing deployment on widely available hardware. More critically, constrained KV cache budgets restrict the effective working memory, directly degrading long horizon consistency in identity, layout, and motion. To address this challenge, we present Quant VideoGen (QVG), a training free KV cache quantization framework for autoregressive video diffusion models. QVG leverages video spatiotemporal redundancy through Semantic Aware Smoothing, producing low magnitude, quantization friendly residuals. It further introduces Progressive Residual Quantization, a coarse to fine multi stage scheme that reduces quantization error while enabling a smooth quality memory trade off. Across LongCat Video, HY WorldPlay, and Self Forcing benchmarks, QVG establishes a new Pareto frontier between quality and memory efficiency, reducing KV cache memory by up to 7.0 times with less than 4% end to end latency overhead while consistently outperforming existing baselines in generation quality."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-03T00:54:32Z",
                "published_parsed": [
                    2026,
                    2,
                    3,
                    0,
                    54,
                    32,
                    1,
                    34,
                    0
                ],
                "arxiv_comment": "11 pages, 7 figures",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Haocheng Xi"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Yilong Zhao"
                    },
                    {
                        "name": "Muyang Li"
                    },
                    {
                        "name": "Han Cai"
                    },
                    {
                        "name": "Xingyang Li"
                    },
                    {
                        "name": "Yujun Lin"
                    },
                    {
                        "name": "Zhuoyang Zhang"
                    },
                    {
                        "name": "Jintao Zhang"
                    },
                    {
                        "name": "Xiuyu Li"
                    },
                    {
                        "name": "Zhiying Xu"
                    },
                    {
                        "name": "Jun Wu"
                    },
                    {
                        "name": "Chenfeng Xu"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Kurt Keutzer"
                    }
                ],
                "author_detail": {
                    "name": "Kurt Keutzer"
                },
                "author": "Kurt Keutzer"
            },
            {
                "id": "http://arxiv.org/abs/2602.16092v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16092v1",
                "title": "Why Any-Order Autoregressive Models Need Two-Stream Attention: A Structural-Semantic Tradeoff",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why Any-Order Autoregressive Models Need Two-Stream Attention: A Structural-Semantic Tradeoff"
                },
                "updated": "2026-02-17T23:39:39Z",
                "updated_parsed": [
                    2026,
                    2,
                    17,
                    23,
                    39,
                    39,
                    1,
                    48,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16092v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16092v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Any-order autoregressive models (AO-ARMs) offer a promising path toward efficient masked diffusion by enabling native key-value caching, but competitive performance has so far required two-stream attention, typically motivated as a means of decoupling token content from position. In this work, we argue that two-stream attention may be serving a more subtle role. We identify a structural-semantic tradeoff in any-order generation: the hidden representation at each step must simultaneously attend to semantically informative tokens for prediction and structurally recent tokens for summarization, objectives that compete for attention capacity in a single stream but can specialize across two streams. To isolate this tradeoff from position-content separation, we propose Decoupled RoPE, a modification to rotary position embeddings that provides target position information without revealing target content. Decoupled RoPE performs competitively at short sequence lengths--where semantic and structural proximity coincide--but degrades as sequence length increases and the two orderings diverge. These results suggest that the success of two-stream attention stems not merely from separating position from content, but from circumventing the deeper structural-semantic tradeoff inherent to any-order generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Any-order autoregressive models (AO-ARMs) offer a promising path toward efficient masked diffusion by enabling native key-value caching, but competitive performance has so far required two-stream attention, typically motivated as a means of decoupling token content from position. In this work, we argue that two-stream attention may be serving a more subtle role. We identify a structural-semantic tradeoff in any-order generation: the hidden representation at each step must simultaneously attend to semantically informative tokens for prediction and structurally recent tokens for summarization, objectives that compete for attention capacity in a single stream but can specialize across two streams. To isolate this tradeoff from position-content separation, we propose Decoupled RoPE, a modification to rotary position embeddings that provides target position information without revealing target content. Decoupled RoPE performs competitively at short sequence lengths--where semantic and structural proximity coincide--but degrades as sequence length increases and the two orderings diverge. These results suggest that the success of two-stream attention stems not merely from separating position from content, but from circumventing the deeper structural-semantic tradeoff inherent to any-order generation."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-17T23:39:39Z",
                "published_parsed": [
                    2026,
                    2,
                    17,
                    23,
                    39,
                    39,
                    1,
                    48,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Patrick Pynadath"
                    },
                    {
                        "name": "Ruqi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ruqi Zhang"
                },
                "author": "Ruqi Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2602.16054v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16054v1",
                "title": "CLAA: Cross-Layer Attention Aggregation for Accelerating LLM Prefill",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLAA: Cross-Layer Attention Aggregation for Accelerating LLM Prefill"
                },
                "updated": "2026-02-17T22:08:16Z",
                "updated_parsed": [
                    2026,
                    2,
                    17,
                    22,
                    8,
                    16,
                    1,
                    48,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16054v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16054v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The prefill stage in long-context LLM inference remains a computational bottleneck. Recent token-ranking heuristics accelerate inference by selectively processing a subset of semantically relevant tokens. However, existing methods suffer from unstable token importance estimation, often varying between layers. Evaluating token-ranking quality independently from heuristic-specific architectures is challenging. To address this, we introduce an Answer-Informed Oracle, which defines ground-truth token importance by measuring attention from generated answers back to the prompt. This oracle reveals that existing heuristics exhibit high variance across layers: rankings can degrade sharply at specific layers, a failure mode invisible to end-to-end benchmarks. The diagnosis suggests a simple fix: aggregate scores across layers rather than relying on any single one. We implement this as Cross-Layer Attention Aggregation (CLAA), which closes the gap to the oracle upper bound and reduces Time-to-First-Token (TTFT) by up to 39\\% compared to the Full KV Cache baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The prefill stage in long-context LLM inference remains a computational bottleneck. Recent token-ranking heuristics accelerate inference by selectively processing a subset of semantically relevant tokens. However, existing methods suffer from unstable token importance estimation, often varying between layers. Evaluating token-ranking quality independently from heuristic-specific architectures is challenging. To address this, we introduce an Answer-Informed Oracle, which defines ground-truth token importance by measuring attention from generated answers back to the prompt. This oracle reveals that existing heuristics exhibit high variance across layers: rankings can degrade sharply at specific layers, a failure mode invisible to end-to-end benchmarks. The diagnosis suggests a simple fix: aggregate scores across layers rather than relying on any single one. We implement this as Cross-Layer Attention Aggregation (CLAA), which closes the gap to the oracle upper bound and reduces Time-to-First-Token (TTFT) by up to 39\\% compared to the Full KV Cache baseline."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-17T22:08:16Z",
                "published_parsed": [
                    2026,
                    2,
                    17,
                    22,
                    8,
                    16,
                    1,
                    48,
                    0
                ],
                "arxiv_comment": "15 pages, 8 figures",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Bradley McDanel"
                    },
                    {
                        "name": "Steven Li"
                    },
                    {
                        "name": "Harshit Khaitan"
                    }
                ],
                "author_detail": {
                    "name": "Harshit Khaitan"
                },
                "author": "Harshit Khaitan"
            },
            {
                "id": "http://arxiv.org/abs/2601.15311v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.15311v3",
                "title": "Aeon: High-Performance Neuro-Symbolic Memory Management for Long-Horizon LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aeon: High-Performance Neuro-Symbolic Memory Management for Long-Horizon LLM Agents"
                },
                "updated": "2026-02-17T15:21:45Z",
                "updated_parsed": [
                    2026,
                    2,
                    17,
                    15,
                    21,
                    45,
                    1,
                    48,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.15311v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.15311v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) are fundamentally constrained by the quadratic computational cost of self-attention and the \"Lost in the Middle\" phenomenon, where reasoning capabilities degrade as context windows expand. Existing solutions, primarily \"Flat RAG\" architectures relying on vector databases, treat memory as an unstructured bag of embeddings, failing to capture the hierarchical and temporal structure of long-horizon interactions. This paper presents Aeon, a Neuro-Symbolic Cognitive Operating System that redefines memory as a managed OS resource. Aeon structures memory into a Memory Palace (a spatial index implemented via Atlas, a SIMD-accelerated Page-Clustered Vector Index) and a Trace (a neuro-symbolic episodic graph). This architecture introduces three advances: (1) Symmetric INT8 Scalar Quantization, achieving 3.1x spatial compression and 5.6x math acceleration via NEON SDOT intrinsics; (2) a decoupled Write-Ahead Log (WAL) ensuring crash-recoverability with statistically negligible overhead (<1%); and (3) a Sidecar Blob Arena eliminating the prior 440-character text ceiling via an append-only mmap-backed blob file with generational garbage collection. The Semantic Lookaside Buffer (SLB) exploits conversational locality to achieve sub-5us retrieval latencies, with INT8 vectors dequantized to FP32 on cache insertion to preserve L1-resident lookup performance. Benchmarks on Apple M4 Max demonstrate that the combined architecture achieves 4.70ns INT8 dot product latency, 3.09us tree traversal at 100K nodes (3.4x over FP32), and P99 read latency of 750ns under hostile 16-thread contention via epoch-based reclamation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are fundamentally constrained by the quadratic computational cost of self-attention and the \"Lost in the Middle\" phenomenon, where reasoning capabilities degrade as context windows expand. Existing solutions, primarily \"Flat RAG\" architectures relying on vector databases, treat memory as an unstructured bag of embeddings, failing to capture the hierarchical and temporal structure of long-horizon interactions. This paper presents Aeon, a Neuro-Symbolic Cognitive Operating System that redefines memory as a managed OS resource. Aeon structures memory into a Memory Palace (a spatial index implemented via Atlas, a SIMD-accelerated Page-Clustered Vector Index) and a Trace (a neuro-symbolic episodic graph). This architecture introduces three advances: (1) Symmetric INT8 Scalar Quantization, achieving 3.1x spatial compression and 5.6x math acceleration via NEON SDOT intrinsics; (2) a decoupled Write-Ahead Log (WAL) ensuring crash-recoverability with statistically negligible overhead (<1%); and (3) a Sidecar Blob Arena eliminating the prior 440-character text ceiling via an append-only mmap-backed blob file with generational garbage collection. The Semantic Lookaside Buffer (SLB) exploits conversational locality to achieve sub-5us retrieval latencies, with INT8 vectors dequantized to FP32 on cache insertion to preserve L1-resident lookup performance. Benchmarks on Apple M4 Max demonstrate that the combined architecture achieves 4.70ns INT8 dot product latency, 3.09us tree traversal at 100K nodes (3.4x over FP32), and P99 read latency of 750ns under hostile 16-thread contention via epoch-based reclamation."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-14T15:23:22Z",
                "published_parsed": [
                    2026,
                    1,
                    14,
                    15,
                    23,
                    22,
                    2,
                    14,
                    0
                ],
                "arxiv_comment": "v3: Production hardening. Added INT8 quantization (5.6x dot product speedup, 3.1x compression), crash recovery via decoupled WAL (<1% overhead), unlimited text storage via sidecar blob arena with generational GC, and epoch-based reclamation for lock-free reads (P99 750ns under 16-thread contention). Revised for systems engineering clarity",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Mustafa Arslan"
                    }
                ],
                "author_detail": {
                    "name": "Mustafa Arslan"
                },
                "author": "Mustafa Arslan"
            },
            {
                "id": "http://arxiv.org/abs/2408.00539v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2408.00539v2",
                "title": "Intermittent Semi-Working Mask: A New Masking Paradigm for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intermittent Semi-Working Mask: A New Masking Paradigm for LLMs"
                },
                "updated": "2026-02-17T13:11:05Z",
                "updated_parsed": [
                    2026,
                    2,
                    17,
                    13,
                    11,
                    5,
                    1,
                    48,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2408.00539v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2408.00539v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Multi-turn dialogues and context-intensive tasks challenge Large Language Models (LLMs) to integrate long histories without sacrificing generation quality. Although prefix LLMs can better exploit historical context via bidirectional attention on prefix tokens, they are rarely used in practice because multi-turn training requires many duplicated triplets, and its bidirectional prefix prevents KV-cache reuse at inference time, driving up high cost and latency. To retain the contextual understanding of prefix mask while preserving the inference-time efficiency of causal mask, we introduce Intermittent Semi-working Mask (ISM), a masking scheme that injects sparse bidirectional attention into the causal backbone. ISM alternates bidirectional attention over query segments with unidirectional attention over answer segments, enabling the synthesis of in-context while preserving global causality. This design eliminates triplet expansion during training and maintains KV-cache reuse during inference, yielding latency comparable to standard causal LLMs. ISM is architecture-agnostic and parameter-free, adding only minimal latency. Across extensive evaluations, ISM outperforms causal baselines not only on multi-turn dialogue, but also on context-intensive tasks like mathematical reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-turn dialogues and context-intensive tasks challenge Large Language Models (LLMs) to integrate long histories without sacrificing generation quality. Although prefix LLMs can better exploit historical context via bidirectional attention on prefix tokens, they are rarely used in practice because multi-turn training requires many duplicated triplets, and its bidirectional prefix prevents KV-cache reuse at inference time, driving up high cost and latency. To retain the contextual understanding of prefix mask while preserving the inference-time efficiency of causal mask, we introduce Intermittent Semi-working Mask (ISM), a masking scheme that injects sparse bidirectional attention into the causal backbone. ISM alternates bidirectional attention over query segments with unidirectional attention over answer segments, enabling the synthesis of in-context while preserving global causality. This design eliminates triplet expansion during training and maintains KV-cache reuse during inference, yielding latency comparable to standard causal LLMs. ISM is architecture-agnostic and parameter-free, adding only minimal latency. Across extensive evaluations, ISM outperforms causal baselines not only on multi-turn dialogue, but also on context-intensive tasks like mathematical reasoning."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-08-01T13:22:01Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    22,
                    1,
                    3,
                    214,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "HaoYuan Hu"
                    },
                    {
                        "name": "Mingcong Lu"
                    },
                    {
                        "name": "Di Luo"
                    },
                    {
                        "name": "XinYa Wu"
                    },
                    {
                        "name": "Jiangcai Zhu"
                    },
                    {
                        "name": "Taoye Yin"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Shusheng Zhang"
                    },
                    {
                        "name": "KeZun Zhang"
                    },
                    {
                        "name": "KaiLai Shao"
                    },
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Feng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Feng Wang"
                },
                "author": "Feng Wang"
            },
            {
                "id": "http://arxiv.org/abs/2507.01110v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.01110v4",
                "title": "A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale Reconstruction with External Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale Reconstruction with External Memory"
                },
                "updated": "2026-02-17T12:40:07Z",
                "updated_parsed": [
                    2026,
                    2,
                    17,
                    12,
                    40,
                    7,
                    1,
                    48,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.01110v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.01110v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Gaussian Splatting has emerged as a high-performance technique for novel view synthesis, enabling real-time rendering and high-quality reconstruction of small scenes. However, scaling to larger environments has so far relied on partitioning the scene into chunks -- a strategy that introduces artifacts at chunk boundaries, complicates training across varying scales, and is poorly suited to unstructured scenarios such as city-scale flyovers combined with street-level views. Moreover, rendering remains fundamentally limited by GPU memory, as all visible chunks must reside in VRAM simultaneously. We introduce A LoD of Gaussians, a framework for training and rendering ultra-large-scale Gaussian scenes on a single consumer-grade GPU -- without partitioning. Our method stores the full scene out-of-core (e.g., in CPU memory) and trains a Level-of-Detail (LoD) representation directly, dynamically streaming only the relevant Gaussians. A hybrid data structure combining Gaussian hierarchies with Sequential Point Trees enables efficient, view-dependent LoD selection, while a lightweight caching and view scheduling system exploits temporal coherence to support real-time streaming and rendering. Together, these innovations enable seamless multi-scale reconstruction and interactive visualization of complex scenes -- from broad aerial views to fine-grained ground-level details.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian Splatting has emerged as a high-performance technique for novel view synthesis, enabling real-time rendering and high-quality reconstruction of small scenes. However, scaling to larger environments has so far relied on partitioning the scene into chunks -- a strategy that introduces artifacts at chunk boundaries, complicates training across varying scales, and is poorly suited to unstructured scenarios such as city-scale flyovers combined with street-level views. Moreover, rendering remains fundamentally limited by GPU memory, as all visible chunks must reside in VRAM simultaneously. We introduce A LoD of Gaussians, a framework for training and rendering ultra-large-scale Gaussian scenes on a single consumer-grade GPU -- without partitioning. Our method stores the full scene out-of-core (e.g., in CPU memory) and trains a Level-of-Detail (LoD) representation directly, dynamically streaming only the relevant Gaussians. A hybrid data structure combining Gaussian hierarchies with Sequential Point Trees enables efficient, view-dependent LoD selection, while a lightweight caching and view scheduling system exploits temporal coherence to support real-time streaming and rendering. Together, these innovations enable seamless multi-scale reconstruction and interactive visualization of complex scenes -- from broad aerial views to fine-grained ground-level details."
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-01T18:12:43Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    18,
                    12,
                    43,
                    1,
                    182,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR"
                },
                "authors": [
                    {
                        "name": "Felix Windisch"
                    },
                    {
                        "name": "Thomas Köhler"
                    },
                    {
                        "name": "Lukas Radl"
                    },
                    {
                        "name": "Mattia D'Urso"
                    },
                    {
                        "name": "Michael Steiner"
                    },
                    {
                        "name": "Dieter Schmalstieg"
                    },
                    {
                        "name": "Markus Steinberger"
                    }
                ],
                "author_detail": {
                    "name": "Markus Steinberger"
                },
                "author": "Markus Steinberger"
            },
            {
                "id": "http://arxiv.org/abs/2602.02108v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.02108v3",
                "title": "Out of the Memory Barrier: A Highly Memory Efficient Training System for LLMs with Million-Token Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Out of the Memory Barrier: A Highly Memory Efficient Training System for LLMs with Million-Token Contexts"
                },
                "updated": "2026-02-17T11:41:00Z",
                "updated_parsed": [
                    2026,
                    2,
                    17,
                    11,
                    41,
                    0,
                    1,
                    48,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.02108v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.02108v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Training Large Language Models (LLMs) on long contexts is severely constrained by prohibitive GPU memory overhead, not training time. The primary culprits are the activations, whose memory footprints scale linearly with sequence length. We introduce OOMB, a highly memory-efficient training system that directly confronts this barrier. Our approach employs a chunk-recurrent training framework with on-the-fly activation recomputation, which maintains a constant activation memory footprint (O(1)) and shifts the primary bottleneck to the growing KV cache. To manage the KV cache, OOMB integrates a suite of synergistic optimizations: a paged memory manager for both the KV cache and its gradients to eliminate fragmentation, asynchronous CPU offloading to hide data transfer latency, and page-level sparse attention to reduce both computational complexity and communication overhead. The synergy of these techniques yields exceptional efficiency. Our empirical results show that for every additional 10K tokens of context, the end-to-end training memory overhead increases by a mere 10MB for Qwen2.5-7B. This allows training Qwen2.5-7B with a 4M-token context on a single H200 GPU, a feat that would otherwise require a large cluster using context parallelism. This work represents a substantial advance in resource efficiency for long-context LLM training. The source code is available at https://github.com/wenhaoli-xmu/OOMB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Large Language Models (LLMs) on long contexts is severely constrained by prohibitive GPU memory overhead, not training time. The primary culprits are the activations, whose memory footprints scale linearly with sequence length. We introduce OOMB, a highly memory-efficient training system that directly confronts this barrier. Our approach employs a chunk-recurrent training framework with on-the-fly activation recomputation, which maintains a constant activation memory footprint (O(1)) and shifts the primary bottleneck to the growing KV cache. To manage the KV cache, OOMB integrates a suite of synergistic optimizations: a paged memory manager for both the KV cache and its gradients to eliminate fragmentation, asynchronous CPU offloading to hide data transfer latency, and page-level sparse attention to reduce both computational complexity and communication overhead. The synergy of these techniques yields exceptional efficiency. Our empirical results show that for every additional 10K tokens of context, the end-to-end training memory overhead increases by a mere 10MB for Qwen2.5-7B. This allows training Qwen2.5-7B with a 4M-token context on a single H200 GPU, a feat that would otherwise require a large cluster using context parallelism. This work represents a substantial advance in resource efficiency for long-context LLM training. The source code is available at https://github.com/wenhaoli-xmu/OOMB."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-02T13:52:40Z",
                "published_parsed": [
                    2026,
                    2,
                    2,
                    13,
                    52,
                    40,
                    0,
                    33,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Daohai Yu"
                    },
                    {
                        "name": "Gen Luo"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Fei Chao"
                    },
                    {
                        "name": "Rongrong Ji"
                    },
                    {
                        "name": "Yifan Wu"
                    },
                    {
                        "name": "Jiaxin Liu"
                    },
                    {
                        "name": "Ziyang Gong"
                    },
                    {
                        "name": "Zimu Liao"
                    }
                ],
                "author_detail": {
                    "name": "Zimu Liao"
                },
                "author": "Zimu Liao"
            },
            {
                "id": "http://arxiv.org/abs/2505.11695v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.11695v3",
                "title": "Qronos: Correcting the Past by Shaping the Future... in Post-Training Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qronos: Correcting the Past by Shaping the Future... in Post-Training Quantization"
                },
                "updated": "2026-02-17T05:04:13Z",
                "updated_parsed": [
                    2026,
                    2,
                    17,
                    5,
                    4,
                    13,
                    1,
                    48,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.11695v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.11695v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce Qronos -- a new state-of-the-art post-training quantization algorithm that sequentially rounds and updates neural network weights. Qronos not only explicitly corrects errors due to both weight and activation quantization, but also errors resulting from quantizing previous layers. Our iterative algorithm is based on an interpretable and disciplined optimization framework that subsumes and surpasses existing data-driven approaches. At each step, Qronos alternates between error correction and diffusion via optimal update rules. Importantly, we prove that Qronos admits an efficient implementation that uses the Cholesky decomposition for solving least-squares problems. We also demonstrate that Qronos is compatible with existing transformation techniques such as Hadamard-based incoherence processing and weight-activation scaling equalization, among others. We evaluate Qronos using recent autoregressive language generation models in the Llama3 family; Qronos consistently outperforms previous state-of-the-art adaptive rounding methods when quantizing the weights, activations, and/or KV caches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Qronos -- a new state-of-the-art post-training quantization algorithm that sequentially rounds and updates neural network weights. Qronos not only explicitly corrects errors due to both weight and activation quantization, but also errors resulting from quantizing previous layers. Our iterative algorithm is based on an interpretable and disciplined optimization framework that subsumes and surpasses existing data-driven approaches. At each step, Qronos alternates between error correction and diffusion via optimal update rules. Importantly, we prove that Qronos admits an efficient implementation that uses the Cholesky decomposition for solving least-squares problems. We also demonstrate that Qronos is compatible with existing transformation techniques such as Hadamard-based incoherence processing and weight-activation scaling equalization, among others. We evaluate Qronos using recent autoregressive language generation models in the Llama3 family; Qronos consistently outperforms previous state-of-the-art adaptive rounding methods when quantizing the weights, activations, and/or KV caches."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-16T21:04:25Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    21,
                    4,
                    25,
                    4,
                    136,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Shihao Zhang"
                    },
                    {
                        "name": "Haoyu Zhang"
                    },
                    {
                        "name": "Ian Colbert"
                    },
                    {
                        "name": "Rayan Saab"
                    }
                ],
                "author_detail": {
                    "name": "Rayan Saab"
                },
                "author": "Rayan Saab"
            },
            {
                "id": "http://arxiv.org/abs/2602.15318v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.15318v1",
                "title": "Sparrow: Text-Anchored Window Attention with Visual-Semantic Glimpsing for Speculative Decoding in Video LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparrow: Text-Anchored Window Attention with Visual-Semantic Glimpsing for Speculative Decoding in Video LLMs"
                },
                "updated": "2026-02-17T02:51:36Z",
                "updated_parsed": [
                    2026,
                    2,
                    17,
                    2,
                    51,
                    36,
                    1,
                    48,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.15318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.15318v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Although speculative decoding is widely used to accelerate Vision-Language Models (VLMs) inference, it faces severe performance collapse when applied to Video Large Language Models (Vid-LLMs). The draft model typically falls into the trap of attention dilution and negative visual gain due to key-value cache explosion and context window mismatches. We observe a visual semantic internalization phenomenon in Vid-LLMs, indicating that critical visual semantics are implicitly encoded into text hidden states during deep-layer interactions, which renders raw visual inputs structurally redundant during deep inference. To address this, we propose the Sparrow framework, which first utilizes visually-aware text-anchored window attention via hidden state reuse to fully offload visual computation to the target model, and leverages intermediate-layer visual state bridging to train the draft model with semantic-rich intermediate states, thereby filtering out low-level visual noise. Additionally, a multi-token prediction strategy is introduced to bridge the training-inference distribution shift. Experiments show that Sparrow achieves an average speedup of 2.82x even with 25k visual tokens, effectively resolving the performance degradation in long sequences and offering a practical solution for real-time long video tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although speculative decoding is widely used to accelerate Vision-Language Models (VLMs) inference, it faces severe performance collapse when applied to Video Large Language Models (Vid-LLMs). The draft model typically falls into the trap of attention dilution and negative visual gain due to key-value cache explosion and context window mismatches. We observe a visual semantic internalization phenomenon in Vid-LLMs, indicating that critical visual semantics are implicitly encoded into text hidden states during deep-layer interactions, which renders raw visual inputs structurally redundant during deep inference. To address this, we propose the Sparrow framework, which first utilizes visually-aware text-anchored window attention via hidden state reuse to fully offload visual computation to the target model, and leverages intermediate-layer visual state bridging to train the draft model with semantic-rich intermediate states, thereby filtering out low-level visual noise. Additionally, a multi-token prediction strategy is introduced to bridge the training-inference distribution shift. Experiments show that Sparrow achieves an average speedup of 2.82x even with 25k visual tokens, effectively resolving the performance degradation in long sequences and offering a practical solution for real-time long video tasks."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-17T02:51:36Z",
                "published_parsed": [
                    2026,
                    2,
                    17,
                    2,
                    51,
                    36,
                    1,
                    48,
                    0
                ],
                "arxiv_comment": "15 pages , 6 figures",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Libo Zhang"
                    },
                    {
                        "name": "Zhaoning Zhang"
                    },
                    {
                        "name": "Wangyang Hong"
                    },
                    {
                        "name": "Peng Qiao"
                    },
                    {
                        "name": "Dongsheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Dongsheng Li"
                },
                "author": "Dongsheng Li"
            },
            {
                "id": "http://arxiv.org/abs/2602.14492v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.14492v2",
                "title": "Query as Anchor: Scenario-Adaptive User Representation via Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query as Anchor: Scenario-Adaptive User Representation via Large Language Model"
                },
                "updated": "2026-02-17T02:44:08Z",
                "updated_parsed": [
                    2026,
                    2,
                    17,
                    2,
                    44,
                    8,
                    1,
                    48,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.14492v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.14492v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Industrial-scale user representation learning requires balancing robust universality with acute task-sensitivity. However, existing paradigms primarily yield static, task-agnostic embeddings that struggle to reconcile the divergent requirements of downstream scenarios within unified vector spaces. Furthermore, heterogeneous multi-source data introduces inherent noise and modality conflicts, degrading representation. We propose Query-as-Anchor, a framework shifting user modeling from static encoding to dynamic, query-aware synthesis. To empower Large Language Models (LLMs) with deep user understanding, we first construct UserU, an industrial-scale pre-training dataset that aligns multi-modal behavioral sequences with user understanding semantics, and our Q-Anchor Embedding architecture integrates hierarchical coarse-to-fine encoders into dual-tower LLMs via joint contrastive-autoregressive optimization for query-aware user representation. To bridge the gap between general pre-training and specialized business logic, we further introduce Cluster-based Soft Prompt Tuning to enforce discriminative latent structures, effectively aligning model attention with scenario-specific modalities. For deployment, anchoring queries at sequence termini enables KV-cache-accelerated inference with negligible incremental latency. Evaluations on 10 Alipay industrial benchmarks show consistent SOTA performance, strong scalability, and efficient deployment. Large-scale online A/B testing in Alipay's production system across two real-world scenarios further validates its practical effectiveness. Our code is prepared for public release and will be available at: https://github.com/JhCircle/Q-Anchor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Industrial-scale user representation learning requires balancing robust universality with acute task-sensitivity. However, existing paradigms primarily yield static, task-agnostic embeddings that struggle to reconcile the divergent requirements of downstream scenarios within unified vector spaces. Furthermore, heterogeneous multi-source data introduces inherent noise and modality conflicts, degrading representation. We propose Query-as-Anchor, a framework shifting user modeling from static encoding to dynamic, query-aware synthesis. To empower Large Language Models (LLMs) with deep user understanding, we first construct UserU, an industrial-scale pre-training dataset that aligns multi-modal behavioral sequences with user understanding semantics, and our Q-Anchor Embedding architecture integrates hierarchical coarse-to-fine encoders into dual-tower LLMs via joint contrastive-autoregressive optimization for query-aware user representation. To bridge the gap between general pre-training and specialized business logic, we further introduce Cluster-based Soft Prompt Tuning to enforce discriminative latent structures, effectively aligning model attention with scenario-specific modalities. For deployment, anchoring queries at sequence termini enables KV-cache-accelerated inference with negligible incremental latency. Evaluations on 10 Alipay industrial benchmarks show consistent SOTA performance, strong scalability, and efficient deployment. Large-scale online A/B testing in Alipay's production system across two real-world scenarios further validates its practical effectiveness. Our code is prepared for public release and will be available at: https://github.com/JhCircle/Q-Anchor."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-16T06:09:31Z",
                "published_parsed": [
                    2026,
                    2,
                    16,
                    6,
                    9,
                    31,
                    0,
                    47,
                    0
                ],
                "arxiv_comment": "15 pages, 12 figures",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Jiahao Yuan"
                    },
                    {
                        "name": "Yike Xu"
                    },
                    {
                        "name": "Jinyong Wen"
                    },
                    {
                        "name": "Baokun Wang"
                    },
                    {
                        "name": "Ziyi Gao"
                    },
                    {
                        "name": "Xiaotong Lin"
                    },
                    {
                        "name": "Yun Liu"
                    },
                    {
                        "name": "Xing Fu"
                    },
                    {
                        "name": "Yu Cheng"
                    },
                    {
                        "name": "Yongchao Liu"
                    },
                    {
                        "name": "Weiqiang Wang"
                    },
                    {
                        "name": "Zhongle Xie"
                    }
                ],
                "author_detail": {
                    "name": "Zhongle Xie"
                },
                "author": "Zhongle Xie"
            },
            {
                "id": "http://arxiv.org/abs/2602.01872v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.01872v2",
                "title": "Grappa: Gradient-Only Communication for Scalable Graph Neural Network Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grappa: Gradient-Only Communication for Scalable Graph Neural Network Training"
                },
                "updated": "2026-02-16T21:24:43Z",
                "updated_parsed": [
                    2026,
                    2,
                    16,
                    21,
                    24,
                    43,
                    0,
                    47,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.01872v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.01872v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Cross-partition edges dominate the cost of distributed GNN training: fetching remote features and activations per iteration overwhelms the network as graphs deepen and partition counts grow. Grappa is a distributed GNN training framework that enforces gradient-only communication: during each iteration, partitions train in isolation and exchange only gradients for the global update. To recover accuracy lost to isolation, Grappa (i) periodically repartitions to expose new neighborhoods and (ii) applies a lightweight coverage-corrected gradient aggregation inspired by importance sampling. We present an asymptotically unbiased estimator for gradient correction, which we use to develop a minimum-distance batch-level variant that is compatible with common deep-learning packages. We also introduce a shrinkage version that improves stability in practice. Empirical results on real and synthetic graphs show that Grappa trains GNNs 4x faster on average (up to 13x) than state-of-the-art systems, achieves better accuracy especially for deeper models, and sustains training at the trillion-edge scale on commodity hardware. Grappa is model-agnostic, supports full-graph and mini-batch training, and does not rely on high-bandwidth interconnects or caching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-partition edges dominate the cost of distributed GNN training: fetching remote features and activations per iteration overwhelms the network as graphs deepen and partition counts grow. Grappa is a distributed GNN training framework that enforces gradient-only communication: during each iteration, partitions train in isolation and exchange only gradients for the global update. To recover accuracy lost to isolation, Grappa (i) periodically repartitions to expose new neighborhoods and (ii) applies a lightweight coverage-corrected gradient aggregation inspired by importance sampling. We present an asymptotically unbiased estimator for gradient correction, which we use to develop a minimum-distance batch-level variant that is compatible with common deep-learning packages. We also introduce a shrinkage version that improves stability in practice. Empirical results on real and synthetic graphs show that Grappa trains GNNs 4x faster on average (up to 13x) than state-of-the-art systems, achieves better accuracy especially for deeper models, and sustains training at the trillion-edge scale on commodity hardware. Grappa is model-agnostic, supports full-graph and mini-batch training, and does not rely on high-bandwidth interconnects or caching."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-02T09:44:12Z",
                "published_parsed": [
                    2026,
                    2,
                    2,
                    9,
                    44,
                    12,
                    0,
                    33,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Chongyang Xu"
                    },
                    {
                        "name": "Christoph Siebenbrunner"
                    },
                    {
                        "name": "Laurent Bindschaedler"
                    }
                ],
                "author_detail": {
                    "name": "Laurent Bindschaedler"
                },
                "author": "Laurent Bindschaedler"
            },
            {
                "id": "http://arxiv.org/abs/2602.14934v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.14934v1",
                "title": "Activation-Space Uncertainty Quantification for Pretrained Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation-Space Uncertainty Quantification for Pretrained Networks"
                },
                "updated": "2026-02-16T17:17:08Z",
                "updated_parsed": [
                    2026,
                    2,
                    16,
                    17,
                    17,
                    8,
                    0,
                    47,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.14934v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.14934v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Reliable uncertainty estimates are crucial for deploying pretrained models; yet, many strong methods for quantifying uncertainty require retraining, Monte Carlo sampling, or expensive second-order computations and may alter a frozen backbone's predictions. To address this, we introduce Gaussian Process Activations (GAPA), a post-hoc method that shifts Bayesian modeling from weights to activations. GAPA replaces standard nonlinearities with Gaussian-process activations whose posterior mean exactly matches the original activation, preserving the backbone's point predictions by construction while providing closed-form epistemic variances in activation space. To scale to modern architectures, we use a sparse variational inducing-point approximation over cached training activations, combined with local k-nearest-neighbor subset conditioning, enabling deterministic single-pass uncertainty propagation without sampling, backpropagation, or second-order information. Across regression, classification, image segmentation, and language modeling, GAPA matches or outperforms strong post-hoc baselines in calibration and out-of-distribution detection while remaining efficient at test time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable uncertainty estimates are crucial for deploying pretrained models; yet, many strong methods for quantifying uncertainty require retraining, Monte Carlo sampling, or expensive second-order computations and may alter a frozen backbone's predictions. To address this, we introduce Gaussian Process Activations (GAPA), a post-hoc method that shifts Bayesian modeling from weights to activations. GAPA replaces standard nonlinearities with Gaussian-process activations whose posterior mean exactly matches the original activation, preserving the backbone's point predictions by construction while providing closed-form epistemic variances in activation space. To scale to modern architectures, we use a sparse variational inducing-point approximation over cached training activations, combined with local k-nearest-neighbor subset conditioning, enabling deterministic single-pass uncertainty propagation without sampling, backpropagation, or second-order information. Across regression, classification, image segmentation, and language modeling, GAPA matches or outperforms strong post-hoc baselines in calibration and out-of-distribution detection while remaining efficient at test time."
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-16T17:17:08Z",
                "published_parsed": [
                    2026,
                    2,
                    16,
                    17,
                    17,
                    8,
                    0,
                    47,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML"
                },
                "authors": [
                    {
                        "name": "Richard Bergna"
                    },
                    {
                        "name": "Stefan Depeweg"
                    },
                    {
                        "name": "Sergio Calvo-Ordoñez"
                    },
                    {
                        "name": "Jonathan Plenk"
                    },
                    {
                        "name": "Alvaro Cartea"
                    },
                    {
                        "name": "Jose Miguel Hernández-Lobato"
                    }
                ],
                "author_detail": {
                    "name": "Jose Miguel Hernández-Lobato"
                },
                "author": "Jose Miguel Hernández-Lobato"
            },
            {
                "id": "http://arxiv.org/abs/2602.14683v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.14683v1",
                "title": "Joint Majorization-Minimization for Nonnegative CP and Tucker Decompositions under $β$-Divergences: Unfolding-Free Updates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Majorization-Minimization for Nonnegative CP and Tucker Decompositions under $β$-Divergences: Unfolding-Free Updates"
                },
                "updated": "2026-02-16T12:16:01Z",
                "updated_parsed": [
                    2026,
                    2,
                    16,
                    12,
                    16,
                    1,
                    0,
                    47,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.14683v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.14683v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We study majorization-minimization methods for nonnegative tensor decompositions under the $β$-divergence family, focusing on nonnegative CP and Tucker models. Our aim is to avoid explicit mode unfoldings and large auxiliary matrices by deriving separable surrogates whose multiplicative updates can be implemented using only tensor contractions (einsum-style operations). We present both classical block-MM updates in contraction-only form and a joint majorization strategy, inspired by joint MM for matrix $β$-NMF, that reuses cached reference quantities across inexpensive inner updates. We prove tightness of the proposed majorizers, establish monotonic decrease of the objective, and show convergence of the sequence of objective values; we also discuss how BSUM theory applies to the block-MM scheme for analyzing limit points. Finally, experiments on synthetic tensors and the Uber spatiotemporal count tensor demonstrate substantial speedups over unfolding-based baselines and a recent einsum-factorization framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study majorization-minimization methods for nonnegative tensor decompositions under the $β$-divergence family, focusing on nonnegative CP and Tucker models. Our aim is to avoid explicit mode unfoldings and large auxiliary matrices by deriving separable surrogates whose multiplicative updates can be implemented using only tensor contractions (einsum-style operations). We present both classical block-MM updates in contraction-only form and a joint majorization strategy, inspired by joint MM for matrix $β$-NMF, that reuses cached reference quantities across inexpensive inner updates. We prove tightness of the proposed majorizers, establish monotonic decrease of the objective, and show convergence of the sequence of objective values; we also discuss how BSUM theory applies to the block-MM scheme for analyzing limit points. Finally, experiments on synthetic tensors and the Uber spatiotemporal count tensor demonstrate substantial speedups over unfolding-based baselines and a recent einsum-factorization framework."
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-16T12:16:01Z",
                "published_parsed": [
                    2026,
                    2,
                    16,
                    12,
                    16,
                    1,
                    0,
                    47,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "math.OC"
                },
                "authors": [
                    {
                        "name": "Valentin Leplat"
                    }
                ],
                "author_detail": {
                    "name": "Valentin Leplat"
                },
                "author": "Valentin Leplat"
            },
            {
                "id": "http://arxiv.org/abs/2601.01112v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.01112v2",
                "title": "EmoLoom-2B: Fast Base-Model Screening for Emotion Classification and VAD with Lexicon-Weak Supervision and KV-Off Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EmoLoom-2B: Fast Base-Model Screening for Emotion Classification and VAD with Lexicon-Weak Supervision and KV-Off Evaluation"
                },
                "updated": "2026-02-16T10:23:22Z",
                "updated_parsed": [
                    2026,
                    2,
                    16,
                    10,
                    23,
                    22,
                    0,
                    47,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.01112v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.01112v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce EmoLoom-2B, a lightweight and reproducible pipeline that turns small language models under 2B parameters into fast screening candidates for joint emotion classification and Valence-Arousal-Dominance prediction. To ensure protocol-faithful and fair evaluation, we unify data loading, training, and inference under a single JSON input-output contract and remove avoidable variance by adopting KV-off decoding as the default setting. We incorporate two orthogonal semantic regularizers: a VAD-preserving constraint that aligns generated text with target VAD triples, and a lightweight external appraisal classifier that provides training-time guidance on goal attainment, controllability, certainty, and fairness without injecting long rationales. To improve polarity sensitivity, we introduce Valence Flip augmentation based on mirrored emotional pairs. During supervised fine-tuning, we apply A/B mixture sampling with entropy-aware temperature scheduling to balance coverage and convergence. Using Qwen-1.8B-Chat as the base model, EmoLoom-2B achieves strong performance on GoEmotions and EmpatheticDialogues, and demonstrates robust cross-corpus generalization on DailyDialog. The proposed recipe is budget-aware, auditable, and re-entrant, serving as a dependable screening pass before heavier training or multimodal fusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce EmoLoom-2B, a lightweight and reproducible pipeline that turns small language models under 2B parameters into fast screening candidates for joint emotion classification and Valence-Arousal-Dominance prediction. To ensure protocol-faithful and fair evaluation, we unify data loading, training, and inference under a single JSON input-output contract and remove avoidable variance by adopting KV-off decoding as the default setting. We incorporate two orthogonal semantic regularizers: a VAD-preserving constraint that aligns generated text with target VAD triples, and a lightweight external appraisal classifier that provides training-time guidance on goal attainment, controllability, certainty, and fairness without injecting long rationales. To improve polarity sensitivity, we introduce Valence Flip augmentation based on mirrored emotional pairs. During supervised fine-tuning, we apply A/B mixture sampling with entropy-aware temperature scheduling to balance coverage and convergence. Using Qwen-1.8B-Chat as the base model, EmoLoom-2B achieves strong performance on GoEmotions and EmpatheticDialogues, and demonstrates robust cross-corpus generalization on DailyDialog. The proposed recipe is budget-aware, auditable, and re-entrant, serving as a dependable screening pass before heavier training or multimodal fusion."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-03T08:25:58Z",
                "published_parsed": [
                    2026,
                    1,
                    3,
                    8,
                    25,
                    58,
                    5,
                    3,
                    0
                ],
                "arxiv_comment": "This paper presents an initial and self-contained study of a lightweight screening pipeline for emotion-aware language modeling, intended as a reproducible baseline and system-level design reference. This latest version corrects and updates certain personal information",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Zilin Li"
                    },
                    {
                        "name": "Weiwei Xu"
                    },
                    {
                        "name": "Xuanbo Lu"
                    },
                    {
                        "name": "Zheda Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zheda Liu"
                },
                "author": "Zheda Liu"
            },
            {
                "id": "http://arxiv.org/abs/2501.16909v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2501.16909v3",
                "title": "Understanding GPU Resource Interference One Level Deeper",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding GPU Resource Interference One Level Deeper"
                },
                "updated": "2026-02-16T09:26:53Z",
                "updated_parsed": [
                    2026,
                    2,
                    16,
                    9,
                    26,
                    53,
                    0,
                    47,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2501.16909v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2501.16909v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "GPUs are vastly underutilized, even when running resource-intensive AI applications, as GPU kernels within each job have diverse resource profiles that may saturate some parts of a device while often leaving other parts idle. Colocating applications is known to improve GPU utilization, but is not common practice as it becomes difficult to provide predictable performance due to workload interference. Providing predictable performance guarantees requires a deep understanding of how applications contend for shared GPU resources such as block schedulers, compute units, L1/L2 caches, and memory bandwidth. We study the key types of GPU resource interference and develop a methodology to quantify the sensitivity of a workload to each type. We discuss how this methodology can serve as the foundation for GPU schedulers that enforce strict performance guarantees and how application developers can design GPU kernels with colocation in mind to improve efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPUs are vastly underutilized, even when running resource-intensive AI applications, as GPU kernels within each job have diverse resource profiles that may saturate some parts of a device while often leaving other parts idle. Colocating applications is known to improve GPU utilization, but is not common practice as it becomes difficult to provide predictable performance due to workload interference. Providing predictable performance guarantees requires a deep understanding of how applications contend for shared GPU resources such as block schedulers, compute units, L1/L2 caches, and memory bandwidth. We study the key types of GPU resource interference and develop a methodology to quantify the sensitivity of a workload to each type. We discuss how this methodology can serve as the foundation for GPU schedulers that enforce strict performance guarantees and how application developers can design GPU kernels with colocation in mind to improve efficiency."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-01-28T12:57:53Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    12,
                    57,
                    53,
                    1,
                    28,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Paul Elvinger"
                    },
                    {
                        "name": "Foteini Strati"
                    },
                    {
                        "name": "Natalie Enright Jerger"
                    },
                    {
                        "name": "Ana Klimovic"
                    }
                ],
                "author_detail": {
                    "name": "Ana Klimovic"
                },
                "author": "Ana Klimovic"
            },
            {
                "id": "http://arxiv.org/abs/2509.23094v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.23094v2",
                "title": "d$^2$Cache: Accelerating Diffusion-Based LLMs via Dual Adaptive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "d$^2$Cache: Accelerating Diffusion-Based LLMs via Dual Adaptive Caching"
                },
                "updated": "2026-02-16T06:46:00Z",
                "updated_parsed": [
                    2026,
                    2,
                    16,
                    6,
                    46,
                    0,
                    0,
                    47,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.23094v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.23094v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion-based large language models (dLLMs), despite their promising performance, still suffer from inferior inference efficiency. This is because dLLMs rely on bidirectional attention and cannot directly benefit from the standard key-value (KV) cache as autoregressive models (ARMs) do. To tackle this issue, we introduce \\textit{Dual aDaptive Cache} (d$^2$Cache), which is a training-free approximate KV cache framework for accelerating dLLM inference. d$^2$Cache features a two-stage fine-grained selection strategy to identify tokens and adaptively update their KV states at each decoding step, while caching the KV states of the remaining tokens for reuse. Furthermore, d$^2$Cache naturally offers a more reliable decoding alternative, which can enable quasi left-to-right generation and mitigate premature overconfidence in tokens at the end of the sequence. Extensive experimental results on two representative dLLMs (\\ie, LLaDA and Dream) demonstrate that d$^2$Cache not only achieves substantial inference speedups, but also yields consistent improvements in generation quality. The code is available at https://github.com/Kamichanw/d2Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based large language models (dLLMs), despite their promising performance, still suffer from inferior inference efficiency. This is because dLLMs rely on bidirectional attention and cannot directly benefit from the standard key-value (KV) cache as autoregressive models (ARMs) do. To tackle this issue, we introduce \\textit{Dual aDaptive Cache} (d$^2$Cache), which is a training-free approximate KV cache framework for accelerating dLLM inference. d$^2$Cache features a two-stage fine-grained selection strategy to identify tokens and adaptively update their KV states at each decoding step, while caching the KV states of the remaining tokens for reuse. Furthermore, d$^2$Cache naturally offers a more reliable decoding alternative, which can enable quasi left-to-right generation and mitigate premature overconfidence in tokens at the end of the sequence. Extensive experimental results on two representative dLLMs (\\ie, LLaDA and Dream) demonstrate that d$^2$Cache not only achieves substantial inference speedups, but also yields consistent improvements in generation quality. The code is available at https://github.com/Kamichanw/d2Cache."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-27T04:07:23Z",
                "published_parsed": [
                    2025,
                    9,
                    27,
                    4,
                    7,
                    23,
                    5,
                    270,
                    0
                ],
                "arxiv_comment": "Accepted by ICLR 2026, 21 pages, 9 figures",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yuchu Jiang"
                    },
                    {
                        "name": "Yue Cai"
                    },
                    {
                        "name": "Xiangzhong Luo"
                    },
                    {
                        "name": "Jiale Fu"
                    },
                    {
                        "name": "Jiarui Wang"
                    },
                    {
                        "name": "Chonghan Liu"
                    },
                    {
                        "name": "Xu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xu Yang"
                },
                "author": "Xu Yang"
            },
            {
                "id": "http://arxiv.org/abs/2502.01068v6",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2502.01068v6",
                "title": "FastKV: Decoupling of Context Reduction and KV Cache Compression for Prefill-Decoding Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastKV: Decoupling of Context Reduction and KV Cache Compression for Prefill-Decoding Acceleration"
                },
                "updated": "2026-02-16T06:37:44Z",
                "updated_parsed": [
                    2026,
                    2,
                    16,
                    6,
                    37,
                    44,
                    0,
                    47,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2502.01068v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2502.01068v6",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "While large language models (LLMs) excel at handling long-context sequences, they require substantial prefill computation and key-value (KV) cache, which can heavily burden computational efficiency and memory usage in both prefill and decoding stages. Recent works that compress KV caches with prefill acceleration reduce this cost but inadvertently tie the prefill compute reduction to the decoding KV budget. This coupling arises from overlooking the layer-dependent variation of critical context, often leading to accuracy degradation. To address this issue, we introduce FastKV, a KV cache compression framework designed to reduce latency in both prefill and decoding by leveraging the stabilization of token importance in later layers. FastKV performs full-context computation until a Token-Selective Propagation (TSP) layer, which forwards only the most informative tokens to subsequent layers. From these propagated tokens, FastKV independently selects salient KV entries for caching, thereby decoupling KV budget from the prefill compute reduction based on the TSP decision. This independent control of the TSP rate and KV retention rate enables flexible optimization of efficiency and accuracy. Experimental results show that FastKV achieves speedups of up to 1.82$\\times$ in prefill and 2.87$\\times$ in decoding compared to the full-context baseline, while matching the accuracy of the baselines that only accelerate the decoding stage. Our code is available at https://github.com/dongwonjo/FastKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) excel at handling long-context sequences, they require substantial prefill computation and key-value (KV) cache, which can heavily burden computational efficiency and memory usage in both prefill and decoding stages. Recent works that compress KV caches with prefill acceleration reduce this cost but inadvertently tie the prefill compute reduction to the decoding KV budget. This coupling arises from overlooking the layer-dependent variation of critical context, often leading to accuracy degradation. To address this issue, we introduce FastKV, a KV cache compression framework designed to reduce latency in both prefill and decoding by leveraging the stabilization of token importance in later layers. FastKV performs full-context computation until a Token-Selective Propagation (TSP) layer, which forwards only the most informative tokens to subsequent layers. From these propagated tokens, FastKV independently selects salient KV entries for caching, thereby decoupling KV budget from the prefill compute reduction based on the TSP decision. This independent control of the TSP rate and KV retention rate enables flexible optimization of efficiency and accuracy. Experimental results show that FastKV achieves speedups of up to 1.82$\\times$ in prefill and 2.87$\\times$ in decoding compared to the full-context baseline, while matching the accuracy of the baselines that only accelerate the decoding stage. Our code is available at https://github.com/dongwonjo/FastKV."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-02-03T05:25:09Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    5,
                    25,
                    9,
                    0,
                    34,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Dongwon Jo"
                    },
                    {
                        "name": "Jiwon Song"
                    },
                    {
                        "name": "Yulhwa Kim"
                    },
                    {
                        "name": "Jae-Joon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Joon Kim"
                },
                "author": "Jae-Joon Kim"
            },
            {
                "id": "http://arxiv.org/abs/2602.14381v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.14381v1",
                "title": "Adapting VACE for Real-Time Autoregressive Video Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting VACE for Real-Time Autoregressive Video Diffusion"
                },
                "updated": "2026-02-16T01:13:33Z",
                "updated_parsed": [
                    2026,
                    2,
                    16,
                    1,
                    13,
                    33,
                    0,
                    47,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.14381v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.14381v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We describe an adaptation of VACE (Video All-in-one Creation and Editing) for real-time autoregressive video generation. VACE provides unified video control (reference guidance, structural conditioning, inpainting, and temporal extension) but assumes bidirectional attention over full sequences, making it incompatible with streaming pipelines that require fixed chunk sizes and causal attention. The key modification moves reference frames from the diffusion latent space into a parallel conditioning pathway, preserving the fixed chunk sizes and KV caching that autoregressive models require. This adaptation reuses existing pretrained VACE weights without additional training. Across 1.3B and 14B model scales, VACE adds 20-30% latency overhead for structural control and inpainting, with negligible VRAM cost relative to the base model. Reference-to-video fidelity is severely degraded compared to batch VACE due to causal attention constraints. A reference implementation is available at https://github.com/daydreamlive/scope.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe an adaptation of VACE (Video All-in-one Creation and Editing) for real-time autoregressive video generation. VACE provides unified video control (reference guidance, structural conditioning, inpainting, and temporal extension) but assumes bidirectional attention over full sequences, making it incompatible with streaming pipelines that require fixed chunk sizes and causal attention. The key modification moves reference frames from the diffusion latent space into a parallel conditioning pathway, preserving the fixed chunk sizes and KV caching that autoregressive models require. This adaptation reuses existing pretrained VACE weights without additional training. Across 1.3B and 14B model scales, VACE adds 20-30% latency overhead for structural control and inpainting, with negligible VRAM cost relative to the base model. Reference-to-video fidelity is severely degraded compared to batch VACE due to causal attention constraints. A reference implementation is available at https://github.com/daydreamlive/scope."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-16T01:13:33Z",
                "published_parsed": [
                    2026,
                    2,
                    16,
                    1,
                    13,
                    33,
                    0,
                    47,
                    0
                ],
                "arxiv_comment": "10 pages, 4 figures, 7 tables",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Ryan Fosdick"
                    }
                ],
                "author_detail": {
                    "name": "Ryan Fosdick"
                },
                "arxiv_affiliation": "Daydream",
                "author": "Ryan Fosdick"
            },
            {
                "id": "http://arxiv.org/abs/2510.22876v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.22876v3",
                "title": "Batch Speculative Decoding Done Right",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch Speculative Decoding Done Right"
                },
                "updated": "2026-02-15T22:53:25Z",
                "updated_parsed": [
                    2026,
                    2,
                    15,
                    22,
                    53,
                    25,
                    6,
                    46,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.22876v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.22876v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Speculative decoding must produce outputs distribution identical to standard autoregressive generation-this output equivalence is not an optimization target but the defining criterion of valid speculative decoding. We demonstrate that all existing batch speculative decoding implementations violate this fundamental requirement, producing corrupted outputs ranging from repetitive tokens to gibberish. These failures stem from the ragged tensor problem: sequences in the same batch accept different numbers of draft tokens, desynchronizing position IDs, attention masks, and KV-cache state. We present the first authentic batch speculative decoding framework. We (1) formalize the synchronization invariants that valid batch speculative decoding must satisfy, (2) present EQSPEC, the first algorithm that guarantees output equivalence, and analyze its cost structure to show that alignment overhead grows superlinearly and consumes up to 40\\% of computation, and (3) introduce EXSPEC, which reduces this overhead through cross-batch scheduling that dynamically groups same-length sequences. On SpecBench across Vicuna-7B/68M, Qwen3-8B/0.6B, and GLM-4-9B/0.6B pairs, our methods achieve up to 3x throughput improvement at batch size 8 while maintaining algorithmic correctness. Our methods achieve 95\\% decoding-equivalence, with residual divergence attributable to floating-point non-determinism in GPU inference, not the synchronization failures that cause near-zero equivalence of prior methods. Our code is available at https://github.com/eBay/spec_dec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding must produce outputs distribution identical to standard autoregressive generation-this output equivalence is not an optimization target but the defining criterion of valid speculative decoding. We demonstrate that all existing batch speculative decoding implementations violate this fundamental requirement, producing corrupted outputs ranging from repetitive tokens to gibberish. These failures stem from the ragged tensor problem: sequences in the same batch accept different numbers of draft tokens, desynchronizing position IDs, attention masks, and KV-cache state. We present the first authentic batch speculative decoding framework. We (1) formalize the synchronization invariants that valid batch speculative decoding must satisfy, (2) present EQSPEC, the first algorithm that guarantees output equivalence, and analyze its cost structure to show that alignment overhead grows superlinearly and consumes up to 40\\% of computation, and (3) introduce EXSPEC, which reduces this overhead through cross-batch scheduling that dynamically groups same-length sequences. On SpecBench across Vicuna-7B/68M, Qwen3-8B/0.6B, and GLM-4-9B/0.6B pairs, our methods achieve up to 3x throughput improvement at batch size 8 while maintaining algorithmic correctness. Our methods achieve 95\\% decoding-equivalence, with residual divergence attributable to floating-point non-determinism in GPU inference, not the synchronization failures that cause near-zero equivalence of prior methods. Our code is available at https://github.com/eBay/spec_dec."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-26T23:59:23Z",
                "published_parsed": [
                    2025,
                    10,
                    26,
                    23,
                    59,
                    23,
                    6,
                    299,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Ranran Haoran Zhang"
                    },
                    {
                        "name": "Soumik Dey"
                    },
                    {
                        "name": "Ashirbad Mishra"
                    },
                    {
                        "name": "Hansi Wu"
                    },
                    {
                        "name": "Binbin Li"
                    },
                    {
                        "name": "Rui Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Zhang"
                },
                "author": "Rui Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2602.14262v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.14262v1",
                "title": "ABI: A tightly integrated, unified, sparsity-aware, reconfigurable, compute near-register file/cache GPU architecture with light-weight softmax for deep learning, linear algebra, and Ising compute",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ABI: A tightly integrated, unified, sparsity-aware, reconfigurable, compute near-register file/cache GPU architecture with light-weight softmax for deep learning, linear algebra, and Ising compute"
                },
                "updated": "2026-02-15T18:19:06Z",
                "updated_parsed": [
                    2026,
                    2,
                    15,
                    18,
                    19,
                    6,
                    6,
                    46,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.14262v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.14262v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present a tightly integrated and unified near-memory GPU architecture that delivers 6 to 16 times speedup and 6 to 13 times energy savings across Convolutional Neural Networks, Graph Convolutional Networks, Linear Programming, Large Language Models, and Ising workloads compared to MIAOW GPU. The design includes a custom sparsity-aware near-memory circuit providing about 1.5 times energy savings, and a lightweight softmax circuit providing about 1.6 times energy savings. The architecture supports reconfigurable compute up to INT16 with dynamic resolution updates and scales efficiently across problem sizes. ABI-enabled MI300 and Blackwell systems achieve about 4.5 times speedup over baseline MI300 and Blackwell.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a tightly integrated and unified near-memory GPU architecture that delivers 6 to 16 times speedup and 6 to 13 times energy savings across Convolutional Neural Networks, Graph Convolutional Networks, Linear Programming, Large Language Models, and Ising workloads compared to MIAOW GPU. The design includes a custom sparsity-aware near-memory circuit providing about 1.5 times energy savings, and a lightweight softmax circuit providing about 1.6 times energy savings. The architecture supports reconfigurable compute up to INT16 with dynamic resolution updates and scales efficiently across problem sizes. ABI-enabled MI300 and Blackwell systems achieve about 4.5 times speedup over baseline MI300 and Blackwell."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-15T18:19:06Z",
                "published_parsed": [
                    2026,
                    2,
                    15,
                    18,
                    19,
                    6,
                    6,
                    46,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Siddhartha Raman Sundara Raman"
                    },
                    {
                        "name": "Jaydeep P. Kulkarni"
                    }
                ],
                "author_detail": {
                    "name": "Jaydeep P. Kulkarni"
                },
                "author": "Jaydeep P. Kulkarni"
            },
            {
                "id": "http://arxiv.org/abs/2602.14236v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.14236v1",
                "title": "Dual-Signal Adaptive KV-Cache Optimization for Long-Form Video Understanding in Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dual-Signal Adaptive KV-Cache Optimization for Long-Form Video Understanding in Vision-Language Models"
                },
                "updated": "2026-02-15T17:06:02Z",
                "updated_parsed": [
                    2026,
                    2,
                    15,
                    17,
                    6,
                    2,
                    6,
                    46,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.14236v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.14236v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Vision-Language Models (VLMs) face a critical memory bottleneck when processing long-form video content due to the linear growth of the Key-Value (KV) cache with sequence length. Existing solutions predominantly employ reactive eviction strategies that compute full attention matrices before discarding tokens, resulting in substantial computational waste. We propose Sali-Cache, a novel a priori optimization framework that implements dual-signal adaptive caching through proactive memory management. By integrating a temporal filter based on optical flow analysis for detecting inter-frame redundancy and a spatial filter leveraging saliency detection for identifying visually significant regions, Sali-Cache intelligently manages memory allocation before entering computationally expensive attention operations. Experimental evaluation on the LLaVA 1.6 architecture demonstrates that our method achieves a 2.20x compression ratio in effective memory usage while maintaining 100% accuracy across BLEU, ROUGE-L, and Exact Match metrics. Furthermore, under identical memory budget constraints, Sali-Cache preserves context-rich features over extended temporal durations without degrading model performance, enabling efficient processing of long-form video content on consumer-grade hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) face a critical memory bottleneck when processing long-form video content due to the linear growth of the Key-Value (KV) cache with sequence length. Existing solutions predominantly employ reactive eviction strategies that compute full attention matrices before discarding tokens, resulting in substantial computational waste. We propose Sali-Cache, a novel a priori optimization framework that implements dual-signal adaptive caching through proactive memory management. By integrating a temporal filter based on optical flow analysis for detecting inter-frame redundancy and a spatial filter leveraging saliency detection for identifying visually significant regions, Sali-Cache intelligently manages memory allocation before entering computationally expensive attention operations. Experimental evaluation on the LLaVA 1.6 architecture demonstrates that our method achieves a 2.20x compression ratio in effective memory usage while maintaining 100% accuracy across BLEU, ROUGE-L, and Exact Match metrics. Furthermore, under identical memory budget constraints, Sali-Cache preserves context-rich features over extended temporal durations without degrading model performance, enabling efficient processing of long-form video content on consumer-grade hardware."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-15T17:06:02Z",
                "published_parsed": [
                    2026,
                    2,
                    15,
                    17,
                    6,
                    2,
                    6,
                    46,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Vishnu Sai"
                    },
                    {
                        "name": "Dheeraj Sai"
                    },
                    {
                        "name": "Srinath B"
                    },
                    {
                        "name": "Girish Varma"
                    },
                    {
                        "name": "Priyesh Shukla"
                    }
                ],
                "author_detail": {
                    "name": "Priyesh Shukla"
                },
                "author": "Priyesh Shukla"
            },
            {
                "id": "http://arxiv.org/abs/2602.14209v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.14209v1",
                "title": "MAGE: All-[MASK] Block Already Knows Where to Look in Diffusion LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAGE: All-[MASK] Block Already Knows Where to Look in Diffusion LLM"
                },
                "updated": "2026-02-15T16:07:51Z",
                "updated_parsed": [
                    2026,
                    2,
                    15,
                    16,
                    7,
                    51,
                    6,
                    46,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.14209v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.14209v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Block diffusion LLMs are emerging as a promising next paradigm for language generation, but their use of KV caching makes memory access a dominant bottleneck in long-context settings. While dynamic sparse attention has been actively explored, existing methods designed for autoregressive LLMs rely on approximate importance estimation and perform poorly when adapted to block diffusion. This work identifies a key opportunity unique to block diffusion: attention at the first All-[MASK] denoising step reliably predicts important KV entries and budget requirements, enabling MAGE to perform a single exact attention pass per block and reuse it for training-free sparse denoising. Across long-context benchmarks including LongBench and Needle-in-a-Haystack, MAGE achieves near-lossless accuracy with a fraction of the KV budget while delivering up to 3-4x end-to-end speedup, consistently outperforming AR-oriented sparse attention baselines. A lightweight fine-tuning strategy further strengthens [MASK]-guided patterns with minimal cost, requiring only a few hours of training on a single NVIDIA H100 GPU for both 1.5B and 7B models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block diffusion LLMs are emerging as a promising next paradigm for language generation, but their use of KV caching makes memory access a dominant bottleneck in long-context settings. While dynamic sparse attention has been actively explored, existing methods designed for autoregressive LLMs rely on approximate importance estimation and perform poorly when adapted to block diffusion. This work identifies a key opportunity unique to block diffusion: attention at the first All-[MASK] denoising step reliably predicts important KV entries and budget requirements, enabling MAGE to perform a single exact attention pass per block and reuse it for training-free sparse denoising. Across long-context benchmarks including LongBench and Needle-in-a-Haystack, MAGE achieves near-lossless accuracy with a fraction of the KV budget while delivering up to 3-4x end-to-end speedup, consistently outperforming AR-oriented sparse attention baselines. A lightweight fine-tuning strategy further strengthens [MASK]-guided patterns with minimal cost, requiring only a few hours of training on a single NVIDIA H100 GPU for both 1.5B and 7B models."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-15T16:07:51Z",
                "published_parsed": [
                    2026,
                    2,
                    15,
                    16,
                    7,
                    51,
                    6,
                    46,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Omin Kwon"
                    },
                    {
                        "name": "Yeonjae Kim"
                    },
                    {
                        "name": "Doyeon Kim"
                    },
                    {
                        "name": "Minseo Kim"
                    },
                    {
                        "name": "Yeonhong Park"
                    },
                    {
                        "name": "Jae W. Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jae W. Lee"
                },
                "author": "Jae W. Lee"
            },
            {
                "id": "http://arxiv.org/abs/2602.14162v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.14162v1",
                "title": "Index Light, Reason Deep: Deferred Visual Ingestion for Visual-Dense Document Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Index Light, Reason Deep: Deferred Visual Ingestion for Visual-Dense Document Question Answering"
                },
                "updated": "2026-02-15T14:23:50Z",
                "updated_parsed": [
                    2026,
                    2,
                    15,
                    14,
                    23,
                    50,
                    6,
                    46,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.14162v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.14162v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Existing multimodal document question answering methods universally adopt a supply-side ingestion strategy: running a Vision-Language Model (VLM) on every page during indexing to generate comprehensive descriptions, then answering questions through text retrieval. However, this \"pre-ingestion\" approach is costly (a 113-page engineering drawing package requires approximately 80,000 VLM tokens), end-to-end unreliable (VLM outputs may fail to be correctly retrieved due to format mismatches in the retrieval infrastructure), and irrecoverable once it fails. This paper proposes the Deferred Visual Ingestion (DVI) framework, adopting a demand-side ingestion strategy: the indexing phase performs only lightweight metadata extraction, deferring visual understanding to the moment users pose specific questions. DVI's core principle is \"Index for locating, not understanding\"--achieving page localization through structured metadata indexes and BM25 full-text search, then sending original images along with specific questions to a VLM for targeted analysis. Experiments on two real industrial engineering drawings (113 pages + 7 pages) demonstrate that DVI achieves comparable overall accuracy at zero ingestion VLM cost (46.7% vs. 48.9%), an effectiveness rate of 50% on visually necessary queries (vs. 0% for pre-ingestion), and 100% page localization (98% search space compression). DVI also supports interactive refinement and progressive caching, transforming the \"QA accuracy\" problem into a \"page localization\" problem--once the correct drawing page is found, obtaining the answer becomes a matter of interaction rounds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing multimodal document question answering methods universally adopt a supply-side ingestion strategy: running a Vision-Language Model (VLM) on every page during indexing to generate comprehensive descriptions, then answering questions through text retrieval. However, this \"pre-ingestion\" approach is costly (a 113-page engineering drawing package requires approximately 80,000 VLM tokens), end-to-end unreliable (VLM outputs may fail to be correctly retrieved due to format mismatches in the retrieval infrastructure), and irrecoverable once it fails. This paper proposes the Deferred Visual Ingestion (DVI) framework, adopting a demand-side ingestion strategy: the indexing phase performs only lightweight metadata extraction, deferring visual understanding to the moment users pose specific questions. DVI's core principle is \"Index for locating, not understanding\"--achieving page localization through structured metadata indexes and BM25 full-text search, then sending original images along with specific questions to a VLM for targeted analysis. Experiments on two real industrial engineering drawings (113 pages + 7 pages) demonstrate that DVI achieves comparable overall accuracy at zero ingestion VLM cost (46.7% vs. 48.9%), an effectiveness rate of 50% on visually necessary queries (vs. 0% for pre-ingestion), and 100% page localization (98% search space compression). DVI also supports interactive refinement and progressive caching, transforming the \"QA accuracy\" problem into a \"page localization\" problem--once the correct drawing page is found, obtaining the answer becomes a matter of interaction rounds."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-15T14:23:50Z",
                "published_parsed": [
                    2026,
                    2,
                    15,
                    14,
                    23,
                    50,
                    6,
                    46,
                    0
                ],
                "arxiv_comment": "24 pages, 9 figures, 9 tables",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Tao Xu"
                    }
                ],
                "author_detail": {
                    "name": "Tao Xu"
                },
                "author": "Tao Xu"
            },
            {
                "id": "http://arxiv.org/abs/2602.13993v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.13993v1",
                "title": "Elastic Diffusion Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Elastic Diffusion Transformer"
                },
                "updated": "2026-02-15T05:19:17Z",
                "updated_parsed": [
                    2026,
                    2,
                    15,
                    5,
                    19,
                    17,
                    6,
                    46,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.13993v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.13993v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion Transformers (DiT) have demonstrated remarkable generative capabilities but remain highly computationally expensive. Previous acceleration methods, such as pruning and distillation, typically rely on a fixed computational capacity, leading to insufficient acceleration and degraded generation quality. To address this limitation, we propose \\textbf{Elastic Diffusion Transformer (E-DiT)}, an adaptive acceleration framework for DiT that effectively improves efficiency while maintaining generation quality. Specifically, we observe that the generative process of DiT exhibits substantial sparsity (i.e., some computations can be skipped with minimal impact on quality), and this sparsity varies significantly across samples. Motivated by this observation, E-DiT equips each DiT block with a lightweight router that dynamically identifies sample-dependent sparsity from the input latent. Each router adaptively determines whether the corresponding block can be skipped. If the block is not skipped, the router then predicts the optimal MLP width reduction ratio within the block. During inference, we further introduce a block-level feature caching mechanism that leverages router predictions to eliminate redundant computations in a training-free manner. Extensive experiments across 2D image (Qwen-Image and FLUX) and 3D asset (Hunyuan3D-3.0) demonstrate the effectiveness of E-DiT, achieving up to $\\sim$2$\\times$ speedup with negligible loss in generation quality. Code will be available at https://github.com/wangjiangshan0725/Elastic-DiT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have demonstrated remarkable generative capabilities but remain highly computationally expensive. Previous acceleration methods, such as pruning and distillation, typically rely on a fixed computational capacity, leading to insufficient acceleration and degraded generation quality. To address this limitation, we propose \\textbf{Elastic Diffusion Transformer (E-DiT)}, an adaptive acceleration framework for DiT that effectively improves efficiency while maintaining generation quality. Specifically, we observe that the generative process of DiT exhibits substantial sparsity (i.e., some computations can be skipped with minimal impact on quality), and this sparsity varies significantly across samples. Motivated by this observation, E-DiT equips each DiT block with a lightweight router that dynamically identifies sample-dependent sparsity from the input latent. Each router adaptively determines whether the corresponding block can be skipped. If the block is not skipped, the router then predicts the optimal MLP width reduction ratio within the block. During inference, we further introduce a block-level feature caching mechanism that leverages router predictions to eliminate redundant computations in a training-free manner. Extensive experiments across 2D image (Qwen-Image and FLUX) and 3D asset (Hunyuan3D-3.0) demonstrate the effectiveness of E-DiT, achieving up to $\\sim$2$\\times$ speedup with negligible loss in generation quality. Code will be available at https://github.com/wangjiangshan0725/Elastic-DiT."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-15T05:19:17Z",
                "published_parsed": [
                    2026,
                    2,
                    15,
                    5,
                    19,
                    17,
                    6,
                    46,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Jiangshan Wang"
                    },
                    {
                        "name": "Zeqiang Lai"
                    },
                    {
                        "name": "Jiarui Chen"
                    },
                    {
                        "name": "Jiayi Guo"
                    },
                    {
                        "name": "Hang Guo"
                    },
                    {
                        "name": "Xiu Li"
                    },
                    {
                        "name": "Xiangyu Yue"
                    },
                    {
                        "name": "Chunchao Guo"
                    }
                ],
                "author_detail": {
                    "name": "Chunchao Guo"
                },
                "author": "Chunchao Guo"
            },
            {
                "id": "http://arxiv.org/abs/2602.13692v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.13692v1",
                "title": "ThunderAgent: A Simple, Fast and Program-Aware Agentic Inference System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThunderAgent: A Simple, Fast and Program-Aware Agentic Inference System"
                },
                "updated": "2026-02-14T09:26:41Z",
                "updated_parsed": [
                    2026,
                    2,
                    14,
                    9,
                    26,
                    41,
                    5,
                    45,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.13692v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.13692v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models(LLMs) are now used to power complex multi-turn agentic workflows. Existing systems run agentic inference by loosely assembling isolated components: an LLM inference engine (e.g., vLLM) and a tool orchestrator (e.g., Kubernetes). Although agentic workflows involve multiple LLM and tool requests, these systems schedule and allocate resources separately on a per-request basis, without end-to-end knowledge of the workflow. This leads to sub-optimal management of KV cache and tool execution environments. To address the challenges, we propose ThunderAgent, a fast, simple, and program-aware agentic inference system. We first abstract agentic workflows as LLM Programs, enabling a unified view of heterogeneous resources, including KV caches, system states, and external tool assets such as disk memory and network ports. Built upon this abstraction, ThunderAgent introduces a program-aware scheduler and a tool resource manager designed to maximize KV cache hit rates, mitigate memory imbalances, and enable asynchronous environment preparation. Evaluations across coding, routing, and scientific discovery agents demonstrate that ThunderAgent achieves 1.5-3.6x throughput improvements in serving, 1.8-3.9x in RL rollout, and up to 4.2x disk memory savings compared to state-of-the-art inference systems. To facilitate reproducibility and support future development, we open-source the system implementations of the whole ThunderAgent at: https://github.com/Agentic-Kinetics/ThunderAgent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models(LLMs) are now used to power complex multi-turn agentic workflows. Existing systems run agentic inference by loosely assembling isolated components: an LLM inference engine (e.g., vLLM) and a tool orchestrator (e.g., Kubernetes). Although agentic workflows involve multiple LLM and tool requests, these systems schedule and allocate resources separately on a per-request basis, without end-to-end knowledge of the workflow. This leads to sub-optimal management of KV cache and tool execution environments. To address the challenges, we propose ThunderAgent, a fast, simple, and program-aware agentic inference system. We first abstract agentic workflows as LLM Programs, enabling a unified view of heterogeneous resources, including KV caches, system states, and external tool assets such as disk memory and network ports. Built upon this abstraction, ThunderAgent introduces a program-aware scheduler and a tool resource manager designed to maximize KV cache hit rates, mitigate memory imbalances, and enable asynchronous environment preparation. Evaluations across coding, routing, and scientific discovery agents demonstrate that ThunderAgent achieves 1.5-3.6x throughput improvements in serving, 1.8-3.9x in RL rollout, and up to 4.2x disk memory savings compared to state-of-the-art inference systems. To facilitate reproducibility and support future development, we open-source the system implementations of the whole ThunderAgent at: https://github.com/Agentic-Kinetics/ThunderAgent."
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-14T09:26:41Z",
                "published_parsed": [
                    2026,
                    2,
                    14,
                    9,
                    26,
                    41,
                    5,
                    45,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS"
                },
                "authors": [
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Ziyang Li"
                    },
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Weili Xu"
                    },
                    {
                        "name": "Yinfang Chen"
                    },
                    {
                        "name": "Junxiong Wang"
                    },
                    {
                        "name": "Beidi Chen"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Chenfeng Xu"
                    },
                    {
                        "name": "Simran Arora"
                    }
                ],
                "author_detail": {
                    "name": "Simran Arora"
                },
                "author": "Simran Arora"
            },
            {
                "id": "http://arxiv.org/abs/2408.10746v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2408.10746v2",
                "title": "Resource-Efficient Personal Large Language Models Fine-Tuning with Collaborative Edge Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource-Efficient Personal Large Language Models Fine-Tuning with Collaborative Edge Computing"
                },
                "updated": "2026-02-14T07:14:40Z",
                "updated_parsed": [
                    2026,
                    2,
                    14,
                    7,
                    14,
                    40,
                    5,
                    45,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2408.10746v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2408.10746v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) have unlocked a plethora of powerful applications at the network edge, such as intelligent personal assistants. Data privacy and security concerns have prompted a shift towards edge-based fine-tuning of personal LLMs, away from cloud reliance. However, this raises issues of computational intensity and resource scarcity, hindering training efficiency and feasibility. While current studies investigate parameter-efficient fine-tuning (PEFT) techniques to mitigate resource constraints, our analysis indicates that these techniques are not sufficiently resource-efficient for edge devices. To tackle these challenges, we propose Pluto and Charon (PAC), a time and memory efficient collaborative edge AI framework for personal LLMs fine-tuning. PAC breaks the resource wall of personal LLMs fine-tuning with a sophisticated algorithm-system co-design. (1) Algorithmically, PAC implements a personal LLMs fine-tuning technique that is efficient in terms of parameters, time, and memory. It utilizes Parallel Adapters to circumvent the need for a full backward pass through the LLM backbone. Additionally, an activation cache mechanism further streamlining the process by negating the necessity for repeated forward passes across multiple epochs. (2) Systematically, PAC leverages edge devices in close proximity, pooling them as a collective resource for in-situ personal LLMs fine-tuning, utilizing a hybrid data and pipeline parallelism to orchestrate distributed training. The use of the activation cache eliminates the need for forward pass through the LLM backbone,enabling exclusive fine-tuning of the Parallel Adapters using data parallelism. Extensive evaluation based on prototype implementation demonstrates that PAC remarkably outperforms state-of-the-art approaches, achieving up to 8.64x end-to-end speedup and up to 88.16% reduction in memory footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have unlocked a plethora of powerful applications at the network edge, such as intelligent personal assistants. Data privacy and security concerns have prompted a shift towards edge-based fine-tuning of personal LLMs, away from cloud reliance. However, this raises issues of computational intensity and resource scarcity, hindering training efficiency and feasibility. While current studies investigate parameter-efficient fine-tuning (PEFT) techniques to mitigate resource constraints, our analysis indicates that these techniques are not sufficiently resource-efficient for edge devices. To tackle these challenges, we propose Pluto and Charon (PAC), a time and memory efficient collaborative edge AI framework for personal LLMs fine-tuning. PAC breaks the resource wall of personal LLMs fine-tuning with a sophisticated algorithm-system co-design. (1) Algorithmically, PAC implements a personal LLMs fine-tuning technique that is efficient in terms of parameters, time, and memory. It utilizes Parallel Adapters to circumvent the need for a full backward pass through the LLM backbone. Additionally, an activation cache mechanism further streamlining the process by negating the necessity for repeated forward passes across multiple epochs. (2) Systematically, PAC leverages edge devices in close proximity, pooling them as a collective resource for in-situ personal LLMs fine-tuning, utilizing a hybrid data and pipeline parallelism to orchestrate distributed training. The use of the activation cache eliminates the need for forward pass through the LLM backbone,enabling exclusive fine-tuning of the Parallel Adapters using data parallelism. Extensive evaluation based on prototype implementation demonstrates that PAC remarkably outperforms state-of-the-art approaches, achieving up to 8.64x end-to-end speedup and up to 88.16% reduction in memory footprint."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-08-20T11:30:12Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    11,
                    30,
                    12,
                    1,
                    233,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Shengyuan Ye"
                    },
                    {
                        "name": "Bei Ouyang"
                    },
                    {
                        "name": "Tianyi Qian"
                    },
                    {
                        "name": "Liekang Zeng"
                    },
                    {
                        "name": "Jingyi Li"
                    },
                    {
                        "name": "Jiangsu Du"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Guoliang Xing"
                    },
                    {
                        "name": "Xu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xu Chen"
                },
                "author": "Xu Chen"
            },
            {
                "id": "http://arxiv.org/abs/2506.02634v5",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.02634v5",
                "title": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache at a Large Cloud Provider",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache at a Large Cloud Provider"
                },
                "updated": "2026-02-14T05:32:46Z",
                "updated_parsed": [
                    2026,
                    2,
                    14,
                    5,
                    32,
                    46,
                    5,
                    45,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.02634v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.02634v5",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Serving large language models (LLMs) is important for cloud providers, and caching intermediate results (KV\\$) after processing each request substantially improves serving throughput and latency. However, there is limited understanding of how LLM serving benefits from KV\\$ caching, where system design decisions like cache eviction policies are highly workload-dependent. In this paper, we present the first systematic characterization of the KV\\$ workload patterns from one of the leading LLM service providers. We draw observations that were not covered by previous studies focusing on synthetic workloads, including: KV\\$ reuses are skewed across requests, where reuses between single-turn requests are equally important as multi-turn requests; the reuse time and probability are diverse considering all requests, but for a specific request category, the pattern tends to be predictable; and the overall cache size required for an ideal cache hit ratio is moderate. Based on the characterization, we further propose a workload-aware cache eviction policy that improves the serving performance under real-world traces, especially with limited cache capacity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving large language models (LLMs) is important for cloud providers, and caching intermediate results (KV\\$) after processing each request substantially improves serving throughput and latency. However, there is limited understanding of how LLM serving benefits from KV\\$ caching, where system design decisions like cache eviction policies are highly workload-dependent. In this paper, we present the first systematic characterization of the KV\\$ workload patterns from one of the leading LLM service providers. We draw observations that were not covered by previous studies focusing on synthetic workloads, including: KV\\$ reuses are skewed across requests, where reuses between single-turn requests are equally important as multi-turn requests; the reuse time and probability are diverse considering all requests, but for a specific request category, the pattern tends to be predictable; and the overall cache size required for an ideal cache hit ratio is moderate. Based on the characterization, we further propose a workload-aware cache eviction policy that improves the serving performance under real-world traces, especially with limited cache capacity."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-03T08:51:38Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    8,
                    51,
                    38,
                    1,
                    154,
                    0
                ],
                "arxiv_comment": "Accepted by USENIX ATC'25",
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Jinbo Han"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Sijie Shen"
                    },
                    {
                        "name": "Dingyan Zhang"
                    },
                    {
                        "name": "Chenguang Fang"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen"
            },
            {
                "id": "http://arxiv.org/abs/2602.03983v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.03983v2",
                "title": "Efficient Long-Horizon Vision-Language-Action Models via Static-Dynamic Disentanglement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Long-Horizon Vision-Language-Action Models via Static-Dynamic Disentanglement"
                },
                "updated": "2026-02-14T03:09:51Z",
                "updated_parsed": [
                    2026,
                    2,
                    14,
                    3,
                    9,
                    51,
                    5,
                    45,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.03983v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.03983v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Vision-Language-Action (VLA) models have recently emerged as a promising paradigm for generalist robotic control. Built upon vision-language model (VLM) architectures, VLAs predict actions conditioned on visual observations and language instructions, achieving strong performance and generalization across tasks. However, VLAs face two major challenges: limited long-horizon context and inefficient inference due to the quadratic attention complexity and large parameter counts. Our work is motivated by the observation that much of the visual information in a trajectory remains static across timesteps (e.g., the background). Leveraging this property, we propose SD-VLA, a framework that disentangles visual inputs into multi-level static and dynamic tokens, which enables (1) retaining a single copy of static tokens across frames to significantly reduce context length, and (2) reusing the key-value (KV) cache of static tokens through a lightweight recache gate that updates only when necessary. This design enables efficient multi-frame integration and efficient inference. In addition, we introduce a new benchmark that more effectively evaluates the long-horizon temporal dependency modeling ability of VLAs. Experimental results show that our approach outperforms baselines on this benchmark by 39.8% absolute improvement in success rate, and achieves a 3.9% gain on the SimplerEnv benchmark. Moreover, SD-VLA delivers a 2.26x inference speedup over the base VLA model on the same benchmark, enabling faster and more practical real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models have recently emerged as a promising paradigm for generalist robotic control. Built upon vision-language model (VLM) architectures, VLAs predict actions conditioned on visual observations and language instructions, achieving strong performance and generalization across tasks. However, VLAs face two major challenges: limited long-horizon context and inefficient inference due to the quadratic attention complexity and large parameter counts. Our work is motivated by the observation that much of the visual information in a trajectory remains static across timesteps (e.g., the background). Leveraging this property, we propose SD-VLA, a framework that disentangles visual inputs into multi-level static and dynamic tokens, which enables (1) retaining a single copy of static tokens across frames to significantly reduce context length, and (2) reusing the key-value (KV) cache of static tokens through a lightweight recache gate that updates only when necessary. This design enables efficient multi-frame integration and efficient inference. In addition, we introduce a new benchmark that more effectively evaluates the long-horizon temporal dependency modeling ability of VLAs. Experimental results show that our approach outperforms baselines on this benchmark by 39.8% absolute improvement in success rate, and achieves a 3.9% gain on the SimplerEnv benchmark. Moreover, SD-VLA delivers a 2.26x inference speedup over the base VLA model on the same benchmark, enabling faster and more practical real-world deployment."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-03T20:17:47Z",
                "published_parsed": [
                    2026,
                    2,
                    3,
                    20,
                    17,
                    47,
                    1,
                    34,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Weikang Qiu"
                    },
                    {
                        "name": "Tinglin Huang"
                    },
                    {
                        "name": "Rex Ying"
                    }
                ],
                "author_detail": {
                    "name": "Rex Ying"
                },
                "author": "Rex Ying"
            },
            {
                "id": "http://arxiv.org/abs/2503.10568v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2503.10568v3",
                "title": "Autoregressive Image Generation with Randomized Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Image Generation with Randomized Parallel Decoding"
                },
                "updated": "2026-02-14T02:23:24Z",
                "updated_parsed": [
                    2026,
                    2,
                    14,
                    2,
                    23,
                    24,
                    5,
                    45,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2503.10568v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2503.10568v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce ARPG, a novel visual autoregressive model that enables randomized parallel generation, addressing the inherent limitations of conventional raster-order approaches, which hinder inference efficiency and zero-shot generalization due to their sequential, predefined token generation order. Our key insight is that effective random-order modeling necessitates explicit guidance for determining the position of the next predicted token. To this end, we propose a novel decoupled decoding framework that decouples positional guidance from content representation, encoding them separately as queries and key-value pairs. By directly incorporating this guidance into the causal attention mechanism, our approach enables fully random-order training and generation, eliminating the need for bidirectional attention. Consequently, ARPG readily generalizes to zero-shot inference tasks such as image inpainting, outpainting, and resolution expansion. Furthermore, it supports parallel inference by concurrently processing multiple queries using a shared KV cache. On the ImageNet-1K 256 benchmark, our approach attains an FID of 1.83 with only 32 sampling steps, achieving over a 30 times speedup in inference and a 75 percent reduction in memory consumption compared to representative recent autoregressive models at a similar scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce ARPG, a novel visual autoregressive model that enables randomized parallel generation, addressing the inherent limitations of conventional raster-order approaches, which hinder inference efficiency and zero-shot generalization due to their sequential, predefined token generation order. Our key insight is that effective random-order modeling necessitates explicit guidance for determining the position of the next predicted token. To this end, we propose a novel decoupled decoding framework that decouples positional guidance from content representation, encoding them separately as queries and key-value pairs. By directly incorporating this guidance into the causal attention mechanism, our approach enables fully random-order training and generation, eliminating the need for bidirectional attention. Consequently, ARPG readily generalizes to zero-shot inference tasks such as image inpainting, outpainting, and resolution expansion. Furthermore, it supports parallel inference by concurrently processing multiple queries using a shared KV cache. On the ImageNet-1K 256 benchmark, our approach attains an FID of 1.83 with only 32 sampling steps, achieving over a 30 times speedup in inference and a 75 percent reduction in memory consumption compared to representative recent autoregressive models at a similar scale."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-03-13T17:19:51Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    19,
                    51,
                    3,
                    72,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Haopeng Li"
                    },
                    {
                        "name": "Jinyue Yang"
                    },
                    {
                        "name": "Guoqi Li"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang"
            },
            {
                "id": "http://arxiv.org/abs/2602.13434v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.13434v1",
                "title": "ORAP: Optimized Row Access Prefetching for Rowhammer-mitigated Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ORAP: Optimized Row Access Prefetching for Rowhammer-mitigated Memory"
                },
                "updated": "2026-02-13T20:22:44Z",
                "updated_parsed": [
                    2026,
                    2,
                    13,
                    20,
                    22,
                    44,
                    4,
                    44,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.13434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.13434v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Rowhammer is a well-studied DRAM phenomenon wherein multiple activations to a given row can cause bit flips in adjacent rows. Many mitigation techniques have been introduced to address Rowhammer, with some support being incorporated into the JEDEC DDR5 standard for per-row-activation-counter (PRAC) and refresh-management (RFM) systems. Mitigation schemes built on these mechanisms claim to have various levels of area, power, and performance overheads. To date the evaluation of existing mitigation schemes typically neglects the impact of other memory system components such as hardware prefetchers. Nearly all modern systems incorporate hardware prefetching and these can significantly improve processor performance through speculative cache population. These prefetchers induce higher numbers of downstream memory requests and increase DRAM activation rates. The performance overhead of Rowhammer mitigations are tied directly to memory access patterns, exposing both hardware prefetchers and Rowhammer mitigations to cross-interaction. We find that the performance improvement provided by prior-work hardware prefetchers is often severely impacted by Rowhammer mitigations. In effect, much of the benefit of speculative memory references from prefetching lies in accelerating and reordering DRAM references in ways that trigger mitigations, significantly reducing the benefits of prefetching. This work proposes the Optimized Row Access Prefetcher (ORAP), leveraging last-level-cache (LLC) space to cache large portions of DRAM rowbuffer contents to reduce the need for future activations. Working with the state-of-the-art Berti prefetcher, ORAP reduces DRAM activation rates by 51.3% and achieves a 4.6% speedup over the prefetcher configuration of Berti and SPP-PPF when prefetching in an RFM-mitigated memory system. Under PRAC mitigations, ORAP reduces energy overheads by 11.8%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rowhammer is a well-studied DRAM phenomenon wherein multiple activations to a given row can cause bit flips in adjacent rows. Many mitigation techniques have been introduced to address Rowhammer, with some support being incorporated into the JEDEC DDR5 standard for per-row-activation-counter (PRAC) and refresh-management (RFM) systems. Mitigation schemes built on these mechanisms claim to have various levels of area, power, and performance overheads. To date the evaluation of existing mitigation schemes typically neglects the impact of other memory system components such as hardware prefetchers. Nearly all modern systems incorporate hardware prefetching and these can significantly improve processor performance through speculative cache population. These prefetchers induce higher numbers of downstream memory requests and increase DRAM activation rates. The performance overhead of Rowhammer mitigations are tied directly to memory access patterns, exposing both hardware prefetchers and Rowhammer mitigations to cross-interaction. We find that the performance improvement provided by prior-work hardware prefetchers is often severely impacted by Rowhammer mitigations. In effect, much of the benefit of speculative memory references from prefetching lies in accelerating and reordering DRAM references in ways that trigger mitigations, significantly reducing the benefits of prefetching. This work proposes the Optimized Row Access Prefetcher (ORAP), leveraging last-level-cache (LLC) space to cache large portions of DRAM rowbuffer contents to reduce the need for future activations. Working with the state-of-the-art Berti prefetcher, ORAP reduces DRAM activation rates by 51.3% and achieves a 4.6% speedup over the prefetcher configuration of Berti and SPP-PPF when prefetching in an RFM-mitigated memory system. Under PRAC mitigations, ORAP reduces energy overheads by 11.8%."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-13T20:22:44Z",
                "published_parsed": [
                    2026,
                    2,
                    13,
                    20,
                    22,
                    44,
                    4,
                    44,
                    0
                ],
                "arxiv_comment": "15 pages, 19 figures",
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Maccoy Merrell"
                    },
                    {
                        "name": "Daniel Puckett"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Jeffrey Stuecheli"
                    },
                    {
                        "name": "Stavros Kalafatis"
                    },
                    {
                        "name": "Paul V. Gratz"
                    }
                ],
                "author_detail": {
                    "name": "Paul V. Gratz"
                },
                "author": "Paul V. Gratz"
            },
            {
                "id": "http://arxiv.org/abs/2602.13172v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.13172v1",
                "title": "LongStream: Long-Sequence Streaming Autoregressive Visual Geometry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongStream: Long-Sequence Streaming Autoregressive Visual Geometry"
                },
                "updated": "2026-02-13T18:30:51Z",
                "updated_parsed": [
                    2026,
                    2,
                    13,
                    18,
                    30,
                    51,
                    4,
                    44,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.13172v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.13172v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Long-sequence streaming 3D reconstruction remains a significant open challenge. Existing autoregressive models often fail when processing long sequences. They typically anchor poses to the first frame, which leads to attention decay, scale drift, and extrapolation errors. We introduce LongStream, a novel gauge-decoupled streaming visual geometry model for metric-scale scene reconstruction across thousands of frames. Our approach is threefold. First, we discard the first-frame anchor and predict keyframe-relative poses. This reformulates long-range extrapolation into a constant-difficulty local task. Second, we introduce orthogonal scale learning. This method fully disentangles geometry from scale estimation to suppress drift. Finally, we solve Transformer cache issues such as attention-sink reliance and long-term KV-cache contamination. We propose cache-consistent training combined with periodic cache refresh. This approach suppresses attention degradation over ultra-long sequences and reduces the gap between training and inference. Experiments show LongStream achieves state-of-the-art performance. It delivers stable, metric-scale reconstruction over kilometer-scale sequences at 18 FPS. Project Page: https://3dagentworld.github.io/longstream/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-sequence streaming 3D reconstruction remains a significant open challenge. Existing autoregressive models often fail when processing long sequences. They typically anchor poses to the first frame, which leads to attention decay, scale drift, and extrapolation errors. We introduce LongStream, a novel gauge-decoupled streaming visual geometry model for metric-scale scene reconstruction across thousands of frames. Our approach is threefold. First, we discard the first-frame anchor and predict keyframe-relative poses. This reformulates long-range extrapolation into a constant-difficulty local task. Second, we introduce orthogonal scale learning. This method fully disentangles geometry from scale estimation to suppress drift. Finally, we solve Transformer cache issues such as attention-sink reliance and long-term KV-cache contamination. We propose cache-consistent training combined with periodic cache refresh. This approach suppresses attention degradation over ultra-long sequences and reduces the gap between training and inference. Experiments show LongStream achieves state-of-the-art performance. It delivers stable, metric-scale reconstruction over kilometer-scale sequences at 18 FPS. Project Page: https://3dagentworld.github.io/longstream/"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-13T18:30:51Z",
                "published_parsed": [
                    2026,
                    2,
                    13,
                    18,
                    30,
                    51,
                    4,
                    44,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Chong Cheng"
                    },
                    {
                        "name": "Xianda Chen"
                    },
                    {
                        "name": "Tao Xie"
                    },
                    {
                        "name": "Wei Yin"
                    },
                    {
                        "name": "Weiqiang Ren"
                    },
                    {
                        "name": "Qian Zhang"
                    },
                    {
                        "name": "Xiaoyuang Guo"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang"
            },
            {
                "id": "http://arxiv.org/abs/2602.13165v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.13165v1",
                "title": "Asynchronous Verified Semantic Caching for Tiered LLM Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asynchronous Verified Semantic Caching for Tiered LLM Architectures"
                },
                "updated": "2026-02-13T18:25:00Z",
                "updated_parsed": [
                    2026,
                    2,
                    13,
                    18,
                    25,
                    0,
                    4,
                    44,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.13165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.13165v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) now sit in the critical path of search, assistance, and agentic workflows, making semantic caching essential for reducing inference cost and latency. Production deployments typically use a tiered static-dynamic design: a static cache of curated, offline vetted responses mined from logs, backed by a dynamic cache populated online. In practice, both tiers are commonly governed by a single embedding similarity threshold, which induces a hard tradeoff: conservative thresholds miss safe reuse opportunities, while aggressive thresholds risk serving semantically incorrect responses. We introduce \\textbf{Krites}, an asynchronous, LLM-judged caching policy that expands static coverage without changing serving decisions. On the critical path, Krites behaves exactly like a standard static threshold policy. When the nearest static neighbor of the prompt falls just below the static threshold, Krites asynchronously invokes an LLM judge to verify whether the static response is acceptable for the new prompt. Approved matches are promoted into the dynamic cache, allowing future repeats and paraphrases to reuse curated static answers and expanding static reach over time. In trace-driven simulations on conversational and search workloads, Krites increases the fraction of requests served with curated static answers (direct static hits plus verified promotions) by up to $\\textbf{3.9}$ times for conversational traffic and search-style queries relative to tuned baselines, with unchanged critical path latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) now sit in the critical path of search, assistance, and agentic workflows, making semantic caching essential for reducing inference cost and latency. Production deployments typically use a tiered static-dynamic design: a static cache of curated, offline vetted responses mined from logs, backed by a dynamic cache populated online. In practice, both tiers are commonly governed by a single embedding similarity threshold, which induces a hard tradeoff: conservative thresholds miss safe reuse opportunities, while aggressive thresholds risk serving semantically incorrect responses. We introduce \\textbf{Krites}, an asynchronous, LLM-judged caching policy that expands static coverage without changing serving decisions. On the critical path, Krites behaves exactly like a standard static threshold policy. When the nearest static neighbor of the prompt falls just below the static threshold, Krites asynchronously invokes an LLM judge to verify whether the static response is acceptable for the new prompt. Approved matches are promoted into the dynamic cache, allowing future repeats and paraphrases to reuse curated static answers and expanding static reach over time. In trace-driven simulations on conversational and search workloads, Krites increases the fraction of requests served with curated static answers (direct static hits plus verified promotions) by up to $\\textbf{3.9}$ times for conversational traffic and search-style queries relative to tuned baselines, with unchanged critical path latency."
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-13T18:25:00Z",
                "published_parsed": [
                    2026,
                    2,
                    13,
                    18,
                    25,
                    0,
                    4,
                    44,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR"
                },
                "authors": [
                    {
                        "name": "Asmit Kumar Singh"
                    },
                    {
                        "name": "Haozhe Wang"
                    },
                    {
                        "name": "Laxmi Naga Santosh Attaluri"
                    },
                    {
                        "name": "Tak Chiam"
                    },
                    {
                        "name": "Weihua Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Weihua Zhu"
                },
                "author": "Weihua Zhu"
            },
            {
                "id": "http://arxiv.org/abs/2508.07675v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.07675v3",
                "title": "Semantic Caching for Low-Cost LLM Serving: From Offline Learning to Online Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Caching for Low-Cost LLM Serving: From Offline Learning to Online Adaptation"
                },
                "updated": "2026-02-13T17:03:20Z",
                "updated_parsed": [
                    2026,
                    2,
                    13,
                    17,
                    3,
                    20,
                    4,
                    44,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.07675v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.07675v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) are revolutionizing how users interact with information systems, yet their high inference cost poses serious scalability and sustainability challenges. Caching inference responses, allowing them to be retrieved without another forward pass through the LLM, has emerged as one possible solution. Traditional exact-match caching, however, overlooks the semantic similarity between queries, leading to unnecessary recomputation. Semantic caching addresses this by retrieving responses based on semantic similarity, but introduces a fundamentally different cache eviction problem: one must account for mismatch costs between incoming queries and cached responses. Moreover, key system parameters, such as query arrival probabilities and serving costs, are often unknown and must be learned over time. Existing semantic caching methods are largely ad-hoc, lacking theoretical foundations and unable to adapt to real-world uncertainty. In this paper, we present a principled, learning-based framework for semantic cache eviction under unknown query and cost distributions. We formulate both offline optimization and online learning variants of the problem, and develop provably efficient algorithms with state-of-the-art guarantees. We also evaluate our framework on a synthetic dataset, showing that our proposed algorithms perform matching or superior performance compared with baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are revolutionizing how users interact with information systems, yet their high inference cost poses serious scalability and sustainability challenges. Caching inference responses, allowing them to be retrieved without another forward pass through the LLM, has emerged as one possible solution. Traditional exact-match caching, however, overlooks the semantic similarity between queries, leading to unnecessary recomputation. Semantic caching addresses this by retrieving responses based on semantic similarity, but introduces a fundamentally different cache eviction problem: one must account for mismatch costs between incoming queries and cached responses. Moreover, key system parameters, such as query arrival probabilities and serving costs, are often unknown and must be learned over time. Existing semantic caching methods are largely ad-hoc, lacking theoretical foundations and unable to adapt to real-world uncertainty. In this paper, we present a principled, learning-based framework for semantic cache eviction under unknown query and cost distributions. We formulate both offline optimization and online learning variants of the problem, and develop provably efficient algorithms with state-of-the-art guarantees. We also evaluate our framework on a synthetic dataset, showing that our proposed algorithms perform matching or superior performance compared with baselines."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-11T06:53:27Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    6,
                    53,
                    27,
                    0,
                    223,
                    0
                ],
                "arxiv_comment": "Accepted to INFOCOM 2026",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Xutong Liu"
                    },
                    {
                        "name": "Baran Atalar"
                    },
                    {
                        "name": "Xiangxiang Dai"
                    },
                    {
                        "name": "Jinhang Zuo"
                    },
                    {
                        "name": "Siwei Wang"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Carlee Joe-Wong"
                    }
                ],
                "author_detail": {
                    "name": "Carlee Joe-Wong"
                },
                "author": "Carlee Joe-Wong"
            },
            {
                "id": "http://arxiv.org/abs/2601.20577v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.20577v2",
                "title": "MeCo: Enhancing LLM-Empowered Multi-Robot Collaboration via Similar Task Memoization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MeCo: Enhancing LLM-Empowered Multi-Robot Collaboration via Similar Task Memoization"
                },
                "updated": "2026-02-13T09:56:37Z",
                "updated_parsed": [
                    2026,
                    2,
                    13,
                    9,
                    56,
                    37,
                    4,
                    44,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.20577v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.20577v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Multi-robot systems have been widely deployed in real-world applications, providing significant improvements in efficiency and reductions in labor costs. However, most existing multi-robot collaboration methods rely on extensive task-specific training, which limits their adaptability to new or diverse scenarios. Recent research leverages the language understanding and reasoning capabilities of large language models (LLMs) to enable more flexible collaboration without specialized training. Yet, current LLM-empowered approaches remain inefficient: when confronted with identical or similar tasks, they must replan from scratch because they omit task-level similarities. To address this limitation, we propose MeCo, a similarity-aware multi-robot collaboration framework that applies the principle of ``cache and reuse'' (a.k.a., memoization) to reduce redundant computation. Unlike simple task repetition, identifying and reusing solutions for similar but not identical tasks is far more challenging, particularly in multi-robot settings. To this end, MeCo introduces a new similarity testing method that retrieves previously solved tasks with high relevance, enabling effective plan reuse without re-invoking LLMs. Furthermore, we present MeCoBench, the first benchmark designed to evaluate performance on similar-task collaboration scenarios. Experimental results show that MeCo substantially reduces planning costs and improves success rates compared with state-of-the-art approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-robot systems have been widely deployed in real-world applications, providing significant improvements in efficiency and reductions in labor costs. However, most existing multi-robot collaboration methods rely on extensive task-specific training, which limits their adaptability to new or diverse scenarios. Recent research leverages the language understanding and reasoning capabilities of large language models (LLMs) to enable more flexible collaboration without specialized training. Yet, current LLM-empowered approaches remain inefficient: when confronted with identical or similar tasks, they must replan from scratch because they omit task-level similarities. To address this limitation, we propose MeCo, a similarity-aware multi-robot collaboration framework that applies the principle of ``cache and reuse'' (a.k.a., memoization) to reduce redundant computation. Unlike simple task repetition, identifying and reusing solutions for similar but not identical tasks is far more challenging, particularly in multi-robot settings. To this end, MeCo introduces a new similarity testing method that retrieves previously solved tasks with high relevance, enabling effective plan reuse without re-invoking LLMs. Furthermore, we present MeCoBench, the first benchmark designed to evaluate performance on similar-task collaboration scenarios. Experimental results show that MeCo substantially reduces planning costs and improves success rates compared with state-of-the-art approaches."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-28T13:15:58Z",
                "published_parsed": [
                    2026,
                    1,
                    28,
                    13,
                    15,
                    58,
                    2,
                    28,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Baiqing Wang"
                    },
                    {
                        "name": "Helei Cui"
                    },
                    {
                        "name": "Bo Zhang"
                    },
                    {
                        "name": "Xiaolong Zheng"
                    },
                    {
                        "name": "Bin Guo"
                    },
                    {
                        "name": "Zhiwen Yu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiwen Yu"
                },
                "author": "Zhiwen Yu"
            },
            {
                "id": "http://arxiv.org/abs/2602.11605v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.11605v2",
                "title": "Recurrent Preference Memory for Efficient Long-Sequence Generative Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recurrent Preference Memory for Efficient Long-Sequence Generative Recommendation"
                },
                "updated": "2026-02-13T09:30:22Z",
                "updated_parsed": [
                    2026,
                    2,
                    13,
                    9,
                    30,
                    22,
                    4,
                    44,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.11605v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.11605v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Generative recommendation (GenRec) models typically model user behavior via full attention, but scaling to lifelong sequences is hindered by prohibitive computational costs and noise accumulation from stochastic interactions. To address these challenges, we introduce Rec2PM, a framework that compresses long user interaction histories into compact Preference Memory tokens. Unlike traditional recurrent methods that suffer from serial training, Rec2PM employs a novel self-referential teacher-forcing strategy: it leverages a global view of the history to generate reference memories, which serve as supervision targets for parallelized recurrent updates. This allows for fully parallel training while maintaining the capability for iterative updates during inference. Additionally, by representing memory as token embeddings rather than extensive KV caches, Rec2PM achieves extreme storage efficiency. Experiments on large-scale benchmarks show that Rec2PM significantly reduces inference latency and memory footprint while achieving superior accuracy compared to full-sequence models. Analysis reveals that the Preference Memory functions as a denoising Information Bottleneck, effectively filtering interaction noise to capture robust long-term interests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative recommendation (GenRec) models typically model user behavior via full attention, but scaling to lifelong sequences is hindered by prohibitive computational costs and noise accumulation from stochastic interactions. To address these challenges, we introduce Rec2PM, a framework that compresses long user interaction histories into compact Preference Memory tokens. Unlike traditional recurrent methods that suffer from serial training, Rec2PM employs a novel self-referential teacher-forcing strategy: it leverages a global view of the history to generate reference memories, which serve as supervision targets for parallelized recurrent updates. This allows for fully parallel training while maintaining the capability for iterative updates during inference. Additionally, by representing memory as token embeddings rather than extensive KV caches, Rec2PM achieves extreme storage efficiency. Experiments on large-scale benchmarks show that Rec2PM significantly reduces inference latency and memory footprint while achieving superior accuracy compared to full-sequence models. Analysis reveals that the Preference Memory functions as a denoising Information Bottleneck, effectively filtering interaction noise to capture robust long-term interests."
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-12T05:51:52Z",
                "published_parsed": [
                    2026,
                    2,
                    12,
                    5,
                    51,
                    52,
                    3,
                    43,
                    0
                ],
                "arxiv_comment": "12 pages, 6figures",
                "arxiv_primary_category": {
                    "term": "cs.IR"
                },
                "authors": [
                    {
                        "name": "Yixiao Chen"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Qiyao Wang"
                    },
                    {
                        "name": "Ke Cheng"
                    },
                    {
                        "name": "Xin Xu"
                    },
                    {
                        "name": "Juntong Yan"
                    },
                    {
                        "name": "Shuojin Yang"
                    },
                    {
                        "name": "Menghao Guo"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Huan Yu"
                    },
                    {
                        "name": "Jie Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Jiang"
                },
                "author": "Jie Jiang"
            },
            {
                "id": "http://arxiv.org/abs/2602.13357v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.13357v1",
                "title": "AdaCorrection: Adaptive Offset Cache Correction for Accurate Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaCorrection: Adaptive Offset Cache Correction for Accurate Diffusion Transformers"
                },
                "updated": "2026-02-13T08:11:54Z",
                "updated_parsed": [
                    2026,
                    2,
                    13,
                    8,
                    11,
                    54,
                    4,
                    44,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.13357v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.13357v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion Transformers (DiTs) achieve state-of-the-art performance in high-fidelity image and video generation but suffer from expensive inference due to their iterative denoising structure. While prior methods accelerate sampling by caching intermediate features, they rely on static reuse schedules or coarse-grained heuristics, which often lead to temporal drift and cache misalignment that significantly degrade generation quality. We introduce \\textbf{AdaCorrection}, an adaptive offset cache correction framework that maintains high generation fidelity while enabling efficient cache reuse across Transformer layers during diffusion inference. At each timestep, AdaCorrection estimates cache validity with lightweight spatio-temporal signals and adaptively blends cached and fresh activations. This correction is computed on-the-fly without additional supervision or retraining. Our approach achieves strong generation quality with minimal computational overhead, maintaining near-original FID while providing moderate acceleration. Experiments on image and video diffusion benchmarks show that AdaCorrection consistently improves generation performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) achieve state-of-the-art performance in high-fidelity image and video generation but suffer from expensive inference due to their iterative denoising structure. While prior methods accelerate sampling by caching intermediate features, they rely on static reuse schedules or coarse-grained heuristics, which often lead to temporal drift and cache misalignment that significantly degrade generation quality. We introduce \\textbf{AdaCorrection}, an adaptive offset cache correction framework that maintains high generation fidelity while enabling efficient cache reuse across Transformer layers during diffusion inference. At each timestep, AdaCorrection estimates cache validity with lightweight spatio-temporal signals and adaptively blends cached and fresh activations. This correction is computed on-the-fly without additional supervision or retraining. Our approach achieves strong generation quality with minimal computational overhead, maintaining near-original FID while providing moderate acceleration. Experiments on image and video diffusion benchmarks show that AdaCorrection consistently improves generation performance."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-13T08:11:54Z",
                "published_parsed": [
                    2026,
                    2,
                    13,
                    8,
                    11,
                    54,
                    4,
                    44,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Yanxuan Yu"
                    },
                    {
                        "name": "Ben Lengerich"
                    },
                    {
                        "name": "Ying Nian Wu"
                    }
                ],
                "author_detail": {
                    "name": "Ying Nian Wu"
                },
                "author": "Ying Nian Wu"
            },
            {
                "id": "http://arxiv.org/abs/2602.15902v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.15902v1",
                "title": "Doc-to-LoRA: Learning to Instantly Internalize Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Doc-to-LoRA: Learning to Instantly Internalize Contexts"
                },
                "updated": "2026-02-13T06:54:20Z",
                "updated_parsed": [
                    2026,
                    2,
                    13,
                    6,
                    54,
                    20,
                    4,
                    44,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.15902v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.15902v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Long input sequences are central to in-context learning, document understanding, and multi-step reasoning of Large Language Models (LLMs). However, the quadratic attention cost of Transformers makes inference memory-intensive and slow. While context distillation (CD) can transfer information into model parameters, per-prompt distillation is impractical due to training costs and latency. To address these limitations, we propose Doc-to-LoRA (D2L), a lightweight hypernetwork that meta-learns to perform approximate CD within a single forward pass. Given an unseen prompt, D2L generates a LoRA adapter for a target LLM, enabling subsequent queries to be answered without re-consuming the original context, reducing latency and KV-cache memory consumption during inference of the target LLM. On a long-context needle-in-a-haystack task, D2L successfully learns to map contexts into adapters that store the needle information, achieving near-perfect zero-shot accuracy at sequence lengths exceeding the target LLM's native context window by more than 4x. On real-world QA datasets with limited compute, D2L outperforms standard CD while significantly reducing peak memory consumption and update latency. We envision that D2L can facilitate rapid adaptation of LLMs, opening up the possibility of frequent knowledge updates and personalized chat behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long input sequences are central to in-context learning, document understanding, and multi-step reasoning of Large Language Models (LLMs). However, the quadratic attention cost of Transformers makes inference memory-intensive and slow. While context distillation (CD) can transfer information into model parameters, per-prompt distillation is impractical due to training costs and latency. To address these limitations, we propose Doc-to-LoRA (D2L), a lightweight hypernetwork that meta-learns to perform approximate CD within a single forward pass. Given an unseen prompt, D2L generates a LoRA adapter for a target LLM, enabling subsequent queries to be answered without re-consuming the original context, reducing latency and KV-cache memory consumption during inference of the target LLM. On a long-context needle-in-a-haystack task, D2L successfully learns to map contexts into adapters that store the needle information, achieving near-perfect zero-shot accuracy at sequence lengths exceeding the target LLM's native context window by more than 4x. On real-world QA datasets with limited compute, D2L outperforms standard CD while significantly reducing peak memory consumption and update latency. We envision that D2L can facilitate rapid adaptation of LLMs, opening up the possibility of frequent knowledge updates and personalized chat behavior."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-13T06:54:20Z",
                "published_parsed": [
                    2026,
                    2,
                    13,
                    6,
                    54,
                    20,
                    4,
                    44,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Rujikorn Charakorn"
                    },
                    {
                        "name": "Edoardo Cetin"
                    },
                    {
                        "name": "Shinnosuke Uesaka"
                    },
                    {
                        "name": "Robert Tjarko Lange"
                    }
                ],
                "author_detail": {
                    "name": "Robert Tjarko Lange"
                },
                "author": "Robert Tjarko Lange"
            },
            {
                "id": "http://arxiv.org/abs/2602.12635v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.12635v1",
                "title": "Unleashing Low-Bit Inference on Ascend NPUs: A Comprehensive Evaluation of HiFloat Formats",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing Low-Bit Inference on Ascend NPUs: A Comprehensive Evaluation of HiFloat Formats"
                },
                "updated": "2026-02-13T05:41:31Z",
                "updated_parsed": [
                    2026,
                    2,
                    13,
                    5,
                    41,
                    31,
                    4,
                    44,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.12635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.12635v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As LLMs scale, low-bit floating-point formats like MXFP and NVFP4 offer new opportunities for precision and efficiency. In this work, we evaluate HiFloat (HiF8 and HiF4), a family of formats tailored for Ascend NPUs. Through rigorous comparison across weight-activation and KV-cache tasks, we provide three key insights: (1) INT8 suits narrow-range data, while floating-point formats excel with high-variance data; (2) in 4-bit regimes, HiF4's hierarchical scaling prevents the accuracy collapse seen in integer formats; and (3) HiFloat is fully compatible with state-of-the-art post-training quantization frameworks. Overall, HiFloat provides a solution for high-efficiency LLM inference on NPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As LLMs scale, low-bit floating-point formats like MXFP and NVFP4 offer new opportunities for precision and efficiency. In this work, we evaluate HiFloat (HiF8 and HiF4), a family of formats tailored for Ascend NPUs. Through rigorous comparison across weight-activation and KV-cache tasks, we provide three key insights: (1) INT8 suits narrow-range data, while floating-point formats excel with high-variance data; (2) in 4-bit regimes, HiF4's hierarchical scaling prevents the accuracy collapse seen in integer formats; and (3) HiFloat is fully compatible with state-of-the-art post-training quantization frameworks. Overall, HiFloat provides a solution for high-efficiency LLM inference on NPUs."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-13T05:41:31Z",
                "published_parsed": [
                    2026,
                    2,
                    13,
                    5,
                    41,
                    31,
                    4,
                    44,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Pengxiang Zhao"
                    },
                    {
                        "name": "Hui-Ling Zhen"
                    },
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Han Bao"
                    },
                    {
                        "name": "Weizhe Lin"
                    },
                    {
                        "name": "Zhiyuan Yang"
                    },
                    {
                        "name": "Ziwei Yu"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Zhenhua Dong"
                    }
                ],
                "author_detail": {
                    "name": "Zhenhua Dong"
                },
                "author": "Zhenhua Dong"
            },
            {
                "id": "http://arxiv.org/abs/2602.12618v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.12618v1",
                "title": "Vision Token Reduction via Attention-Driven Self-Compression for Efficient Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Token Reduction via Attention-Driven Self-Compression for Efficient Multimodal Large Language Models"
                },
                "updated": "2026-02-13T04:49:27Z",
                "updated_parsed": [
                    2026,
                    2,
                    13,
                    4,
                    49,
                    27,
                    4,
                    44,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.12618v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.12618v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Multimodal Large Language Models (MLLMs) incur significant computational cost from processing numerous vision tokens through all LLM layers. Prior pruning methods operate either before the LLM, limiting generality due to diverse encoder-projector designs or within the LLM using heuristics that are incompatible with FlashAttention. We take a different approach: rather than identifying unimportant tokens, we treat the LLM itself as the optimal guide for compression. Observing that deeper layers naturally transmit vision-to-text information, we introduce Attention-Driven Self-Compression (ADSC), a simple, broadly applicable method that progressively reduces vision tokens using only the LLM's attention mechanism. Our method applies uniform token downsampling at selected layers, forming bottlenecks that encourage the model to reorganize and compress information into the remaining tokens. It requires no score computation, auxiliary modules, or attention modification, and remains fully compatible with FlashAttention. Applied to LLaVA-1.5, ADSC reduces FLOPs by 53.7% and peak KV-cache memory by 56.7%, while preserving 98.2% of the original model performance. Across multiple benchmarks, it outperforms prior pruning approaches in both efficiency and accuracy. Crucially, under high compression ratios, our method remains robust while heuristic-based techniques degrade sharply.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) incur significant computational cost from processing numerous vision tokens through all LLM layers. Prior pruning methods operate either before the LLM, limiting generality due to diverse encoder-projector designs or within the LLM using heuristics that are incompatible with FlashAttention. We take a different approach: rather than identifying unimportant tokens, we treat the LLM itself as the optimal guide for compression. Observing that deeper layers naturally transmit vision-to-text information, we introduce Attention-Driven Self-Compression (ADSC), a simple, broadly applicable method that progressively reduces vision tokens using only the LLM's attention mechanism. Our method applies uniform token downsampling at selected layers, forming bottlenecks that encourage the model to reorganize and compress information into the remaining tokens. It requires no score computation, auxiliary modules, or attention modification, and remains fully compatible with FlashAttention. Applied to LLaVA-1.5, ADSC reduces FLOPs by 53.7% and peak KV-cache memory by 56.7%, while preserving 98.2% of the original model performance. Across multiple benchmarks, it outperforms prior pruning approaches in both efficiency and accuracy. Crucially, under high compression ratios, our method remains robust while heuristic-based techniques degrade sharply."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-13T04:49:27Z",
                "published_parsed": [
                    2026,
                    2,
                    13,
                    4,
                    49,
                    27,
                    4,
                    44,
                    0
                ],
                "arxiv_comment": "2025 IEEE International Conference on Big Data (BigData)",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Omer Faruk Deniz"
                    },
                    {
                        "name": "Ruiyu Mao"
                    },
                    {
                        "name": "Ruochen Li"
                    },
                    {
                        "name": "Yapeng Tian"
                    },
                    {
                        "name": "Latifur Khan"
                    }
                ],
                "author_detail": {
                    "name": "Latifur Khan"
                },
                "author": "Latifur Khan"
            },
            {
                "id": "http://arxiv.org/abs/2602.12596v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.12596v1",
                "title": "Arcalis: Accelerating Remote Procedure Calls Using a Lightweight Near-Cache Solution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arcalis: Accelerating Remote Procedure Calls Using a Lightweight Near-Cache Solution"
                },
                "updated": "2026-02-13T04:14:42Z",
                "updated_parsed": [
                    2026,
                    2,
                    13,
                    4,
                    14,
                    42,
                    4,
                    44,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.12596v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.12596v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Modern microservices increasingly depend on high-performance remote procedure calls (RPCs) to coordinate fine-grained, distributed computation. As network bandwidths continue to scale, the CPU overhead associated with RPC processing, particularly serialization, deserialization, and protocol handling, has become a critical bottleneck. This challenge is exacerbated by fast user-space networking stacks such as DPDK, which expose RPC processing as the dominant performance limiter. While prior work has explored software optimizations and FPGA-based offload engines, these approaches remain physically distant from the CPU's memory hierarchy, incurring unnecessary data movement and cache pollution. We present Arcalis, a near-cache RPC accelerator that positions a lightweight hardware engine adjacent to the last-level cache (LLC). Arcalis offloads RPC processing to dedicated microengines on receive and transmit paths that operate with cache-line latency while preserving programmability. By decoupling RPC processing logic, enabling microservice-specific execution, and positioning itself near the LLC to immediately consume data injected by network cards, Arcalis achieves 1.79-4.16$\\times$ end-to-end speedup compared to the CPU baseline, while significantly reducing microarchitectural overhead by up to 88%, and achieves up to a 1.62$\\times$ higher throughput than prior solutions. These results highlight the potential of near-cache RPC acceleration as a practical solution for high-performance microservice deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern microservices increasingly depend on high-performance remote procedure calls (RPCs) to coordinate fine-grained, distributed computation. As network bandwidths continue to scale, the CPU overhead associated with RPC processing, particularly serialization, deserialization, and protocol handling, has become a critical bottleneck. This challenge is exacerbated by fast user-space networking stacks such as DPDK, which expose RPC processing as the dominant performance limiter. While prior work has explored software optimizations and FPGA-based offload engines, these approaches remain physically distant from the CPU's memory hierarchy, incurring unnecessary data movement and cache pollution. We present Arcalis, a near-cache RPC accelerator that positions a lightweight hardware engine adjacent to the last-level cache (LLC). Arcalis offloads RPC processing to dedicated microengines on receive and transmit paths that operate with cache-line latency while preserving programmability. By decoupling RPC processing logic, enabling microservice-specific execution, and positioning itself near the LLC to immediately consume data injected by network cards, Arcalis achieves 1.79-4.16$\\times$ end-to-end speedup compared to the CPU baseline, while significantly reducing microarchitectural overhead by up to 88%, and achieves up to a 1.62$\\times$ higher throughput than prior solutions. These results highlight the potential of near-cache RPC acceleration as a practical solution for high-performance microservice deployment."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-13T04:14:42Z",
                "published_parsed": [
                    2026,
                    2,
                    13,
                    4,
                    14,
                    42,
                    4,
                    44,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Johnson Umeike"
                    },
                    {
                        "name": "Pongstorn Maidee"
                    },
                    {
                        "name": "Bahar Asgari"
                    }
                ],
                "author_detail": {
                    "name": "Bahar Asgari"
                },
                "arxiv_affiliation": "University of Maryland, College Park",
                "author": "Bahar Asgari"
            },
            {
                "id": "http://arxiv.org/abs/2512.17298v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.17298v3",
                "title": "ProCache: Constraint-Aware Feature Caching with Selective Computation for Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProCache: Constraint-Aware Feature Caching with Selective Computation for Diffusion Transformer Acceleration"
                },
                "updated": "2026-02-13T01:46:03Z",
                "updated_parsed": [
                    2026,
                    2,
                    13,
                    1,
                    46,
                    3,
                    4,
                    44,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.17298v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.17298v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion Transformers (DiTs) have achieved state-of-the-art performance in generative modeling, yet their high computational cost hinders real-time deployment. While feature caching offers a promising training-free acceleration solution by exploiting temporal redundancy, existing methods suffer from two key limitations: (1) uniform caching intervals fail to align with the non-uniform temporal dynamics of DiT, and (2) naive feature reuse with excessively large caching intervals can lead to severe error accumulation. In this work, we analyze the evolution of DiT features during denoising and reveal that both feature changes and error propagation are highly time- and depth-varying. Motivated by this, we propose ProCache, a training-free dynamic feature caching framework that addresses these issues via two core components: (i) a constraint-aware caching pattern search module that generates non-uniform activation schedules through offline constrained sampling, tailored to the model's temporal characteristics; and (ii) a selective computation module that selectively computes within deep blocks and high-importance tokens for cached segments to mitigate error accumulation with minimal overhead. Extensive experiments on PixArt-alpha and DiT demonstrate that ProCache achieves up to 1.96x and 2.90x acceleration with negligible quality degradation, significantly outperforming prior caching-based methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have achieved state-of-the-art performance in generative modeling, yet their high computational cost hinders real-time deployment. While feature caching offers a promising training-free acceleration solution by exploiting temporal redundancy, existing methods suffer from two key limitations: (1) uniform caching intervals fail to align with the non-uniform temporal dynamics of DiT, and (2) naive feature reuse with excessively large caching intervals can lead to severe error accumulation. In this work, we analyze the evolution of DiT features during denoising and reveal that both feature changes and error propagation are highly time- and depth-varying. Motivated by this, we propose ProCache, a training-free dynamic feature caching framework that addresses these issues via two core components: (i) a constraint-aware caching pattern search module that generates non-uniform activation schedules through offline constrained sampling, tailored to the model's temporal characteristics; and (ii) a selective computation module that selectively computes within deep blocks and high-importance tokens for cached segments to mitigate error accumulation with minimal overhead. Extensive experiments on PixArt-alpha and DiT demonstrate that ProCache achieves up to 1.96x and 2.90x acceleration with negligible quality degradation, significantly outperforming prior caching-based methods."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-19T07:27:19Z",
                "published_parsed": [
                    2025,
                    12,
                    19,
                    7,
                    27,
                    19,
                    4,
                    353,
                    0
                ],
                "arxiv_comment": "Accepted for poster presentation at AAAI 2026",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Fanpu Cao"
                    },
                    {
                        "name": "Yaofo Chen"
                    },
                    {
                        "name": "Zeng You"
                    },
                    {
                        "name": "Wei Luo"
                    }
                ],
                "author_detail": {
                    "name": "Wei Luo"
                },
                "author": "Wei Luo"
            },
            {
                "id": "http://arxiv.org/abs/2512.07841v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.07841v2",
                "title": "Impact of Data-Oriented and Object-Oriented Design on Performance and Cache Utilization with Artificial Intelligence Algorithms in Multi-Threaded CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Impact of Data-Oriented and Object-Oriented Design on Performance and Cache Utilization with Artificial Intelligence Algorithms in Multi-Threaded CPUs"
                },
                "updated": "2026-02-13T00:12:14Z",
                "updated_parsed": [
                    2026,
                    2,
                    13,
                    0,
                    12,
                    14,
                    4,
                    44,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.07841v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.07841v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The growing performance gap between multi-core CPUs and main memory necessitates hardware-aware software design paradigms. This study provides a comprehensive performance analysis of Data Oriented Design (DOD) versus the traditional Object-Oriented Design (OOD), focusing on cache utilization and efficiency in multi-threaded environments. We developed and compared four distinct versions of the A* search algorithm: single-threaded OOD (ST-OOD), single-threaded DOD (ST-DOD), multi-threaded OOD (MT-OOD), and multi-threaded DOD (MT-DOD). The evaluation was based on metrics including execution time, memory usage, and CPU cache misses. In multi-threaded tests, the DOD implementation demonstrated considerable performance gains, with faster execution times and a lower number of raw system calls and cache misses. While OOD occasionally showed marginal advantages in memory usage or percentage-based cache miss rates, DOD's efficiency in data-intensive operations was more evident. Furthermore, our findings reveal that for a fine-grained task like the A* algorithm, the overhead associated with thread management led to single-threaded versions significantly outperforming their multi-threaded counterparts in both paradigms. We conclude that even when performance differences appear subtle in simple algorithms, the consistent advantages of DOD in critical metrics highlight its foundational architectural superiority, suggesting it is a more effective approach for maximizing hardware efficiency in complex, large-scale AI and parallel computing tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing performance gap between multi-core CPUs and main memory necessitates hardware-aware software design paradigms. This study provides a comprehensive performance analysis of Data Oriented Design (DOD) versus the traditional Object-Oriented Design (OOD), focusing on cache utilization and efficiency in multi-threaded environments. We developed and compared four distinct versions of the A* search algorithm: single-threaded OOD (ST-OOD), single-threaded DOD (ST-DOD), multi-threaded OOD (MT-OOD), and multi-threaded DOD (MT-DOD). The evaluation was based on metrics including execution time, memory usage, and CPU cache misses. In multi-threaded tests, the DOD implementation demonstrated considerable performance gains, with faster execution times and a lower number of raw system calls and cache misses. While OOD occasionally showed marginal advantages in memory usage or percentage-based cache miss rates, DOD's efficiency in data-intensive operations was more evident. Furthermore, our findings reveal that for a fine-grained task like the A* algorithm, the overhead associated with thread management led to single-threaded versions significantly outperforming their multi-threaded counterparts in both paradigms. We conclude that even when performance differences appear subtle in simple algorithms, the consistent advantages of DOD in critical metrics highlight its foundational architectural superiority, suggesting it is a more effective approach for maximizing hardware efficiency in complex, large-scale AI and parallel computing tasks."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-22T17:48:25Z",
                "published_parsed": [
                    2025,
                    11,
                    22,
                    17,
                    48,
                    25,
                    5,
                    326,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "arxiv_journal_ref": "v. 1 n. 26 (2025): Revista Junior de Iniciação Científica em Ciências Exatas e Engenharia",
                "authors": [
                    {
                        "name": "Gabriel M. Arantes"
                    },
                    {
                        "name": "Giancarlo Lucca"
                    },
                    {
                        "name": "Eduardo N. Borges"
                    },
                    {
                        "name": "Richard F. Pinto"
                    },
                    {
                        "name": "Bruno L. Dalmazo"
                    },
                    {
                        "name": "Rafael A. Berri"
                    }
                ],
                "author_detail": {
                    "name": "Rafael A. Berri"
                },
                "author": "Rafael A. Berri"
            },
            {
                "id": "http://arxiv.org/abs/2602.12422v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.12422v1",
                "title": "CacheMind: From Miss Rates to Why -- Natural-Language, Trace-Grounded Reasoning for Cache Replacement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheMind: From Miss Rates to Why -- Natural-Language, Trace-Grounded Reasoning for Cache Replacement"
                },
                "updated": "2026-02-12T21:28:23Z",
                "updated_parsed": [
                    2026,
                    2,
                    12,
                    21,
                    28,
                    23,
                    3,
                    43,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.12422v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.12422v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1145/3779212.3790136",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Cache replacement remains a challenging problem in CPU microarchitecture, often addressed using hand-crafted heuristics, limiting cache performance. Cache data analysis requires parsing millions of trace entries with manual filtering, making the process slow and non-interactive. To address this, we introduce CacheMind, a conversational tool that uses Retrieval-Augmented Generation (RAG) and Large Language Models (LLMs) to enable semantic reasoning over cache traces. Architects can now ask natural language questions like, \"Why is the memory access associated with PC X causing more evictions?\", and receive trace-grounded, human-readable answers linked to program semantics for the first time. To evaluate CacheMind, we present CacheMindBench, the first verified benchmark suite for LLM-based reasoning for the cache replacement problem. Using the SIEVE retriever, CacheMind achieves 66.67% on 75 unseen trace-grounded questions and 84.80% on 25 unseen policy-specific reasoning tasks; with RANGER, it achieves 89.33% and 64.80% on the same evaluations. Additionally, with RANGER, CacheMind achieves 100% accuracy on 4 out of 6 categories in the trace-grounded tier of CacheMindBench. Compared to LlamaIndex (10% retrieval success), SIEVE achieves 60% and RANGER achieves 90%, demonstrating that existing Retrieval-Augmented Generation (RAGs) are insufficient for precise, trace-grounded microarchitectural reasoning. We provided four concrete actionable insights derived using CacheMind, wherein bypassing use case improved cache hit rate by 7.66% and speedup by 2.04%, software fix use case gives speedup of 76%, and Mockingjay replacement policy use case gives speedup of 0.7%; showing the utility of CacheMind on non-trivial queries that require a natural-language interface.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache replacement remains a challenging problem in CPU microarchitecture, often addressed using hand-crafted heuristics, limiting cache performance. Cache data analysis requires parsing millions of trace entries with manual filtering, making the process slow and non-interactive. To address this, we introduce CacheMind, a conversational tool that uses Retrieval-Augmented Generation (RAG) and Large Language Models (LLMs) to enable semantic reasoning over cache traces. Architects can now ask natural language questions like, \"Why is the memory access associated with PC X causing more evictions?\", and receive trace-grounded, human-readable answers linked to program semantics for the first time. To evaluate CacheMind, we present CacheMindBench, the first verified benchmark suite for LLM-based reasoning for the cache replacement problem. Using the SIEVE retriever, CacheMind achieves 66.67% on 75 unseen trace-grounded questions and 84.80% on 25 unseen policy-specific reasoning tasks; with RANGER, it achieves 89.33% and 64.80% on the same evaluations. Additionally, with RANGER, CacheMind achieves 100% accuracy on 4 out of 6 categories in the trace-grounded tier of CacheMindBench. Compared to LlamaIndex (10% retrieval success), SIEVE achieves 60% and RANGER achieves 90%, demonstrating that existing Retrieval-Augmented Generation (RAGs) are insufficient for precise, trace-grounded microarchitectural reasoning. We provided four concrete actionable insights derived using CacheMind, wherein bypassing use case improved cache hit rate by 7.66% and speedup by 2.04%, software fix use case gives speedup of 76%, and Mockingjay replacement policy use case gives speedup of 0.7%; showing the utility of CacheMind on non-trivial queries that require a natural-language interface."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-12T21:28:23Z",
                "published_parsed": [
                    2026,
                    2,
                    12,
                    21,
                    28,
                    23,
                    3,
                    43,
                    0
                ],
                "arxiv_comment": "16 pages, 13 figures, ASPLOS 2026",
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Kaushal Mhapsekar"
                    },
                    {
                        "name": "Azam Ghanbari"
                    },
                    {
                        "name": "Bita Aslrousta"
                    },
                    {
                        "name": "Samira Mirbagher-Ajorpaz"
                    }
                ],
                "author_detail": {
                    "name": "Samira Mirbagher-Ajorpaz"
                },
                "author": "Samira Mirbagher-Ajorpaz",
                "arxiv_doi": "10.1145/3779212.3790136"
            },
            {
                "id": "http://arxiv.org/abs/2602.08798v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.08798v2",
                "title": "CryptoGen: Secure Transformer Generation with Encrypted KV-Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CryptoGen: Secure Transformer Generation with Encrypted KV-Cache Reuse"
                },
                "updated": "2026-02-12T20:32:18Z",
                "updated_parsed": [
                    2026,
                    2,
                    12,
                    20,
                    32,
                    18,
                    3,
                    43,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.08798v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.08798v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The widespread deployment of cloud-hosted generative models raises a fundamental challenge: enabling efficient autoregressive generation while preserving the privacy of both user prompts and model parameters in untrusted environments. We address this challenge in a client-server setting where an untrusted server hosts an autoregressive Transformer and the client requires cryptographic protection for both inputs and inference. We present CryptoGen, the first system to enable scalable privacy-preserving neural generation with persistent encrypted key-value (KV) cache reuse. Discriminative-task secure inference systems incur quadratic latency and memory growth when adapted to autoregressive decoding due to the lack of native encrypted KV-cache support. In contrast, CryptoGen achieves near-linear scaling by securely reusing and updating encrypted KV caches throughout generation. CryptoGen integrates homomorphic encryption and secret sharing to support both prefilling and generation. Key techniques include a unified encrypted KV-cache framework, heterogeneous SIMD encodings for different phases, optimized cipher-cipher matrix-matrix and matrix-vector operations, and efficient noise refresh and ciphertext concatenation mechanisms. Evaluation on generative Transformer models trained on WikiText-2, PTB, and LAMBADA shows that for input lengths of 128-512 tokens, CryptoGen achieves 4.4x-7.6x lower per-token latency than state-of-the-art discriminative secure inference systems, while maintaining near-linear latency and memory scaling, with advantages increasing for longer sequences. CryptoGen is released as an open-source library.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread deployment of cloud-hosted generative models raises a fundamental challenge: enabling efficient autoregressive generation while preserving the privacy of both user prompts and model parameters in untrusted environments. We address this challenge in a client-server setting where an untrusted server hosts an autoregressive Transformer and the client requires cryptographic protection for both inputs and inference. We present CryptoGen, the first system to enable scalable privacy-preserving neural generation with persistent encrypted key-value (KV) cache reuse. Discriminative-task secure inference systems incur quadratic latency and memory growth when adapted to autoregressive decoding due to the lack of native encrypted KV-cache support. In contrast, CryptoGen achieves near-linear scaling by securely reusing and updating encrypted KV caches throughout generation. CryptoGen integrates homomorphic encryption and secret sharing to support both prefilling and generation. Key techniques include a unified encrypted KV-cache framework, heterogeneous SIMD encodings for different phases, optimized cipher-cipher matrix-matrix and matrix-vector operations, and efficient noise refresh and ciphertext concatenation mechanisms. Evaluation on generative Transformer models trained on WikiText-2, PTB, and LAMBADA shows that for input lengths of 128-512 tokens, CryptoGen achieves 4.4x-7.6x lower per-token latency than state-of-the-art discriminative secure inference systems, while maintaining near-linear latency and memory scaling, with advantages increasing for longer sequences. CryptoGen is released as an open-source library."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-09T15:38:13Z",
                "published_parsed": [
                    2026,
                    2,
                    9,
                    15,
                    38,
                    13,
                    0,
                    40,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Hedong Zhang"
                    },
                    {
                        "name": "Neusha Javidnia"
                    },
                    {
                        "name": "Shweta Pardeshi"
                    },
                    {
                        "name": "Qian Lou"
                    },
                    {
                        "name": "Farinaz Koushanfar"
                    }
                ],
                "author_detail": {
                    "name": "Farinaz Koushanfar"
                },
                "author": "Farinaz Koushanfar"
            },
            {
                "id": "http://arxiv.org/abs/2602.12220v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.12220v1",
                "title": "Taming Subpacketization without Sacrificing Communication: A Packet Type-based Framework for D2D Coded Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Taming Subpacketization without Sacrificing Communication: A Packet Type-based Framework for D2D Coded Caching"
                },
                "updated": "2026-02-12T17:58:37Z",
                "updated_parsed": [
                    2026,
                    2,
                    12,
                    17,
                    58,
                    37,
                    3,
                    43,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.12220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.12220v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Finite-length analysis is critical for bringing coded caching closer to practical deployment. In this work, we study the design of communication rate-optimal device-to-device (D2D) coded caching schemes with minimal subpacketization levels, a key bottleneck in finite-length settings. We present a novel \\tit{packet type-based} (PT) design framework that (i) strategically introduces \\tit{asymmetry} into file splitting through user grouping, and (ii) systematically exploits such asymmetry in both cache placement and multicast delivery to create subpacketization reduction opportunities. In particular, the induced asymmetry gives rise to two fundamental forms of subpacketization reduction gains: the \\emph{subfile saving gain}, achieved by eliminating certain types of subfiles through careful user grouping and transmitter selection, and the \\emph{further splitting saving gain}, attained by reducing the splitting granularity for the remaining subfiles. The combined effect of these two reduction gains yields an overall subpacketization improvement over the original Ji-Caire-Molisch (JCM) caching scheme~\\cite{ji2016fundamental}, as well as various state-of-the-art schemes, while preserving optimal communication rates.\n  Under the PT framework, we formulate the caching scheme design as an integer linear program (ILP), where each feasible solution corresponds to a valid rate-optimal D2D coded caching scheme with potentially reduced subpacketization relative to the JCM baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finite-length analysis is critical for bringing coded caching closer to practical deployment. In this work, we study the design of communication rate-optimal device-to-device (D2D) coded caching schemes with minimal subpacketization levels, a key bottleneck in finite-length settings. We present a novel \\tit{packet type-based} (PT) design framework that (i) strategically introduces \\tit{asymmetry} into file splitting through user grouping, and (ii) systematically exploits such asymmetry in both cache placement and multicast delivery to create subpacketization reduction opportunities. In particular, the induced asymmetry gives rise to two fundamental forms of subpacketization reduction gains: the \\emph{subfile saving gain}, achieved by eliminating certain types of subfiles through careful user grouping and transmitter selection, and the \\emph{further splitting saving gain}, attained by reducing the splitting granularity for the remaining subfiles. The combined effect of these two reduction gains yields an overall subpacketization improvement over the original Ji-Caire-Molisch (JCM) caching scheme~\\cite{ji2016fundamental}, as well as various state-of-the-art schemes, while preserving optimal communication rates.\n  Under the PT framework, we formulate the caching scheme design as an integer linear program (ILP), where each feasible solution corresponds to a valid rate-optimal D2D coded caching scheme with potentially reduced subpacketization relative to the JCM baseline."
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-12T17:58:37Z",
                "published_parsed": [
                    2026,
                    2,
                    12,
                    17,
                    58,
                    37,
                    3,
                    43,
                    0
                ],
                "arxiv_comment": "Submitted to IEEE Transactions on Information Theory",
                "arxiv_primary_category": {
                    "term": "cs.IT"
                },
                "authors": [
                    {
                        "name": "Xiang Zhang"
                    },
                    {
                        "name": "Giuseppe Caire"
                    },
                    {
                        "name": "Mingyue Ji"
                    }
                ],
                "author_detail": {
                    "name": "Mingyue Ji"
                },
                "author": "Mingyue Ji"
            },
            {
                "id": "http://arxiv.org/abs/2602.12029v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.12029v1",
                "title": "PrefillShare: A Shared Prefill Module for KV Reuse in Multi-LLM Disaggregated Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefillShare: A Shared Prefill Module for KV Reuse in Multi-LLM Disaggregated Serving"
                },
                "updated": "2026-02-12T14:59:50Z",
                "updated_parsed": [
                    2026,
                    2,
                    12,
                    14,
                    59,
                    50,
                    3,
                    43,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.12029v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.12029v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Multi-agent systems increasingly orchestrate multiple specialized language models to solve complex real-world problems, often invoking them over a shared context. This execution pattern repeatedly processes the same prompt prefix across models. Consequently, each model redundantly executes the prefill stage and maintains its own key-value (KV) cache, increasing aggregate prefill load and worsening tail latency by intensifying prefill-decode interference in existing LLM serving stacks. Disaggregated serving reduces such interference by placing prefill and decode on separate GPUs, but disaggregation does not fundamentally eliminate inter-model redundancy in computation and KV storage for the same prompt. To address this issue, we propose PrefillShare, a novel algorithm that enables sharing the prefill stage across multiple models in a disaggregated setting. PrefillShare factorizes the model into prefill and decode modules, freezes the prefill module, and fine-tunes only the decode module. This design allows multiple task-specific models to share a prefill module and the KV cache generated for the same prompt. We further introduce a routing mechanism that enables effective prefill sharing across heterogeneous models in a vLLM-based disaggregated system. PrefillShare not only matches full fine-tuning accuracy on a broad range of tasks and models, but also delivers 4.5x lower p95 latency and 3.9x higher throughput in multi-model agent workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent systems increasingly orchestrate multiple specialized language models to solve complex real-world problems, often invoking them over a shared context. This execution pattern repeatedly processes the same prompt prefix across models. Consequently, each model redundantly executes the prefill stage and maintains its own key-value (KV) cache, increasing aggregate prefill load and worsening tail latency by intensifying prefill-decode interference in existing LLM serving stacks. Disaggregated serving reduces such interference by placing prefill and decode on separate GPUs, but disaggregation does not fundamentally eliminate inter-model redundancy in computation and KV storage for the same prompt. To address this issue, we propose PrefillShare, a novel algorithm that enables sharing the prefill stage across multiple models in a disaggregated setting. PrefillShare factorizes the model into prefill and decode modules, freezes the prefill module, and fine-tunes only the decode module. This design allows multiple task-specific models to share a prefill module and the KV cache generated for the same prompt. We further introduce a routing mechanism that enables effective prefill sharing across heterogeneous models in a vLLM-based disaggregated system. PrefillShare not only matches full fine-tuning accuracy on a broad range of tasks and models, but also delivers 4.5x lower p95 latency and 3.9x higher throughput in multi-model agent workloads."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-12T14:59:50Z",
                "published_parsed": [
                    2026,
                    2,
                    12,
                    14,
                    59,
                    50,
                    3,
                    43,
                    0
                ],
                "arxiv_comment": "Preprint. 13 pages, 6 figures",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Sunghyeon Woo"
                    },
                    {
                        "name": "Hoseung Kim"
                    },
                    {
                        "name": "Sunghwan Shim"
                    },
                    {
                        "name": "Minjung Jo"
                    },
                    {
                        "name": "Hyunjoon Jeong"
                    },
                    {
                        "name": "Jeongtae Lee"
                    },
                    {
                        "name": "Joonghoon Kim"
                    },
                    {
                        "name": "Sungjae Lee"
                    },
                    {
                        "name": "Baeseong Park"
                    },
                    {
                        "name": "Se Jung Kwon"
                    },
                    {
                        "name": "Dongsoo Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongsoo Lee"
                },
                "author": "Dongsoo Lee"
            },
            {
                "id": "http://arxiv.org/abs/2602.11937v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.11937v1",
                "title": "Extending Puzzle for Mixture-of-Experts Reasoning Models with Application to GPT-OSS Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extending Puzzle for Mixture-of-Experts Reasoning Models with Application to GPT-OSS Acceleration"
                },
                "updated": "2026-02-12T13:36:19Z",
                "updated_parsed": [
                    2026,
                    2,
                    12,
                    13,
                    36,
                    19,
                    3,
                    43,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.11937v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.11937v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Reasoning-focused LLMs improve answer quality by generating longer reasoning traces, but the additional tokens dramatically increase serving cost, motivating inference optimization. We extend and apply Puzzle, a post-training neural architecture search (NAS) framework, to gpt-oss-120B to produce gpt-oss-puzzle-88B, a deployment-optimized derivative. Our approach combines heterogeneous MoE expert pruning, selective replacement of full-context attention with window attention, FP8 KV-cache quantization with calibrated scales, and post-training reinforcement learning to recover accuracy, while maintaining low generation length. In terms of per-token speeds, on an 8XH100 node we achieve 1.63X and 1.22X throughput speedups in long-context and short-context settings, respectively. gpt-oss-puzzle-88B also delivers throughput speedups of 2.82X on a single NVIDIA H100 GPU. However, because token counts can change with reasoning effort and model variants, per-token throughput (tok/s) and latency (ms/token) do not necessarily lead to end-to-end speedups: a 2X throughput gain is erased if traces grow 2X. Conversely, throughput gains can be spent on more reasoning tokens to improve accuracy; we therefore advocate request-level efficiency metrics that normalize throughput by tokens generated and trace an accuracy--speed frontier across reasoning efforts. We show that gpt-oss-puzzle-88B improves over gpt-oss-120B along the entire frontier, delivering up to 1.29X higher request-level efficiency. Across various benchmarks, gpt-oss-puzzle-88B matches or slightly exceeds the parent on suite-average accuracy across reasoning efforts, with retention ranging from 100.8% (high) to 108.2% (low), showing that post-training architecture search can substantially reduce inference costs without sacrificing quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning-focused LLMs improve answer quality by generating longer reasoning traces, but the additional tokens dramatically increase serving cost, motivating inference optimization. We extend and apply Puzzle, a post-training neural architecture search (NAS) framework, to gpt-oss-120B to produce gpt-oss-puzzle-88B, a deployment-optimized derivative. Our approach combines heterogeneous MoE expert pruning, selective replacement of full-context attention with window attention, FP8 KV-cache quantization with calibrated scales, and post-training reinforcement learning to recover accuracy, while maintaining low generation length. In terms of per-token speeds, on an 8XH100 node we achieve 1.63X and 1.22X throughput speedups in long-context and short-context settings, respectively. gpt-oss-puzzle-88B also delivers throughput speedups of 2.82X on a single NVIDIA H100 GPU. However, because token counts can change with reasoning effort and model variants, per-token throughput (tok/s) and latency (ms/token) do not necessarily lead to end-to-end speedups: a 2X throughput gain is erased if traces grow 2X. Conversely, throughput gains can be spent on more reasoning tokens to improve accuracy; we therefore advocate request-level efficiency metrics that normalize throughput by tokens generated and trace an accuracy--speed frontier across reasoning efforts. We show that gpt-oss-puzzle-88B improves over gpt-oss-120B along the entire frontier, delivering up to 1.29X higher request-level efficiency. Across various benchmarks, gpt-oss-puzzle-88B matches or slightly exceeds the parent on suite-average accuracy across reasoning efforts, with retention ranging from 100.8% (high) to 108.2% (low), showing that post-training architecture search can substantially reduce inference costs without sacrificing quality."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-12T13:36:19Z",
                "published_parsed": [
                    2026,
                    2,
                    12,
                    13,
                    36,
                    19,
                    3,
                    43,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Akhiad Bercovich"
                    },
                    {
                        "name": "Nir Ailon"
                    },
                    {
                        "name": "Vladimir Anisimov"
                    },
                    {
                        "name": "Tomer Asida"
                    },
                    {
                        "name": "Nave Assaf"
                    },
                    {
                        "name": "Mohammad Dabbah"
                    },
                    {
                        "name": "Ido Galil"
                    },
                    {
                        "name": "Amnon Geifman"
                    },
                    {
                        "name": "Yonatan Geifman"
                    },
                    {
                        "name": "Izhak Golan"
                    },
                    {
                        "name": "Roi Koren"
                    },
                    {
                        "name": "Itay Levy"
                    },
                    {
                        "name": "Zach Moshe"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Najeeb Nabwani"
                    },
                    {
                        "name": "Mostofa Patwari"
                    },
                    {
                        "name": "Omri Puny"
                    },
                    {
                        "name": "Tomer Ronen"
                    },
                    {
                        "name": "Itamar Schen"
                    },
                    {
                        "name": "Elad Segal"
                    },
                    {
                        "name": "Ido Shahaf"
                    },
                    {
                        "name": "Oren Tropp"
                    },
                    {
                        "name": "Ran Zilberstein"
                    },
                    {
                        "name": "Ran El-Yaniv"
                    }
                ],
                "author_detail": {
                    "name": "Ran El-Yaniv"
                },
                "author": "Ran El-Yaniv"
            },
            {
                "id": "http://arxiv.org/abs/2602.11808v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.11808v1",
                "title": "Deep Kernel Fusion for Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Kernel Fusion for Transformers"
                },
                "updated": "2026-02-12T10:43:59Z",
                "updated_parsed": [
                    2026,
                    2,
                    12,
                    10,
                    43,
                    59,
                    3,
                    43,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.11808v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.11808v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Agentic LLM inference with long contexts is increasingly limited by memory bandwidth rather than compute. In this setting, SwiGLU MLP blocks, whose large weights exceed cache capacity, become a major yet under-optimized bottleneck. We propose DeepFusionKernel, a deeply fused kernel that cuts HBM traffic and boosts cache reuse, delivering up to 13.2% speedup on H100 and 9.7% on A100 over SGLang. Integrated with SGLang and paired with a kernel scheduler, DeepFusionKernel ensures consistent accelerations over generation lengths, while remaining adaptable to diverse models, inference configurations, and hardware platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic LLM inference with long contexts is increasingly limited by memory bandwidth rather than compute. In this setting, SwiGLU MLP blocks, whose large weights exceed cache capacity, become a major yet under-optimized bottleneck. We propose DeepFusionKernel, a deeply fused kernel that cuts HBM traffic and boosts cache reuse, delivering up to 13.2% speedup on H100 and 9.7% on A100 over SGLang. Integrated with SGLang and paired with a kernel scheduler, DeepFusionKernel ensures consistent accelerations over generation lengths, while remaining adaptable to diverse models, inference configurations, and hardware platforms."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-12T10:43:59Z",
                "published_parsed": [
                    2026,
                    2,
                    12,
                    10,
                    43,
                    59,
                    3,
                    43,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Zixi Zhang"
                    },
                    {
                        "name": "Zhiwen Mo"
                    },
                    {
                        "name": "Yiren Zhao"
                    },
                    {
                        "name": "Robert Mullins"
                    }
                ],
                "author_detail": {
                    "name": "Robert Mullins"
                },
                "author": "Robert Mullins"
            },
            {
                "id": "http://arxiv.org/abs/2510.03346v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.03346v2",
                "title": "KVComm: Enabling Efficient LLM Communication through Selective KV Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVComm: Enabling Efficient LLM Communication through Selective KV Sharing"
                },
                "updated": "2026-02-12T09:38:03Z",
                "updated_parsed": [
                    2026,
                    2,
                    12,
                    9,
                    38,
                    3,
                    3,
                    43,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.03346v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.03346v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) are increasingly deployed in multi-agent systems, where effective inter-model communication is crucial. Existing communication protocols either rely on natural language, incurring high inference costs and information loss, or on hidden states, which suffer from information concentration bias and inefficiency. To address these limitations, we propose KVComm, a novel communication framework that enables efficient communication between LLMs through selective sharing of KV pairs. KVComm leverages the rich information encoded in the KV pairs while avoiding the pitfalls of hidden states. We introduce a KV layer-wise selection strategy based on attention importance scores with a Gaussian prior to identify the most informative KV pairs for communication. Extensive experiments across diverse tasks and model pairs demonstrate that KVComm achieves comparable performance to the upper-bound method, which directly merges inputs to one model without any communication, while transmitting as few as 30\\% of layers' KV pairs. Our study highlights the potential of KV pairs as an effective medium for inter-LLM communication, paving the way for scalable and efficient multi-agent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in multi-agent systems, where effective inter-model communication is crucial. Existing communication protocols either rely on natural language, incurring high inference costs and information loss, or on hidden states, which suffer from information concentration bias and inefficiency. To address these limitations, we propose KVComm, a novel communication framework that enables efficient communication between LLMs through selective sharing of KV pairs. KVComm leverages the rich information encoded in the KV pairs while avoiding the pitfalls of hidden states. We introduce a KV layer-wise selection strategy based on attention importance scores with a Gaussian prior to identify the most informative KV pairs for communication. Extensive experiments across diverse tasks and model pairs demonstrate that KVComm achieves comparable performance to the upper-bound method, which directly merges inputs to one model without any communication, while transmitting as few as 30\\% of layers' KV pairs. Our study highlights the potential of KV pairs as an effective medium for inter-LLM communication, paving the way for scalable and efficient multi-agent systems."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-02T16:01:54Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    1,
                    54,
                    3,
                    275,
                    0
                ],
                "arxiv_comment": "ICLR 2026",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Xiangyu Shi"
                    },
                    {
                        "name": "Marco Chiesa"
                    },
                    {
                        "name": "Gerald Q. Maguire"
                    },
                    {
                        "name": "Dejan Kostic"
                    }
                ],
                "author_detail": {
                    "name": "Dejan Kostic"
                },
                "author": "Dejan Kostic"
            },
            {
                "id": "http://arxiv.org/abs/2602.11741v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.11741v1",
                "title": "Designing Scalable Rate Limiting Systems: Algorithms, Architecture, and Distributed Solutions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing Scalable Rate Limiting Systems: Algorithms, Architecture, and Distributed Solutions"
                },
                "updated": "2026-02-12T09:11:08Z",
                "updated_parsed": [
                    2026,
                    2,
                    12,
                    9,
                    11,
                    8,
                    3,
                    43,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.11741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.11741v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Designing a rate limiter that is simultaneously accurate, available, and scalable presents a fundamental challenge in distributed systems, primarily due to the trade-offs between algorithmic precision, availability, consistency, and partition tolerance. This article presents a concrete architecture for a distributed rate limiting system in a production-grade environment. Our design chooses the in-memory cache database, the Redis, along with its Sorted Set data structure, which provides $O(log (N))$ time complexity operation for the key-value pair dataset with efficiency and low latency, and maintains precision. The core contribution is quantifying the accuracy and memory cost trade-off of the chosen Rolling Window as the implemented rate limiting algorithm against the Token Bucket and Fixed Window algorithms. In addition, we explain how server-side Lua scripting is critical to bundling cleanup, counting, and insertion into a single atomic operation, thereby eliminating race conditions in concurrent environments. In the system architecture, we propose a three-layer architecture that manages the storage and updating of the limit rules. Through script load by hashing the rule parameters, rules can be changed without modifying the cached scripts. Furthermore, we analyze the deployment of this architecture on a Redis Cluster, which provides the availability and scalability by data sharding and replication. We explain the acceptance of AP (Availability and Partition Tolerance) from the CAP theorem as the pragmatic engineering trade-off for this use case.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing a rate limiter that is simultaneously accurate, available, and scalable presents a fundamental challenge in distributed systems, primarily due to the trade-offs between algorithmic precision, availability, consistency, and partition tolerance. This article presents a concrete architecture for a distributed rate limiting system in a production-grade environment. Our design chooses the in-memory cache database, the Redis, along with its Sorted Set data structure, which provides $O(log (N))$ time complexity operation for the key-value pair dataset with efficiency and low latency, and maintains precision. The core contribution is quantifying the accuracy and memory cost trade-off of the chosen Rolling Window as the implemented rate limiting algorithm against the Token Bucket and Fixed Window algorithms. In addition, we explain how server-side Lua scripting is critical to bundling cleanup, counting, and insertion into a single atomic operation, thereby eliminating race conditions in concurrent environments. In the system architecture, we propose a three-layer architecture that manages the storage and updating of the limit rules. Through script load by hashing the rule parameters, rules can be changed without modifying the cached scripts. Furthermore, we analyze the deployment of this architecture on a Redis Cluster, which provides the availability and scalability by data sharding and replication. We explain the acceptance of AP (Availability and Partition Tolerance) from the CAP theorem as the pragmatic engineering trade-off for this use case."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-12T09:11:08Z",
                "published_parsed": [
                    2026,
                    2,
                    12,
                    9,
                    11,
                    8,
                    3,
                    43,
                    0
                ],
                "arxiv_comment": "27 pages, 8 figures, 2 tables",
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Bo Guan"
                    }
                ],
                "author_detail": {
                    "name": "Bo Guan"
                },
                "author": "Bo Guan"
            },
            {
                "id": "http://arxiv.org/abs/2602.11694v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.11694v1",
                "title": "Emergent spin-resolved electronic charge density waves and pseudogap phenomena from strong $d$-wave altermagnetism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emergent spin-resolved electronic charge density waves and pseudogap phenomena from strong $d$-wave altermagnetism"
                },
                "updated": "2026-02-12T08:19:55Z",
                "updated_parsed": [
                    2026,
                    2,
                    12,
                    8,
                    19,
                    55,
                    3,
                    43,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.11694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.11694v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Inspired by recent discovery of metallic $d$-wave altermagnetism in KV$_2$Se$_2$O, we develop a self-consistent microscopic many-body calculation of density-wave order for an itinerant altermagnetic metal. We show that the strong $d$-wave spin-momentum locking inherent to the altermagnetic band structure reconstructs the Fermi surface into spin-selective quasi-1D open sheets. This unique topology of Fermi surface drives an instability toward spin-resolved electronic charge density waves (CDWs), in which the ordering wave vectors for spin-up and spin-down electrons condense along two mutually orthogonal directions, forming spin-resolved stripe phases. As a consequence, this results in pronounced gap openings near the Fermi surface, and the superposition of these spin-resolved stripe orders leads to a checkerboard CDW in the charge channel and an antiphase spin-density-wave modulation in the spin channel. Upon increasing temperature, the density-wave order melts at $T_c$ due to thermal phase fluctuation while the gap opening persists, giving rise to a robust pseudogap regime, which eventually closes at a higher temperature $T_g$. The resulting simulations quantitatively reproduce the key features observed in the spectroscopic measurements, offering a consistent and generic understanding of the reported phenomena in KV$_2$Se$_2$O and, more broadly, in metallic altermagnets with strong spin-momentum locking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inspired by recent discovery of metallic $d$-wave altermagnetism in KV$_2$Se$_2$O, we develop a self-consistent microscopic many-body calculation of density-wave order for an itinerant altermagnetic metal. We show that the strong $d$-wave spin-momentum locking inherent to the altermagnetic band structure reconstructs the Fermi surface into spin-selective quasi-1D open sheets. This unique topology of Fermi surface drives an instability toward spin-resolved electronic charge density waves (CDWs), in which the ordering wave vectors for spin-up and spin-down electrons condense along two mutually orthogonal directions, forming spin-resolved stripe phases. As a consequence, this results in pronounced gap openings near the Fermi surface, and the superposition of these spin-resolved stripe orders leads to a checkerboard CDW in the charge channel and an antiphase spin-density-wave modulation in the spin channel. Upon increasing temperature, the density-wave order melts at $T_c$ due to thermal phase fluctuation while the gap opening persists, giving rise to a robust pseudogap regime, which eventually closes at a higher temperature $T_g$. The resulting simulations quantitatively reproduce the key features observed in the spectroscopic measurements, offering a consistent and generic understanding of the reported phenomena in KV$_2$Se$_2$O and, more broadly, in metallic altermagnets with strong spin-momentum locking."
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-12T08:19:55Z",
                "published_parsed": [
                    2026,
                    2,
                    12,
                    8,
                    19,
                    55,
                    3,
                    43,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el"
                },
                "authors": [
                    {
                        "name": "Fei Yang"
                    },
                    {
                        "name": "Guo-Dong Zhao"
                    },
                    {
                        "name": "Binghai Yan"
                    },
                    {
                        "name": "Long-Qing Chen"
                    }
                ],
                "author_detail": {
                    "name": "Long-Qing Chen"
                },
                "author": "Long-Qing Chen"
            },
            {
                "id": "http://arxiv.org/abs/2602.11688v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.11688v1",
                "title": "GORGO: Maximizing KV-Cache Reuse While Minimizing Network Latency in Cross-Region LLM Load Balancing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GORGO: Maximizing KV-Cache Reuse While Minimizing Network Latency in Cross-Region LLM Load Balancing"
                },
                "updated": "2026-02-12T08:09:14Z",
                "updated_parsed": [
                    2026,
                    2,
                    12,
                    8,
                    9,
                    14,
                    3,
                    43,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.11688v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.11688v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Distributing LLM inference across geographical regions can improve Time-to-First-Token (TTFT) by regionalizing service deployments. While existing multi-region load balancers save prefill computation by prioritizing Key--Value (KV) Cache hit rate, they ignore cluster networking latency, a critical factor in routing decisions. We introduce GORGO, a method for minimizing TTFT by optimizing a total serving cost as a function of available compute, network latency, and prefix caching. Using extensive profiling on custom infrastructure, we analyze component-level latency bottlenecks and benchmark GORGO against three baselines: (1) naive least-load routing, which ignores prefix-cache overlap; (2) prefix-similarity routing, which selectively pushes requests to the replica with the highest cached-prefix overlap; and (3) a centralized HTTP proxy that runs the GORGO policy while tracking requests across all nodes. We demonstrate that GORGO reduces P99 TTFT through network-aware routing and improves average TTFT by preventing pathological cross-region forwarding. Additionally, we find that GORGO-proxy overcomes synchronization overhead in previous methods and is 2.5x faster on median TTFT, demonstrating the success of a centralized router.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributing LLM inference across geographical regions can improve Time-to-First-Token (TTFT) by regionalizing service deployments. While existing multi-region load balancers save prefill computation by prioritizing Key--Value (KV) Cache hit rate, they ignore cluster networking latency, a critical factor in routing decisions. We introduce GORGO, a method for minimizing TTFT by optimizing a total serving cost as a function of available compute, network latency, and prefix caching. Using extensive profiling on custom infrastructure, we analyze component-level latency bottlenecks and benchmark GORGO against three baselines: (1) naive least-load routing, which ignores prefix-cache overlap; (2) prefix-similarity routing, which selectively pushes requests to the replica with the highest cached-prefix overlap; and (3) a centralized HTTP proxy that runs the GORGO policy while tracking requests across all nodes. We demonstrate that GORGO reduces P99 TTFT through network-aware routing and improves average TTFT by preventing pathological cross-region forwarding. Additionally, we find that GORGO-proxy overcomes synchronization overhead in previous methods and is 2.5x faster on median TTFT, demonstrating the success of a centralized router."
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-12T08:09:14Z",
                "published_parsed": [
                    2026,
                    2,
                    12,
                    8,
                    9,
                    14,
                    3,
                    43,
                    0
                ],
                "arxiv_comment": "12 pages, 4 figures. Code: https://github.com/atoniolo76/gotoni/tree/benchmark-load-balancing",
                "arxiv_primary_category": {
                    "term": "cs.NI"
                },
                "authors": [
                    {
                        "name": "Alessio Ricci Toniolo"
                    },
                    {
                        "name": "Abinaya Dinesh"
                    },
                    {
                        "name": "Rome Thorstenson"
                    }
                ],
                "author_detail": {
                    "name": "Rome Thorstenson"
                },
                "author": "Rome Thorstenson"
            },
            {
                "id": "http://arxiv.org/abs/2602.07837v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.07837v3",
                "title": "RLinf-USER: A Unified and Extensible System for Real-World Online Policy Learning in Embodied AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RLinf-USER: A Unified and Extensible System for Real-World Online Policy Learning in Embodied AI"
                },
                "updated": "2026-02-12T08:08:43Z",
                "updated_parsed": [
                    2026,
                    2,
                    12,
                    8,
                    8,
                    43,
                    3,
                    43,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.07837v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.07837v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Online policy learning directly in the physical world is a promising yet challenging direction for embodied intelligence. Unlike simulation, real-world systems cannot be arbitrarily accelerated, cheaply reset, or massively replicated, which makes scalable data collection, heterogeneous deployment, and long-horizon effective training difficult. These challenges suggest that real-world policy learning is not only an algorithmic issue but fundamentally a systems problem. We present USER, a Unified and extensible SystEm for Real-world online policy learning. USER treats physical robots as first-class hardware resources alongside GPUs through a unified hardware abstraction layer, enabling automatic discovery, management, and scheduling of heterogeneous robots. To address cloud-edge communication, USER introduces an adaptive communication plane with tunneling-based networking, distributed data channels for traffic localization, and streaming-multiprocessor-aware weight synchronization to regulate GPU-side overhead. On top of this infrastructure, USER organizes learning as a fully asynchronous framework with a persistent, cache-aware buffer, enabling efficient long-horizon experiments with robust crash recovery and reuse of historical data. In addition, USER provides extensible abstractions for rewards, algorithms, and policies, supporting online imitation or reinforcement learning of CNN/MLP, generative policies, and large vision-language-action (VLA) models within a unified pipeline. Results in both simulation and the real world show that USER enables multi-robot coordination, heterogeneous manipulators, edge-cloud collaboration with large models, and long-running asynchronous training, offering a unified and extensible systems foundation for real-world online policy learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online policy learning directly in the physical world is a promising yet challenging direction for embodied intelligence. Unlike simulation, real-world systems cannot be arbitrarily accelerated, cheaply reset, or massively replicated, which makes scalable data collection, heterogeneous deployment, and long-horizon effective training difficult. These challenges suggest that real-world policy learning is not only an algorithmic issue but fundamentally a systems problem. We present USER, a Unified and extensible SystEm for Real-world online policy learning. USER treats physical robots as first-class hardware resources alongside GPUs through a unified hardware abstraction layer, enabling automatic discovery, management, and scheduling of heterogeneous robots. To address cloud-edge communication, USER introduces an adaptive communication plane with tunneling-based networking, distributed data channels for traffic localization, and streaming-multiprocessor-aware weight synchronization to regulate GPU-side overhead. On top of this infrastructure, USER organizes learning as a fully asynchronous framework with a persistent, cache-aware buffer, enabling efficient long-horizon experiments with robust crash recovery and reuse of historical data. In addition, USER provides extensible abstractions for rewards, algorithms, and policies, supporting online imitation or reinforcement learning of CNN/MLP, generative policies, and large vision-language-action (VLA) models within a unified pipeline. Results in both simulation and the real world show that USER enables multi-robot coordination, heterogeneous manipulators, edge-cloud collaboration with large models, and long-running asynchronous training, offering a unified and extensible systems foundation for real-world online policy learning."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-08T06:23:43Z",
                "published_parsed": [
                    2026,
                    2,
                    8,
                    6,
                    23,
                    43,
                    6,
                    39,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Hongzhi Zang"
                    },
                    {
                        "name": "Shu'ang Yu"
                    },
                    {
                        "name": "Hao Lin"
                    },
                    {
                        "name": "Tianxing Zhou"
                    },
                    {
                        "name": "Zefang Huang"
                    },
                    {
                        "name": "Zhen Guo"
                    },
                    {
                        "name": "Xin Xu"
                    },
                    {
                        "name": "Jiakai Zhou"
                    },
                    {
                        "name": "Yuze Sheng"
                    },
                    {
                        "name": "Shizhe Zhang"
                    },
                    {
                        "name": "Feng Gao"
                    },
                    {
                        "name": "Wenhao Tang"
                    },
                    {
                        "name": "Yufeng Yue"
                    },
                    {
                        "name": "Quanlu Zhang"
                    },
                    {
                        "name": "Xinlei Chen"
                    },
                    {
                        "name": "Chao Yu"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang"
            },
            {
                "id": "http://arxiv.org/abs/2602.09725v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.09725v2",
                "title": "Efficient Remote Prefix Fetching with GPU-native Media ASICs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Remote Prefix Fetching with GPU-native Media ASICs"
                },
                "updated": "2026-02-12T03:30:35Z",
                "updated_parsed": [
                    2026,
                    2,
                    12,
                    3,
                    30,
                    35,
                    3,
                    43,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.09725v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.09725v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Remote KV cache reuse fetches KV cache for identical contexts from remote storage, avoiding recomputation, accelerating LLM inference. While it excels in high-speed networks, its performance degrades significantly in bandwidth-limited scenarios. Recent studies address this by transmitting KV caches in compressed form, but the associated heavyweight decompression counteracts the KV reuse benefits. In this paper, we propose an efficient and widely deployable remote KV cache reuse solution that leverages GPU-native video codecs. Our system, KVFetcher, enables effective KV cache coding with two techniques. The codec-friendly tensor layout compresses the KV cache in a highly compact video format, enabling fast transmission. The efficient KV fetcher orchestrates the transmission, decoding, and restoration of compressed KV caches in an efficient pipelined manner, eliminating resource contention, masking network fluctuations, and achieving minimum time-to-first-token (TTFT). We prototype KVFetcher on diverse GPUs from high- to low-end. Experiments reveal that it reduces TTFT by up to 3.51 times while maintaining lossless accuracy, compared to SOTA methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Remote KV cache reuse fetches KV cache for identical contexts from remote storage, avoiding recomputation, accelerating LLM inference. While it excels in high-speed networks, its performance degrades significantly in bandwidth-limited scenarios. Recent studies address this by transmitting KV caches in compressed form, but the associated heavyweight decompression counteracts the KV reuse benefits. In this paper, we propose an efficient and widely deployable remote KV cache reuse solution that leverages GPU-native video codecs. Our system, KVFetcher, enables effective KV cache coding with two techniques. The codec-friendly tensor layout compresses the KV cache in a highly compact video format, enabling fast transmission. The efficient KV fetcher orchestrates the transmission, decoding, and restoration of compressed KV caches in an efficient pipelined manner, eliminating resource contention, masking network fluctuations, and achieving minimum time-to-first-token (TTFT). We prototype KVFetcher on diverse GPUs from high- to low-end. Experiments reveal that it reduces TTFT by up to 3.51 times while maintaining lossless accuracy, compared to SOTA methods."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-10T12:29:02Z",
                "published_parsed": [
                    2026,
                    2,
                    10,
                    12,
                    29,
                    2,
                    1,
                    41,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Liang Mi"
                    },
                    {
                        "name": "Weijun Wang"
                    },
                    {
                        "name": "Jinghan Chen"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Haipeng Dai"
                    },
                    {
                        "name": "Yunxin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yunxin Liu"
                },
                "author": "Yunxin Liu"
            },
            {
                "id": "http://arxiv.org/abs/2602.11521v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.11521v1",
                "title": "PAM: Processing Across Memory Hierarchy for Efficient KV-centric LLM Serving System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PAM: Processing Across Memory Hierarchy for Efficient KV-centric LLM Serving System"
                },
                "updated": "2026-02-12T03:30:11Z",
                "updated_parsed": [
                    2026,
                    2,
                    12,
                    3,
                    30,
                    11,
                    3,
                    43,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.11521v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.11521v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The widespread adoption of Large Language Models (LLMs) has exponentially increased the demand for efficient serving systems. With growing requests and context lengths, key-value (KV)-related operations, including attention computation and KV cache storage, have emerged as critical bottlenecks. They require massive memory bandwidth and capacity. Unfortunately, existing LLM serving systems, optimized for compute-bound workloads, fail to handle these memory-intensive operations effectively. Even with Processing-In-Memory (PIM) technology, current single-level memory designs cannot simultaneously satisfy the bandwidth and capacity requirements.\n  To address these challenges, we propose Processing Across Memory (PAM), a KV-centric LLM serving system that coordinates heterogeneous PIM-enabled memory devices within a hierarchical architecture. PAM introduces a novel computing paradigm to balance high memory bandwidth with scalable capacity. First, PAM exploits the inherent context locality in KV access patterns to intelligently distribute KV tokens across the memory hierarchy. Second, to further exploit context locality, it introduces the PAMattention algorithm, enabling fine-grained parallel attention computation across heterogeneous PIM devices. Finally, PAM incorporates an intra-device KV mapping, inter-device KV migration interface, and an inter-device online KV scheduling algorithm to dynamically balance computational workloads. By addressing both bandwidth and capacity demands simultaneously, PAM significantly enhances the efficiency and scalability of LLM serving systems, paving the way for cost-effective, high-performance solutions in the era of large-scale AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread adoption of Large Language Models (LLMs) has exponentially increased the demand for efficient serving systems. With growing requests and context lengths, key-value (KV)-related operations, including attention computation and KV cache storage, have emerged as critical bottlenecks. They require massive memory bandwidth and capacity. Unfortunately, existing LLM serving systems, optimized for compute-bound workloads, fail to handle these memory-intensive operations effectively. Even with Processing-In-Memory (PIM) technology, current single-level memory designs cannot simultaneously satisfy the bandwidth and capacity requirements.\n  To address these challenges, we propose Processing Across Memory (PAM), a KV-centric LLM serving system that coordinates heterogeneous PIM-enabled memory devices within a hierarchical architecture. PAM introduces a novel computing paradigm to balance high memory bandwidth with scalable capacity. First, PAM exploits the inherent context locality in KV access patterns to intelligently distribute KV tokens across the memory hierarchy. Second, to further exploit context locality, it introduces the PAMattention algorithm, enabling fine-grained parallel attention computation across heterogeneous PIM devices. Finally, PAM incorporates an intra-device KV mapping, inter-device KV migration interface, and an inter-device online KV scheduling algorithm to dynamically balance computational workloads. By addressing both bandwidth and capacity demands simultaneously, PAM significantly enhances the efficiency and scalability of LLM serving systems, paving the way for cost-effective, high-performance solutions in the era of large-scale AI."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-12T03:30:11Z",
                "published_parsed": [
                    2026,
                    2,
                    12,
                    3,
                    30,
                    11,
                    3,
                    43,
                    0
                ],
                "arxiv_comment": "15 pages, 13 figures",
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Lian Liu"
                    },
                    {
                        "name": "Shixin Zhao"
                    },
                    {
                        "name": "Yutian Zhou"
                    },
                    {
                        "name": "Yintao He"
                    },
                    {
                        "name": "Mengdi Wang"
                    },
                    {
                        "name": "Yinhe Han"
                    },
                    {
                        "name": "Ying Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ying Wang"
                },
                "author": "Ying Wang"
            },
            {
                "id": "http://arxiv.org/abs/2510.02388v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.02388v2",
                "title": "Learning to Route: A Rule-Driven Agent Framework for Hybrid-Source Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Route: A Rule-Driven Agent Framework for Hybrid-Source Retrieval-Augmented Generation"
                },
                "updated": "2026-02-12T02:52:55Z",
                "updated_parsed": [
                    2026,
                    2,
                    12,
                    2,
                    52,
                    55,
                    3,
                    43,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.02388v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.02388v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) have shown remarkable performance on general Question Answering (QA), yet they often struggle in domain-specific scenarios where accurate and up-to-date information is required. Retrieval-Augmented Generation (RAG) addresses this limitation by enriching LLMs with external knowledge, but existing systems primarily rely on unstructured documents, while largely overlooking relational databases, which provide precise, timely, and efficiently queryable factual information, serving as indispensable infrastructure in domains such as finance, healthcare, and scientific research. Motivated by this gap, we conduct a systematic analysis that reveals three central observations: (i) databases and documents offer complementary strengths across queries, (ii) naively combining both sources introduces noise and cost without consistent accuracy gains, and (iii) selecting the most suitable source for each query is crucial to balance effectiveness and efficiency. We further observe that query types show consistent regularities in their alignment with retrieval paths, suggesting that routing decisions can be effectively guided by systematic rules that capture these patterns. Building on these insights, we propose a rule-driven routing framework. A routing agent scores candidate augmentation paths based on explicit rules and selects the most suitable one; a rule-making expert agent refines the rules over time using QA feedback to maintain adaptability; and a path-level meta-cache reuses past routing decisions for semantically similar queries to reduce latency and cost. Experiments on three QA benchmarks demonstrate that our framework consistently outperforms static strategies and learned routing baselines, achieving higher accuracy while maintaining moderate computational cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable performance on general Question Answering (QA), yet they often struggle in domain-specific scenarios where accurate and up-to-date information is required. Retrieval-Augmented Generation (RAG) addresses this limitation by enriching LLMs with external knowledge, but existing systems primarily rely on unstructured documents, while largely overlooking relational databases, which provide precise, timely, and efficiently queryable factual information, serving as indispensable infrastructure in domains such as finance, healthcare, and scientific research. Motivated by this gap, we conduct a systematic analysis that reveals three central observations: (i) databases and documents offer complementary strengths across queries, (ii) naively combining both sources introduces noise and cost without consistent accuracy gains, and (iii) selecting the most suitable source for each query is crucial to balance effectiveness and efficiency. We further observe that query types show consistent regularities in their alignment with retrieval paths, suggesting that routing decisions can be effectively guided by systematic rules that capture these patterns. Building on these insights, we propose a rule-driven routing framework. A routing agent scores candidate augmentation paths based on explicit rules and selects the most suitable one; a rule-making expert agent refines the rules over time using QA feedback to maintain adaptability; and a path-level meta-cache reuses past routing decisions for semantically similar queries to reduce latency and cost. Experiments on three QA benchmarks demonstrate that our framework consistently outperforms static strategies and learned routing baselines, achieving higher accuracy while maintaining moderate computational cost."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-30T22:19:44Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    22,
                    19,
                    44,
                    1,
                    273,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Haoyue Bai"
                    },
                    {
                        "name": "Haoyu Wang"
                    },
                    {
                        "name": "Shengyu Chen"
                    },
                    {
                        "name": "Zhengzhang Chen"
                    },
                    {
                        "name": "Lu-An Tang"
                    },
                    {
                        "name": "Wei Cheng"
                    },
                    {
                        "name": "Haifeng Chen"
                    },
                    {
                        "name": "Yanjie Fu"
                    }
                ],
                "author_detail": {
                    "name": "Yanjie Fu"
                },
                "author": "Yanjie Fu"
            },
            {
                "id": "http://arxiv.org/abs/2602.10718v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.10718v2",
                "title": "SnapMLA: Efficient Long-Context MLA Decoding via Hardware-Aware FP8 Quantized Pipelining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SnapMLA: Efficient Long-Context MLA Decoding via Hardware-Aware FP8 Quantized Pipelining"
                },
                "updated": "2026-02-12T02:38:42Z",
                "updated_parsed": [
                    2026,
                    2,
                    12,
                    2,
                    38,
                    42,
                    3,
                    43,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.10718v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.10718v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "While FP8 attention has shown substantial promise in innovations like FlashAttention-3, its integration into the decoding phase of the DeepSeek Multi-head Latent Attention (MLA) architecture presents notable challenges. These challenges include numerical heterogeneity arising from the decoupling of positional embeddings, misalignment of quantization scales in FP8 PV GEMM, and the need for optimized system-level support. In this paper, we introduce SnapMLA, an FP8 MLA decoding framework optimized to improve long-context efficiency through the following hardware-aware algorithm-kernel co-optimization techniques: (i) RoPE-Aware Per-Token KV Quantization, where the RoPE part is maintained in high precision, motivated by our comprehensive analysis of the heterogeneous quantization sensitivity inherent to the MLA KV cache. Furthermore, per-token granularity is employed to align with the autoregressive decoding process and maintain quantization accuracy. (ii) Quantized PV Computation Pipeline Reconstruction, which resolves the misalignment of quantization scale in FP8 PV computation stemming from the shared KV structure of the MLA KV cache. (iii) End-to-End Dataflow Optimization, where we establish an efficient data read-and-write workflow using specialized kernels, ensuring efficient data flow and performance gains. Extensive experiments on state-of-the-art MLA LLMs show that SnapMLA achieves up to a 1.91x improvement in throughput, with negligible risk of performance degradation in challenging long-context tasks, including mathematical reasoning and code generation benchmarks. Code is available at https://github.com/meituan-longcat/SGLang-FluentLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While FP8 attention has shown substantial promise in innovations like FlashAttention-3, its integration into the decoding phase of the DeepSeek Multi-head Latent Attention (MLA) architecture presents notable challenges. These challenges include numerical heterogeneity arising from the decoupling of positional embeddings, misalignment of quantization scales in FP8 PV GEMM, and the need for optimized system-level support. In this paper, we introduce SnapMLA, an FP8 MLA decoding framework optimized to improve long-context efficiency through the following hardware-aware algorithm-kernel co-optimization techniques: (i) RoPE-Aware Per-Token KV Quantization, where the RoPE part is maintained in high precision, motivated by our comprehensive analysis of the heterogeneous quantization sensitivity inherent to the MLA KV cache. Furthermore, per-token granularity is employed to align with the autoregressive decoding process and maintain quantization accuracy. (ii) Quantized PV Computation Pipeline Reconstruction, which resolves the misalignment of quantization scale in FP8 PV computation stemming from the shared KV structure of the MLA KV cache. (iii) End-to-End Dataflow Optimization, where we establish an efficient data read-and-write workflow using specialized kernels, ensuring efficient data flow and performance gains. Extensive experiments on state-of-the-art MLA LLMs show that SnapMLA achieves up to a 1.91x improvement in throughput, with negligible risk of performance degradation in challenging long-context tasks, including mathematical reasoning and code generation benchmarks. Code is available at https://github.com/meituan-longcat/SGLang-FluentLLM."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-11T10:24:42Z",
                "published_parsed": [
                    2026,
                    2,
                    11,
                    10,
                    24,
                    42,
                    2,
                    42,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Zunhai Su"
                    },
                    {
                        "name": "Shuhao Hu"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Yulei Qian"
                    },
                    {
                        "name": "Yuchen Xie"
                    },
                    {
                        "name": "Xunliang Cai"
                    }
                ],
                "author_detail": {
                    "name": "Xunliang Cai"
                },
                "author": "Xunliang Cai"
            },
            {
                "id": "http://arxiv.org/abs/2602.11470v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.11470v1",
                "title": "Cachemir: Fully Homomorphic Encrypted Inference of Generative Large Language Model with KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cachemir: Fully Homomorphic Encrypted Inference of Generative Large Language Model with KV Cache"
                },
                "updated": "2026-02-12T01:01:38Z",
                "updated_parsed": [
                    2026,
                    2,
                    12,
                    1,
                    1,
                    38,
                    3,
                    43,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.11470v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.11470v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Generative large language models (LLMs) have revolutionized multiple domains. Modern LLMs predominantly rely on an autoregressive decoding strategy, which generates output tokens sequentially and employs a key-value cache (KV cache) to avoid redundant computation. However, the widespread deployment of LLMs has raised serious privacy concerns, as users are feeding all types of data into the model, motivating the development of secure inference frameworks based on fully homomorphic encryption (FHE). A major limitation of existing FHE-based frameworks is their inability to effectively integrate the KV cache, resulting in prohibitively high latency for autoregressive decoding. In this paper, we propose Cachemir, a KV Cache Accelerated Homomorphic Encrypted LLM Inference Regime to overcome this limitation. Cachemir comprises three key technical contributions: 1) a set of novel HE packing algorithms specifically designed to leverage the computational advantages of the KV cache; 2) an interleaved replicated packing algorithm to efficiently compute the vector-matrix multiplications that result from using the KV cache in Transformer linear layers; and 3) an augmented bootstrapping placement strategy that accounts for the KV cache to minimize bootstrapping cost. We demonstrate that Cachemir achieves $48.83\\times$ and $67.16\\times$ speedup over MOAI (ICML'25) and THOR (CCS'25) respectively on CPU and consumes less than 100 seconds on GPU to generate an output token for Llama-3-8B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative large language models (LLMs) have revolutionized multiple domains. Modern LLMs predominantly rely on an autoregressive decoding strategy, which generates output tokens sequentially and employs a key-value cache (KV cache) to avoid redundant computation. However, the widespread deployment of LLMs has raised serious privacy concerns, as users are feeding all types of data into the model, motivating the development of secure inference frameworks based on fully homomorphic encryption (FHE). A major limitation of existing FHE-based frameworks is their inability to effectively integrate the KV cache, resulting in prohibitively high latency for autoregressive decoding. In this paper, we propose Cachemir, a KV Cache Accelerated Homomorphic Encrypted LLM Inference Regime to overcome this limitation. Cachemir comprises three key technical contributions: 1) a set of novel HE packing algorithms specifically designed to leverage the computational advantages of the KV cache; 2) an interleaved replicated packing algorithm to efficiently compute the vector-matrix multiplications that result from using the KV cache in Transformer linear layers; and 3) an augmented bootstrapping placement strategy that accounts for the KV cache to minimize bootstrapping cost. We demonstrate that Cachemir achieves $48.83\\times$ and $67.16\\times$ speedup over MOAI (ICML'25) and THOR (CCS'25) respectively on CPU and consumes less than 100 seconds on GPU to generate an output token for Llama-3-8B."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-12T01:01:38Z",
                "published_parsed": [
                    2026,
                    2,
                    12,
                    1,
                    1,
                    38,
                    3,
                    43,
                    0
                ],
                "arxiv_comment": "16 pages, 10 figures, 6 tables. Under review",
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Ye Yu"
                    },
                    {
                        "name": "Yifan Zhou"
                    },
                    {
                        "name": "Yi Chen"
                    },
                    {
                        "name": "Pedro Soto"
                    },
                    {
                        "name": "Wenjie Xiong"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li"
            },
            {
                "id": "http://arxiv.org/abs/2602.11374v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.11374v1",
                "title": "Retrieval-Aware Distillation for Transformer-SSM Hybrids",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Aware Distillation for Transformer-SSM Hybrids"
                },
                "updated": "2026-02-11T21:05:00Z",
                "updated_parsed": [
                    2026,
                    2,
                    11,
                    21,
                    5,
                    0,
                    2,
                    42,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.11374v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.11374v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "State-space models (SSMs) offer efficient sequence modeling but lag behind Transformers on benchmarks that require in-context retrieval. Prior work links this gap to a small set of attention heads, termed Gather-and-Aggregate (G&A), which SSMs struggle to reproduce. We propose *retrieval-aware distillation*, which converts a pretrained Transformer into a hybrid student by preserving only these retrieval-critical heads and distilling the rest into recurrent heads. We identify the essential heads via ablation on a synthetic retrieval task, producing a hybrid with sparse, non-uniform attention placement. We show that preserving **just 2% of attention heads recovers over 95% of teacher performance on retrieval-heavy tasks** (10 heads in a 1B model), requiring far fewer heads than hybrids that retain at least 25%. We further find that large recurrent states often compensate for missing retrieval: once retrieval is handled by these heads, the SSM backbone can be simplified with limited loss, even with an $8\\times$ reduction in state dimension. By reducing both the attention cache and the SSM state, the resulting hybrid is $5$--$6\\times$ more memory-efficient than comparable hybrids, closing the Transformer--SSM gap at a fraction of the memory cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State-space models (SSMs) offer efficient sequence modeling but lag behind Transformers on benchmarks that require in-context retrieval. Prior work links this gap to a small set of attention heads, termed Gather-and-Aggregate (G&A), which SSMs struggle to reproduce. We propose *retrieval-aware distillation*, which converts a pretrained Transformer into a hybrid student by preserving only these retrieval-critical heads and distilling the rest into recurrent heads. We identify the essential heads via ablation on a synthetic retrieval task, producing a hybrid with sparse, non-uniform attention placement. We show that preserving **just 2% of attention heads recovers over 95% of teacher performance on retrieval-heavy tasks** (10 heads in a 1B model), requiring far fewer heads than hybrids that retain at least 25%. We further find that large recurrent states often compensate for missing retrieval: once retrieval is handled by these heads, the SSM backbone can be simplified with limited loss, even with an $8\\times$ reduction in state dimension. By reducing both the attention cache and the SSM state, the resulting hybrid is $5$--$6\\times$ more memory-efficient than comparable hybrids, closing the Transformer--SSM gap at a fraction of the memory cost."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-11T21:05:00Z",
                "published_parsed": [
                    2026,
                    2,
                    11,
                    21,
                    5,
                    0,
                    2,
                    42,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Aviv Bick"
                    },
                    {
                        "name": "Eric P. Xing"
                    },
                    {
                        "name": "Albert Gu"
                    }
                ],
                "author_detail": {
                    "name": "Albert Gu"
                },
                "author": "Albert Gu"
            },
            {
                "id": "http://arxiv.org/abs/2602.11029v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.11029v1",
                "title": "Bounding the Average Move Structure Query for Faster and Smaller RLBWT Permutations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bounding the Average Move Structure Query for Faster and Smaller RLBWT Permutations"
                },
                "updated": "2026-02-11T16:55:18Z",
                "updated_parsed": [
                    2026,
                    2,
                    11,
                    16,
                    55,
                    18,
                    2,
                    42,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.11029v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.11029v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The move structure represents permutations with long contiguously permuted intervals in compressed space with optimal query time. They have become an important feature of compressed text indexes using space proportional to the number of Burrows-Wheeler Transform (BWT) runs, often applied in genomics. This is in thanks not only to theoretical improvements over past approaches, but great cache efficiency and average case query time in practice. This is true even without using the worst case guarantees provided by the interval splitting balancing of the original result. In this paper, we show that an even simpler type of splitting, length capping by truncating long intervals, bounds the average move structure query time to optimal whilst obtaining a superior construction time than the traditional approach. This also proves constant query time when amortized over a full traversal of a single cycle permutation from an arbitrary starting position.\n  Such a scheme has surprising benefits both in theory and practice. We leverage the approach to improve the representation of any move structure with $r$ runs over a domain $n$ to $O(r \\log r + r \\log \\frac{n}{r})$-bits of space. The worst case query time is also improved to $O(\\log \\frac{n}{r})$ without balancing. An $O(r)$-time and $O(r)$-space construction lets us apply the method to run-length encoded BWT (RLBWT) permutations such as LF and $φ$ to obtain optimal-time algorithms for BWT inversion and suffix array (SA) enumeration in $O(r)$ additional working space. Finally, we provide the RunPerm library, providing flexible plug and play move structure support, and use it to evaluate our splitting approach. Experiments find length capping results in faster move structures, but also a space reduction: at least $\\sim 40\\%$ for LF across large repetitive genomic collections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The move structure represents permutations with long contiguously permuted intervals in compressed space with optimal query time. They have become an important feature of compressed text indexes using space proportional to the number of Burrows-Wheeler Transform (BWT) runs, often applied in genomics. This is in thanks not only to theoretical improvements over past approaches, but great cache efficiency and average case query time in practice. This is true even without using the worst case guarantees provided by the interval splitting balancing of the original result. In this paper, we show that an even simpler type of splitting, length capping by truncating long intervals, bounds the average move structure query time to optimal whilst obtaining a superior construction time than the traditional approach. This also proves constant query time when amortized over a full traversal of a single cycle permutation from an arbitrary starting position.\n  Such a scheme has surprising benefits both in theory and practice. We leverage the approach to improve the representation of any move structure with $r$ runs over a domain $n$ to $O(r \\log r + r \\log \\frac{n}{r})$-bits of space. The worst case query time is also improved to $O(\\log \\frac{n}{r})$ without balancing. An $O(r)$-time and $O(r)$-space construction lets us apply the method to run-length encoded BWT (RLBWT) permutations such as LF and $φ$ to obtain optimal-time algorithms for BWT inversion and suffix array (SA) enumeration in $O(r)$ additional working space. Finally, we provide the RunPerm library, providing flexible plug and play move structure support, and use it to evaluate our splitting approach. Experiments find length capping results in faster move structures, but also a space reduction: at least $\\sim 40\\%$ for LF across large repetitive genomic collections."
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-11T16:55:18Z",
                "published_parsed": [
                    2026,
                    2,
                    11,
                    16,
                    55,
                    18,
                    2,
                    42,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS"
                },
                "authors": [
                    {
                        "name": "Nathaniel K. Brown"
                    },
                    {
                        "name": "Ben Langmead"
                    }
                ],
                "author_detail": {
                    "name": "Ben Langmead"
                },
                "author": "Ben Langmead"
            },
            {
                "id": "http://arxiv.org/abs/2602.11016v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.11016v1",
                "title": "From Buffers to Registers: Unlocking Fine-Grained FlashAttention with Hybrid-Bonded 3D NPU Co-Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Buffers to Registers: Unlocking Fine-Grained FlashAttention with Hybrid-Bonded 3D NPU Co-Design"
                },
                "updated": "2026-02-11T16:40:34Z",
                "updated_parsed": [
                    2026,
                    2,
                    11,
                    16,
                    40,
                    34,
                    2,
                    42,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.11016v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.11016v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Transformer-based models dominate modern AI workloads but exacerbate memory bottlenecks due to their quadratic attention complexity and ever-growing model sizes. Existing accelerators, such as Groq and Cerebras, mitigate off-chip traffic with large on-chip caches, while algorithmic innovations such as FlashAttention fuse operators to avoid materializing large attention matrices. However, as off-chip traffic decreases, our measurements show that on-chip SRAM accesses account for over 60% of energy in long-sequence workloads, making cache access the new bottleneck. We propose 3D-Flow, a hybrid-bonded, 3D-stacked spatial accelerator that enables register-to-register communication across vertically partitioned PE tiers. Unlike 2D multi-array architectures limited by NoC-based router-to-router transfers, 3D-Flow leverages sub-10 um vertical TSVs to sustain cycle-level operator pipelining with minimal overhead. On top of this architecture, we design 3D-FlashAttention, a fine-grained scheduling method that balances latency across tiers, forming a bubble-free vertical dataflow without on-chip SRAM roundtrips. Evaluations on Transformer workloads (OPT and QWEN models) show that our 3D spatial accelerator reduces 46-93% energy consumption and achieves 1.4x-7.6x speedups compared to state-of-the-art 2D and 3D designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based models dominate modern AI workloads but exacerbate memory bottlenecks due to their quadratic attention complexity and ever-growing model sizes. Existing accelerators, such as Groq and Cerebras, mitigate off-chip traffic with large on-chip caches, while algorithmic innovations such as FlashAttention fuse operators to avoid materializing large attention matrices. However, as off-chip traffic decreases, our measurements show that on-chip SRAM accesses account for over 60% of energy in long-sequence workloads, making cache access the new bottleneck. We propose 3D-Flow, a hybrid-bonded, 3D-stacked spatial accelerator that enables register-to-register communication across vertically partitioned PE tiers. Unlike 2D multi-array architectures limited by NoC-based router-to-router transfers, 3D-Flow leverages sub-10 um vertical TSVs to sustain cycle-level operator pipelining with minimal overhead. On top of this architecture, we design 3D-FlashAttention, a fine-grained scheduling method that balances latency across tiers, forming a bubble-free vertical dataflow without on-chip SRAM roundtrips. Evaluations on Transformer workloads (OPT and QWEN models) show that our 3D spatial accelerator reduces 46-93% energy consumption and achieves 1.4x-7.6x speedups compared to state-of-the-art 2D and 3D designs."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-11T16:40:34Z",
                "published_parsed": [
                    2026,
                    2,
                    11,
                    16,
                    40,
                    34,
                    2,
                    42,
                    0
                ],
                "arxiv_comment": "Accepted to DATE 2026",
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Jinxin Yu"
                    },
                    {
                        "name": "Yudong Pan"
                    },
                    {
                        "name": "Mengdi Wang"
                    },
                    {
                        "name": "Huawei Li"
                    },
                    {
                        "name": "Yinhe Han"
                    },
                    {
                        "name": "Xiaowei Li"
                    },
                    {
                        "name": "Ying Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ying Wang"
                },
                "author": "Ying Wang"
            },
            {
                "id": "http://arxiv.org/abs/2602.10986v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.10986v1",
                "title": "TVCACHE: A Stateful Tool-Value Cache for Post-Training LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TVCACHE: A Stateful Tool-Value Cache for Post-Training LLM Agents"
                },
                "updated": "2026-02-11T16:13:44Z",
                "updated_parsed": [
                    2026,
                    2,
                    11,
                    16,
                    13,
                    44,
                    2,
                    42,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.10986v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.10986v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In RL post-training of LLM agents, calls to external tools take several seconds or even minutes, leaving allocated GPUs idle and inflating post-training time and cost. While many tool invocations repeat across parallel rollouts and could in principle be cached, naively caching their outputs for reuse is incorrect since tool outputs depend on the environment state induced by prior agent interactions. We present TVCACHE, a stateful tool-value cache for LLM agent post-training. TVCACHE maintains a tree of observed tool-call sequences and performs longest-prefix matching for cache lookups: a hit occurs only when the agent's full tool history matches a previously executed sequence, guaranteeing identical environment state. On three diverse workloads-terminal-based tasks, SQL generation, and video understanding. TVCACHE achieves cache hit rates of up to 70% and reduces median tool call execution time by up to 6.9X, with no degradation in post-training reward accumulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In RL post-training of LLM agents, calls to external tools take several seconds or even minutes, leaving allocated GPUs idle and inflating post-training time and cost. While many tool invocations repeat across parallel rollouts and could in principle be cached, naively caching their outputs for reuse is incorrect since tool outputs depend on the environment state induced by prior agent interactions. We present TVCACHE, a stateful tool-value cache for LLM agent post-training. TVCACHE maintains a tree of observed tool-call sequences and performs longest-prefix matching for cache lookups: a hit occurs only when the agent's full tool history matches a previously executed sequence, guaranteeing identical environment state. On three diverse workloads-terminal-based tasks, SQL generation, and video understanding. TVCACHE achieves cache hit rates of up to 70% and reduces median tool call execution time by up to 6.9X, with no degradation in post-training reward accumulation."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-11T16:13:44Z",
                "published_parsed": [
                    2026,
                    2,
                    11,
                    16,
                    13,
                    44,
                    2,
                    42,
                    0
                ],
                "arxiv_comment": "Abhishek Vijaya Kumar and Bhaskar Kataria have equal contribution",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Abhishek Vijaya Kumar"
                    },
                    {
                        "name": "Bhaskar Kataria"
                    },
                    {
                        "name": "Byungsoo Oh"
                    },
                    {
                        "name": "Emaad Manzoor"
                    },
                    {
                        "name": "Rachee Singh"
                    }
                ],
                "author_detail": {
                    "name": "Rachee Singh"
                },
                "author": "Rachee Singh"
            },
            {
                "id": "http://arxiv.org/abs/2602.10852v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.10852v1",
                "title": "Enhanced effective masses, spin-orbit polarization, and dispersion relations in 2D hole gases under strongly asymmetric confinement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhanced effective masses, spin-orbit polarization, and dispersion relations in 2D hole gases under strongly asymmetric confinement"
                },
                "updated": "2026-02-11T13:39:07Z",
                "updated_parsed": [
                    2026,
                    2,
                    11,
                    13,
                    39,
                    7,
                    2,
                    42,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.10852v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.10852v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The dispersion of Rashba-split heavy-hole subbands in GaAs two-dimensional hole gases (2DHGs) is difficult to access experimentally because strong heavy-hole-light-hole mixing produces non-parabolicity and breaks the usual correspondence between carrier density and Fermi wave vector. Here we use low-field magnetotransport (B < 1 T) to reconstruct the dispersions of the two spin-orbit-split heavy-hole branches (HH-, HH+) in undoped (100) GaAs/AlGaAs single heterojunction 2DHGs operated in an accumulation-mode field-effect geometry. The dopant-free devices sustain out-of-plane electric fields up to 26 kV/cm while maintaining mobilities up to 84 m$^2$/Vs and exhibiting a spin-orbit polarization as large as 36%. Fourier analysis of Shubnikov-de Haas (SdH) oscillations resolves the individual HH-/HH+ subband densities; fitting the temperature dependence of the corresponding Fourier amplitudes yields both branch-resolved SdH effective masses over the same magnetic field window. SdH regimes in which reliable subband parameters can be extracted are delineated. Over 2DHG densities (0.76-1.9) $\\times$ 10$^{15}$ /m$^2$, the HH- mass is nearly density independent ($\\approx 0.34m_e$), implying a near-parabolic HH- dispersion below the first LH+/HH- anticrossing, whereas HH+ exhibits strong non-parabolicity with an effective mass that increases with density. Combining the extracted dispersions yields a transport-based determination of the spin-orbit splitting energy $Δ_\\text{HH}$ between HH and HH+ as a function of in-plane wave vector. Parameter-free Luttinger-model calculations reproduce the qualitative trends but underestimate both masses by a common factor $\\approx$ 2, suggesting a many-body renormalization of the heavy-hole mass in this strongly asymmetric regime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The dispersion of Rashba-split heavy-hole subbands in GaAs two-dimensional hole gases (2DHGs) is difficult to access experimentally because strong heavy-hole-light-hole mixing produces non-parabolicity and breaks the usual correspondence between carrier density and Fermi wave vector. Here we use low-field magnetotransport (B < 1 T) to reconstruct the dispersions of the two spin-orbit-split heavy-hole branches (HH-, HH+) in undoped (100) GaAs/AlGaAs single heterojunction 2DHGs operated in an accumulation-mode field-effect geometry. The dopant-free devices sustain out-of-plane electric fields up to 26 kV/cm while maintaining mobilities up to 84 m$^2$/Vs and exhibiting a spin-orbit polarization as large as 36%. Fourier analysis of Shubnikov-de Haas (SdH) oscillations resolves the individual HH-/HH+ subband densities; fitting the temperature dependence of the corresponding Fourier amplitudes yields both branch-resolved SdH effective masses over the same magnetic field window. SdH regimes in which reliable subband parameters can be extracted are delineated. Over 2DHG densities (0.76-1.9) $\\times$ 10$^{15}$ /m$^2$, the HH- mass is nearly density independent ($\\approx 0.34m_e$), implying a near-parabolic HH- dispersion below the first LH+/HH- anticrossing, whereas HH+ exhibits strong non-parabolicity with an effective mass that increases with density. Combining the extracted dispersions yields a transport-based determination of the spin-orbit splitting energy $Δ_\\text{HH}$ between HH and HH+ as a function of in-plane wave vector. Parameter-free Luttinger-model calculations reproduce the qualitative trends but underestimate both masses by a common factor $\\approx$ 2, suggesting a many-body renormalization of the heavy-hole mass in this strongly asymmetric regime."
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-11T13:39:07Z",
                "published_parsed": [
                    2026,
                    2,
                    11,
                    13,
                    39,
                    7,
                    2,
                    42,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall"
                },
                "authors": [
                    {
                        "name": "N. A. Cockton"
                    },
                    {
                        "name": "F. Sfigakis"
                    },
                    {
                        "name": "M. Korkusinski"
                    },
                    {
                        "name": "S. R. Harrigan"
                    },
                    {
                        "name": "G. Nichols"
                    },
                    {
                        "name": "Z. D. Merino"
                    },
                    {
                        "name": "T. Zou"
                    },
                    {
                        "name": "A. C. Coschizza"
                    },
                    {
                        "name": "T. Joshi"
                    },
                    {
                        "name": "A. Shetty"
                    },
                    {
                        "name": "M. C. Tam"
                    },
                    {
                        "name": "Z. R. Wasilewski"
                    },
                    {
                        "name": "S. A. Studenikin"
                    },
                    {
                        "name": "D. G. Austing"
                    },
                    {
                        "name": "J. Baugh"
                    },
                    {
                        "name": "J. B. Kycia"
                    }
                ],
                "author_detail": {
                    "name": "J. B. Kycia"
                },
                "author": "J. B. Kycia"
            },
            {
                "id": "http://arxiv.org/abs/2602.10825v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.10825v1",
                "title": "Flow caching for autoregressive video generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flow caching for autoregressive video generation"
                },
                "updated": "2026-02-11T13:11:04Z",
                "updated_parsed": [
                    2026,
                    2,
                    11,
                    13,
                    11,
                    4,
                    2,
                    42,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.10825v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.10825v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Autoregressive models, often built on Transformer architectures, represent a powerful paradigm for generating ultra-long videos by synthesizing content in sequential chunks. However, this sequential generation process is notoriously slow. While caching strategies have proven effective for accelerating traditional video diffusion models, existing methods assume uniform denoising across all frames-an assumption that breaks down in autoregressive models where different video chunks exhibit varying similarity patterns at identical timesteps. In this paper, we present FlowCache, the first caching framework specifically designed for autoregressive video generation. Our key insight is that each video chunk should maintain independent caching policies, allowing fine-grained control over which chunks require recomputation at each timestep. We introduce a chunkwise caching strategy that dynamically adapts to the unique denoising characteristics of each chunk, complemented by a joint importance-redundancy optimized KV cache compression mechanism that maintains fixed memory bounds while preserving generation quality. Our method achieves remarkable speedups of 2.38 times on MAGI-1 and 6.7 times on SkyReels-V2, with negligible quality degradation (VBench: 0.87 increase and 0.79 decrease respectively). These results demonstrate that FlowCache successfully unlocks the potential of autoregressive models for real-time, ultra-long video generation-establishing a new benchmark for efficient video synthesis at scale. The code is available at https://github.com/mikeallen39/FlowCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive models, often built on Transformer architectures, represent a powerful paradigm for generating ultra-long videos by synthesizing content in sequential chunks. However, this sequential generation process is notoriously slow. While caching strategies have proven effective for accelerating traditional video diffusion models, existing methods assume uniform denoising across all frames-an assumption that breaks down in autoregressive models where different video chunks exhibit varying similarity patterns at identical timesteps. In this paper, we present FlowCache, the first caching framework specifically designed for autoregressive video generation. Our key insight is that each video chunk should maintain independent caching policies, allowing fine-grained control over which chunks require recomputation at each timestep. We introduce a chunkwise caching strategy that dynamically adapts to the unique denoising characteristics of each chunk, complemented by a joint importance-redundancy optimized KV cache compression mechanism that maintains fixed memory bounds while preserving generation quality. Our method achieves remarkable speedups of 2.38 times on MAGI-1 and 6.7 times on SkyReels-V2, with negligible quality degradation (VBench: 0.87 increase and 0.79 decrease respectively). These results demonstrate that FlowCache successfully unlocks the potential of autoregressive models for real-time, ultra-long video generation-establishing a new benchmark for efficient video synthesis at scale. The code is available at https://github.com/mikeallen39/FlowCache."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-11T13:11:04Z",
                "published_parsed": [
                    2026,
                    2,
                    11,
                    13,
                    11,
                    4,
                    2,
                    42,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Yuexiao Ma"
                    },
                    {
                        "name": "Xuzhe Zheng"
                    },
                    {
                        "name": "Jing Xu"
                    },
                    {
                        "name": "Xiwei Xu"
                    },
                    {
                        "name": "Feng Ling"
                    },
                    {
                        "name": "Xiawu Zheng"
                    },
                    {
                        "name": "Huafeng Kuang"
                    },
                    {
                        "name": "Huixia Li"
                    },
                    {
                        "name": "Xing Wang"
                    },
                    {
                        "name": "Xuefeng Xiao"
                    },
                    {
                        "name": "Fei Chao"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji"
            },
            {
                "id": "http://arxiv.org/abs/2602.07449v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.07449v3",
                "title": "SoulX-FlashHead: Oracle-guided Generation of Infinite Real-time Streaming Talking Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SoulX-FlashHead: Oracle-guided Generation of Infinite Real-time Streaming Talking Heads"
                },
                "updated": "2026-02-11T12:34:52Z",
                "updated_parsed": [
                    2026,
                    2,
                    11,
                    12,
                    34,
                    52,
                    2,
                    42,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.07449v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.07449v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Achieving a balance between high-fidelity visual quality and low-latency streaming remains a formidable challenge in audio-driven portrait generation. Existing large-scale models often suffer from prohibitive computational costs, while lightweight alternatives typically compromise on holistic facial representations and temporal stability. In this paper, we propose SoulX-FlashHead, a unified 1.3B-parameter framework designed for real-time, infinite-length, and high-fidelity streaming video generation. To address the instability of audio features in streaming scenarios, we introduce Streaming-Aware Spatiotemporal Pre-training equipped with a Temporal Audio Context Cache mechanism, which ensures robust feature extraction from short audio fragments. Furthermore, to mitigate the error accumulation and identity drift inherent in long-sequence autoregressive generation, we propose Oracle-Guided Bidirectional Distillation, leveraging ground-truth motion priors to provide precise physical guidance. We also present VividHead, a large-scale, high-quality dataset containing 782 hours of strictly aligned footage to support robust training. Extensive experiments demonstrate that SoulX-FlashHead achieves state-of-the-art performance on HDTF and VFHQ benchmarks. Notably, our Lite variant achieves an inference speed of 96 FPS on a single NVIDIA RTX 4090, facilitating ultra-fast interaction without sacrificing visual coherence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Achieving a balance between high-fidelity visual quality and low-latency streaming remains a formidable challenge in audio-driven portrait generation. Existing large-scale models often suffer from prohibitive computational costs, while lightweight alternatives typically compromise on holistic facial representations and temporal stability. In this paper, we propose SoulX-FlashHead, a unified 1.3B-parameter framework designed for real-time, infinite-length, and high-fidelity streaming video generation. To address the instability of audio features in streaming scenarios, we introduce Streaming-Aware Spatiotemporal Pre-training equipped with a Temporal Audio Context Cache mechanism, which ensures robust feature extraction from short audio fragments. Furthermore, to mitigate the error accumulation and identity drift inherent in long-sequence autoregressive generation, we propose Oracle-Guided Bidirectional Distillation, leveraging ground-truth motion priors to provide precise physical guidance. We also present VividHead, a large-scale, high-quality dataset containing 782 hours of strictly aligned footage to support robust training. Extensive experiments demonstrate that SoulX-FlashHead achieves state-of-the-art performance on HDTF and VFHQ benchmarks. Notably, our Lite variant achieves an inference speed of 96 FPS on a single NVIDIA RTX 4090, facilitating ultra-fast interaction without sacrificing visual coherence."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-07T08:58:16Z",
                "published_parsed": [
                    2026,
                    2,
                    7,
                    8,
                    58,
                    16,
                    5,
                    38,
                    0
                ],
                "arxiv_comment": "11 pages, 3 figures",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Tan Yu"
                    },
                    {
                        "name": "Qian Qiao"
                    },
                    {
                        "name": "Le Shen"
                    },
                    {
                        "name": "Ke Zhou"
                    },
                    {
                        "name": "Jincheng Hu"
                    },
                    {
                        "name": "Dian Sheng"
                    },
                    {
                        "name": "Bo Hu"
                    },
                    {
                        "name": "Haoming Qin"
                    },
                    {
                        "name": "Jun Gao"
                    },
                    {
                        "name": "Changhai Zhou"
                    },
                    {
                        "name": "Shunshun Yin"
                    },
                    {
                        "name": "Siyuan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Siyuan Liu"
                },
                "author": "Siyuan Liu"
            },
            {
                "id": "http://arxiv.org/abs/2602.10681v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.10681v1",
                "title": "High-voltage generation system for a traveling-wave Stark decelerator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-voltage generation system for a traveling-wave Stark decelerator"
                },
                "updated": "2026-02-11T09:32:47Z",
                "updated_parsed": [
                    2026,
                    2,
                    11,
                    9,
                    32,
                    47,
                    2,
                    42,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.10681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.10681v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In this paper we describe the high-voltage generation system we have developed for a traveling-wave Stark decelerator (TWSD). The TWSD can reduce the forward velocity of a molecular beam of heavy neutral polar molecules such as strontium monofluoride (SrF) and barium monofluoride (BaF) from $\\sim$ 200 m/s down to $\\sim$ 6 m/s. The main motivation for the development of this device is the increased sensitivity from precision spectroscopy of the decelerated molecules to test fundamental physics. The high-voltage generation system can produce eight pulsed sinusoidal waveforms with a maximum amplitude of 10 kV and a linear frequency sweep from 16.7 kHz down to 500 Hz over the span of 40 ms at a repetition rate of 10 Hz. The eight waveforms are phase-offset to each other by 45 degrees. To slow down the heavy molecules, the decelerator is required to have a length of $\\sim$ 4 m, which results in a significant capacitive coupling between adjacent channels of $\\sim$ 160 pF. As a consequence, the control and stability of the waveforms is extra challenging. We designed a method that compensates for the frequency-dependent coupling between the eight channels. Allowing for amplitude and phase-offsets that do not deviate more than 1% and 2 degrees, respectively, from their design values during the frequency sweep. The system outperforms commercially available options in terms of stability, output voltage amplitude, cost and ease of maintenance. This approach is also relevant for other fields where precise control of high-voltage waveforms is required, such as particle accelerator physics, plasma physics and mass spectroscopy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper we describe the high-voltage generation system we have developed for a traveling-wave Stark decelerator (TWSD). The TWSD can reduce the forward velocity of a molecular beam of heavy neutral polar molecules such as strontium monofluoride (SrF) and barium monofluoride (BaF) from $\\sim$ 200 m/s down to $\\sim$ 6 m/s. The main motivation for the development of this device is the increased sensitivity from precision spectroscopy of the decelerated molecules to test fundamental physics. The high-voltage generation system can produce eight pulsed sinusoidal waveforms with a maximum amplitude of 10 kV and a linear frequency sweep from 16.7 kHz down to 500 Hz over the span of 40 ms at a repetition rate of 10 Hz. The eight waveforms are phase-offset to each other by 45 degrees. To slow down the heavy molecules, the decelerator is required to have a length of $\\sim$ 4 m, which results in a significant capacitive coupling between adjacent channels of $\\sim$ 160 pF. As a consequence, the control and stability of the waveforms is extra challenging. We designed a method that compensates for the frequency-dependent coupling between the eight channels. Allowing for amplitude and phase-offsets that do not deviate more than 1% and 2 degrees, respectively, from their design values during the frequency sweep. The system outperforms commercially available options in terms of stability, output voltage amplitude, cost and ease of maintenance. This approach is also relevant for other fields where precise control of high-voltage waveforms is required, such as particle accelerator physics, plasma physics and mass spectroscopy."
                },
                "tags": [
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-11T09:32:47Z",
                "published_parsed": [
                    2026,
                    2,
                    11,
                    9,
                    32,
                    47,
                    2,
                    42,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "physics.atom-ph"
                },
                "authors": [
                    {
                        "name": "Lucas van Sloten"
                    },
                    {
                        "name": "Leo Huisman"
                    },
                    {
                        "name": "Steven Hoekstra"
                    }
                ],
                "author_detail": {
                    "name": "Steven Hoekstra"
                },
                "author": "Steven Hoekstra"
            },
            {
                "id": "http://arxiv.org/abs/2512.00891v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.00891v2",
                "title": "Accelerating Streaming Video Large Language Models via Hierarchical Token Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Streaming Video Large Language Models via Hierarchical Token Compression"
                },
                "updated": "2026-02-11T08:35:07Z",
                "updated_parsed": [
                    2026,
                    2,
                    11,
                    8,
                    35,
                    7,
                    2,
                    42,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.00891v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.00891v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Streaming Video Large Language Models (VideoLLMs) have demonstrated impressive performance across various video understanding tasks, but they face significant challenges in real-time deployment due to the high computational cost of processing dense visual tokens from continuous video streams. In streaming video scenarios, the primary bottleneck lies in the Vision Transformer (ViT) encoding stage, where redundant processing of temporally similar frames leads to inefficiency. Additionally, inflated token sequences during LLM pre-filling further exacerbate latency and memory overhead. To address these challenges, we propose \\textbf{S}treaming \\textbf{T}oken \\textbf{C}ompression (\\textbf{STC}), a plug-and-play hierarchical framework that seamlessly integrates into existing streaming VideoLLMs, optimizing both ViT encoding and LLM pre-filling stages to accelerate processing. STC introduces two token-level accelerators: \\textbf{STC-Cacher}, which reduces ViT encoding overhead by caching and reusing features from temporally similar frames, and \\textbf{STC-Pruner}, which compresses the visual token sequence before it enters the LLM, preserving only the most salient tokens based on both spatial and temporal relevance. Extensive experiments on four baseline streaming VideoLLMs across five benchmarks demonstrate that STC outperforms other compression methods. Notably, STC retains up to \\textbf{99\\%} of accuracy on the ReKV framework while reducing ViT encoding latency and LLM pre-filling latency by \\textbf{24.5\\%} and \\textbf{45.3\\%}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming Video Large Language Models (VideoLLMs) have demonstrated impressive performance across various video understanding tasks, but they face significant challenges in real-time deployment due to the high computational cost of processing dense visual tokens from continuous video streams. In streaming video scenarios, the primary bottleneck lies in the Vision Transformer (ViT) encoding stage, where redundant processing of temporally similar frames leads to inefficiency. Additionally, inflated token sequences during LLM pre-filling further exacerbate latency and memory overhead. To address these challenges, we propose \\textbf{S}treaming \\textbf{T}oken \\textbf{C}ompression (\\textbf{STC}), a plug-and-play hierarchical framework that seamlessly integrates into existing streaming VideoLLMs, optimizing both ViT encoding and LLM pre-filling stages to accelerate processing. STC introduces two token-level accelerators: \\textbf{STC-Cacher}, which reduces ViT encoding overhead by caching and reusing features from temporally similar frames, and \\textbf{STC-Pruner}, which compresses the visual token sequence before it enters the LLM, preserving only the most salient tokens based on both spatial and temporal relevance. Extensive experiments on four baseline streaming VideoLLMs across five benchmarks demonstrate that STC outperforms other compression methods. Notably, STC retains up to \\textbf{99\\%} of accuracy on the ReKV framework while reducing ViT encoding latency and LLM pre-filling latency by \\textbf{24.5\\%} and \\textbf{45.3\\%}."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-30T13:44:28Z",
                "published_parsed": [
                    2025,
                    11,
                    30,
                    13,
                    44,
                    28,
                    6,
                    334,
                    0
                ],
                "arxiv_comment": "Code is avaliable at \\url{https://github.com/lern-to-write/STC}",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Yiyu Wang"
                    },
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Xiyan Gui"
                    },
                    {
                        "name": "Xinying Lin"
                    },
                    {
                        "name": "Boxue Yang"
                    },
                    {
                        "name": "Chenfei Liao"
                    },
                    {
                        "name": "Tailai Chen"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2602.10455v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.10455v1",
                "title": "Compute Only Once: UG-Separation for Efficient Large Recommendation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Only Once: UG-Separation for Efficient Large Recommendation Models"
                },
                "updated": "2026-02-11T02:53:59Z",
                "updated_parsed": [
                    2026,
                    2,
                    11,
                    2,
                    53,
                    59,
                    2,
                    42,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.10455v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.10455v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Driven by scaling laws, recommender systems increasingly rely on large-scale models to capture complex feature interactions and user behaviors, but this trend also leads to prohibitive training and inference costs. While long-sequence models(e.g., LONGER) can reuse user-side computation through KV caching, such reuse is difficult in dense feature interaction architectures(e.g., RankMixer), where user and group (candidate item) features are deeply entangled across layers. In this work, we propose User-Group Separation (UG-Sep), a novel framework that enables reusable user-side computation in dense interaction models for the first time. UG-Sep introduces a masking mechanism that explicitly disentangles user-side and item-side information flows within token-mixing layers, ensuring that a subset of tokens to preserve purely user-side representations across layers. This design enables corresponding token computations to be reused across multiple samples, significantly reducing redundant inference cost. To compensate for potential expressiveness loss induced by masking, we further propose an Information Compensation strategy that adaptively reconstructs suppressed user-item interactions. Moreover, as UG-Sep substantially reduces user-side FLOPs and exposes memory-bound components, we incorporate W8A16 (8-bit weight, 16-bit activation) weight-only quantization to alleviate memory bandwidth bottlenecks and achieve additional acceleration. We conduct extensive offline evaluations and large-scale online A/B experiments at ByteDance, demonstrating that UG-Sep reduces inference latency by up to 20 percent without degrading online user experience or commercial metrics across multiple business scenarios, including feed recommendation and advertising systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Driven by scaling laws, recommender systems increasingly rely on large-scale models to capture complex feature interactions and user behaviors, but this trend also leads to prohibitive training and inference costs. While long-sequence models(e.g., LONGER) can reuse user-side computation through KV caching, such reuse is difficult in dense feature interaction architectures(e.g., RankMixer), where user and group (candidate item) features are deeply entangled across layers. In this work, we propose User-Group Separation (UG-Sep), a novel framework that enables reusable user-side computation in dense interaction models for the first time. UG-Sep introduces a masking mechanism that explicitly disentangles user-side and item-side information flows within token-mixing layers, ensuring that a subset of tokens to preserve purely user-side representations across layers. This design enables corresponding token computations to be reused across multiple samples, significantly reducing redundant inference cost. To compensate for potential expressiveness loss induced by masking, we further propose an Information Compensation strategy that adaptively reconstructs suppressed user-item interactions. Moreover, as UG-Sep substantially reduces user-side FLOPs and exposes memory-bound components, we incorporate W8A16 (8-bit weight, 16-bit activation) weight-only quantization to alleviate memory bandwidth bottlenecks and achieve additional acceleration. We conduct extensive offline evaluations and large-scale online A/B experiments at ByteDance, demonstrating that UG-Sep reduces inference latency by up to 20 percent without degrading online user experience or commercial metrics across multiple business scenarios, including feed recommendation and advertising systems."
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-11T02:53:59Z",
                "published_parsed": [
                    2026,
                    2,
                    11,
                    2,
                    53,
                    59,
                    2,
                    42,
                    0
                ],
                "arxiv_comment": "Large Recommender Model, Industrial Recommenders, Scaling Law",
                "arxiv_primary_category": {
                    "term": "cs.IR"
                },
                "authors": [
                    {
                        "name": "Hui Lu"
                    },
                    {
                        "name": "Zheng Chai"
                    },
                    {
                        "name": "Shipeng Bai"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Zhifang Fan"
                    },
                    {
                        "name": "Kunmin Bai"
                    },
                    {
                        "name": "Yingwen Wu"
                    },
                    {
                        "name": "Bingzheng Wei"
                    },
                    {
                        "name": "Xiang Sun"
                    },
                    {
                        "name": "Ziyan Gong"
                    },
                    {
                        "name": "Tianyi Liu"
                    },
                    {
                        "name": "Hua Chen"
                    },
                    {
                        "name": "Deping Xie"
                    },
                    {
                        "name": "Zhongkai Chen"
                    },
                    {
                        "name": "Zhiliang Guo"
                    },
                    {
                        "name": "Qiwei Chen"
                    },
                    {
                        "name": "Yuchao Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Yuchao Zheng"
                },
                "author": "Yuchao Zheng"
            },
            {
                "id": "http://arxiv.org/abs/2602.11210v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.11210v1",
                "title": "SWE-MiniSandbox: Container-Free Reinforcement Learning for Building Software Engineering Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWE-MiniSandbox: Container-Free Reinforcement Learning for Building Software Engineering Agents"
                },
                "updated": "2026-02-11T02:33:04Z",
                "updated_parsed": [
                    2026,
                    2,
                    11,
                    2,
                    33,
                    4,
                    2,
                    42,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.11210v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.11210v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Reinforcement learning (RL) has become a key paradigm for training software engineering (SWE) agents, but existing pipelines typically rely on per-task containers for isolation. At scale, pre-built container images incur substantial storage overhead, slow environment setup, and require container-management privileges. We propose SWE-MiniSandbox, a lightweight, container-free method that enables scalable RL training of SWE agents without sacrificing isolation. Instead of relying on per-instance containers, SWE-MiniSandbox executes each task in an isolated workspace backed by kernel-level mechanisms, substantially reducing system overhead. It leverages lightweight environment pre-caching techniques to eliminate the need for bulky container images. As a result, our approach lowers disk usage to approximately 5\\% of that required by container-based pipelines and reduces environment preparation time to about 25\\% of the container baseline. Empirical results demonstrate that SWE-MiniSandbox achieves evaluation performance comparable to standard container-based pipelines. By removing the dependency on heavy container infrastructure, SWE-MiniSandbox offers a practical and accessible foundation for scaling RL-based SWE agents, particularly in resource-constrained research environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has become a key paradigm for training software engineering (SWE) agents, but existing pipelines typically rely on per-task containers for isolation. At scale, pre-built container images incur substantial storage overhead, slow environment setup, and require container-management privileges. We propose SWE-MiniSandbox, a lightweight, container-free method that enables scalable RL training of SWE agents without sacrificing isolation. Instead of relying on per-instance containers, SWE-MiniSandbox executes each task in an isolated workspace backed by kernel-level mechanisms, substantially reducing system overhead. It leverages lightweight environment pre-caching techniques to eliminate the need for bulky container images. As a result, our approach lowers disk usage to approximately 5\\% of that required by container-based pipelines and reduces environment preparation time to about 25\\% of the container baseline. Empirical results demonstrate that SWE-MiniSandbox achieves evaluation performance comparable to standard container-based pipelines. By removing the dependency on heavy container infrastructure, SWE-MiniSandbox offers a practical and accessible foundation for scaling RL-based SWE agents, particularly in resource-constrained research environments."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-11T02:33:04Z",
                "published_parsed": [
                    2026,
                    2,
                    11,
                    2,
                    33,
                    4,
                    2,
                    42,
                    0
                ],
                "arxiv_comment": "ICML under review",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Danlong Yuan"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Zhengren Wang"
                    },
                    {
                        "name": "Xueliang Zhao"
                    },
                    {
                        "name": "Huishuai Zhang"
                    },
                    {
                        "name": "Dongyan Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Dongyan Zhao"
                },
                "author": "Dongyan Zhao"
            },
            {
                "id": "http://arxiv.org/abs/2505.11739v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.11739v3",
                "title": "ZeroTuning: Unlocking the Initial Token's Power to Enhance Large Language Models Without Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZeroTuning: Unlocking the Initial Token's Power to Enhance Large Language Models Without Training"
                },
                "updated": "2026-02-10T23:22:11Z",
                "updated_parsed": [
                    2026,
                    2,
                    10,
                    23,
                    22,
                    11,
                    1,
                    41,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.11739v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.11739v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Token-level attention tuning, a class of training-free methods including Post-hoc Attention Steering (PASTA) and Attention Calibration (ACT), has emerged as a promising approach for improving frozen LLMs via interpretable interventions. However, these methods rely on auxiliary heuristics to identify important task-specific tokens, which can introduce bias and limit applicability when token importance is ambiguous or when optimized kernels make attention maps inaccessible. We propose a simpler alternative: intervening only on the initial token (e.g., BOS in LLaMA). We theoretically show that adding lightweight biases to this token's attention logits systematically shifts and reshapes downstream attention patterns - an effect amplified by its natural role as an attention sink. Empirically, we find that this tuning can improve LLM performance and better elicit pretrained knowledge, with stronger effects in early layers and distinct scaling preferences across attention heads. Building on these findings, we introduce ZeroTuning, a training-free method that improves LLM performance by applying head-specific attention adjustments to the initial token, requiring no parameter updates. We present two variants: a supervised mode that calibrates on validation examples, and an unsupervised mode that directly minimizes output entropy. ZeroTuning requires no KV-cache or decoding changes and is kernel-agnostic (works with SDPA and FlashAttention). It requires only four lines of modification to the standard LlamaAttention code, achieves gains across 15 datasets, and outperforms prior, more complex methods. For example, on Llama-3.1-8B, it yields relative improvements of 19.9% on classification, 4.5% on question answering, and 2.1% on dialogue. ZeroTuning also works out of the box with quantized inference and maintains its improvements as context length increases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token-level attention tuning, a class of training-free methods including Post-hoc Attention Steering (PASTA) and Attention Calibration (ACT), has emerged as a promising approach for improving frozen LLMs via interpretable interventions. However, these methods rely on auxiliary heuristics to identify important task-specific tokens, which can introduce bias and limit applicability when token importance is ambiguous or when optimized kernels make attention maps inaccessible. We propose a simpler alternative: intervening only on the initial token (e.g., BOS in LLaMA). We theoretically show that adding lightweight biases to this token's attention logits systematically shifts and reshapes downstream attention patterns - an effect amplified by its natural role as an attention sink. Empirically, we find that this tuning can improve LLM performance and better elicit pretrained knowledge, with stronger effects in early layers and distinct scaling preferences across attention heads. Building on these findings, we introduce ZeroTuning, a training-free method that improves LLM performance by applying head-specific attention adjustments to the initial token, requiring no parameter updates. We present two variants: a supervised mode that calibrates on validation examples, and an unsupervised mode that directly minimizes output entropy. ZeroTuning requires no KV-cache or decoding changes and is kernel-agnostic (works with SDPA and FlashAttention). It requires only four lines of modification to the standard LlamaAttention code, achieves gains across 15 datasets, and outperforms prior, more complex methods. For example, on Llama-3.1-8B, it yields relative improvements of 19.9% on classification, 4.5% on question answering, and 2.1% on dialogue. ZeroTuning also works out of the box with quantized inference and maintains its improvements as context length increases."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-16T22:52:24Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    22,
                    52,
                    24,
                    4,
                    136,
                    0
                ],
                "arxiv_comment": "ICLR 2026 Accepted Version: proofread, introduction rewritten, additional experiments and appendix material added",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Feijiang Han"
                    },
                    {
                        "name": "Xiaodong Yu"
                    },
                    {
                        "name": "Jianheng Tang"
                    },
                    {
                        "name": "Delip Rao"
                    },
                    {
                        "name": "Weihua Du"
                    },
                    {
                        "name": "Lyle Ungar"
                    }
                ],
                "author_detail": {
                    "name": "Lyle Ungar"
                },
                "author": "Lyle Ungar"
            },
            {
                "id": "http://arxiv.org/abs/2602.10254v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.10254v1",
                "title": "Area-Efficient In-Memory Computing for Mixture-of-Experts via Multiplexing and Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Area-Efficient In-Memory Computing for Mixture-of-Experts via Multiplexing and Caching"
                },
                "updated": "2026-02-10T19:59:08Z",
                "updated_parsed": [
                    2026,
                    2,
                    10,
                    19,
                    59,
                    8,
                    1,
                    41,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.10254v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.10254v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Mixture-of-Experts (MoE) layers activate a subset of model weights, dubbed experts, to improve model performance. MoE is particularly promising for deployment on process-in-memory (PIM) architectures, because PIM can naturally fit experts separately and provide great benefits for energy efficiency. However, PIM chips often suffer from large area overhead, especially in the peripheral circuits. In this paper, we propose an area-efficient in-memory computing architecture for MoE transformers. First, to reduce area, we propose a crossbar-level multiplexing strategy that exploits MoE sparsity: experts are deployed on crossbars and multiple crossbars share the same peripheral circuits. Second, we propose expert grouping and group-wise scheduling methods to alleviate the load imbalance and contention overhead caused by sharing. In addition, to address the problem that the expert choice router requires access to all hidden states during generation, we propose a gate-output (GO)cache to store necessary results and bypass expensive additional computation. Experiments show that our approaches improve the area efficiency of the MoE part by up to 2.2x compared to a SOTA architecture. During generation, the cache improves performance and energy efficiency by 4.2x and 10.1x, respectively, compared to the baseline when generating 8 tokens. The total performance density achieves 15.6 GOPS/W/mm2. The code is open source at https://github.com/superstarghy/MoEwithPIM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) layers activate a subset of model weights, dubbed experts, to improve model performance. MoE is particularly promising for deployment on process-in-memory (PIM) architectures, because PIM can naturally fit experts separately and provide great benefits for energy efficiency. However, PIM chips often suffer from large area overhead, especially in the peripheral circuits. In this paper, we propose an area-efficient in-memory computing architecture for MoE transformers. First, to reduce area, we propose a crossbar-level multiplexing strategy that exploits MoE sparsity: experts are deployed on crossbars and multiple crossbars share the same peripheral circuits. Second, we propose expert grouping and group-wise scheduling methods to alleviate the load imbalance and contention overhead caused by sharing. In addition, to address the problem that the expert choice router requires access to all hidden states during generation, we propose a gate-output (GO)cache to store necessary results and bypass expensive additional computation. Experiments show that our approaches improve the area efficiency of the MoE part by up to 2.2x compared to a SOTA architecture. During generation, the cache improves performance and energy efficiency by 4.2x and 10.1x, respectively, compared to the baseline when generating 8 tokens. The total performance density achieves 15.6 GOPS/W/mm2. The code is open source at https://github.com/superstarghy/MoEwithPIM."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-10T19:59:08Z",
                "published_parsed": [
                    2026,
                    2,
                    10,
                    19,
                    59,
                    8,
                    1,
                    41,
                    0
                ],
                "arxiv_comment": "Accepted by ISCAS 2026",
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Hanyuan Gao"
                    },
                    {
                        "name": "Xiaoxuan Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoxuan Yang"
                },
                "author": "Xiaoxuan Yang"
            },
            {
                "id": "http://arxiv.org/abs/2602.10238v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.10238v1",
                "title": "Learning to Evict from Key-Value Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Evict from Key-Value Cache"
                },
                "updated": "2026-02-10T19:34:15Z",
                "updated_parsed": [
                    2026,
                    2,
                    10,
                    19,
                    34,
                    15,
                    1,
                    41,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.10238v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.10238v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The growing size of Large Language Models (LLMs) makes efficient inference challenging, primarily due to the memory demands of the autoregressive Key-Value (KV) cache. Existing eviction or compression methods reduce cost but rely on heuristics, such as recency or past attention scores, which serve only as indirect proxies for a token's future utility and introduce computational overhead. We reframe KV cache eviction as a reinforcement learning (RL) problem: learning to rank tokens by their predicted usefulness for future decoding. To this end, we introduce KV Policy (KVP), a framework of lightweight per-head RL agents trained on pre-computed generation traces using only key and value vectors. Each agent learns a specialized eviction policy guided by future utility, which evaluates the quality of the ranking across all cache budgets, requiring no modifications to the underlying LLM or additional inference. Evaluated across two different model families on the long-context benchmark RULER and the multi-turn dialogue benchmark OASST2-4k, KVP significantly outperforms baselines. Furthermore, zero-shot tests on standard downstream tasks (e.g., LongBench, BOOLQ, ARC) indicate that KVP generalizes well beyond its training distribution and to longer context lengths. These results demonstrate that learning to predict future token utility is a powerful and scalable paradigm for adaptive KV cache management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing size of Large Language Models (LLMs) makes efficient inference challenging, primarily due to the memory demands of the autoregressive Key-Value (KV) cache. Existing eviction or compression methods reduce cost but rely on heuristics, such as recency or past attention scores, which serve only as indirect proxies for a token's future utility and introduce computational overhead. We reframe KV cache eviction as a reinforcement learning (RL) problem: learning to rank tokens by their predicted usefulness for future decoding. To this end, we introduce KV Policy (KVP), a framework of lightweight per-head RL agents trained on pre-computed generation traces using only key and value vectors. Each agent learns a specialized eviction policy guided by future utility, which evaluates the quality of the ranking across all cache budgets, requiring no modifications to the underlying LLM or additional inference. Evaluated across two different model families on the long-context benchmark RULER and the multi-turn dialogue benchmark OASST2-4k, KVP significantly outperforms baselines. Furthermore, zero-shot tests on standard downstream tasks (e.g., LongBench, BOOLQ, ARC) indicate that KVP generalizes well beyond its training distribution and to longer context lengths. These results demonstrate that learning to predict future token utility is a powerful and scalable paradigm for adaptive KV cache management."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-10T19:34:15Z",
                "published_parsed": [
                    2026,
                    2,
                    10,
                    19,
                    34,
                    15,
                    1,
                    41,
                    0
                ],
                "arxiv_comment": "23 pages, 15 figures",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Luca Moschella"
                    },
                    {
                        "name": "Laura Manduchi"
                    },
                    {
                        "name": "Ozan Sener"
                    }
                ],
                "author_detail": {
                    "name": "Ozan Sener"
                },
                "author": "Ozan Sener"
            },
            {
                "id": "http://arxiv.org/abs/2602.10080v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.10080v1",
                "title": "Beyond a Single Queue: Multi-Level-Multi-Queue as an Effective Design for SSSP problems on GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond a Single Queue: Multi-Level-Multi-Queue as an Effective Design for SSSP problems on GPUs"
                },
                "updated": "2026-02-10T18:46:16Z",
                "updated_parsed": [
                    2026,
                    2,
                    10,
                    18,
                    46,
                    16,
                    1,
                    41,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.10080v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.10080v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As one of the most fundamental problems in graph processing, the Single-Source Shortest Path (SSSP) problem plays a critical role in numerous application scenarios. However, existing GPU-based solutions remain inefficient, as they typically rely on a single, fixed queue design that incurs severe synchronization overhead, high memory latency, and poor adaptivity to diverse inputs. To address these inefficiencies, we propose MultiLevelMultiQueue (MLMQ), a novel data structure that distributes multiple queues across the GPU's multi-level parallelism and memory hierarchy. To realize MLMQ, we introduce a cache-like collaboration mechanism for efficient inter-queue coordination, and develop a modular queue design based on unified Read and Write primitives. Within this framework, we expand the optimization space by designing a set of GPU-friendly queues, composing them across multiple levels, and further providing an input-adaptive MLMQ configuration scheme. Our MLMQ design achieves average speedups of 1.87x to 17.13x over state-of-the-art implementations. Our code is open-sourced at https://github.com/Leo9660/MLMQ.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As one of the most fundamental problems in graph processing, the Single-Source Shortest Path (SSSP) problem plays a critical role in numerous application scenarios. However, existing GPU-based solutions remain inefficient, as they typically rely on a single, fixed queue design that incurs severe synchronization overhead, high memory latency, and poor adaptivity to diverse inputs. To address these inefficiencies, we propose MultiLevelMultiQueue (MLMQ), a novel data structure that distributes multiple queues across the GPU's multi-level parallelism and memory hierarchy. To realize MLMQ, we introduce a cache-like collaboration mechanism for efficient inter-queue coordination, and develop a modular queue design based on unified Read and Write primitives. Within this framework, we expand the optimization space by designing a set of GPU-friendly queues, composing them across multiple levels, and further providing an input-adaptive MLMQ configuration scheme. Our MLMQ design achieves average speedups of 1.87x to 17.13x over state-of-the-art implementations. Our code is open-sourced at https://github.com/Leo9660/MLMQ.git."
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-10T18:46:16Z",
                "published_parsed": [
                    2026,
                    2,
                    10,
                    18,
                    46,
                    16,
                    1,
                    41,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS"
                },
                "authors": [
                    {
                        "name": "Zhengding Hu"
                    },
                    {
                        "name": "Jingwen Sun"
                    },
                    {
                        "name": "Le Jiang"
                    },
                    {
                        "name": "Yuhao Wang"
                    },
                    {
                        "name": "Junqing Lin"
                    },
                    {
                        "name": "Yi Zong"
                    },
                    {
                        "name": "Guangzhong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangzhong Sun"
                },
                "author": "Guangzhong Sun"
            },
            {
                "id": "http://arxiv.org/abs/2602.10056v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.10056v1",
                "title": "WildCat: Near-Linear Attention in Theory and Practice",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WildCat: Near-Linear Attention in Theory and Practice"
                },
                "updated": "2026-02-10T18:22:32Z",
                "updated_parsed": [
                    2026,
                    2,
                    10,
                    18,
                    22,
                    32,
                    1,
                    41,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.10056v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.10056v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce WildCat, a high-accuracy, low-cost approach to compressing the attention mechanism in neural networks. While attention is a staple of modern network architectures, it is also notoriously expensive to deploy due to resource requirements that scale quadratically with the input sequence length $n$. WildCat avoids these quadratic costs by only attending over a small weighted coreset. Crucially, we select the coreset using a fast but spectrally-accurate subsampling algorithm -- randomly pivoted Cholesky -- and weight the elements optimally to minimise reconstruction error. Remarkably, given bounded inputs, WildCat approximates exact attention with super-polynomial $O(n^{-\\sqrt{\\log(\\log(n))}})$ error decay while running in near-linear $O(n^{1+o(1)})$ time. In contrast, prior practical approximations either lack error guarantees or require quadratic runtime to guarantee such high fidelity. We couple this advance with a GPU-optimized PyTorch implementation and a suite of benchmark experiments demonstrating the benefits of WildCat for image generation, image classification, and language model KV cache compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce WildCat, a high-accuracy, low-cost approach to compressing the attention mechanism in neural networks. While attention is a staple of modern network architectures, it is also notoriously expensive to deploy due to resource requirements that scale quadratically with the input sequence length $n$. WildCat avoids these quadratic costs by only attending over a small weighted coreset. Crucially, we select the coreset using a fast but spectrally-accurate subsampling algorithm -- randomly pivoted Cholesky -- and weight the elements optimally to minimise reconstruction error. Remarkably, given bounded inputs, WildCat approximates exact attention with super-polynomial $O(n^{-\\sqrt{\\log(\\log(n))}})$ error decay while running in near-linear $O(n^{1+o(1)})$ time. In contrast, prior practical approximations either lack error guarantees or require quadratic runtime to guarantee such high fidelity. We couple this advance with a GPU-optimized PyTorch implementation and a suite of benchmark experiments demonstrating the benefits of WildCat for image generation, image classification, and language model KV cache compression."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-10T18:22:32Z",
                "published_parsed": [
                    2026,
                    2,
                    10,
                    18,
                    22,
                    32,
                    1,
                    41,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Tobias Schröder"
                    },
                    {
                        "name": "Lester Mackey"
                    }
                ],
                "author_detail": {
                    "name": "Lester Mackey"
                },
                "author": "Lester Mackey"
            },
            {
                "id": "http://arxiv.org/abs/2407.21625v6",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2407.21625v6",
                "title": "REPS: Recycled Entropy Packet Spraying for Adaptive Load Balancing and Failure Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REPS: Recycled Entropy Packet Spraying for Adaptive Load Balancing and Failure Mitigation"
                },
                "updated": "2026-02-10T17:10:16Z",
                "updated_parsed": [
                    2026,
                    2,
                    10,
                    17,
                    10,
                    16,
                    1,
                    41,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2407.21625v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2407.21625v6",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1145/3767295.3769320",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Next-generation datacenters require highly efficient network load balancing to manage the growing scale of artificial intelligence (AI) training and general datacenter traffic. However, existing Ethernet-based solutions, such as Equal Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to maintain high network utilization due to both increasing traffic demands and the expanding scale of datacenter topologies, which also exacerbate network failures. To address these limitations, we propose REPS, a lightweight decentralized per-packet adaptive load balancing algorithm designed to optimize network utilization while ensuring rapid recovery from link failures. REPS adapts to network conditions by caching good-performing paths. In case of a network failure, REPS re-routes traffic away from it in less than 100 microseconds. REPS is designed to be deployed with next-generation out-of-order transports, such as Ultra Ethernet, and uses less than 25 bytes of per-connection state regardless of the topology size. We extensively evaluate REPS in large-scale simulations and FPGA-based NICs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next-generation datacenters require highly efficient network load balancing to manage the growing scale of artificial intelligence (AI) training and general datacenter traffic. However, existing Ethernet-based solutions, such as Equal Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to maintain high network utilization due to both increasing traffic demands and the expanding scale of datacenter topologies, which also exacerbate network failures. To address these limitations, we propose REPS, a lightweight decentralized per-packet adaptive load balancing algorithm designed to optimize network utilization while ensuring rapid recovery from link failures. REPS adapts to network conditions by caching good-performing paths. In case of a network failure, REPS re-routes traffic away from it in less than 100 microseconds. REPS is designed to be deployed with next-generation out-of-order transports, such as Ultra Ethernet, and uses less than 25 bytes of per-connection state regardless of the topology size. We extensively evaluate REPS in large-scale simulations and FPGA-based NICs."
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-07-31T14:17:49Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    14,
                    17,
                    49,
                    2,
                    213,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI"
                },
                "arxiv_journal_ref": "Proc. 21st European Conference on Computer Systems (EuroSys 2026), ACM, 2026",
                "authors": [
                    {
                        "name": "Tommaso Bonato"
                    },
                    {
                        "name": "Abdul Kabbani"
                    },
                    {
                        "name": "Ahmad Ghalayini"
                    },
                    {
                        "name": "Michael Papamichael"
                    },
                    {
                        "name": "Mohammad Dohadwala"
                    },
                    {
                        "name": "Lukas Gianinazzi"
                    },
                    {
                        "name": "Mikhail Khalilov"
                    },
                    {
                        "name": "Elias Achermann"
                    },
                    {
                        "name": "Daniele De Sensi"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "arxiv_doi": "10.1145/3767295.3769320"
            },
            {
                "id": "http://arxiv.org/abs/2511.03475v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.03475v2",
                "title": "RAGBoost: Efficient Retrieval-Augmented Generation with Accuracy-Preserving Context Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAGBoost: Efficient Retrieval-Augmented Generation with Accuracy-Preserving Context Reuse"
                },
                "updated": "2026-02-10T16:55:15Z",
                "updated_parsed": [
                    2026,
                    2,
                    10,
                    16,
                    55,
                    15,
                    1,
                    41,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.03475v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.03475v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) with retrieved context but often suffers from downgraded prefill performance as modern applications demand longer and more complex inputs. Existing caching techniques either preserve accuracy with low cache reuse or improve reuse at the cost of degraded reasoning quality. We present RAGBoost, an efficient RAG system that achieves high cache reuse without sacrificing accuracy through accuracy-preserving context reuse. RAGBoost detects overlapping retrieved items across concurrent sessions and multi-turn interactions, using efficient context indexing, ordering, and de-duplication to maximize reuse, while lightweight contextual hints maintain reasoning fidelity. It integrates seamlessly with existing LLM inference engines and improves their prefill performance by 1.5-3X over state-of-the-art methods, while preserving or even enhancing reasoning accuracy across diverse RAG and agentic AI workloads. Our code is released at: https://github.com/Edinburgh-AgenticAI/RAGBoost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) with retrieved context but often suffers from downgraded prefill performance as modern applications demand longer and more complex inputs. Existing caching techniques either preserve accuracy with low cache reuse or improve reuse at the cost of degraded reasoning quality. We present RAGBoost, an efficient RAG system that achieves high cache reuse without sacrificing accuracy through accuracy-preserving context reuse. RAGBoost detects overlapping retrieved items across concurrent sessions and multi-turn interactions, using efficient context indexing, ordering, and de-duplication to maximize reuse, while lightweight contextual hints maintain reasoning fidelity. It integrates seamlessly with existing LLM inference engines and improves their prefill performance by 1.5-3X over state-of-the-art methods, while preserving or even enhancing reasoning accuracy across diverse RAG and agentic AI workloads. Our code is released at: https://github.com/Edinburgh-AgenticAI/RAGBoost."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-05T13:59:01Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    13,
                    59,
                    1,
                    2,
                    309,
                    0
                ],
                "arxiv_comment": "The paper is no longer valid and the contents will be fused to another different paper",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Yinsicheng Jiang"
                    },
                    {
                        "name": "Yeqi Huang"
                    },
                    {
                        "name": "Liang Cheng"
                    },
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Xuan Sun"
                    },
                    {
                        "name": "Luo Mai"
                    }
                ],
                "author_detail": {
                    "name": "Luo Mai"
                },
                "author": "Luo Mai"
            },
            {
                "id": "http://arxiv.org/abs/2602.07721v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.07721v2",
                "title": "ParisKV: Fast and Drift-Robust KV-Cache Retrieval for Long-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ParisKV: Fast and Drift-Robust KV-Cache Retrieval for Long-Context LLMs"
                },
                "updated": "2026-02-10T16:05:56Z",
                "updated_parsed": [
                    2026,
                    2,
                    10,
                    16,
                    5,
                    56,
                    1,
                    41,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.07721v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.07721v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "KV-cache retrieval is essential for long-context LLM inference, yet existing methods struggle with distribution drift and high latency at scale. We introduce ParisKV, a drift-robust, GPU-native KV-cache retrieval framework based on collision-based candidate selection, followed by a quantized inner-product reranking estimator. For million-token contexts, ParisKV supports CPU-offloaded KV caches via Unified Virtual Addressing (UVA), enabling on-demand top-$k$ fetching with minimal overhead. ParisKV matches or outperforms full attention quality on long-input and long-generation benchmarks. It achieves state-of-the-art long-context decoding efficiency: it matches or exceeds full attention speed even at batch size 1 for long contexts, delivers up to 2.8$\\times$ higher throughput within full attention's runnable range, and scales to million-token contexts where full attention runs out of memory. At million-token scale, ParisKV reduces decode latency by 17$\\times$ and 44$\\times$ compared to MagicPIG and PQCache, respectively, two state-of-the-art KV-cache Top-$k$ retrieval baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-cache retrieval is essential for long-context LLM inference, yet existing methods struggle with distribution drift and high latency at scale. We introduce ParisKV, a drift-robust, GPU-native KV-cache retrieval framework based on collision-based candidate selection, followed by a quantized inner-product reranking estimator. For million-token contexts, ParisKV supports CPU-offloaded KV caches via Unified Virtual Addressing (UVA), enabling on-demand top-$k$ fetching with minimal overhead. ParisKV matches or outperforms full attention quality on long-input and long-generation benchmarks. It achieves state-of-the-art long-context decoding efficiency: it matches or exceeds full attention speed even at batch size 1 for long contexts, delivers up to 2.8$\\times$ higher throughput within full attention's runnable range, and scales to million-token contexts where full attention runs out of memory. At million-token scale, ParisKV reduces decode latency by 17$\\times$ and 44$\\times$ compared to MagicPIG and PQCache, respectively, two state-of-the-art KV-cache Top-$k$ retrieval baselines."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-07T22:26:45Z",
                "published_parsed": [
                    2026,
                    2,
                    7,
                    22,
                    26,
                    45,
                    5,
                    38,
                    0
                ],
                "arxiv_comment": "25 pages, 16 figures. Under review",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Yanlin Qi"
                    },
                    {
                        "name": "Xinhang Chen"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qitong Wang"
                    },
                    {
                        "name": "Botao Peng"
                    },
                    {
                        "name": "Themis Palpanas"
                    }
                ],
                "author_detail": {
                    "name": "Themis Palpanas"
                },
                "author": "Themis Palpanas"
            },
            {
                "id": "http://arxiv.org/abs/2601.05787v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05787v2",
                "title": "From Off-Policy to On-Policy: Enhancing GUI Agents via Bi-level Expert-to-Policy Assimilation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Off-Policy to On-Policy: Enhancing GUI Agents via Bi-level Expert-to-Policy Assimilation"
                },
                "updated": "2026-02-10T15:12:17Z",
                "updated_parsed": [
                    2026,
                    2,
                    10,
                    15,
                    12,
                    17,
                    1,
                    41,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05787v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05787v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Vision-language models are increasingly deployed as computer-use agents (CUAs) that operate desktops and browsers. Top-performing CUAs are framework-based systems that decompose planning and execution, while end-to-end screenshot-to-action policies are easier to deploy but lag behind on benchmarks such as OSWorld-Verified. GUI datasets like OSWorld pose two bottlenecks: they expose only a few hundred interactive, verifiable tasks and environments, and expert trajectories must be gathered by interacting with these environments, making such data hard to scale. We therefore ask how reinforcement learning from verifiable rewards (RLVR) can best exploit a small pool of exist expert trajectories to train end-to-end policies. Naively mixing these off-policy traces into on-policy RLVR is brittle: even after format conversion, expert trajectories exhibit structural mismatch and distribution shift from the learner. We propose BEPA (Bi-Level Expert-to-Policy Assimilation), which turns static expert traces into policy-aligned guidance via self-rolled reachable trajectories under the base policy (LEVEL-1) and a per-task, dynamically updated cache used in RLVR (LEVEL-2). On OSWorld-Verified, BEPA improves UITARS1.5-7B success from 22.87% to 32.13% and raises a held-out split from 5.74% to 10.30%, with consistent gains on MMBench-GUI and Online-Mind2Web. Our code and data are available at: https://github.com/LEON-gittech/Verl_GUI.git",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models are increasingly deployed as computer-use agents (CUAs) that operate desktops and browsers. Top-performing CUAs are framework-based systems that decompose planning and execution, while end-to-end screenshot-to-action policies are easier to deploy but lag behind on benchmarks such as OSWorld-Verified. GUI datasets like OSWorld pose two bottlenecks: they expose only a few hundred interactive, verifiable tasks and environments, and expert trajectories must be gathered by interacting with these environments, making such data hard to scale. We therefore ask how reinforcement learning from verifiable rewards (RLVR) can best exploit a small pool of exist expert trajectories to train end-to-end policies. Naively mixing these off-policy traces into on-policy RLVR is brittle: even after format conversion, expert trajectories exhibit structural mismatch and distribution shift from the learner. We propose BEPA (Bi-Level Expert-to-Policy Assimilation), which turns static expert traces into policy-aligned guidance via self-rolled reachable trajectories under the base policy (LEVEL-1) and a per-task, dynamically updated cache used in RLVR (LEVEL-2). On OSWorld-Verified, BEPA improves UITARS1.5-7B success from 22.87% to 32.13% and raises a held-out split from 5.74% to 10.30%, with consistent gains on MMBench-GUI and Online-Mind2Web. Our code and data are available at: https://github.com/LEON-gittech/Verl_GUI.git"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-09T13:26:38Z",
                "published_parsed": [
                    2026,
                    1,
                    9,
                    13,
                    26,
                    38,
                    4,
                    9,
                    0
                ],
                "arxiv_comment": "Work In Progress",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Zezhou Wang"
                    },
                    {
                        "name": "Ziyun Zhang"
                    },
                    {
                        "name": "Xiaoyi Zhang"
                    },
                    {
                        "name": "Zhuzhong Qian"
                    },
                    {
                        "name": "Yan Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yan Lu"
                },
                "author": "Yan Lu"
            },
            {
                "id": "http://arxiv.org/abs/2511.16054v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.16054v2",
                "title": "Learning Tractable Distributions Of Language Model Continuations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Tractable Distributions Of Language Model Continuations"
                },
                "updated": "2026-02-10T13:57:46Z",
                "updated_parsed": [
                    2026,
                    2,
                    10,
                    13,
                    57,
                    46,
                    1,
                    41,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.16054v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.16054v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Controlled generation imposes sequence-level constraints (syntax, style, safety) that depend on future tokens, making exact conditioning of an autoregressive LM intractable. Tractable surrogates such as HMMs can approximate continuation distributions and steer decoding, but standard surrogates are often weakly context-aware. We propose Learning to Look Ahead (LTLA), a hybrid method that uses base-LM embeddings to condition a globally learned tractable surrogate: a neural head predicts only a prefix-dependent latent prior, while a shared HMM answers continuation queries exactly. LTLA is designed to avoid two common efficiency traps when adding neural context. First, it avoids vocabulary-sized prefix rescoring (V extra LM evaluations) by scoring all next-token candidates via a single batched HMM forward update. Second, it avoids predicting a new HMM per prefix by learning one shared HMM and conditioning only the latent prior, which enables reuse of cached future-likelihood (backward) messages across decoding steps. Empirically, LTLA improves continuation likelihood over standard HMM surrogates, enables lookahead control for vision--language models by incorporating continuous context, achieves 100% syntactic constraint satisfaction, and improves detoxification while adding only a 14% decoding-time overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controlled generation imposes sequence-level constraints (syntax, style, safety) that depend on future tokens, making exact conditioning of an autoregressive LM intractable. Tractable surrogates such as HMMs can approximate continuation distributions and steer decoding, but standard surrogates are often weakly context-aware. We propose Learning to Look Ahead (LTLA), a hybrid method that uses base-LM embeddings to condition a globally learned tractable surrogate: a neural head predicts only a prefix-dependent latent prior, while a shared HMM answers continuation queries exactly. LTLA is designed to avoid two common efficiency traps when adding neural context. First, it avoids vocabulary-sized prefix rescoring (V extra LM evaluations) by scoring all next-token candidates via a single batched HMM forward update. Second, it avoids predicting a new HMM per prefix by learning one shared HMM and conditioning only the latent prior, which enables reuse of cached future-likelihood (backward) messages across decoding steps. Empirically, LTLA improves continuation likelihood over standard HMM surrogates, enables lookahead control for vision--language models by incorporating continuous context, achieves 100% syntactic constraint satisfaction, and improves detoxification while adding only a 14% decoding-time overhead."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-20T05:17:19Z",
                "published_parsed": [
                    2025,
                    11,
                    20,
                    5,
                    17,
                    19,
                    3,
                    324,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Gwen Yidou-Weng"
                    },
                    {
                        "name": "Ian Li"
                    },
                    {
                        "name": "Anji Liu"
                    },
                    {
                        "name": "Oliver Broadrick"
                    },
                    {
                        "name": "Yuchen Cui"
                    },
                    {
                        "name": "Guy Van den Broeck"
                    },
                    {
                        "name": "Benjie Wang"
                    }
                ],
                "author_detail": {
                    "name": "Benjie Wang"
                },
                "author": "Benjie Wang"
            },
            {
                "id": "http://arxiv.org/abs/2508.06133v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.06133v3",
                "title": "LLM Serving Optimization with Variable Prefill and Decode Lengths",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Serving Optimization with Variable Prefill and Decode Lengths"
                },
                "updated": "2026-02-10T12:57:16Z",
                "updated_parsed": [
                    2026,
                    2,
                    10,
                    12,
                    57,
                    16,
                    1,
                    41,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.06133v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.06133v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We study offline scheduling for large language model (LLM) serving under a fixed KV-cache memory budget, where requests have heterogeneous prompt (prefill) and response (decode) lengths. Prompt tokens determine initial KV usage, and each generated token increases memory by one unit. Given a backlog of n requests arriving together, we schedule mixed prefill and decode batches to minimize total end-to-end latency. We show that heterogeneity in prompt lengths makes the problem computationally intractable and that widely used heuristics such as first-come-first-served and shortest-first can be arbitrarily suboptimal. We propose Sorted-F, which repeatedly forms feasible batches using a new selection metric that balances batch size against downstream decode cost, and prove it achieves a constant-factor guarantee on total latency. We further develop practical variants -- an exact solver for small instances and fast heuristics for larger ones -- and evaluate them on a public workload spanning short conversations and long-document summarization, where they consistently reduce average latency relative to standard baselines. Our results highlight that during peak-hour tidal backlogs, greedy GPU packing or short-request prioritization can perform poorly when prompt lengths vary widely, and provide a principled, tunable framework for designing production batch schedulers and planning capacity in memory-constrained LLM serving systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study offline scheduling for large language model (LLM) serving under a fixed KV-cache memory budget, where requests have heterogeneous prompt (prefill) and response (decode) lengths. Prompt tokens determine initial KV usage, and each generated token increases memory by one unit. Given a backlog of n requests arriving together, we schedule mixed prefill and decode batches to minimize total end-to-end latency. We show that heterogeneity in prompt lengths makes the problem computationally intractable and that widely used heuristics such as first-come-first-served and shortest-first can be arbitrarily suboptimal. We propose Sorted-F, which repeatedly forms feasible batches using a new selection metric that balances batch size against downstream decode cost, and prove it achieves a constant-factor guarantee on total latency. We further develop practical variants -- an exact solver for small instances and fast heuristics for larger ones -- and evaluate them on a public workload spanning short conversations and long-document summarization, where they consistently reduce average latency relative to standard baselines. Our results highlight that during peak-hour tidal backlogs, greedy GPU packing or short-request prioritization can perform poorly when prompt lengths vary widely, and provide a principled, tunable framework for designing production batch schedulers and planning capacity in memory-constrained LLM serving systems."
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-08T08:54:21Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    8,
                    54,
                    21,
                    4,
                    220,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "math.OC"
                },
                "authors": [
                    {
                        "name": "Meixuan Wang"
                    },
                    {
                        "name": "Yinyu Ye"
                    },
                    {
                        "name": "Zijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zijie Zhou"
                },
                "author": "Zijie Zhou"
            },
            {
                "id": "http://arxiv.org/abs/2602.02599v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.02599v3",
                "title": "RAP: KV-Cache Compression via RoPE-Aligned Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAP: KV-Cache Compression via RoPE-Aligned Pruning"
                },
                "updated": "2026-02-10T12:21:14Z",
                "updated_parsed": [
                    2026,
                    2,
                    10,
                    12,
                    21,
                    14,
                    1,
                    41,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.02599v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.02599v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Long-context inference in large language models is increasingly bottlenecked by the memory and compute cost of the KV-Cache. Low-rank factorization compresses KV projections by writing $W \\approx A * B$, where A produces latent KV states and B can be absorbed into downstream weights. In modern RoPE-based LLMs, this absorption fails: RoPE forces latent KV states to be reconstructed to full dimension, reintroducing substantial memory and compute overhead. We propose RoPE-Aligned Pruning (RAP), which prunes entire RoPE-aligned column pairs to preserve RoPE's 2x2 rotation structure, restore B absorption, and eliminate reconstruction. Our evaluation on LLaMA-3-8B and Mistral-7B shows that RAP enables joint reduction of KV-Cache, attention parameters, and FLOPs by 20-30%, all at once, while maintaining strong accuracy. Notably, RAP reduces attention latency to 83% (prefill) and 77% (decode) of baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context inference in large language models is increasingly bottlenecked by the memory and compute cost of the KV-Cache. Low-rank factorization compresses KV projections by writing $W \\approx A * B$, where A produces latent KV states and B can be absorbed into downstream weights. In modern RoPE-based LLMs, this absorption fails: RoPE forces latent KV states to be reconstructed to full dimension, reintroducing substantial memory and compute overhead. We propose RoPE-Aligned Pruning (RAP), which prunes entire RoPE-aligned column pairs to preserve RoPE's 2x2 rotation structure, restore B absorption, and eliminate reconstruction. Our evaluation on LLaMA-3-8B and Mistral-7B shows that RAP enables joint reduction of KV-Cache, attention parameters, and FLOPs by 20-30%, all at once, while maintaining strong accuracy. Notably, RAP reduces attention latency to 83% (prefill) and 77% (decode) of baseline."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-01T17:15:40Z",
                "published_parsed": [
                    2026,
                    2,
                    1,
                    17,
                    15,
                    40,
                    6,
                    32,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Jihao Xin"
                    },
                    {
                        "name": "Tian Lyu"
                    },
                    {
                        "name": "David Keyes"
                    },
                    {
                        "name": "Hatem Ltaief"
                    },
                    {
                        "name": "Marco Canini"
                    }
                ],
                "author_detail": {
                    "name": "Marco Canini"
                },
                "author": "Marco Canini"
            },
            {
                "id": "http://arxiv.org/abs/2508.21091v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.21091v2",
                "title": "ERTACache: Error Rectification and Timesteps Adjustment for Efficient Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ERTACache: Error Rectification and Timesteps Adjustment for Efficient Diffusion"
                },
                "updated": "2026-02-10T02:48:05Z",
                "updated_parsed": [
                    2026,
                    2,
                    10,
                    2,
                    48,
                    5,
                    1,
                    41,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.21091v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.21091v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion models suffer from substantial computational overhead due to their inherently iterative inference process. While feature caching offers a promising acceleration strategy by reusing intermediate outputs across timesteps, naive reuse often incurs noticeable quality degradation. In this work, we formally analyze the cumulative error introduced by caching and decompose it into two principal components: feature shift error, caused by inaccuracies in cached outputs, and step amplification error, which arises from error propagation under fixed timestep schedules. To address these issues, we propose ERTACache, a principled caching framework that jointly rectifies both error types. Our method employs an offline residual profiling stage to identify reusable steps, dynamically adjusts integration intervals via a trajectory-aware correction coefficient, and analytically approximates cache-induced errors through a closed-form residual linearization model. Together, these components enable accurate and efficient sampling under aggressive cache reuse. Extensive experiments across standard image and video generation benchmarks show that ERTACache achieves up to 2x inference speedup while consistently preserving or even improving visual quality. Notably, on the state-of-the-art Wan2.1 video diffusion model, ERTACache delivers 2x acceleration with minimal VBench degradation, effectively maintaining baseline fidelity while significantly improving efficiency. The code is available at https://github.com/bytedance/ERTACache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models suffer from substantial computational overhead due to their inherently iterative inference process. While feature caching offers a promising acceleration strategy by reusing intermediate outputs across timesteps, naive reuse often incurs noticeable quality degradation. In this work, we formally analyze the cumulative error introduced by caching and decompose it into two principal components: feature shift error, caused by inaccuracies in cached outputs, and step amplification error, which arises from error propagation under fixed timestep schedules. To address these issues, we propose ERTACache, a principled caching framework that jointly rectifies both error types. Our method employs an offline residual profiling stage to identify reusable steps, dynamically adjusts integration intervals via a trajectory-aware correction coefficient, and analytically approximates cache-induced errors through a closed-form residual linearization model. Together, these components enable accurate and efficient sampling under aggressive cache reuse. Extensive experiments across standard image and video generation benchmarks show that ERTACache achieves up to 2x inference speedup while consistently preserving or even improving visual quality. Notably, on the state-of-the-art Wan2.1 video diffusion model, ERTACache delivers 2x acceleration with minimal VBench degradation, effectively maintaining baseline fidelity while significantly improving efficiency. The code is available at https://github.com/bytedance/ERTACache."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-27T10:37:24Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    10,
                    37,
                    24,
                    2,
                    239,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Xurui Peng"
                    },
                    {
                        "name": "Chenqian Yan"
                    },
                    {
                        "name": "Hong Liu"
                    },
                    {
                        "name": "Rui Ma"
                    },
                    {
                        "name": "Fangmin Chen"
                    },
                    {
                        "name": "Xing Wang"
                    },
                    {
                        "name": "Zhihua Wu"
                    },
                    {
                        "name": "Songwei Liu"
                    },
                    {
                        "name": "Mingbao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Mingbao Lin"
                },
                "author": "Mingbao Lin"
            },
            {
                "id": "http://arxiv.org/abs/2602.09323v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.09323v1",
                "title": "LLM-CoOpt: A Co-Design and Optimization Framework for Efficient LLM Inference on Heterogeneous Platforms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-CoOpt: A Co-Design and Optimization Framework for Efficient LLM Inference on Heterogeneous Platforms"
                },
                "updated": "2026-02-10T01:31:30Z",
                "updated_parsed": [
                    2026,
                    2,
                    10,
                    1,
                    31,
                    30,
                    1,
                    41,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.09323v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.09323v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Major challenges in LLMs inference remain frequent memory bandwidth bottlenecks, computational redundancy, and inefficiencies in long-sequence processing. To address these issues, we propose LLM-CoOpt, a comprehensive algorithmhardware co-design framework aimed at improving both throughput and latency in LLM inference. LLM-CoOpt integrates three key strategies: (1) Key-Value Cache Optimization, termed Opt-KV, which improves memory access efficiency by optimizing both KV cache write and read paths, and introduces FP8 quantization to reduce memory footprint while maintaining accuracy; (2) Grouped-Query Attention for Computational Efficiency, termed Opt-GQA, which reduces the overall computational complexity by restructuring multi-head self-attention into grouped-query attention with shared key-value projections, enabling higher throughput and lower resource consumption; (3) Paged Attention for Long- Sequence Processing, termed Opt-Pa, which adopts a two-step strategy to first segment long sequences into manageable chunks and then apply lazy memory mapping and computation, significantly reducing memory pressure and improving performance on long-context inputs.Experiments on the LLaMa-13BGPTQ model demonstrate that LLM-CoOpt increases inference throughput by up to 13.43%, reduces latency by up to 16.79%, and maintains model accuracy. These results confirm that LLM-CoOpt provides a practical, high-performance optimization path for real-world inference of large-scale language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Major challenges in LLMs inference remain frequent memory bandwidth bottlenecks, computational redundancy, and inefficiencies in long-sequence processing. To address these issues, we propose LLM-CoOpt, a comprehensive algorithmhardware co-design framework aimed at improving both throughput and latency in LLM inference. LLM-CoOpt integrates three key strategies: (1) Key-Value Cache Optimization, termed Opt-KV, which improves memory access efficiency by optimizing both KV cache write and read paths, and introduces FP8 quantization to reduce memory footprint while maintaining accuracy; (2) Grouped-Query Attention for Computational Efficiency, termed Opt-GQA, which reduces the overall computational complexity by restructuring multi-head self-attention into grouped-query attention with shared key-value projections, enabling higher throughput and lower resource consumption; (3) Paged Attention for Long- Sequence Processing, termed Opt-Pa, which adopts a two-step strategy to first segment long sequences into manageable chunks and then apply lazy memory mapping and computation, significantly reducing memory pressure and improving performance on long-context inputs.Experiments on the LLaMa-13BGPTQ model demonstrate that LLM-CoOpt increases inference throughput by up to 13.43%, reduces latency by up to 16.79%, and maintains model accuracy. These results confirm that LLM-CoOpt provides a practical, high-performance optimization path for real-world inference of large-scale language models."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-10T01:31:30Z",
                "published_parsed": [
                    2026,
                    2,
                    10,
                    1,
                    31,
                    30,
                    1,
                    41,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Jie Kong"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Jiehan Zhou"
                    },
                    {
                        "name": "Chen Yu"
                    }
                ],
                "author_detail": {
                    "name": "Chen Yu"
                },
                "author": "Chen Yu"
            },
            {
                "id": "http://arxiv.org/abs/2602.13307v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.13307v1",
                "title": "Cooperative Edge Caching with Large Language Model in Wireless Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cooperative Edge Caching with Large Language Model in Wireless Networks"
                },
                "updated": "2026-02-10T00:15:58Z",
                "updated_parsed": [
                    2026,
                    2,
                    10,
                    0,
                    15,
                    58,
                    1,
                    41,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.13307v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.13307v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Cooperative edge caching in overlapping zones creates intricate coupling among Base Station (BS) decisions, making content replacement highly sensitive to topology and temporal reuse. While heuristics are often myopic and Deep Reinforcement Learning lacks robustness under dynamics, this paper proposes a Large Language Model (LLM)-based multi-BS orchestrator. The LLM acts as the sole autonomous engine, interacting with the environment via a validated text-to-action interface. Each time slot, the system renders environmental states -- including cache inventories and frequency statistics -- into prompts, parsing LLM-generated decisions against strict feasibility constraints. We align the model through a two-stage paradigm: Supervised Fine-Tuning on oracle trajectories for syntax and initialization, followed by Group Relative Policy Optimization. The latter employs an ``opportunity-aware'' reward that prioritizes multi-step cooperative gains relative to a No-Operation baseline. Evaluated on identical request traces, the orchestrator approaches exhaustive-search performance (0.610 vs.\\ 0.617 in a 5-BS scenario), outperforms classical baselines (e.g., +4.1\\% over least-frequently used), and demonstrates robust zero-shot transfer across varying cache capacities, library sizes, and user densities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cooperative edge caching in overlapping zones creates intricate coupling among Base Station (BS) decisions, making content replacement highly sensitive to topology and temporal reuse. While heuristics are often myopic and Deep Reinforcement Learning lacks robustness under dynamics, this paper proposes a Large Language Model (LLM)-based multi-BS orchestrator. The LLM acts as the sole autonomous engine, interacting with the environment via a validated text-to-action interface. Each time slot, the system renders environmental states -- including cache inventories and frequency statistics -- into prompts, parsing LLM-generated decisions against strict feasibility constraints. We align the model through a two-stage paradigm: Supervised Fine-Tuning on oracle trajectories for syntax and initialization, followed by Group Relative Policy Optimization. The latter employs an ``opportunity-aware'' reward that prioritizes multi-step cooperative gains relative to a No-Operation baseline. Evaluated on identical request traces, the orchestrator approaches exhaustive-search performance (0.610 vs.\\ 0.617 in a 5-BS scenario), outperforms classical baselines (e.g., +4.1\\% over least-frequently used), and demonstrates robust zero-shot transfer across varying cache capacities, library sizes, and user densities."
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-10T00:15:58Z",
                "published_parsed": [
                    2026,
                    2,
                    10,
                    0,
                    15,
                    58,
                    1,
                    41,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI"
                },
                "authors": [
                    {
                        "name": "Ning Yang"
                    },
                    {
                        "name": "Wentao Wang"
                    },
                    {
                        "name": "Lingtao Ouyang"
                    },
                    {
                        "name": "Haijun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Haijun Zhang"
                },
                "author": "Haijun Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2508.08438v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.08438v2",
                "title": "Selective KV-Cache Sharing to Mitigate Timing Side-Channels in LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Selective KV-Cache Sharing to Mitigate Timing Side-Channels in LLM Inference"
                },
                "updated": "2026-02-09T21:48:43Z",
                "updated_parsed": [
                    2026,
                    2,
                    9,
                    21,
                    48,
                    43,
                    0,
                    40,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.08438v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.08438v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Global KV-cache sharing is an effective optimization for accelerating large language model (LLM) inference, yet it introduces an API-visible timing side channel that lets adversaries infer sensitive user inputs from shared entries, leading to cross-tenant privacy risks. To address this problem, we introduce SafeKV (Secure and Flexible KV-cache Sharing), a system-level co-design of privacy enforcement and KV-cache management. SafeKV integrates lightweight detection and isolation directly into the serving runtime to eliminate cross-tenant reuse of sensitive KV-cache blocks under our threat model, while recovering most of the performance benefits of global sharing. Our key contributions are: (1) a three-tier asynchronous detection pipeline that decouples privacy classification from inference and supports streaming workloads, (2) a unified radix-tree-based memory manager with path compression and sensitivity-aware eviction for scalable selective isolation, and (3) an RDR-guided (Reuse Diversity Ratio) runtime safeguard that detects and bounds residual leakage. On large LLM backends, SafeKV reduces the time-to-first-token (TTFT) overhead compared to full isolation by up to 40.58% and raises throughput by up to 2.66x. Overall, SafeKV restores the efficiency of KV reuse while enforcing strong, practical privacy for multi-tenant LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Global KV-cache sharing is an effective optimization for accelerating large language model (LLM) inference, yet it introduces an API-visible timing side channel that lets adversaries infer sensitive user inputs from shared entries, leading to cross-tenant privacy risks. To address this problem, we introduce SafeKV (Secure and Flexible KV-cache Sharing), a system-level co-design of privacy enforcement and KV-cache management. SafeKV integrates lightweight detection and isolation directly into the serving runtime to eliminate cross-tenant reuse of sensitive KV-cache blocks under our threat model, while recovering most of the performance benefits of global sharing. Our key contributions are: (1) a three-tier asynchronous detection pipeline that decouples privacy classification from inference and supports streaming workloads, (2) a unified radix-tree-based memory manager with path compression and sensitivity-aware eviction for scalable selective isolation, and (3) an RDR-guided (Reuse Diversity Ratio) runtime safeguard that detects and bounds residual leakage. On large LLM backends, SafeKV reduces the time-to-first-token (TTFT) overhead compared to full isolation by up to 40.58% and raises throughput by up to 2.66x. Overall, SafeKV restores the efficiency of KV reuse while enforcing strong, practical privacy for multi-tenant LLM inference."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-11T19:55:44Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    19,
                    55,
                    44,
                    0,
                    223,
                    0
                ],
                "arxiv_comment": "14 pages,15 figures",
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Kexin Chu"
                    },
                    {
                        "name": "Zecheng Lin"
                    },
                    {
                        "name": "Dawei Xiang"
                    },
                    {
                        "name": "Zixu Shen"
                    },
                    {
                        "name": "Jianchang Su"
                    },
                    {
                        "name": "Cheng Chu"
                    },
                    {
                        "name": "Yiwei Yang"
                    },
                    {
                        "name": "Wenhui Zhang"
                    },
                    {
                        "name": "Wenfei Wu"
                    },
                    {
                        "name": "Wei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhang"
                },
                "author": "Wei Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2602.09205v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.09205v1",
                "title": "Development of a Reduced Multi-Fluid Equilibrium Model and Its Application to Proton-Boron Spherical Tokamaks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development of a Reduced Multi-Fluid Equilibrium Model and Its Application to Proton-Boron Spherical Tokamaks"
                },
                "updated": "2026-02-09T21:16:59Z",
                "updated_parsed": [
                    2026,
                    2,
                    9,
                    21,
                    16,
                    59,
                    0,
                    40,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.09205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.09205v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Proton-Boron fusion requires extreme ion temperatures and robust confinement, making Spherical Tokamaks (ST) with high-power neutral beam injection primary candidates. In these devices, strong toroidal rotation and the large mass disparity between protons and boron ions drive complex multi-fluid effects - specifically centrifugal species separation and electrostatic polarization - that standard single-fluid magnetohydrodynamic (MHD) models fail to capture. While comprehensive multi-fluid models are often numerically stiff, we develop a reduced model balancing physical fidelity with computational robustness. By retaining dominant toroidal rotation and self-consistent potential while neglecting poloidal inertia and pressure anisotropy, the model couples a generalized Grad-Shafranov equation with species-specific Bernoulli relations and a quasi-neutrality constraint. The model is applied to two representative p-B ST configurations: the experimental EHL-2 and reactor-scale EHL-3B. Simulation results demonstrate that equilibrium modifications are governed by the ion Mach number ($M$). In the low-rotation regime ($M < 0.5$), multi-fluid effects are weak and solutions approach the single-fluid limit. However, at $M > 2$, strong centrifugal forces drive significant boron accumulation at the low-field side (LFS) and generate an internal electrostatic potential on the order of 10 kV. These findings confirm the necessity of multi-fluid modeling for accurate p-$^{11}$B reactor design and establish a theoretical foundation for future investigations into stability, transport, and free-boundary dynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proton-Boron fusion requires extreme ion temperatures and robust confinement, making Spherical Tokamaks (ST) with high-power neutral beam injection primary candidates. In these devices, strong toroidal rotation and the large mass disparity between protons and boron ions drive complex multi-fluid effects - specifically centrifugal species separation and electrostatic polarization - that standard single-fluid magnetohydrodynamic (MHD) models fail to capture. While comprehensive multi-fluid models are often numerically stiff, we develop a reduced model balancing physical fidelity with computational robustness. By retaining dominant toroidal rotation and self-consistent potential while neglecting poloidal inertia and pressure anisotropy, the model couples a generalized Grad-Shafranov equation with species-specific Bernoulli relations and a quasi-neutrality constraint. The model is applied to two representative p-B ST configurations: the experimental EHL-2 and reactor-scale EHL-3B. Simulation results demonstrate that equilibrium modifications are governed by the ion Mach number ($M$). In the low-rotation regime ($M < 0.5$), multi-fluid effects are weak and solutions approach the single-fluid limit. However, at $M > 2$, strong centrifugal forces drive significant boron accumulation at the low-field side (LFS) and generate an internal electrostatic potential on the order of 10 kV. These findings confirm the necessity of multi-fluid modeling for accurate p-$^{11}$B reactor design and establish a theoretical foundation for future investigations into stability, transport, and free-boundary dynamics."
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-09T21:16:59Z",
                "published_parsed": [
                    2026,
                    2,
                    9,
                    21,
                    16,
                    59,
                    0,
                    40,
                    0
                ],
                "arxiv_comment": "11 pages, 7 figures",
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph"
                },
                "authors": [
                    {
                        "name": "Huasheng Xie"
                    },
                    {
                        "name": "Xingyu Li"
                    },
                    {
                        "name": "Jiaqi Dong"
                    },
                    {
                        "name": "Zhiwei Ma"
                    },
                    {
                        "name": "Yunfeng Liang"
                    },
                    {
                        "name": "Yuejiang Shi"
                    },
                    {
                        "name": "Wenjun Liu"
                    },
                    {
                        "name": "Yueng-Kay Martin Peng"
                    },
                    {
                        "name": "Lai Wei"
                    },
                    {
                        "name": "Zhengxiong Wang"
                    },
                    {
                        "name": "Hanyue Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hanyue Zhao"
                },
                "author": "Hanyue Zhao"
            },
            {
                "id": "http://arxiv.org/abs/2602.08841v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.08841v1",
                "title": "Flash annealing-engineered wafer-scale relaxor antiferroelectrics for enhanced energy storage performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flash annealing-engineered wafer-scale relaxor antiferroelectrics for enhanced energy storage performance"
                },
                "updated": "2026-02-09T16:09:04Z",
                "updated_parsed": [
                    2026,
                    2,
                    9,
                    16,
                    9,
                    4,
                    0,
                    40,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.08841v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.08841v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1126/sciadv.ady2349",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Dielectric capacitors are essential for energy storage systems due to their high-power density and fast operation speed. However, optimizing energy storage density with concurrent thermal stability remains a substantial challenge. Here, we develop a flash annealing process with ultrafast heating and cooling rates of 1000 oC/s, which facilitates the rapid crystallization of PbZrO3 film within a mere second, while locking its high-temperature microstructure to room temperature. This produces compact films with sub-grain boundaries fraction of 36%, nanodomains of several nanometers, and negligible lead volatilization. These contribute to relaxor antiferroelectric film with a high breakdown strength (4800 kV/cm) and large polarization (70 uC/cm2). Consequently, we have achieved a high energy storage density of 63.5 J/cm3 and outstanding thermal stability with performance degradation less than 3% up to 250 oC. Our approach is extendable to ferroelectrics like Pb(Zr0.52Ti0.48)O3 and on wafer scale, providing on-chip nonlinear dielectric energy storage solutions with industrial scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dielectric capacitors are essential for energy storage systems due to their high-power density and fast operation speed. However, optimizing energy storage density with concurrent thermal stability remains a substantial challenge. Here, we develop a flash annealing process with ultrafast heating and cooling rates of 1000 oC/s, which facilitates the rapid crystallization of PbZrO3 film within a mere second, while locking its high-temperature microstructure to room temperature. This produces compact films with sub-grain boundaries fraction of 36%, nanodomains of several nanometers, and negligible lead volatilization. These contribute to relaxor antiferroelectric film with a high breakdown strength (4800 kV/cm) and large polarization (70 uC/cm2). Consequently, we have achieved a high energy storage density of 63.5 J/cm3 and outstanding thermal stability with performance degradation less than 3% up to 250 oC. Our approach is extendable to ferroelectrics like Pb(Zr0.52Ti0.48)O3 and on wafer scale, providing on-chip nonlinear dielectric energy storage solutions with industrial scalability."
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-09T16:09:04Z",
                "published_parsed": [
                    2026,
                    2,
                    9,
                    16,
                    9,
                    4,
                    0,
                    40,
                    0
                ],
                "arxiv_comment": "49 pages, 29 figures, 3 tables",
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci"
                },
                "arxiv_journal_ref": "Science Advances, 11, eady2349, 2025",
                "authors": [
                    {
                        "name": "Yizhuo Li"
                    },
                    {
                        "name": "Kepeng Song"
                    },
                    {
                        "name": "Meixiong Zhu"
                    },
                    {
                        "name": "Xiaoqi Li"
                    },
                    {
                        "name": "Zhaowei Zeng"
                    },
                    {
                        "name": "KangMing Luo"
                    },
                    {
                        "name": "Yuxuan Jiang"
                    },
                    {
                        "name": "Zhe Zhang"
                    },
                    {
                        "name": "Cuihong Li"
                    },
                    {
                        "name": "Yujia Wang"
                    },
                    {
                        "name": "Bing Li"
                    },
                    {
                        "name": "Zhihong Wang"
                    },
                    {
                        "name": "Zhidong Zhang"
                    },
                    {
                        "name": "Weijin Hu"
                    }
                ],
                "author_detail": {
                    "name": "Weijin Hu"
                },
                "author": "Weijin Hu",
                "arxiv_doi": "10.1126/sciadv.ady2349"
            },
            {
                "id": "http://arxiv.org/abs/2602.08722v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.08722v1",
                "title": "QUOKA: Query-Oriented KV Selection For Efficient LLM Prefill",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QUOKA: Query-Oriented KV Selection For Efficient LLM Prefill"
                },
                "updated": "2026-02-09T14:32:26Z",
                "updated_parsed": [
                    2026,
                    2,
                    9,
                    14,
                    32,
                    26,
                    0,
                    40,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.08722v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.08722v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present QUOKA: Query-oriented KV selection for efficient attention, a training-free and hardware agnostic sparse attention algorithm for accelerating transformer inference under chunked prefill. While many queries focus on a smaller group of keys in the attention operator, we observe that queries with low cosine similarity with respect to the mean query interact more strongly with more keys and have the greatest contribution to final attention logits. By prioritizing these low cosine similarity queries, the behavior of full attention during the prefill stage can be closely approximated. QUOKA leverages this observation, accelerating attention by (1) first retaining a small set of representative queries and (2) then subselectin the keys most aligned with those queries. Through experiments on Needle-In-A-Haystack, LongBench, RULER, and Math500, we show that, while realizing a 3x reduction in time-to-first-token, 5x speedup in attention on Nvidia GPUs and up to nearly a 7x speedup on Intel Xeon CPUs, QUOKA achieves near-baseline accuracy, utilizing 88% fewer key-value pairs per attention evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present QUOKA: Query-oriented KV selection for efficient attention, a training-free and hardware agnostic sparse attention algorithm for accelerating transformer inference under chunked prefill. While many queries focus on a smaller group of keys in the attention operator, we observe that queries with low cosine similarity with respect to the mean query interact more strongly with more keys and have the greatest contribution to final attention logits. By prioritizing these low cosine similarity queries, the behavior of full attention during the prefill stage can be closely approximated. QUOKA leverages this observation, accelerating attention by (1) first retaining a small set of representative queries and (2) then subselectin the keys most aligned with those queries. Through experiments on Needle-In-A-Haystack, LongBench, RULER, and Math500, we show that, while realizing a 3x reduction in time-to-first-token, 5x speedup in attention on Nvidia GPUs and up to nearly a 7x speedup on Intel Xeon CPUs, QUOKA achieves near-baseline accuracy, utilizing 88% fewer key-value pairs per attention evaluation."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-09T14:32:26Z",
                "published_parsed": [
                    2026,
                    2,
                    9,
                    14,
                    32,
                    26,
                    0,
                    40,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Matthew Morse"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    },
                    {
                        "name": "Harper Langston"
                    }
                ],
                "author_detail": {
                    "name": "Harper Langston"
                },
                "author": "Harper Langston"
            },
            {
                "id": "http://arxiv.org/abs/2602.08686v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.08686v1",
                "title": "CompilerKV: Risk-Adaptive KV Compression via Offline Experience Compilation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CompilerKV: Risk-Adaptive KV Compression via Offline Experience Compilation"
                },
                "updated": "2026-02-09T14:07:55Z",
                "updated_parsed": [
                    2026,
                    2,
                    9,
                    14,
                    7,
                    55,
                    0,
                    40,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.08686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.08686v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) in long-context scenarios are severely constrained by the linear growth of Key-Value (KV) cache memory. Existing KV compression methods rely either on static thresholds and attention-only heuristics or on coarse memory budget allocation. Under tight memory budgets, these methods overlook two key factors: prompt-dependent variation in compression risk and functional heterogeneity across attention heads, which destabilize token selection and lead to tail failures. To address these challenges, we propose CompilerKV, a risk-adaptive and head-aware compression framework that compiles offline experience into reusable decision tables for prefill-only deployment. CompilerKV integrates two key synergistic components: (i) a Head Heterogeneity Table, learned via offline contextual bandits, which assigns head-specific reliability weights to govern functional differences across attention heads explicitly; and (ii) a Risk-Adaptive Threshold Gating mechanism that jointly models attention entropy and local perplexity, transforming prompt-level risk into deployable retention thresholds. Experiments on LongBench show CompilerKV dominates SOTA methods under a 512-token budget, recovering 97.7\\% of FullKV performance while achieving up to +5.2 points gain over the strongest competitor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) in long-context scenarios are severely constrained by the linear growth of Key-Value (KV) cache memory. Existing KV compression methods rely either on static thresholds and attention-only heuristics or on coarse memory budget allocation. Under tight memory budgets, these methods overlook two key factors: prompt-dependent variation in compression risk and functional heterogeneity across attention heads, which destabilize token selection and lead to tail failures. To address these challenges, we propose CompilerKV, a risk-adaptive and head-aware compression framework that compiles offline experience into reusable decision tables for prefill-only deployment. CompilerKV integrates two key synergistic components: (i) a Head Heterogeneity Table, learned via offline contextual bandits, which assigns head-specific reliability weights to govern functional differences across attention heads explicitly; and (ii) a Risk-Adaptive Threshold Gating mechanism that jointly models attention entropy and local perplexity, transforming prompt-level risk into deployable retention thresholds. Experiments on LongBench show CompilerKV dominates SOTA methods under a 512-token budget, recovering 97.7\\% of FullKV performance while achieving up to +5.2 points gain over the strongest competitor."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-09T14:07:55Z",
                "published_parsed": [
                    2026,
                    2,
                    9,
                    14,
                    7,
                    55,
                    0,
                    40,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Ning Yang"
                    },
                    {
                        "name": "Chengzhi Wang"
                    },
                    {
                        "name": "Yibo Liu"
                    },
                    {
                        "name": "Baoliang Tian"
                    },
                    {
                        "name": "Haijun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Haijun Zhang"
                },
                "author": "Haijun Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2602.08585v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.08585v1",
                "title": "Predicting Future Utility: Global Combinatorial Optimization for Task-Agnostic KV Cache Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting Future Utility: Global Combinatorial Optimization for Task-Agnostic KV Cache Eviction"
                },
                "updated": "2026-02-09T12:23:38Z",
                "updated_parsed": [
                    2026,
                    2,
                    9,
                    12,
                    23,
                    38,
                    0,
                    40,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.08585v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.08585v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Given the quadratic complexity of attention, KV cache eviction is vital to accelerate model inference. Current KV cache eviction methods typically rely on instantaneous heuristic metrics, implicitly assuming that score magnitudes are consistent proxies for importance across all heads. However, this overlooks the heterogeneity in predictive fidelity across attention heads. While certain heads prioritize the instantaneous contribution of tokens, others are dedicated to capturing long-horizon utility. In this paper, we propose that optimal budget allocation should be governed by the marginal utility in preserving long-term semantic information. Based on this insight, we propose LU-KV, a novel framework that optimizes head-level budget allocation through a convex-hull relaxation and a marginal-utility-based greedy solver to achieve near-optimal precision. Furthermore, we implement a data-driven offline profiling protocol to facilitate the practical deployment of LU-KV. Extensive evaluations on LongBench and RULER benchmarks demonstrate that LU-KV achieves an 80% reduction in KV cache size with minimal performance degradation, while simultaneously reducing inference latency and GPU memory footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given the quadratic complexity of attention, KV cache eviction is vital to accelerate model inference. Current KV cache eviction methods typically rely on instantaneous heuristic metrics, implicitly assuming that score magnitudes are consistent proxies for importance across all heads. However, this overlooks the heterogeneity in predictive fidelity across attention heads. While certain heads prioritize the instantaneous contribution of tokens, others are dedicated to capturing long-horizon utility. In this paper, we propose that optimal budget allocation should be governed by the marginal utility in preserving long-term semantic information. Based on this insight, we propose LU-KV, a novel framework that optimizes head-level budget allocation through a convex-hull relaxation and a marginal-utility-based greedy solver to achieve near-optimal precision. Furthermore, we implement a data-driven offline profiling protocol to facilitate the practical deployment of LU-KV. Extensive evaluations on LongBench and RULER benchmarks demonstrate that LU-KV achieves an 80% reduction in KV cache size with minimal performance degradation, while simultaneously reducing inference latency and GPU memory footprint."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-09T12:23:38Z",
                "published_parsed": [
                    2026,
                    2,
                    9,
                    12,
                    23,
                    38,
                    0,
                    40,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Ziyao Tang"
                    },
                    {
                        "name": "Pengkun Jiao"
                    },
                    {
                        "name": "Xinhang Chen"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Shiyong Li"
                    },
                    {
                        "name": "Jingjing Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jingjing Chen"
                },
                "author": "Jingjing Chen"
            },
            {
                "id": "http://arxiv.org/abs/2602.08343v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.08343v1",
                "title": "ManifoldKV: Training-Free KV Cache Compression via Euclidean Outlier Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ManifoldKV: Training-Free KV Cache Compression via Euclidean Outlier Detection"
                },
                "updated": "2026-02-09T07:28:55Z",
                "updated_parsed": [
                    2026,
                    2,
                    9,
                    7,
                    28,
                    55,
                    0,
                    40,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.08343v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.08343v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Long-context inference is constrained by KV-cache memory, which grows linearly with sequence length; KV-cache compression therefore hinges on reliably selecting which past tokens to retain. Most geometry-based eviction methods score keys by cosine similarity to a global centroid, but cosine is scale-invariant and can discard magnitude cues that distinguish semantically salient tokens. We propose ManifoldKV, a training-free scorer that ranks tokens by Euclidean distance to the key centroid, capturing both angular and radial deviations.\n  On the RULER benchmark, ManifoldKV achieves 95.7% accuracy at 4K-16K contexts with 20% compression; matching the best geometric baseline while improving robustness in two regimes where cosine scoring fails. First, on multi-key retrieval, ManifoldKV reduces directional collisions, achieving 92.4% vs KeyDiff's 77.0% (+15.4 points) on 3-key NIAH at 50% compression. Second, to address dilution and performance collapse of global centroids at 64K context, we introduce WindowedManifoldKV, which restores accuracy to 84.3% at 25% compression, a 49-point recovery over global L2 and +3.2 points over KeyDiff. The method requires only 3 lines of code and works across 4 architectures without tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context inference is constrained by KV-cache memory, which grows linearly with sequence length; KV-cache compression therefore hinges on reliably selecting which past tokens to retain. Most geometry-based eviction methods score keys by cosine similarity to a global centroid, but cosine is scale-invariant and can discard magnitude cues that distinguish semantically salient tokens. We propose ManifoldKV, a training-free scorer that ranks tokens by Euclidean distance to the key centroid, capturing both angular and radial deviations.\n  On the RULER benchmark, ManifoldKV achieves 95.7% accuracy at 4K-16K contexts with 20% compression; matching the best geometric baseline while improving robustness in two regimes where cosine scoring fails. First, on multi-key retrieval, ManifoldKV reduces directional collisions, achieving 92.4% vs KeyDiff's 77.0% (+15.4 points) on 3-key NIAH at 50% compression. Second, to address dilution and performance collapse of global centroids at 64K context, we introduce WindowedManifoldKV, which restores accuracy to 84.3% at 25% compression, a 49-point recovery over global L2 and +3.2 points over KeyDiff. The method requires only 3 lines of code and works across 4 architectures without tuning."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-09T07:28:55Z",
                "published_parsed": [
                    2026,
                    2,
                    9,
                    7,
                    28,
                    55,
                    0,
                    40,
                    0
                ],
                "arxiv_comment": "18 pages, 5 figures, 18 tables",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Debajyoti Datta"
                    },
                    {
                        "name": "Trishala Neeraj"
                    },
                    {
                        "name": "Bibek Paudel"
                    },
                    {
                        "name": "Vyom Sharma"
                    },
                    {
                        "name": "Subhabrata Mukherjee"
                    }
                ],
                "author_detail": {
                    "name": "Subhabrata Mukherjee"
                },
                "author": "Subhabrata Mukherjee"
            },
            {
                "id": "http://arxiv.org/abs/2602.08334v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.08334v1",
                "title": "Vec-QMDP: Vectorized POMDP Planning on CPUs for Real-Time Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vec-QMDP: Vectorized POMDP Planning on CPUs for Real-Time Autonomous Driving"
                },
                "updated": "2026-02-09T07:15:19Z",
                "updated_parsed": [
                    2026,
                    2,
                    9,
                    7,
                    15,
                    19,
                    0,
                    40,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.08334v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.08334v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Planning under uncertainty for real-world robotics tasks, such as autonomous driving, requires reasoning in enormous high-dimensional belief spaces, rendering the problem computationally intensive. While parallelization offers scalability, existing hybrid CPU-GPU solvers face critical bottlenecks due to host-device synchronization latency and branch divergence on SIMT architectures, limiting their utility for real-time planning and hindering real-robot deployment. We present Vec-QMDP, a CPU-native parallel planner that aligns POMDP search with modern CPUs' SIMD architecture, achieving $227\\times$--$1073\\times$ speedup over state-of-the-art serial planners. Vec-QMDP adopts a Data-Oriented Design (DOD), refactoring scattered, pointer-based data structures into contiguous, cache-efficient memory layouts. We further introduce a hierarchical parallelism scheme: distributing sub-trees across independent CPU cores and SIMD lanes, enabling fully vectorized tree expansion and collision checking. Efficiency is maximized with the help of UCB load balancing across trees and a vectorized STR-tree for coarse-level collision checking. Evaluated on large-scale autonomous driving benchmarks, Vec-QMDP achieves state-of-the-art planning performance with millisecond-level latency, establishing CPUs as a high-performance computing platform for large-scale planning under uncertainty.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Planning under uncertainty for real-world robotics tasks, such as autonomous driving, requires reasoning in enormous high-dimensional belief spaces, rendering the problem computationally intensive. While parallelization offers scalability, existing hybrid CPU-GPU solvers face critical bottlenecks due to host-device synchronization latency and branch divergence on SIMT architectures, limiting their utility for real-time planning and hindering real-robot deployment. We present Vec-QMDP, a CPU-native parallel planner that aligns POMDP search with modern CPUs' SIMD architecture, achieving $227\\times$--$1073\\times$ speedup over state-of-the-art serial planners. Vec-QMDP adopts a Data-Oriented Design (DOD), refactoring scattered, pointer-based data structures into contiguous, cache-efficient memory layouts. We further introduce a hierarchical parallelism scheme: distributing sub-trees across independent CPU cores and SIMD lanes, enabling fully vectorized tree expansion and collision checking. Efficiency is maximized with the help of UCB load balancing across trees and a vectorized STR-tree for coarse-level collision checking. Evaluated on large-scale autonomous driving benchmarks, Vec-QMDP achieves state-of-the-art planning performance with millisecond-level latency, establishing CPUs as a high-performance computing platform for large-scale planning under uncertainty."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-09T07:15:19Z",
                "published_parsed": [
                    2026,
                    2,
                    9,
                    7,
                    15,
                    19,
                    0,
                    40,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Xuanjin Jin"
                    },
                    {
                        "name": "Yanxin Dong"
                    },
                    {
                        "name": "Bin Sun"
                    },
                    {
                        "name": "Huan Xu"
                    },
                    {
                        "name": "Zhihui Hao"
                    },
                    {
                        "name": "XianPeng Lang"
                    },
                    {
                        "name": "Panpan Cai"
                    }
                ],
                "author_detail": {
                    "name": "Panpan Cai"
                },
                "author": "Panpan Cai"
            },
            {
                "id": "http://arxiv.org/abs/2602.08329v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.08329v1",
                "title": "Near-Oracle KV Selection via Pre-hoc Sparsity for Long-Context Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Near-Oracle KV Selection via Pre-hoc Sparsity for Long-Context Inference"
                },
                "updated": "2026-02-09T07:05:23Z",
                "updated_parsed": [
                    2026,
                    2,
                    9,
                    7,
                    5,
                    23,
                    0,
                    40,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.08329v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.08329v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "A core bottleneck in large language model (LLM) inference is the cost of attending over the ever-growing key-value (KV) cache. Although near-oracle top-k KV selection can preserve the quality of dense attention while sharply reducing computation and bandwidth, existing sparse methods generally rely on posterior heuristics, i.e., selectors conditioned on observed attention or proxy scores. Such conditioning introduces posterior bias: it tends to distort true token importance and miss salient tokens, thereby impairing long-range reasoning. To tackle this problem, we propose Pre-hoc Sparsity (PrHS), which selects KV entries before attention scoring and provides explicit accuracy control. Let the attention mass of discarded entries be delta (the dropped mass). Through a marginal-to-mutual-information analysis, we derive an upper bound on the mutual-information loss that depends only on the dropped mass. This relation explains failure modes of posterior heuristics and enables verifiable guarantees by controlling the dropped mass in advance. Within PrHS, we instantiate three orthogonal pre-hoc selectors along the axes of time, depth, and layer. Extensive experiments on LLaMA and Mistral families validate PrHS. Across GSM8K and CoQA, PrHS reduces retrieval overhead by over 90%, achieving 3x higher retrieval sparsity than HShare at matched or better accuracy. It incurs under 1% average degradation on LongBench, lowers attention FLOPs by about 15% versus prior sparse baselines, and yields a 9.9x speedup in attention-operator latency and 2.8x higher throughput on NVIDIA A100-80GB GPUs than the dense baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A core bottleneck in large language model (LLM) inference is the cost of attending over the ever-growing key-value (KV) cache. Although near-oracle top-k KV selection can preserve the quality of dense attention while sharply reducing computation and bandwidth, existing sparse methods generally rely on posterior heuristics, i.e., selectors conditioned on observed attention or proxy scores. Such conditioning introduces posterior bias: it tends to distort true token importance and miss salient tokens, thereby impairing long-range reasoning. To tackle this problem, we propose Pre-hoc Sparsity (PrHS), which selects KV entries before attention scoring and provides explicit accuracy control. Let the attention mass of discarded entries be delta (the dropped mass). Through a marginal-to-mutual-information analysis, we derive an upper bound on the mutual-information loss that depends only on the dropped mass. This relation explains failure modes of posterior heuristics and enables verifiable guarantees by controlling the dropped mass in advance. Within PrHS, we instantiate three orthogonal pre-hoc selectors along the axes of time, depth, and layer. Extensive experiments on LLaMA and Mistral families validate PrHS. Across GSM8K and CoQA, PrHS reduces retrieval overhead by over 90%, achieving 3x higher retrieval sparsity than HShare at matched or better accuracy. It incurs under 1% average degradation on LongBench, lowers attention FLOPs by about 15% versus prior sparse baselines, and yields a 9.9x speedup in attention-operator latency and 2.8x higher throughput on NVIDIA A100-80GB GPUs than the dense baseline."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-09T07:05:23Z",
                "published_parsed": [
                    2026,
                    2,
                    9,
                    7,
                    5,
                    23,
                    0,
                    40,
                    0
                ],
                "arxiv_comment": "An effective method for accelerating LLM's inference via selective KV processing",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Yifei Gao"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Rong-Cheng Tu"
                    },
                    {
                        "name": "Qixin Zhang"
                    },
                    {
                        "name": "Jun Cheng"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao"
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2602.13194v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.13194v2",
                "title": "Semantic Chunking and the Entropy of Natural Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Chunking and the Entropy of Natural Language"
                },
                "updated": "2026-02-18T18:59:22Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    18,
                    59,
                    22,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.13194v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.13194v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The entropy rate of printed English is famously estimated to be about one bit per character, a benchmark that modern large language models (LLMs) have only recently approached. This entropy rate implies that English contains nearly 80 percent redundancy relative to the five bits per character expected for random text. We introduce a statistical model that attempts to capture the intricate multi-scale structure of natural language, providing a first-principles account of this redundancy level. Our model describes a procedure of self-similarly segmenting text into semantically coherent chunks down to the single-word level. The semantic structure of the text can then be hierarchically decomposed, allowing for analytical treatment. Numerical experiments with modern LLMs and open datasets suggest that our model quantitatively captures the structure of real texts at different levels of the semantic hierarchy. The entropy rate predicted by our model agrees with the estimated entropy rate of printed English. Moreover, our theory further reveals that the entropy rate of natural language is not fixed but should increase systematically with the semantic complexity of corpora, which are captured by the only free parameter in our model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The entropy rate of printed English is famously estimated to be about one bit per character, a benchmark that modern large language models (LLMs) have only recently approached. This entropy rate implies that English contains nearly 80 percent redundancy relative to the five bits per character expected for random text. We introduce a statistical model that attempts to capture the intricate multi-scale structure of natural language, providing a first-principles account of this redundancy level. Our model describes a procedure of self-similarly segmenting text into semantically coherent chunks down to the single-word level. The semantic structure of the text can then be hierarchically decomposed, allowing for analytical treatment. Numerical experiments with modern LLMs and open datasets suggest that our model quantitatively captures the structure of real texts at different levels of the semantic hierarchy. The entropy rate predicted by our model agrees with the estimated entropy rate of printed English. Moreover, our theory further reveals that the entropy rate of natural language is not fixed but should increase systematically with the semantic complexity of corpora, which are captured by the only free parameter in our model."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-13T18:58:10Z",
                "published_parsed": [
                    2026,
                    2,
                    13,
                    18,
                    58,
                    10,
                    4,
                    44,
                    0
                ],
                "arxiv_comment": "29 pages, 9 figures; typos fixed",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Weishun Zhong"
                    },
                    {
                        "name": "Doron Sivan"
                    },
                    {
                        "name": "Tankut Can"
                    },
                    {
                        "name": "Mikhail Katkov"
                    },
                    {
                        "name": "Misha Tsodyks"
                    }
                ],
                "author_detail": {
                    "name": "Misha Tsodyks"
                },
                "author": "Misha Tsodyks"
            },
            {
                "id": "http://arxiv.org/abs/2602.16708v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16708v1",
                "title": "Policy Compiler for Secure Agentic Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Policy Compiler for Secure Agentic Systems"
                },
                "updated": "2026-02-18T18:57:12Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    18,
                    57,
                    12,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16708v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16708v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "LLM-based agents are increasingly being deployed in contexts requiring complex authorization policies: customer service protocols, approval workflows, data access restrictions, and regulatory compliance. Embedding these policies in prompts provides no enforcement guarantees. We present PCAS, a Policy Compiler for Agentic Systems that provides deterministic policy enforcement.\n  Enforcing such policies requires tracking information flow across agents, which linear message histories cannot capture. Instead, PCAS models the agentic system state as a dependency graph capturing causal relationships among events such as tool calls, tool results, and messages. Policies are expressed in a Datalog-derived language, as declarative rules that account for transitive information flow and cross-agent provenance. A reference monitor intercepts all actions and blocks violations before execution, providing deterministic enforcement independent of model reasoning.\n  PCAS takes an existing agent implementation and a policy specification, and compiles them into an instrumented system that is policy-compliant by construction, with no security-specific restructuring required. We evaluate PCAS on three case studies: information flow policies for prompt injection defense, approval workflows in a multi-agent pharmacovigilance system, and organizational policies for customer service. On customer service tasks, PCAS improves policy compliance from 48% to 93% across frontier models, with zero policy violations in instrumented runs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based agents are increasingly being deployed in contexts requiring complex authorization policies: customer service protocols, approval workflows, data access restrictions, and regulatory compliance. Embedding these policies in prompts provides no enforcement guarantees. We present PCAS, a Policy Compiler for Agentic Systems that provides deterministic policy enforcement.\n  Enforcing such policies requires tracking information flow across agents, which linear message histories cannot capture. Instead, PCAS models the agentic system state as a dependency graph capturing causal relationships among events such as tool calls, tool results, and messages. Policies are expressed in a Datalog-derived language, as declarative rules that account for transitive information flow and cross-agent provenance. A reference monitor intercepts all actions and blocks violations before execution, providing deterministic enforcement independent of model reasoning.\n  PCAS takes an existing agent implementation and a policy specification, and compiles them into an instrumented system that is policy-compliant by construction, with no security-specific restructuring required. We evaluate PCAS on three case studies: information flow policies for prompt injection defense, approval workflows in a multi-agent pharmacovigilance system, and organizational policies for customer service. On customer service tasks, PCAS improves policy compliance from 48% to 93% across frontier models, with zero policy violations in instrumented runs."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T18:57:12Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    18,
                    57,
                    12,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Nils Palumbo"
                    },
                    {
                        "name": "Sarthak Choudhary"
                    },
                    {
                        "name": "Jihye Choi"
                    },
                    {
                        "name": "Prasad Chalasani"
                    },
                    {
                        "name": "Mihai Christodorescu"
                    },
                    {
                        "name": "Somesh Jha"
                    }
                ],
                "author_detail": {
                    "name": "Somesh Jha"
                },
                "author": "Somesh Jha"
            },
            {
                "id": "http://arxiv.org/abs/2602.16703v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16703v1",
                "title": "Measuring Mid-2025 LLM-Assistance on Novice Performance in Biology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring Mid-2025 LLM-Assistance on Novice Performance in Biology"
                },
                "updated": "2026-02-18T18:51:28Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    18,
                    51,
                    28,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16703v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16703v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) perform strongly on biological benchmarks, raising concerns that they may help novice actors acquire dual-use laboratory skills. Yet, whether this translates to improved human performance in the physical laboratory remains unclear. To address this, we conducted a pre-registered, investigator-blinded, randomized controlled trial (June-August 2025; n = 153) evaluating whether LLMs improve novice performance in tasks that collectively model a viral reverse genetics workflow. We observed no significant difference in the primary endpoint of workflow completion (5.2% LLM vs. 6.6% Internet; P = 0.759), nor in the success rate of individual tasks. However, the LLM arm had numerically higher success rates in four of the five tasks, most notably for the cell culture task (68.8% LLM vs. 55.3% Internet; P = 0.059). Post-hoc Bayesian modeling of pooled data estimates an approximate 1.4-fold increase (95% CrI 0.74-2.62) in success for a \"typical\" reverse genetics task under LLM assistance. Ordinal regression modelling suggests that participants in the LLM arm were more likely to progress through intermediate steps across all tasks (posterior probability of a positive effect: 81%-96%). Overall, mid-2025 LLMs did not substantially increase novice completion of complex laboratory procedures but were associated with a modest performance benefit. These results reveal a gap between in silico benchmarks and real-world utility, underscoring the need for physical-world validation of AI biosecurity assessments as model capabilities and user proficiency evolve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) perform strongly on biological benchmarks, raising concerns that they may help novice actors acquire dual-use laboratory skills. Yet, whether this translates to improved human performance in the physical laboratory remains unclear. To address this, we conducted a pre-registered, investigator-blinded, randomized controlled trial (June-August 2025; n = 153) evaluating whether LLMs improve novice performance in tasks that collectively model a viral reverse genetics workflow. We observed no significant difference in the primary endpoint of workflow completion (5.2% LLM vs. 6.6% Internet; P = 0.759), nor in the success rate of individual tasks. However, the LLM arm had numerically higher success rates in four of the five tasks, most notably for the cell culture task (68.8% LLM vs. 55.3% Internet; P = 0.059). Post-hoc Bayesian modeling of pooled data estimates an approximate 1.4-fold increase (95% CrI 0.74-2.62) in success for a \"typical\" reverse genetics task under LLM assistance. Ordinal regression modelling suggests that participants in the LLM arm were more likely to progress through intermediate steps across all tasks (posterior probability of a positive effect: 81%-96%). Overall, mid-2025 LLMs did not substantially increase novice completion of complex laboratory procedures but were associated with a modest performance benefit. These results reveal a gap between in silico benchmarks and real-world utility, underscoring the need for physical-world validation of AI biosecurity assessments as model capabilities and user proficiency evolve."
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T18:51:28Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    18,
                    51,
                    28,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY"
                },
                "authors": [
                    {
                        "name": "Shen Zhou Hong"
                    },
                    {
                        "name": "Alex Kleinman"
                    },
                    {
                        "name": "Alyssa Mathiowetz"
                    },
                    {
                        "name": "Adam Howes"
                    },
                    {
                        "name": "Julian Cohen"
                    },
                    {
                        "name": "Suveer Ganta"
                    },
                    {
                        "name": "Alex Letizia"
                    },
                    {
                        "name": "Dora Liao"
                    },
                    {
                        "name": "Deepika Pahari"
                    },
                    {
                        "name": "Xavier Roberts-Gaal"
                    },
                    {
                        "name": "Luca Righetti"
                    },
                    {
                        "name": "Joe Torres"
                    }
                ],
                "author_detail": {
                    "name": "Joe Torres"
                },
                "author": "Joe Torres"
            },
            {
                "id": "http://arxiv.org/abs/2402.18060v6",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2402.18060v6",
                "title": "Benchmarking Large Language Models on Answering and Explaining Challenging Medical Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Large Language Models on Answering and Explaining Challenging Medical Questions"
                },
                "updated": "2026-02-18T18:50:32Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    18,
                    50,
                    32,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2402.18060v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2402.18060v6",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "LLMs have demonstrated impressive performance in answering medical questions, such as achieving passing scores on medical licensing examinations. However, medical board exams or general clinical questions do not capture the complexity of realistic clinical cases. Moreover, the lack of reference explanations means we cannot easily evaluate the reasoning of model decisions, a crucial component of supporting doctors in making complex medical decisions. To address these challenges, we construct two new datasets: JAMA Clinical Challenge and Medbullets. Datasets and code are available at https://github.com/HanjieChen/ChallengeClinicalQA. JAMA Clinical Challenge consists of questions based on challenging clinical cases, while Medbullets comprises simulated clinical questions. Both datasets are structured as multiple-choice question-answering tasks, accompanied by expert-written explanations. We evaluate seven LLMs on the two datasets using various prompts. Experiments demonstrate that our datasets are harder than previous benchmarks. In-depth automatic and human evaluations of model-generated explanations provide insights into the promise and deficiency of LLMs for explainable medical QA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs have demonstrated impressive performance in answering medical questions, such as achieving passing scores on medical licensing examinations. However, medical board exams or general clinical questions do not capture the complexity of realistic clinical cases. Moreover, the lack of reference explanations means we cannot easily evaluate the reasoning of model decisions, a crucial component of supporting doctors in making complex medical decisions. To address these challenges, we construct two new datasets: JAMA Clinical Challenge and Medbullets. Datasets and code are available at https://github.com/HanjieChen/ChallengeClinicalQA. JAMA Clinical Challenge consists of questions based on challenging clinical cases, while Medbullets comprises simulated clinical questions. Both datasets are structured as multiple-choice question-answering tasks, accompanied by expert-written explanations. We evaluate seven LLMs on the two datasets using various prompts. Experiments demonstrate that our datasets are harder than previous benchmarks. In-depth automatic and human evaluations of model-generated explanations provide insights into the promise and deficiency of LLMs for explainable medical QA."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-02-28T05:44:41Z",
                "published_parsed": [
                    2024,
                    2,
                    28,
                    5,
                    44,
                    41,
                    2,
                    59,
                    0
                ],
                "arxiv_comment": "NAACL 2025",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Hanjie Chen"
                    },
                    {
                        "name": "Zhouxiang Fang"
                    },
                    {
                        "name": "Yash Singla"
                    },
                    {
                        "name": "Mark Dredze"
                    }
                ],
                "author_detail": {
                    "name": "Mark Dredze"
                },
                "author": "Mark Dredze"
            },
            {
                "id": "http://arxiv.org/abs/2602.16702v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16702v1",
                "title": "Saliency-Aware Multi-Route Thinking: Revisiting Vision-Language Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Saliency-Aware Multi-Route Thinking: Revisiting Vision-Language Reasoning"
                },
                "updated": "2026-02-18T18:49:56Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    18,
                    49,
                    56,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16702v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16702v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Vision-language models (VLMs) aim to reason by jointly leveraging visual and textual modalities. While allocating additional inference-time computation has proven effective for large language models (LLMs), achieving similar scaling in VLMs remains challenging. A key obstacle is that visual inputs are typically provided only once at the start of generation, while textual reasoning (e.g., early visual summaries) is generated autoregressively, causing reasoning to become increasingly text-dominated and allowing early visual grounding errors to accumulate. Moreover, vanilla guidance for visual grounding during inference is often coarse and noisy, making it difficult to steer reasoning over long texts. To address these challenges, we propose \\emph{Saliency-Aware Principle} (SAP) selection. SAP operates on high-level reasoning principles rather than token-level trajectories, which enable stable control over discrete generation under noisy feedback while allowing later reasoning steps to re-consult visual evidence when renewed grounding is required. In addition, SAP supports multi-route inference, enabling parallel exploration of diverse reasoning behaviors. SAP is model-agnostic and data-free, requiring no additional training. Empirical results show that SAP achieves competitive performance, especially in reducing object hallucination, under comparable token-generation budgets while yielding more stable reasoning and lower response latency than CoT-style long sequential reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) aim to reason by jointly leveraging visual and textual modalities. While allocating additional inference-time computation has proven effective for large language models (LLMs), achieving similar scaling in VLMs remains challenging. A key obstacle is that visual inputs are typically provided only once at the start of generation, while textual reasoning (e.g., early visual summaries) is generated autoregressively, causing reasoning to become increasingly text-dominated and allowing early visual grounding errors to accumulate. Moreover, vanilla guidance for visual grounding during inference is often coarse and noisy, making it difficult to steer reasoning over long texts. To address these challenges, we propose \\emph{Saliency-Aware Principle} (SAP) selection. SAP operates on high-level reasoning principles rather than token-level trajectories, which enable stable control over discrete generation under noisy feedback while allowing later reasoning steps to re-consult visual evidence when renewed grounding is required. In addition, SAP supports multi-route inference, enabling parallel exploration of diverse reasoning behaviors. SAP is model-agnostic and data-free, requiring no additional training. Empirical results show that SAP achieves competitive performance, especially in reducing object hallucination, under comparable token-generation budgets while yielding more stable reasoning and lower response latency than CoT-style long sequential reasoning."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T18:49:56Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    18,
                    49,
                    56,
                    2,
                    49,
                    0
                ],
                "arxiv_comment": "preprint 10 pages, 4 figures",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Mingjia Shi"
                    },
                    {
                        "name": "Yinhan He"
                    },
                    {
                        "name": "Yaochen Zhu"
                    },
                    {
                        "name": "Jundong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jundong Li"
                },
                "author": "Jundong Li"
            },
            {
                "id": "http://arxiv.org/abs/2602.16699v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16699v1",
                "title": "Calibrate-Then-Act: Cost-Aware Exploration in LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Calibrate-Then-Act: Cost-Aware Exploration in LLM Agents"
                },
                "updated": "2026-02-18T18:46:14Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    18,
                    46,
                    14,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16699v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16699v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "LLMs are increasingly being used for complex problems which are not necessarily resolved in a single response, but require interacting with an environment to acquire information. In these scenarios, LLMs must reason about inherent cost-uncertainty tradeoffs in when to stop exploring and commit to an answer. For instance, on a programming task, an LLM should test a generated code snippet if it is uncertain about the correctness of that code; the cost of writing a test is nonzero, but typically lower than the cost of making a mistake. In this work, we show that we can induce LLMs to explicitly reason about balancing these cost-uncertainty tradeoffs, then perform more optimal environment exploration. We formalize multiple tasks, including information retrieval and coding, as sequential decision-making problems under uncertainty. Each problem has latent environment state that can be reasoned about via a prior which is passed to the LLM agent. We introduce a framework called Calibrate-Then-Act (CTA), where we feed the LLM this additional context to enable it to act more optimally. This improvement is preserved even under RL training of both the baseline and CTA. Our results on information-seeking QA and on a simplified coding task show that making cost-benefit tradeoffs explicit with CTA can help agents discover more optimal decision-making strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are increasingly being used for complex problems which are not necessarily resolved in a single response, but require interacting with an environment to acquire information. In these scenarios, LLMs must reason about inherent cost-uncertainty tradeoffs in when to stop exploring and commit to an answer. For instance, on a programming task, an LLM should test a generated code snippet if it is uncertain about the correctness of that code; the cost of writing a test is nonzero, but typically lower than the cost of making a mistake. In this work, we show that we can induce LLMs to explicitly reason about balancing these cost-uncertainty tradeoffs, then perform more optimal environment exploration. We formalize multiple tasks, including information retrieval and coding, as sequential decision-making problems under uncertainty. Each problem has latent environment state that can be reasoned about via a prior which is passed to the LLM agent. We introduce a framework called Calibrate-Then-Act (CTA), where we feed the LLM this additional context to enable it to act more optimally. This improvement is preserved even under RL training of both the baseline and CTA. Our results on information-seeking QA and on a simplified coding task show that making cost-benefit tradeoffs explicit with CTA can help agents discover more optimal decision-making strategies."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T18:46:14Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    18,
                    46,
                    14,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Wenxuan Ding"
                    },
                    {
                        "name": "Nicholas Tomlin"
                    },
                    {
                        "name": "Greg Durrett"
                    }
                ],
                "author_detail": {
                    "name": "Greg Durrett"
                },
                "author": "Greg Durrett"
            },
            {
                "id": "http://arxiv.org/abs/2602.16698v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16698v1",
                "title": "Causality is Key for Interpretability Claims to Generalise",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causality is Key for Interpretability Claims to Generalise"
                },
                "updated": "2026-02-18T18:45:04Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    18,
                    45,
                    4,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16698v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16698v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Interpretability research on large language models (LLMs) has yielded important insights into model behaviour, yet recurring pitfalls persist: findings that do not generalise, and causal interpretations that outrun the evidence. Our position is that causal inference specifies what constitutes a valid mapping from model activations to invariant high-level structures, the data or assumptions needed to achieve it, and the inferences it can support. Specifically, Pearl's causal hierarchy clarifies what an interpretability study can justify. Observations establish associations between model behaviour and internal components. Interventions (e.g., ablations or activation patching) support claims how these edits affect a behavioural metric (\\eg, average change in token probabilities) over a set of prompts. However, counterfactual claims -- i.e., asking what the model output would have been for the same prompt under an unobserved intervention -- remain largely unverifiable without controlled supervision. We show how causal representation learning (CRL) operationalises this hierarchy, specifying which variables are recoverable from activations and under what assumptions. Together, these motivate a diagnostic framework that helps practitioners select methods and evaluations matching claims to evidence such that findings generalise.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpretability research on large language models (LLMs) has yielded important insights into model behaviour, yet recurring pitfalls persist: findings that do not generalise, and causal interpretations that outrun the evidence. Our position is that causal inference specifies what constitutes a valid mapping from model activations to invariant high-level structures, the data or assumptions needed to achieve it, and the inferences it can support. Specifically, Pearl's causal hierarchy clarifies what an interpretability study can justify. Observations establish associations between model behaviour and internal components. Interventions (e.g., ablations or activation patching) support claims how these edits affect a behavioural metric (\\eg, average change in token probabilities) over a set of prompts. However, counterfactual claims -- i.e., asking what the model output would have been for the same prompt under an unobserved intervention -- remain largely unverifiable without controlled supervision. We show how causal representation learning (CRL) operationalises this hierarchy, specifying which variables are recoverable from activations and under what assumptions. Together, these motivate a diagnostic framework that helps practitioners select methods and evaluations matching claims to evidence such that findings generalise."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T18:45:04Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    18,
                    45,
                    4,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Shruti Joshi"
                    },
                    {
                        "name": "Aaron Mueller"
                    },
                    {
                        "name": "David Klindt"
                    },
                    {
                        "name": "Wieland Brendel"
                    },
                    {
                        "name": "Patrik Reizinger"
                    },
                    {
                        "name": "Dhanya Sridhar"
                    }
                ],
                "author_detail": {
                    "name": "Dhanya Sridhar"
                },
                "author": "Dhanya Sridhar"
            },
            {
                "id": "http://arxiv.org/abs/2503.18825v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2503.18825v4",
                "title": "EconEvals: Benchmarks and Litmus Tests for Economic Decision-Making by LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EconEvals: Benchmarks and Litmus Tests for Economic Decision-Making by LLM Agents"
                },
                "updated": "2026-02-18T18:37:52Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    18,
                    37,
                    52,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2503.18825v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2503.18825v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We develop evaluation methods for measuring the economic decision-making capabilities and tendencies of LLMs. First, we develop benchmarks derived from key problems in economics -- procurement, scheduling, and pricing -- that test an LLM's ability to learn from the environment in context. Second, we develop the framework of litmus tests, evaluations that quantify an LLM's choice behavior on a stylized decision-making task with multiple conflicting objectives. Each litmus test outputs a litmus score, which quantifies an LLM's tradeoff response, a reliability score, which measures the coherence of an LLM's choice behavior, and a competency score, which measures an LLM's capability at the same task when the conflicting objectives are replaced by a single, well-specified objective. Evaluating a broad array of frontier LLMs, we (1) investigate changes in LLM capabilities and tendencies over time, (2) derive economically meaningful insights from the LLMs' choice behavior and chain-of-thought, (3) validate our litmus test framework by testing self-consistency, robustness, and generalizability. Overall, this work provides a foundation for evaluating LLM agents as they are further integrated into economic decision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop evaluation methods for measuring the economic decision-making capabilities and tendencies of LLMs. First, we develop benchmarks derived from key problems in economics -- procurement, scheduling, and pricing -- that test an LLM's ability to learn from the environment in context. Second, we develop the framework of litmus tests, evaluations that quantify an LLM's choice behavior on a stylized decision-making task with multiple conflicting objectives. Each litmus test outputs a litmus score, which quantifies an LLM's tradeoff response, a reliability score, which measures the coherence of an LLM's choice behavior, and a competency score, which measures an LLM's capability at the same task when the conflicting objectives are replaced by a single, well-specified objective. Evaluating a broad array of frontier LLMs, we (1) investigate changes in LLM capabilities and tendencies over time, (2) derive economically meaningful insights from the LLMs' choice behavior and chain-of-thought, (3) validate our litmus test framework by testing self-consistency, robustness, and generalizability. Overall, this work provides a foundation for evaluating LLM agents as they are further integrated into economic decision-making."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-03-24T16:06:04Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    6,
                    4,
                    0,
                    83,
                    0
                ],
                "arxiv_comment": "v3 was a major revision with updated experiments and analysis; v4 consists of minor edits",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Sara Fish"
                    },
                    {
                        "name": "Julia Shephard"
                    },
                    {
                        "name": "Minkai Li"
                    },
                    {
                        "name": "Ran I. Shorrer"
                    },
                    {
                        "name": "Yannai A. Gonczarowski"
                    }
                ],
                "author_detail": {
                    "name": "Yannai A. Gonczarowski"
                },
                "author": "Yannai A. Gonczarowski"
            },
            {
                "id": "http://arxiv.org/abs/2602.16690v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16690v1",
                "title": "Synthetic-Powered Multiple Testing with FDR Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic-Powered Multiple Testing with FDR Control"
                },
                "updated": "2026-02-18T18:36:24Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    18,
                    36,
                    24,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16690v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16690v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Multiple hypothesis testing with false discovery rate (FDR) control is a fundamental problem in statistical inference, with broad applications in genomics, drug screening, and outlier detection. In many such settings, researchers may have access not only to real experimental observations but also to auxiliary or synthetic data -- from past, related experiments or generated by generative models -- that can provide additional evidence about the hypotheses of interest. We introduce SynthBH, a synthetic-powered multiple testing procedure that safely leverages such synthetic data. We prove that SynthBH guarantees finite-sample, distribution-free FDR control under a mild PRDS-type positive dependence condition, without requiring the pooled-data p-values to be valid under the null. The proposed method adapts to the (unknown) quality of the synthetic data: it enhances the sample efficiency and may boost the power when synthetic data are of high quality, while controlling the FDR at a user-specified level regardless of their quality. We demonstrate the empirical performance of SynthBH on tabular outlier detection benchmarks and on genomic analyses of drug-cancer sensitivity associations, and further study its properties through controlled experiments on simulated data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiple hypothesis testing with false discovery rate (FDR) control is a fundamental problem in statistical inference, with broad applications in genomics, drug screening, and outlier detection. In many such settings, researchers may have access not only to real experimental observations but also to auxiliary or synthetic data -- from past, related experiments or generated by generative models -- that can provide additional evidence about the hypotheses of interest. We introduce SynthBH, a synthetic-powered multiple testing procedure that safely leverages such synthetic data. We prove that SynthBH guarantees finite-sample, distribution-free FDR control under a mild PRDS-type positive dependence condition, without requiring the pooled-data p-values to be valid under the null. The proposed method adapts to the (unknown) quality of the synthetic data: it enhances the sample efficiency and may boost the power when synthetic data are of high quality, while controlling the FDR at a user-specified level regardless of their quality. We demonstrate the empirical performance of SynthBH on tabular outlier detection benchmarks and on genomic analyses of drug-cancer sensitivity associations, and further study its properties through controlled experiments on simulated data."
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T18:36:24Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    18,
                    36,
                    24,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME"
                },
                "authors": [
                    {
                        "name": "Yonghoon Lee"
                    },
                    {
                        "name": "Meshi Bashari"
                    },
                    {
                        "name": "Edgar Dobriban"
                    },
                    {
                        "name": "Yaniv Romano"
                    }
                ],
                "author_detail": {
                    "name": "Yaniv Romano"
                },
                "author": "Yaniv Romano"
            },
            {
                "id": "http://arxiv.org/abs/2411.11706v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2411.11706v4",
                "title": "MC-LLaVA: Multi-Concept Personalized Vision-Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MC-LLaVA: Multi-Concept Personalized Vision-Language Model"
                },
                "updated": "2026-02-18T18:33:19Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    18,
                    33,
                    19,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2411.11706v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2411.11706v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Current vision-language models (VLMs) show exceptional abilities across diverse tasks, such as visual question answering. To enhance user experience, recent studies have investigated VLM personalization to understand user-provided concepts. However, they mainly focus on single concepts, neglecting the existence and interplay of multiple concepts, which limits real-world applicability. This paper proposes MC-LLaVA, a multi-concept personalization paradigm. Specifically, MC-LLaVA employs a multi-concept instruction tuning strategy, effectively integrating multiple concepts in a single training step. To reduce the training costs, we propose a personalized textual prompt that uses visual token information to initialize concept tokens. Additionally, we introduce a personalized visual prompt during inference, aggregating location maps for enhanced recognition and grounding capabilities. To further push the performance upper bound, we incorporate an optional auxiliary loss, better enhancing the proposed personalized prompts. To decorate the VLM personalization research, we contribute a high-quality dataset. We carefully collect images with multiple characters and objects from movies and manually create question-answer samples for multi-concept scenarios, featuring superior diversity. Comprehensive experiments demonstrate that MC-LLaVA achieves impressive multi-concept personalized responses, paving the way for VLMs to become better user assistants. The code and dataset will be released at \\href{https://github.com/arctanxarc/MC-LLaVA}{https://github.com/arctanxarc/MC-LLaVA}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current vision-language models (VLMs) show exceptional abilities across diverse tasks, such as visual question answering. To enhance user experience, recent studies have investigated VLM personalization to understand user-provided concepts. However, they mainly focus on single concepts, neglecting the existence and interplay of multiple concepts, which limits real-world applicability. This paper proposes MC-LLaVA, a multi-concept personalization paradigm. Specifically, MC-LLaVA employs a multi-concept instruction tuning strategy, effectively integrating multiple concepts in a single training step. To reduce the training costs, we propose a personalized textual prompt that uses visual token information to initialize concept tokens. Additionally, we introduce a personalized visual prompt during inference, aggregating location maps for enhanced recognition and grounding capabilities. To further push the performance upper bound, we incorporate an optional auxiliary loss, better enhancing the proposed personalized prompts. To decorate the VLM personalization research, we contribute a high-quality dataset. We carefully collect images with multiple characters and objects from movies and manually create question-answer samples for multi-concept scenarios, featuring superior diversity. Comprehensive experiments demonstrate that MC-LLaVA achieves impressive multi-concept personalized responses, paving the way for VLMs to become better user assistants. The code and dataset will be released at \\href{https://github.com/arctanxarc/MC-LLaVA}{https://github.com/arctanxarc/MC-LLaVA}."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-11-18T16:33:52Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    16,
                    33,
                    52,
                    0,
                    323,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Ruichuan An"
                    },
                    {
                        "name": "Sihan Yang"
                    },
                    {
                        "name": "Renrui Zhang"
                    },
                    {
                        "name": "Ming Lu"
                    },
                    {
                        "name": "Tianyi Jiang"
                    },
                    {
                        "name": "Kai Zeng"
                    },
                    {
                        "name": "Yulin Luo"
                    },
                    {
                        "name": "Jiajun Cao"
                    },
                    {
                        "name": "Hao Liang"
                    },
                    {
                        "name": "Ying Chen"
                    },
                    {
                        "name": "Qi She"
                    },
                    {
                        "name": "Shanghang Zhang"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2602.16687v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16687v1",
                "title": "Scaling Open Discrete Audio Foundation Models with Interleaved Semantic, Acoustic, and Text Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Open Discrete Audio Foundation Models with Interleaved Semantic, Acoustic, and Text Tokens"
                },
                "updated": "2026-02-18T18:32:46Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    18,
                    32,
                    46,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16687v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16687v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Current audio language models are predominantly text-first, either extending pre-trained text LLM backbones or relying on semantic-only audio tokens, limiting general audio modeling. This paper presents a systematic empirical study of native audio foundation models that apply next-token prediction to audio at scale, jointly modeling semantic content, acoustic details, and text to support both general audio generation and cross-modal capabilities. We provide comprehensive empirical insights for building such models: (1) We systematically investigate design choices -- data sources, text mixture ratios, and token composition -- establishing a validated training recipe. (2) We conduct the first scaling law study for discrete audio models via IsoFLOP analysis on 64 models spanning $3{\\times}10^{18}$ to $3{\\times}10^{20}$ FLOPs, finding that optimal data grows 1.6$\\times$ faster than optimal model size. (3) We apply these lessons to train SODA (Scaling Open Discrete Audio), a suite of models from 135M to 4B parameters on 500B tokens, comparing against our scaling predictions and existing models. SODA serves as a flexible backbone for diverse audio/text tasks -- we demonstrate this by fine-tuning for voice-preserving speech-to-speech translation, using the same unified architecture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current audio language models are predominantly text-first, either extending pre-trained text LLM backbones or relying on semantic-only audio tokens, limiting general audio modeling. This paper presents a systematic empirical study of native audio foundation models that apply next-token prediction to audio at scale, jointly modeling semantic content, acoustic details, and text to support both general audio generation and cross-modal capabilities. We provide comprehensive empirical insights for building such models: (1) We systematically investigate design choices -- data sources, text mixture ratios, and token composition -- establishing a validated training recipe. (2) We conduct the first scaling law study for discrete audio models via IsoFLOP analysis on 64 models spanning $3{\\times}10^{18}$ to $3{\\times}10^{20}$ FLOPs, finding that optimal data grows 1.6$\\times$ faster than optimal model size. (3) We apply these lessons to train SODA (Scaling Open Discrete Audio), a suite of models from 135M to 4B parameters on 500B tokens, comparing against our scaling predictions and existing models. SODA serves as a flexible backbone for diverse audio/text tasks -- we demonstrate this by fine-tuning for voice-preserving speech-to-speech translation, using the same unified architecture."
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T18:32:46Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    18,
                    32,
                    46,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD"
                },
                "authors": [
                    {
                        "name": "Potsawee Manakul"
                    },
                    {
                        "name": "Woody Haosheng Gan"
                    },
                    {
                        "name": "Martijn Bartelds"
                    },
                    {
                        "name": "Guangzhi Sun"
                    },
                    {
                        "name": "William Held"
                    },
                    {
                        "name": "Diyi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Diyi Yang"
                },
                "author": "Diyi Yang"
            },
            {
                "id": "http://arxiv.org/abs/2505.21681v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.21681v2",
                "title": "Residual Diffusion Models for Variable-Rate Joint Source Channel Coding of MIMO CSI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Residual Diffusion Models for Variable-Rate Joint Source Channel Coding of MIMO CSI"
                },
                "updated": "2026-02-18T18:26:29Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    18,
                    26,
                    29,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.21681v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.21681v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Despite significant advancements in deep learning based CSI compression, some key limitations remain unaddressed. Current approaches predominantly treat CSI compression as a source-coding problem, thereby neglecting transmission errors. Conventional separate source and channel coding suffers from the cliff effect, leading to significant deterioration in reconstruction performance under challenging channel conditions. While existing autoencoder-based compression schemes can be readily extended to support joint source-channel coding, they struggle to capture complex channel distributions and exhibit poor scalability with increasing parameter count. To overcome these inherent limitations of autoencoder-based approaches, we propose Residual-Diffusion Joint Source-Channel Coding (RD- JSCC), a novel framework that integrates a lightweight autoencoder with a residual diffusion module to iteratively refine CSI reconstruction. Our flexible decoding strategy balances computational efficiency and performance by dynamically switching between low-complexity autoencoder decoding and sophisticated diffusion-based refinement based on channel conditions. Comprehensive simulations demonstrate that RD-JSCC significantly outperforms existing autoencoder-based approaches in challenging wireless environments. Furthermore, RD-JSCC offers several practical features, including a low-latency 2-step diffusion during inference, support for multiple compression rates with a single model, robustness to fixed-bit quantization, and adaptability to imperfect channel estimation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite significant advancements in deep learning based CSI compression, some key limitations remain unaddressed. Current approaches predominantly treat CSI compression as a source-coding problem, thereby neglecting transmission errors. Conventional separate source and channel coding suffers from the cliff effect, leading to significant deterioration in reconstruction performance under challenging channel conditions. While existing autoencoder-based compression schemes can be readily extended to support joint source-channel coding, they struggle to capture complex channel distributions and exhibit poor scalability with increasing parameter count. To overcome these inherent limitations of autoencoder-based approaches, we propose Residual-Diffusion Joint Source-Channel Coding (RD- JSCC), a novel framework that integrates a lightweight autoencoder with a residual diffusion module to iteratively refine CSI reconstruction. Our flexible decoding strategy balances computational efficiency and performance by dynamically switching between low-complexity autoencoder decoding and sophisticated diffusion-based refinement based on channel conditions. Comprehensive simulations demonstrate that RD-JSCC significantly outperforms existing autoencoder-based approaches in challenging wireless environments. Furthermore, RD-JSCC offers several practical features, including a low-latency 2-step diffusion during inference, support for multiple compression rates with a single model, robustness to fixed-bit quantization, and adaptability to imperfect channel estimation."
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-27T19:06:48Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    19,
                    6,
                    48,
                    1,
                    147,
                    0
                ],
                "arxiv_comment": "15 pages, 13 figures, To appear in IEEE Journal on Selected Areas in Communications, 2026",
                "arxiv_primary_category": {
                    "term": "cs.IT"
                },
                "authors": [
                    {
                        "name": "Sravan Kumar Ankireddy"
                    },
                    {
                        "name": "Heasung Kim"
                    },
                    {
                        "name": "Joonyoung Cho"
                    },
                    {
                        "name": "Hyeji Kim"
                    }
                ],
                "author_detail": {
                    "name": "Hyeji Kim"
                },
                "author": "Hyeji Kim"
            },
            {
                "id": "http://arxiv.org/abs/2602.16682v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16682v1",
                "title": "Learning Situated Awareness in the Real World",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Situated Awareness in the Real World"
                },
                "updated": "2026-02-18T18:22:52Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    18,
                    22,
                    52,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16682v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16682v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "A core aspect of human perception is situated awareness, the ability to relate ourselves to the surrounding physical environment and reason over possible actions in context. However, most existing benchmarks for multimodal foundation models (MFMs) emphasize environment-centric spatial relations (relations among objects in a scene), while largely overlooking observer-centric relationships that require reasoning relative to agent's viewpoint, pose, and motion. To bridge this gap, we introduce SAW-Bench (Situated Awareness in the Real World), a novel benchmark for evaluating egocentric situated awareness using real-world videos. SAW-Bench comprises 786 self-recorded videos captured with Ray-Ban Meta (Gen 2) smart glasses spanning diverse indoor and outdoor environments, and over 2,071 human-annotated question-answer pairs. It probes a model's observer-centric understanding with six different awareness tasks. Our comprehensive evaluation reveals a human-model performance gap of 37.66%, even with the best-performing MFM, Gemini 3 Flash. Beyond this gap, our in-depth analysis uncovers several notable findings; for example, while models can exploit partial geometric cues in egocentric videos, they often fail to infer a coherent camera geometry, leading to systematic spatial reasoning errors. We position SAW-Bench as a benchmark for situated spatial intelligence, moving beyond passive observation to understanding physically grounded, observer-centric dynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A core aspect of human perception is situated awareness, the ability to relate ourselves to the surrounding physical environment and reason over possible actions in context. However, most existing benchmarks for multimodal foundation models (MFMs) emphasize environment-centric spatial relations (relations among objects in a scene), while largely overlooking observer-centric relationships that require reasoning relative to agent's viewpoint, pose, and motion. To bridge this gap, we introduce SAW-Bench (Situated Awareness in the Real World), a novel benchmark for evaluating egocentric situated awareness using real-world videos. SAW-Bench comprises 786 self-recorded videos captured with Ray-Ban Meta (Gen 2) smart glasses spanning diverse indoor and outdoor environments, and over 2,071 human-annotated question-answer pairs. It probes a model's observer-centric understanding with six different awareness tasks. Our comprehensive evaluation reveals a human-model performance gap of 37.66%, even with the best-performing MFM, Gemini 3 Flash. Beyond this gap, our in-depth analysis uncovers several notable findings; for example, while models can exploit partial geometric cues in egocentric videos, they often fail to infer a coherent camera geometry, leading to systematic spatial reasoning errors. We position SAW-Bench as a benchmark for situated spatial intelligence, moving beyond passive observation to understanding physically grounded, observer-centric dynamics."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T18:22:52Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    18,
                    22,
                    52,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Chuhan Li"
                    },
                    {
                        "name": "Ruilin Han"
                    },
                    {
                        "name": "Joy Hsu"
                    },
                    {
                        "name": "Yongyuan Liang"
                    },
                    {
                        "name": "Rajiv Dhawan"
                    },
                    {
                        "name": "Jiajun Wu"
                    },
                    {
                        "name": "Ming-Hsuan Yang"
                    },
                    {
                        "name": "Xin Eric Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Eric Wang"
                },
                "author": "Xin Eric Wang"
            },
            {
                "id": "http://arxiv.org/abs/2509.20345v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.20345v2",
                "title": "Statistical Inference Leveraging Synthetic Data with Distribution-Free Guarantees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical Inference Leveraging Synthetic Data with Distribution-Free Guarantees"
                },
                "updated": "2026-02-18T18:13:32Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    18,
                    13,
                    32,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.20345v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.20345v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The rapid proliferation of high-quality synthetic data -- generated by advanced AI models or collected as auxiliary data from related tasks -- presents both opportunities and challenges for statistical inference. This paper introduces a GEneral Synthetic-Powered Inference (GESPI) framework that wraps around any statistical inference procedure to safely enhance sample efficiency by combining synthetic and real data. Our framework leverages high-quality synthetic data to boost statistical power, yet adaptively defaults to the standard inference method using only real data when synthetic data is of low quality. The error of our method remains below a user-specified bound without any distributional assumptions on the synthetic data, and decreases as the quality of the synthetic data improves. This flexibility enables seamless integration with conformal prediction, risk control, hypothesis testing, and multiple testing procedures, all without modifying the base inference method. We demonstrate the benefits of our method on challenging tasks with limited labeled data, including AlphaFold protein structure prediction, and comparing large reasoning models on complex math problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid proliferation of high-quality synthetic data -- generated by advanced AI models or collected as auxiliary data from related tasks -- presents both opportunities and challenges for statistical inference. This paper introduces a GEneral Synthetic-Powered Inference (GESPI) framework that wraps around any statistical inference procedure to safely enhance sample efficiency by combining synthetic and real data. Our framework leverages high-quality synthetic data to boost statistical power, yet adaptively defaults to the standard inference method using only real data when synthetic data is of low quality. The error of our method remains below a user-specified bound without any distributional assumptions on the synthetic data, and decreases as the quality of the synthetic data improves. This flexibility enables seamless integration with conformal prediction, risk control, hypothesis testing, and multiple testing procedures, all without modifying the base inference method. We demonstrate the benefits of our method on challenging tasks with limited labeled data, including AlphaFold protein structure prediction, and comparing large reasoning models on complex math problems."
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-24T17:37:14Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    17,
                    37,
                    14,
                    2,
                    267,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME"
                },
                "authors": [
                    {
                        "name": "Meshi Bashari"
                    },
                    {
                        "name": "Yonghoon Lee"
                    },
                    {
                        "name": "Roy Maor Lotan"
                    },
                    {
                        "name": "Edgar Dobriban"
                    },
                    {
                        "name": "Yaniv Romano"
                    }
                ],
                "author_detail": {
                    "name": "Yaniv Romano"
                },
                "author": "Yaniv Romano"
            },
            {
                "id": "http://arxiv.org/abs/2602.16671v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16671v1",
                "title": "SPARC: Scenario Planning and Reasoning for Automated C Unit Test Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPARC: Scenario Planning and Reasoning for Automated C Unit Test Generation"
                },
                "updated": "2026-02-18T18:09:03Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    18,
                    9,
                    3,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16671v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16671v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Automated unit test generation for C remains a formidable challenge due to the semantic gap between high-level program intent and the rigid syntactic constraints of pointer arithmetic and manual memory management. While Large Language Models (LLMs) exhibit strong generative capabilities, direct intent-to-code synthesis frequently suffers from the leap-to-code failure mode, where models prematurely emit code without grounding in program structure, constraints, and semantics. This will result in non-compilable tests, hallucinated function signatures, low branch coverage, and semantically irrelevant assertions that cannot properly capture bugs. We introduce SPARC, a neuro-symbolic, scenario-based framework that bridges this gap through four stages: (1) Control Flow Graph (CFG) analysis, (2) an Operation Map that grounds LLM reasoning in validated utility helpers, (3) Path-targeted test synthesis, and (4) an iterative, self-correction validation loop using compiler and runtime feedback. We evaluate SPARC on 59 real-world and algorithmic subjects, where it outperforms the vanilla prompt generation baseline by 31.36% in line coverage, 26.01% in branch coverage, and 20.78% in mutation score, matching or exceeding the symbolic execution tool KLEE on complex subjects. SPARC retains 94.3% of tests through iterative repair and produces code with significantly higher developer-rated readability and maintainability. By aligning LLM reasoning with program structure, SPARC provides a scalable path for industrial-grade testing of legacy C codebases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated unit test generation for C remains a formidable challenge due to the semantic gap between high-level program intent and the rigid syntactic constraints of pointer arithmetic and manual memory management. While Large Language Models (LLMs) exhibit strong generative capabilities, direct intent-to-code synthesis frequently suffers from the leap-to-code failure mode, where models prematurely emit code without grounding in program structure, constraints, and semantics. This will result in non-compilable tests, hallucinated function signatures, low branch coverage, and semantically irrelevant assertions that cannot properly capture bugs. We introduce SPARC, a neuro-symbolic, scenario-based framework that bridges this gap through four stages: (1) Control Flow Graph (CFG) analysis, (2) an Operation Map that grounds LLM reasoning in validated utility helpers, (3) Path-targeted test synthesis, and (4) an iterative, self-correction validation loop using compiler and runtime feedback. We evaluate SPARC on 59 real-world and algorithmic subjects, where it outperforms the vanilla prompt generation baseline by 31.36% in line coverage, 26.01% in branch coverage, and 20.78% in mutation score, matching or exceeding the symbolic execution tool KLEE on complex subjects. SPARC retains 94.3% of tests through iterative repair and produces code with significantly higher developer-rated readability and maintainability. By aligning LLM reasoning with program structure, SPARC provides a scalable path for industrial-grade testing of legacy C codebases."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T18:09:03Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    18,
                    9,
                    3,
                    2,
                    49,
                    0
                ],
                "arxiv_comment": "9 pages, 6 figures, 4 tables",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Jaid Monwar Chowdhury"
                    },
                    {
                        "name": "Chi-An Fu"
                    },
                    {
                        "name": "Reyhaneh Jabbarvand"
                    }
                ],
                "author_detail": {
                    "name": "Reyhaneh Jabbarvand"
                },
                "author": "Reyhaneh Jabbarvand"
            },
            {
                "id": "http://arxiv.org/abs/2602.16665v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16665v1",
                "title": "Optimizing p-spin models through hypergraph neural networks and deep reinforcement learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing p-spin models through hypergraph neural networks and deep reinforcement learning"
                },
                "updated": "2026-02-18T18:05:19Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    18,
                    5,
                    19,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16665v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16665v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "p-spin glasses, characterized by frustrated many-body interactions beyond the conventional pairwise case (p>2), are prototypical disordered systems whose ground-state search is NP-hard and computationally prohibitive for large instances. Solving this problem is not only fundamental for understanding high-order disorder, structural glasses, and topological phases, but also central to a wide spectrum of hard combinatorial optimization tasks. Despite decades of progress, there still lacks an efficient and scalable solver for generic large-scale p-spin models. Here we introduce PLANCK, a physics-inspired deep reinforcement learning framework built on hypergraph neural networks. PLANCK directly optimizes arbitrary high-order interactions, and systematically exploits gauge symmetry throughout both training and inference. Trained exclusively on small synthetic instances, PLANCK exhibits strong zero-shot generalization to systems orders of magnitude larger, and consistently outperforms state-of-the-art thermal annealing methods across all tested structural topologies and coupling distributions. Moreover, without any modification, PLANCK achieves near-optimal solutions for a broad class of NP-hard combinatorial problems, including random k-XORSAT, hypergraph max-cut, and conventional max-cut. The presented framework provides a physics-inspired algorithmic paradigm that bridges statistical mechanics and reinforcement learning. The symmetry-aware design not only advances the tractable frontiers of high-order disordered systems, but also opens a promising avenue for machine-learning-based solvers to tackle previously intractable combinatorial optimization challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "p-spin glasses, characterized by frustrated many-body interactions beyond the conventional pairwise case (p>2), are prototypical disordered systems whose ground-state search is NP-hard and computationally prohibitive for large instances. Solving this problem is not only fundamental for understanding high-order disorder, structural glasses, and topological phases, but also central to a wide spectrum of hard combinatorial optimization tasks. Despite decades of progress, there still lacks an efficient and scalable solver for generic large-scale p-spin models. Here we introduce PLANCK, a physics-inspired deep reinforcement learning framework built on hypergraph neural networks. PLANCK directly optimizes arbitrary high-order interactions, and systematically exploits gauge symmetry throughout both training and inference. Trained exclusively on small synthetic instances, PLANCK exhibits strong zero-shot generalization to systems orders of magnitude larger, and consistently outperforms state-of-the-art thermal annealing methods across all tested structural topologies and coupling distributions. Moreover, without any modification, PLANCK achieves near-optimal solutions for a broad class of NP-hard combinatorial problems, including random k-XORSAT, hypergraph max-cut, and conventional max-cut. The presented framework provides a physics-inspired algorithmic paradigm that bridges statistical mechanics and reinforcement learning. The symmetry-aware design not only advances the tractable frontiers of high-order disordered systems, but also opens a promising avenue for machine-learning-based solvers to tackle previously intractable combinatorial optimization challenges."
                },
                "tags": [
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T18:05:19Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    18,
                    5,
                    19,
                    2,
                    49,
                    0
                ],
                "arxiv_comment": "13 pages, 8 figures",
                "arxiv_primary_category": {
                    "term": "cond-mat.dis-nn"
                },
                "authors": [
                    {
                        "name": "Li Zeng"
                    },
                    {
                        "name": "Mutian Shen"
                    },
                    {
                        "name": "Tianle Pu"
                    },
                    {
                        "name": "Zohar Nussinov"
                    },
                    {
                        "name": "Qing Feng"
                    },
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Zhong Liu"
                    },
                    {
                        "name": "Changjun Fan"
                    }
                ],
                "author_detail": {
                    "name": "Changjun Fan"
                },
                "author": "Changjun Fan"
            },
            {
                "id": "http://arxiv.org/abs/2602.16662v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16662v1",
                "title": "Evaluating Collective Behaviour of Hundreds of LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Collective Behaviour of Hundreds of LLM Agents"
                },
                "updated": "2026-02-18T18:02:51Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    18,
                    2,
                    51,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16662v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16662v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As autonomous agents powered by LLM are increasingly deployed in society, understanding their collective behaviour in social dilemmas becomes critical. We introduce an evaluation framework where LLMs generate strategies encoded as algorithms, enabling inspection prior to deployment and scaling to populations of hundreds of agents -- substantially larger than in previous work. We find that more recent models tend to produce worse societal outcomes compared to older models when agents prioritise individual gain over collective benefits. Using cultural evolution to model user selection of agents, our simulations reveal a significant risk of convergence to poor societal equilibria, particularly when the relative benefit of cooperation diminishes and population sizes increase. We release our code as an evaluation suite for developers to assess the emergent collective behaviour of their models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As autonomous agents powered by LLM are increasingly deployed in society, understanding their collective behaviour in social dilemmas becomes critical. We introduce an evaluation framework where LLMs generate strategies encoded as algorithms, enabling inspection prior to deployment and scaling to populations of hundreds of agents -- substantially larger than in previous work. We find that more recent models tend to produce worse societal outcomes compared to older models when agents prioritise individual gain over collective benefits. Using cultural evolution to model user selection of agents, our simulations reveal a significant risk of convergence to poor societal equilibria, particularly when the relative benefit of cooperation diminishes and population sizes increase. We release our code as an evaluation suite for developers to assess the emergent collective behaviour of their models."
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T18:02:51Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    18,
                    2,
                    51,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA"
                },
                "authors": [
                    {
                        "name": "Richard Willis"
                    },
                    {
                        "name": "Jianing Zhao"
                    },
                    {
                        "name": "Yali Du"
                    },
                    {
                        "name": "Joel Z. Leibo"
                    }
                ],
                "author_detail": {
                    "name": "Joel Z. Leibo"
                },
                "author": "Joel Z. Leibo"
            },
            {
                "id": "http://arxiv.org/abs/2602.16660v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16660v1",
                "title": "Align Once, Benefit Multilingually: Enforcing Multilingual Consistency for LLM Safety Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Align Once, Benefit Multilingually: Enforcing Multilingual Consistency for LLM Safety Alignment"
                },
                "updated": "2026-02-18T18:01:23Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    18,
                    1,
                    23,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16660v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16660v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The widespread deployment of large language models (LLMs) across linguistic communities necessitates reliable multilingual safety alignment. However, recent efforts to extend alignment to other languages often require substantial resources, either through large-scale, high-quality supervision in the target language or through pairwise alignment with high-resource languages, which limits scalability. In this work, we propose a resource-efficient method for improving multilingual safety alignment. We introduce a plug-and-play Multi-Lingual Consistency (MLC) loss that can be integrated into existing monolingual alignment pipelines. By improving collinearity between multilingual representation vectors, our method encourages directional consistency at the multilingual semantic level in a single update. This allows simultaneous alignment across multiple languages using only multilingual prompt variants without requiring additional response-level supervision in low-resource languages. We validate the proposed method across different model architectures and alignment paradigms, and demonstrate its effectiveness in enhancing multilingual safety with limited impact on general model utility. Further evaluation across languages and tasks indicates improved cross-lingual generalization, suggesting the proposed approach as a practical solution for multilingual consistency alignment under limited supervision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread deployment of large language models (LLMs) across linguistic communities necessitates reliable multilingual safety alignment. However, recent efforts to extend alignment to other languages often require substantial resources, either through large-scale, high-quality supervision in the target language or through pairwise alignment with high-resource languages, which limits scalability. In this work, we propose a resource-efficient method for improving multilingual safety alignment. We introduce a plug-and-play Multi-Lingual Consistency (MLC) loss that can be integrated into existing monolingual alignment pipelines. By improving collinearity between multilingual representation vectors, our method encourages directional consistency at the multilingual semantic level in a single update. This allows simultaneous alignment across multiple languages using only multilingual prompt variants without requiring additional response-level supervision in low-resource languages. We validate the proposed method across different model architectures and alignment paradigms, and demonstrate its effectiveness in enhancing multilingual safety with limited impact on general model utility. Further evaluation across languages and tasks indicates improved cross-lingual generalization, suggesting the proposed approach as a practical solution for multilingual consistency alignment under limited supervision."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T18:01:23Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    18,
                    1,
                    23,
                    2,
                    49,
                    0
                ],
                "arxiv_comment": "Accepted by ICLR 2026",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yuyan Bu"
                    },
                    {
                        "name": "Xiaohao Liu"
                    },
                    {
                        "name": "ZhaoXing Ren"
                    },
                    {
                        "name": "Yaodong Yang"
                    },
                    {
                        "name": "Juntao Dai"
                    }
                ],
                "author_detail": {
                    "name": "Juntao Dai"
                },
                "author": "Juntao Dai"
            },
            {
                "id": "http://arxiv.org/abs/2602.15238v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.15238v2",
                "title": "Closing the Distribution Gap in Adversarial Training for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Closing the Distribution Gap in Adversarial Training for LLMs"
                },
                "updated": "2026-02-18T17:57:10Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    17,
                    57,
                    10,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.15238v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.15238v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Adversarial training for LLMs is one of the most promising methods to reliably improve robustness against adversaries. However, despite significant progress, models remain vulnerable to simple in-distribution exploits, such as rewriting prompts in the past tense or translating them into other languages. We argue that this persistent fragility stems from a fundamental limitation in current adversarial training algorithms: they minimize adversarial loss on their training set but inadequately cover the data distribution, resulting in vulnerability to seemingly simple attacks. To bridge this gap, we propose Distributional Adversarial Training, DAT. We leverage Diffusion LLMs to approximate the true joint distribution of prompts and responses, enabling generation of diverse, high-likelihood samples that address generalization failures. By combining optimization over the data distribution provided by the diffusion model with continuous adversarial training, DAT achieves substantially higher adversarial robustness than previous methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial training for LLMs is one of the most promising methods to reliably improve robustness against adversaries. However, despite significant progress, models remain vulnerable to simple in-distribution exploits, such as rewriting prompts in the past tense or translating them into other languages. We argue that this persistent fragility stems from a fundamental limitation in current adversarial training algorithms: they minimize adversarial loss on their training set but inadequately cover the data distribution, resulting in vulnerability to seemingly simple attacks. To bridge this gap, we propose Distributional Adversarial Training, DAT. We leverage Diffusion LLMs to approximate the true joint distribution of prompts and responses, enabling generation of diverse, high-likelihood samples that address generalization failures. By combining optimization over the data distribution provided by the diffusion model with continuous adversarial training, DAT achieves substantially higher adversarial robustness than previous methods."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-16T22:34:52Z",
                "published_parsed": [
                    2026,
                    2,
                    16,
                    22,
                    34,
                    52,
                    0,
                    47,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Chengzhi Hu"
                    },
                    {
                        "name": "Jonas Dornbusch"
                    },
                    {
                        "name": "David Lüdke"
                    },
                    {
                        "name": "Stephan Günnemann"
                    },
                    {
                        "name": "Leo Schwinn"
                    }
                ],
                "author_detail": {
                    "name": "Leo Schwinn"
                },
                "author": "Leo Schwinn"
            },
            {
                "id": "http://arxiv.org/abs/2602.16651v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16651v1",
                "title": "Interpreting the HI 21-cm cosmology maps through Largest Cluster Statistics III: Impact of the lightcone effect",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpreting the HI 21-cm cosmology maps through Largest Cluster Statistics III: Impact of the lightcone effect"
                },
                "updated": "2026-02-18T17:47:25Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    17,
                    47,
                    25,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16651v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16651v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The redshifted 21-cm signal emitted by neutral Hydrogen (HI) is a promising probe to understand the evolution of the topology of ionized regions during the Epoch of Reionization (EoR). The topology of ionized regions allows us to infer the nature and properties of ionizing sources, i.e., early galaxies and AGNs. Traditional Fourier statistics, such as the power spectrum, help us quantify the strength of fluctuations in this field at different length scales but do not preserve its phase information. Analyzing the 21-cm brightness temperature field in the image domain retains its non-Gaussian characteristics and morphological information. One such approach is to track the coalescence of multiple ionized regions to form one contiguous ionized region spanning the universe. This is referred to as percolation, and its onset is quantified by a sharp rise in the value of the Largest Cluster Statistic (LCS) approaching unity. In this work, we carry out a percolation analysis of 21-cm brightness temperature fields by studying the redshift evolution of the LCS along a lightcone to distinguish between several simulated reionization scenarios. We have extended previous results on reionization model comparison from the analysis of coeval 21-cm maps to understand how the lightcone effect biases the observed percolation behavior and affects the distinguishability of the source models. We estimate the LCS of subvolumes of different sizes in the 21-cm lightcone maps and study their redshift evolution for different reionization scenarios using a moving volume approach. We find that the percolation transition inferred from a lightcone approaches that from the coeval box as we increase the bandwidth of the moving volume in all but one reionization scenario.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The redshifted 21-cm signal emitted by neutral Hydrogen (HI) is a promising probe to understand the evolution of the topology of ionized regions during the Epoch of Reionization (EoR). The topology of ionized regions allows us to infer the nature and properties of ionizing sources, i.e., early galaxies and AGNs. Traditional Fourier statistics, such as the power spectrum, help us quantify the strength of fluctuations in this field at different length scales but do not preserve its phase information. Analyzing the 21-cm brightness temperature field in the image domain retains its non-Gaussian characteristics and morphological information. One such approach is to track the coalescence of multiple ionized regions to form one contiguous ionized region spanning the universe. This is referred to as percolation, and its onset is quantified by a sharp rise in the value of the Largest Cluster Statistic (LCS) approaching unity. In this work, we carry out a percolation analysis of 21-cm brightness temperature fields by studying the redshift evolution of the LCS along a lightcone to distinguish between several simulated reionization scenarios. We have extended previous results on reionization model comparison from the analysis of coeval 21-cm maps to understand how the lightcone effect biases the observed percolation behavior and affects the distinguishability of the source models. We estimate the LCS of subvolumes of different sizes in the 21-cm lightcone maps and study their redshift evolution for different reionization scenarios using a moving volume approach. We find that the percolation transition inferred from a lightcone approaches that from the coeval box as we increase the bandwidth of the moving volume in all but one reionization scenario."
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T17:47:25Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    17,
                    47,
                    25,
                    2,
                    49,
                    0
                ],
                "arxiv_comment": "23 pages, 5 figures, 1 table. To be submitted to JCAP. Comments and suggestions are welcome",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO"
                },
                "authors": [
                    {
                        "name": "Hemanth Potluri"
                    },
                    {
                        "name": "Manas Mohit Dosibhatla"
                    },
                    {
                        "name": "Leon Noble"
                    },
                    {
                        "name": "Chandra Shekhar Murmu"
                    },
                    {
                        "name": "Suman Majumdar"
                    },
                    {
                        "name": "Samit Kumar Pal"
                    },
                    {
                        "name": "Saswata Dasgupta"
                    },
                    {
                        "name": "Satadru Bag"
                    },
                    {
                        "name": "Abhirup Datta"
                    }
                ],
                "author_detail": {
                    "name": "Abhirup Datta"
                },
                "author": "Abhirup Datta"
            },
            {
                "id": "http://arxiv.org/abs/2412.06004v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2412.06004v2",
                "title": "Large-sample analysis of cost functionals for inference under the coalescent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-sample analysis of cost functionals for inference under the coalescent"
                },
                "updated": "2026-02-18T17:46:09Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    17,
                    46,
                    9,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2412.06004v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2412.06004v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1016/j.spa.2026.104894",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "The coalescent is a foundational model of latent genealogical trees under neutral evolution, but suffers from intractable sampling probabilities. Methods for approximating these sampling probabilities either introduce bias or fail to scale to large sample sizes. We show that a class of cost functionals of the coalescent with recurrent mutation and a finite number of alleles converge to tractable processes in the infinite-sample limit. A particular choice of costs yields insight about importance sampling methods, which are a classical tool for coalescent sampling probability approximation. These insights reveal that the behaviour of coalescent importance sampling algorithms differs markedly from standard sequential importance samplers, with or without resampling. We conduct a simulation study to verify that our asymptotics are accurate for algorithms with finite (and moderate) sample sizes. Our results constitute the first theoretical description of large-sample importance sampling algorithms for the coalescent, provide heuristics for the a priori optimisation of computational effort, and identify settings where resampling is harmful for algorithm performance. We observe strikingly different behaviour for importance sampling methods under the infinite sites model of mutation, which is regarded as a good and more tractable approximation of finite alleles mutation in most respects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The coalescent is a foundational model of latent genealogical trees under neutral evolution, but suffers from intractable sampling probabilities. Methods for approximating these sampling probabilities either introduce bias or fail to scale to large sample sizes. We show that a class of cost functionals of the coalescent with recurrent mutation and a finite number of alleles converge to tractable processes in the infinite-sample limit. A particular choice of costs yields insight about importance sampling methods, which are a classical tool for coalescent sampling probability approximation. These insights reveal that the behaviour of coalescent importance sampling algorithms differs markedly from standard sequential importance samplers, with or without resampling. We conduct a simulation study to verify that our asymptotics are accurate for algorithms with finite (and moderate) sample sizes. Our results constitute the first theoretical description of large-sample importance sampling algorithms for the coalescent, provide heuristics for the a priori optimisation of computational effort, and identify settings where resampling is harmful for algorithm performance. We observe strikingly different behaviour for importance sampling methods under the infinite sites model of mutation, which is regarded as a good and more tractable approximation of finite alleles mutation in most respects."
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-12-08T17:28:18Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    17,
                    28,
                    18,
                    6,
                    343,
                    0
                ],
                "arxiv_comment": "34 pages, 7 figures",
                "arxiv_primary_category": {
                    "term": "math.ST"
                },
                "arxiv_journal_ref": "Stochastic Processes and their Applications, Volume 195, 2026, Stochastic Processes and their Applications 195 (2026) 104894",
                "authors": [
                    {
                        "name": "Martina Favero"
                    },
                    {
                        "name": "Jere Koskela"
                    }
                ],
                "author_detail": {
                    "name": "Jere Koskela"
                },
                "author": "Jere Koskela",
                "arxiv_doi": "10.1016/j.spa.2026.104894"
            },
            {
                "id": "http://arxiv.org/abs/2602.16650v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16650v1",
                "title": "Retrieval Augmented Generation of Literature-derived Polymer Knowledge: The Example of a Biodegradable Polymer Expert System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation of Literature-derived Polymer Knowledge: The Example of a Biodegradable Polymer Expert System"
                },
                "updated": "2026-02-18T17:46:09Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    17,
                    46,
                    9,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16650v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16650v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Polymer literature contains a large and growing body of experimental knowledge, yet much of it is buried in unstructured text and inconsistent terminology, making systematic retrieval and reasoning difficult. Existing tools typically extract narrow, study-specific facts in isolation, failing to preserve the cross-study context required to answer broader scientific questions. Retrieval-augmented generation (RAG) offers a promising way to overcome this limitation by combining large language models (LLMs) with external retrieval, but its effectiveness depends strongly on how domain knowledge is represented. In this work, we develop two retrieval pipelines: a dense semantic vector-based approach (VectorRAG) and a graph-based approach (GraphRAG). Using over 1,000 polyhydroxyalkanoate (PHA) papers, we construct context-preserving paragraph embeddings and a canonicalized structured knowledge graph supporting entity disambiguation and multi-hop reasoning. We evaluate these pipelines through standard retrieval metrics, comparisons with general state-of-the-art systems such as GPT and Gemini, and qualitative validation by a domain chemist. The results show that GraphRAG achieves higher precision and interpretability, while VectorRAG provides broader recall, highlighting complementary trade-offs. Expert validation further confirms that the tailored pipelines, particularly GraphRAG, produce well-grounded, citation-reliable responses with strong domain relevance. By grounding every statement in evidence, these systems enable researchers to navigate the literature, compare findings across studies, and uncover patterns that are difficult to extract manually. More broadly, this work establishes a practical framework for building materials science assistants using curated corpora and retrieval design, reducing reliance on proprietary models while enabling trustworthy literature analysis at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Polymer literature contains a large and growing body of experimental knowledge, yet much of it is buried in unstructured text and inconsistent terminology, making systematic retrieval and reasoning difficult. Existing tools typically extract narrow, study-specific facts in isolation, failing to preserve the cross-study context required to answer broader scientific questions. Retrieval-augmented generation (RAG) offers a promising way to overcome this limitation by combining large language models (LLMs) with external retrieval, but its effectiveness depends strongly on how domain knowledge is represented. In this work, we develop two retrieval pipelines: a dense semantic vector-based approach (VectorRAG) and a graph-based approach (GraphRAG). Using over 1,000 polyhydroxyalkanoate (PHA) papers, we construct context-preserving paragraph embeddings and a canonicalized structured knowledge graph supporting entity disambiguation and multi-hop reasoning. We evaluate these pipelines through standard retrieval metrics, comparisons with general state-of-the-art systems such as GPT and Gemini, and qualitative validation by a domain chemist. The results show that GraphRAG achieves higher precision and interpretability, while VectorRAG provides broader recall, highlighting complementary trade-offs. Expert validation further confirms that the tailored pipelines, particularly GraphRAG, produce well-grounded, citation-reliable responses with strong domain relevance. By grounding every statement in evidence, these systems enable researchers to navigate the literature, compare findings across studies, and uncover patterns that are difficult to extract manually. More broadly, this work establishes a practical framework for building materials science assistants using curated corpora and retrieval design, reducing reliance on proprietary models while enabling trustworthy literature analysis at scale."
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T17:46:09Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    17,
                    46,
                    9,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE"
                },
                "authors": [
                    {
                        "name": "Sonakshi Gupta"
                    },
                    {
                        "name": "Akhlak Mahmood"
                    },
                    {
                        "name": "Wei Xiong"
                    },
                    {
                        "name": "Rampi Ramprasad"
                    }
                ],
                "author_detail": {
                    "name": "Rampi Ramprasad"
                },
                "author": "Rampi Ramprasad"
            },
            {
                "id": "http://arxiv.org/abs/2602.16640v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16640v1",
                "title": "Quecto-V1: Empirical Analysis of 8-bit Quantized Small Language Models for On-Device Legal Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quecto-V1: Empirical Analysis of 8-bit Quantized Small Language Models for On-Device Legal Retrieval"
                },
                "updated": "2026-02-18T17:29:43Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    17,
                    29,
                    43,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16640v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16640v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The rapid proliferation of Large Language Models (LLMs) has revolutionized Natural Language Processing (NLP) but has simultaneously created a \"resource divide.\" State-of-the-art legal intelligence systems typically rely on massive parameter counts (7B+) and cloud-based inference, rendering them inaccessible to practitioners in resource-constrained environments and posing significant data sovereignty risks. This paper introduces Quecto-V1, a domain-specific Small Language Model (SLM) engineered to democratize access to Indian legal intelligence. Built upon a custom configuration of the GPT-2 architecture (124 million parameters), Quecto-V1 was trained from scratch exclusively on a corpus of Indian statutes, including the Indian Penal Code (IPC), the Code of Criminal Procedure (CrPC), and the Constitution of India. Unlike generalist models, which prioritize broad world knowledge, our approach maximizes \"lexical density\" within the legal domain. Furthermore, we address the deployment bottleneck by applying post-training 8-bit quantization (GGUF format), compressing the model to a memory footprint of under 150 MB. Our empirical analysis demonstrates that Quecto-V1 achieves high fidelity in retrieving statutory definitions and penal provisions, outperforming general-purpose SLMs in domain-specific exact match tasks while running entirely offline on consumer-grade CPUs. We further present an ablation study showing that 8-bit quantization yields a 74% reduction in model size with less than 3.5% degradation in retrieval accuracy compared to full-precision baselines. These findings suggest that for specialized, high-stakes domains like law, domain-specific training coupled with aggressive quantization offers a viable, privacy-preserving alternative to monolithic cloud models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid proliferation of Large Language Models (LLMs) has revolutionized Natural Language Processing (NLP) but has simultaneously created a \"resource divide.\" State-of-the-art legal intelligence systems typically rely on massive parameter counts (7B+) and cloud-based inference, rendering them inaccessible to practitioners in resource-constrained environments and posing significant data sovereignty risks. This paper introduces Quecto-V1, a domain-specific Small Language Model (SLM) engineered to democratize access to Indian legal intelligence. Built upon a custom configuration of the GPT-2 architecture (124 million parameters), Quecto-V1 was trained from scratch exclusively on a corpus of Indian statutes, including the Indian Penal Code (IPC), the Code of Criminal Procedure (CrPC), and the Constitution of India. Unlike generalist models, which prioritize broad world knowledge, our approach maximizes \"lexical density\" within the legal domain. Furthermore, we address the deployment bottleneck by applying post-training 8-bit quantization (GGUF format), compressing the model to a memory footprint of under 150 MB. Our empirical analysis demonstrates that Quecto-V1 achieves high fidelity in retrieving statutory definitions and penal provisions, outperforming general-purpose SLMs in domain-specific exact match tasks while running entirely offline on consumer-grade CPUs. We further present an ablation study showing that 8-bit quantization yields a 74% reduction in model size with less than 3.5% degradation in retrieval accuracy compared to full-precision baselines. These findings suggest that for specialized, high-stakes domains like law, domain-specific training coupled with aggressive quantization offers a viable, privacy-preserving alternative to monolithic cloud models."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T17:29:43Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    17,
                    29,
                    43,
                    2,
                    49,
                    0
                ],
                "arxiv_comment": "5 pages, 2 tables",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Subrit Dikshit"
                    }
                ],
                "author_detail": {
                    "name": "Subrit Dikshit"
                },
                "author": "Subrit Dikshit"
            },
            {
                "id": "http://arxiv.org/abs/2602.16639v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16639v1",
                "title": "AREG: Adversarial Resource Extraction Game for Evaluating Persuasion and Resistance in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AREG: Adversarial Resource Extraction Game for Evaluating Persuasion and Resistance in Large Language Models"
                },
                "updated": "2026-02-18T17:28:28Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    17,
                    28,
                    28,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16639v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16639v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Evaluating the social intelligence of Large Language Models (LLMs) increasingly requires moving beyond static text generation toward dynamic, adversarial interaction. We introduce the Adversarial Resource Extraction Game (AREG), a benchmark that operationalizes persuasion and resistance as a multi-turn, zero-sum negotiation over financial resources. Using a round-robin tournament across frontier models, AREG enables joint evaluation of offensive (persuasion) and defensive (resistance) capabilities within a single interactional framework. Our analysis provides evidence that these capabilities are weakly correlated ($ρ= 0.33$) and empirically dissociated: strong persuasive performance does not reliably predict strong resistance, and vice versa. Across all evaluated models, resistance scores exceed persuasion scores, indicating a systematic defensive advantage in adversarial dialogue settings. Further linguistic analysis suggests that interaction structure plays a central role in these outcomes. Incremental commitment-seeking strategies are associated with higher extraction success, while verification-seeking responses are more prevalent in successful defenses than explicit refusal. Together, these findings indicate that social influence in LLMs is not a monolithic capability and that evaluation frameworks focusing on persuasion alone may overlook asymmetric behavioral vulnerabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the social intelligence of Large Language Models (LLMs) increasingly requires moving beyond static text generation toward dynamic, adversarial interaction. We introduce the Adversarial Resource Extraction Game (AREG), a benchmark that operationalizes persuasion and resistance as a multi-turn, zero-sum negotiation over financial resources. Using a round-robin tournament across frontier models, AREG enables joint evaluation of offensive (persuasion) and defensive (resistance) capabilities within a single interactional framework. Our analysis provides evidence that these capabilities are weakly correlated ($ρ= 0.33$) and empirically dissociated: strong persuasive performance does not reliably predict strong resistance, and vice versa. Across all evaluated models, resistance scores exceed persuasion scores, indicating a systematic defensive advantage in adversarial dialogue settings. Further linguistic analysis suggests that interaction structure plays a central role in these outcomes. Incremental commitment-seeking strategies are associated with higher extraction success, while verification-seeking responses are more prevalent in successful defenses than explicit refusal. Together, these findings indicate that social influence in LLMs is not a monolithic capability and that evaluation frameworks focusing on persuasion alone may overlook asymmetric behavioral vulnerabilities."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T17:28:28Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    17,
                    28,
                    28,
                    2,
                    49,
                    0
                ],
                "arxiv_comment": "15 pages, 5 figures, 11 tables. Includes appendix with detailed experimental results and prompts",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Adib Sakhawat"
                    },
                    {
                        "name": "Fardeen Sadab"
                    }
                ],
                "author_detail": {
                    "name": "Fardeen Sadab"
                },
                "author": "Fardeen Sadab"
            },
            {
                "id": "http://arxiv.org/abs/2508.02922v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.02922v2",
                "title": "A multi-stage Bayesian approach to fit spatial point process models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A multi-stage Bayesian approach to fit spatial point process models"
                },
                "updated": "2026-02-18T17:28:15Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    17,
                    28,
                    15,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.02922v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.02922v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Spatial point process (SPP) models are commonly used to analyze point pattern data in many fields, including presence-only data in ecology. Existing exact Bayesian methods for fitting these models are computationally expensive because they require approximating an intractable integral each time parameters are updated and often involve algorithm supervision (i.e., tuning in the Bayesian setting). We propose a flexible, efficient, and exact multi-stage recursive Bayesian approach to fitting SPP models that leverages parallel computing resources to obtain realizations from the joint posterior, which can then be used to obtain inference on derived quantities. We outline potential extensions, including a framework for analyzing study designs with compact observation windows and a neural network basis expansion for increased model flexibility. We demonstrate this approach and its extensions using a simulation study and analyze data from aerial imagery surveys to improve our understanding of spatially explicit abundance of harbor seal (Phoca vitulina) pups in Johns Hopkins Inlet, a protected tidewater glacial fjord in Glacier Bay National Park, Alaska.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatial point process (SPP) models are commonly used to analyze point pattern data in many fields, including presence-only data in ecology. Existing exact Bayesian methods for fitting these models are computationally expensive because they require approximating an intractable integral each time parameters are updated and often involve algorithm supervision (i.e., tuning in the Bayesian setting). We propose a flexible, efficient, and exact multi-stage recursive Bayesian approach to fitting SPP models that leverages parallel computing resources to obtain realizations from the joint posterior, which can then be used to obtain inference on derived quantities. We outline potential extensions, including a framework for analyzing study designs with compact observation windows and a neural network basis expansion for increased model flexibility. We demonstrate this approach and its extensions using a simulation study and analyze data from aerial imagery surveys to improve our understanding of spatially explicit abundance of harbor seal (Phoca vitulina) pups in Johns Hopkins Inlet, a protected tidewater glacial fjord in Glacier Bay National Park, Alaska."
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-04T21:53:05Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    21,
                    53,
                    5,
                    0,
                    216,
                    0
                ],
                "arxiv_comment": "51 pages, 24 figures",
                "arxiv_primary_category": {
                    "term": "stat.ME"
                },
                "authors": [
                    {
                        "name": "Rachael Ren"
                    },
                    {
                        "name": "Mevin B. Hooten"
                    },
                    {
                        "name": "Toryn L. J. Schafer"
                    },
                    {
                        "name": "Nicholas M. Calzada"
                    },
                    {
                        "name": "Benjamin Hoose"
                    },
                    {
                        "name": "Jamie N. Womble"
                    },
                    {
                        "name": "Scott Gende"
                    }
                ],
                "author_detail": {
                    "name": "Scott Gende"
                },
                "author": "Scott Gende"
            },
            {
                "id": "http://arxiv.org/abs/2602.16620v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16620v1",
                "title": "HOLISMOKES XX. Lens models of binary lens galaxies with five images of Supernova Winny",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HOLISMOKES XX. Lens models of binary lens galaxies with five images of Supernova Winny"
                },
                "updated": "2026-02-18T17:15:40Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    17,
                    15,
                    40,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16620v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16620v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Strongly lensed supernovae (SNe) provide a powerful way to study cosmology, SNe and galaxies. Modelling the lens system is key to extracting astrophysical and cosmological information. We present adaptive-optics-assisted high-resolution images of SN Winny (SN 2025wny) in the J and K filters obtained with the Large Binocular Telescope (LBT). The LBT imaging confirms the presence of a fifth point source, whose colour is consistent with that of the other SN images at similar phases, while lens modelling robustly supports its interpretation as an additional image of SN~Winny. We measure the positions of the five SN images with uncertainties varying between 1 and 14 milliarcseconds. We build the first mass models using lenstronomy and GLEE, and explore three classes of mass models for the two lens galaxies G1 and G2. The optimal model class of the three is a singular isothermal ellipsoid for G1, a singular isothermal sphere for G2, and an external shear. We infer the enclosed masses within the Einstein radius as 4.61^{+0.06}_{-0.04} \\times 10^{11}\\,M_\\odot for G1 and 1.01\\pm0.02 \\times 10^{11}\\,M_\\odot for G2. The lensing configuration by the two lens galaxies can produce two additional magnified SN images beyond the five observed ones; the exclusion of such model configurations further constrains the lens model parameters. Our model fits to the observed image positions with an RMS of ~0.0012\" - 0.0025\", within the observed positional uncertainties. The predicted magnifications of the multiple images vary between ~1.6 (for the faintest fifth image E) to ~10 (for the brightest image A). The predicted relative lensing magnifications of the multiple images do not match that of the observed within 2σuncertainties. The differences in the relative magnifications could be due to millilensing/microlensing. Our mass models form the basis for future analyses of this unique system. (abridged)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strongly lensed supernovae (SNe) provide a powerful way to study cosmology, SNe and galaxies. Modelling the lens system is key to extracting astrophysical and cosmological information. We present adaptive-optics-assisted high-resolution images of SN Winny (SN 2025wny) in the J and K filters obtained with the Large Binocular Telescope (LBT). The LBT imaging confirms the presence of a fifth point source, whose colour is consistent with that of the other SN images at similar phases, while lens modelling robustly supports its interpretation as an additional image of SN~Winny. We measure the positions of the five SN images with uncertainties varying between 1 and 14 milliarcseconds. We build the first mass models using lenstronomy and GLEE, and explore three classes of mass models for the two lens galaxies G1 and G2. The optimal model class of the three is a singular isothermal ellipsoid for G1, a singular isothermal sphere for G2, and an external shear. We infer the enclosed masses within the Einstein radius as 4.61^{+0.06}_{-0.04} \\times 10^{11}\\,M_\\odot for G1 and 1.01\\pm0.02 \\times 10^{11}\\,M_\\odot for G2. The lensing configuration by the two lens galaxies can produce two additional magnified SN images beyond the five observed ones; the exclusion of such model configurations further constrains the lens model parameters. Our model fits to the observed image positions with an RMS of ~0.0012\" - 0.0025\", within the observed positional uncertainties. The predicted magnifications of the multiple images vary between ~1.6 (for the faintest fifth image E) to ~10 (for the brightest image A). The predicted relative lensing magnifications of the multiple images do not match that of the observed within 2σuncertainties. The differences in the relative magnifications could be due to millilensing/microlensing. Our mass models form the basis for future analyses of this unique system. (abridged)"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T17:15:40Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    17,
                    15,
                    40,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA"
                },
                "authors": [
                    {
                        "name": "L. R. Ecker"
                    },
                    {
                        "name": "A. G. Schweinfurth"
                    },
                    {
                        "name": "R. Saglia"
                    },
                    {
                        "name": "L. Deng"
                    },
                    {
                        "name": "S. H. Suyu"
                    },
                    {
                        "name": "C. Saulder"
                    },
                    {
                        "name": "J. Snigula"
                    },
                    {
                        "name": "R. Bender"
                    },
                    {
                        "name": "R. Cañameras"
                    },
                    {
                        "name": "T. -W. Chen"
                    },
                    {
                        "name": "A. Galan"
                    },
                    {
                        "name": "A. Halkola"
                    },
                    {
                        "name": "E. Mamuzic"
                    },
                    {
                        "name": "A. Melo"
                    },
                    {
                        "name": "S. Schuldt"
                    },
                    {
                        "name": "S. Taubenberger"
                    }
                ],
                "author_detail": {
                    "name": "S. Taubenberger"
                },
                "author": "S. Taubenberger"
            },
            {
                "id": "http://arxiv.org/abs/2602.16612v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16612v1",
                "title": "Causal and Compositional Abstraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal and Compositional Abstraction"
                },
                "updated": "2026-02-18T17:06:09Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    17,
                    6,
                    9,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16612v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16612v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Abstracting from a low level to a more explanatory high level of description, and ideally while preserving causal structure, is fundamental to scientific practice, to causal inference problems, and to robust, efficient and interpretable AI. We present a general account of abstractions between low and high level models as natural transformations, focusing on the case of causal models. This provides a new formalisation of causal abstraction, unifying several notions in the literature, including constructive causal abstraction, Q-$τ$ consistency, abstractions based on interchange interventions, and `distributed' causal abstractions. Our approach is formalised in terms of category theory, and uses the general notion of a compositional model with a given set of queries and semantics in a monoidal, cd- or Markov category; causal models and their queries such as interventions being special cases. We identify two basic notions of abstraction: downward abstractions mapping queries from high to low level; and upward abstractions, mapping concrete queries such as Do-interventions from low to high. Although usually presented as the latter, we show how common causal abstractions may, more fundamentally, be understood in terms of the former. Our approach also leads us to consider a new stronger notion of `component-level' abstraction, applying to the individual components of a model. In particular, this yields a novel, strengthened form of constructive causal abstraction at the mechanism-level, for which we prove characterisation results. Finally, we show that abstraction can be generalised to further compositional models, including those with a quantum semantics implemented by quantum circuits, and we take first steps in exploring abstractions between quantum compositional circuit models and high-level classical causal models as a means to explainable quantum AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Abstracting from a low level to a more explanatory high level of description, and ideally while preserving causal structure, is fundamental to scientific practice, to causal inference problems, and to robust, efficient and interpretable AI. We present a general account of abstractions between low and high level models as natural transformations, focusing on the case of causal models. This provides a new formalisation of causal abstraction, unifying several notions in the literature, including constructive causal abstraction, Q-$τ$ consistency, abstractions based on interchange interventions, and `distributed' causal abstractions. Our approach is formalised in terms of category theory, and uses the general notion of a compositional model with a given set of queries and semantics in a monoidal, cd- or Markov category; causal models and their queries such as interventions being special cases. We identify two basic notions of abstraction: downward abstractions mapping queries from high to low level; and upward abstractions, mapping concrete queries such as Do-interventions from low to high. Although usually presented as the latter, we show how common causal abstractions may, more fundamentally, be understood in terms of the former. Our approach also leads us to consider a new stronger notion of `component-level' abstraction, applying to the individual components of a model. In particular, this yields a novel, strengthened form of constructive causal abstraction at the mechanism-level, for which we prove characterisation results. Finally, we show that abstraction can be generalised to further compositional models, including those with a quantum semantics implemented by quantum circuits, and we take first steps in exploring abstractions between quantum compositional circuit models and high-level classical causal models as a means to explainable quantum AI."
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.CT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T17:06:09Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    17,
                    6,
                    9,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO"
                },
                "authors": [
                    {
                        "name": "Robin Lorenz"
                    },
                    {
                        "name": "Sean Tull"
                    }
                ],
                "author_detail": {
                    "name": "Sean Tull"
                },
                "author": "Sean Tull"
            },
            {
                "id": "http://arxiv.org/abs/2602.16611v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16611v1",
                "title": "Style-Aware Gloss Control for Generative Non-Photorealistic Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Style-Aware Gloss Control for Generative Non-Photorealistic Rendering"
                },
                "updated": "2026-02-18T17:05:23Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    17,
                    5,
                    23,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16611v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16611v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Humans can infer material characteristics of objects from their visual appearance, and this ability extends to artistic depictions, where similar perceptual strategies guide the interpretation of paintings or drawings. Among the factors that define material appearance, gloss, along with color, is widely regarded as one of the most important, and recent studies indicate that humans can perceive gloss independently of the artistic style used to depict an object. To investigate how gloss and artistic style are represented in learned models, we train an unsupervised generative model on a newly curated dataset of painterly objects designed to systematically vary such factors. Our analysis reveals a hierarchical latent space in which gloss is disentangled from other appearance factors, allowing for a detailed study of how gloss is represented and varies across artistic styles. Building on this representation, we introduce a lightweight adapter that connects our style- and gloss-aware latent space to a latent-diffusion model, enabling the synthesis of non-photorealistic images with fine-grained control of these factors. We compare our approach with previous models and observe improved disentanglement and controllability of the learned factors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans can infer material characteristics of objects from their visual appearance, and this ability extends to artistic depictions, where similar perceptual strategies guide the interpretation of paintings or drawings. Among the factors that define material appearance, gloss, along with color, is widely regarded as one of the most important, and recent studies indicate that humans can perceive gloss independently of the artistic style used to depict an object. To investigate how gloss and artistic style are represented in learned models, we train an unsupervised generative model on a newly curated dataset of painterly objects designed to systematically vary such factors. Our analysis reveals a hierarchical latent space in which gloss is disentangled from other appearance factors, allowing for a detailed study of how gloss is represented and varies across artistic styles. Building on this representation, we introduce a lightweight adapter that connects our style- and gloss-aware latent space to a latent-diffusion model, enabling the synthesis of non-photorealistic images with fine-grained control of these factors. We compare our approach with previous models and observe improved disentanglement and controllability of the learned factors."
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T17:05:23Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    17,
                    5,
                    23,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR"
                },
                "authors": [
                    {
                        "name": "Santiago Jimenez-Navarro"
                    },
                    {
                        "name": "Belen Masia"
                    },
                    {
                        "name": "Ana Serrano"
                    }
                ],
                "author_detail": {
                    "name": "Ana Serrano"
                },
                "author": "Ana Serrano"
            },
            {
                "id": "http://arxiv.org/abs/2602.16610v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16610v1",
                "title": "Who can we trust? LLM-as-a-jury for Comparative Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Who can we trust? LLM-as-a-jury for Comparative Assessment"
                },
                "updated": "2026-02-18T17:04:02Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    17,
                    4,
                    2,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16610v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16610v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) are increasingly applied as automatic evaluators for natural language generation assessment often using pairwise comparative judgements. Existing approaches typically rely on single judges or aggregate multiple judges assuming equal reliability. In practice, LLM judges vary substantially in performance across tasks and aspects, and their judgment probabilities may be biased and inconsistent. Furthermore, human-labelled supervision for judge calibration may be unavailable. We first empirically demonstrate that inconsistencies in LLM comparison probabilities exist and show that it limits the effectiveness of direct probability-based ranking. To address this, we study the LLM-as-a-jury setting and propose BT-sigma, a judge-aware extension of the Bradley-Terry model that introduces a discriminator parameter for each judge to jointly infer item rankings and judge reliability from pairwise comparisons alone. Experiments on benchmark NLG evaluation datasets show that BT-sigma consistently outperforms averaging-based aggregation methods, and that the learned discriminator strongly correlates with independent measures of the cycle consistency of LLM judgments. Further analysis reveals that BT-sigma can be interpreted as an unsupervised calibration mechanism that improves aggregation by modelling judge reliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly applied as automatic evaluators for natural language generation assessment often using pairwise comparative judgements. Existing approaches typically rely on single judges or aggregate multiple judges assuming equal reliability. In practice, LLM judges vary substantially in performance across tasks and aspects, and their judgment probabilities may be biased and inconsistent. Furthermore, human-labelled supervision for judge calibration may be unavailable. We first empirically demonstrate that inconsistencies in LLM comparison probabilities exist and show that it limits the effectiveness of direct probability-based ranking. To address this, we study the LLM-as-a-jury setting and propose BT-sigma, a judge-aware extension of the Bradley-Terry model that introduces a discriminator parameter for each judge to jointly infer item rankings and judge reliability from pairwise comparisons alone. Experiments on benchmark NLG evaluation datasets show that BT-sigma consistently outperforms averaging-based aggregation methods, and that the learned discriminator strongly correlates with independent measures of the cycle consistency of LLM judgments. Further analysis reveals that BT-sigma can be interpreted as an unsupervised calibration mechanism that improves aggregation by modelling judge reliability."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T17:04:02Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    17,
                    4,
                    2,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Mengjie Qian"
                    },
                    {
                        "name": "Guangzhi Sun"
                    },
                    {
                        "name": "Mark J. F. Gales"
                    },
                    {
                        "name": "Kate M. Knill"
                    }
                ],
                "author_detail": {
                    "name": "Kate M. Knill"
                },
                "author": "Kate M. Knill"
            },
            {
                "id": "http://arxiv.org/abs/2602.16607v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16607v1",
                "title": "CitiLink-Summ: Summarization of Discussion Subjects in European Portuguese Municipal Meeting Minutes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CitiLink-Summ: Summarization of Discussion Subjects in European Portuguese Municipal Meeting Minutes"
                },
                "updated": "2026-02-18T17:03:07Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    17,
                    3,
                    7,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16607v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16607v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Municipal meeting minutes are formal records documenting the discussions and decisions of local government, yet their content is often lengthy, dense, and difficult for citizens to navigate. Automatic summarization can help address this challenge by producing concise summaries for each discussion subject. Despite its potential, research on summarizing discussion subjects in municipal meeting minutes remains largely unexplored, especially in low-resource languages, where the inherent complexity of these documents adds further challenges. A major bottleneck is the scarcity of datasets containing high-quality, manually crafted summaries, which limits the development and evaluation of effective summarization models for this domain. In this paper, we present CitiLink-Summ, a new corpus of European Portuguese municipal meeting minutes, comprising 100 documents and 2,322 manually hand-written summaries, each corresponding to a distinct discussion subject. Leveraging this dataset, we establish baseline results for automatic summarization in this domain, employing state-of-the-art generative models (e.g., BART, PRIMERA) as well as large language models (LLMs), evaluated with both lexical and semantic metrics such as ROUGE, BLEU, METEOR, and BERTScore. CitiLink-Summ provides the first benchmark for municipal-domain summarization in European Portuguese, offering a valuable resource for advancing NLP research on complex administrative texts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Municipal meeting minutes are formal records documenting the discussions and decisions of local government, yet their content is often lengthy, dense, and difficult for citizens to navigate. Automatic summarization can help address this challenge by producing concise summaries for each discussion subject. Despite its potential, research on summarizing discussion subjects in municipal meeting minutes remains largely unexplored, especially in low-resource languages, where the inherent complexity of these documents adds further challenges. A major bottleneck is the scarcity of datasets containing high-quality, manually crafted summaries, which limits the development and evaluation of effective summarization models for this domain. In this paper, we present CitiLink-Summ, a new corpus of European Portuguese municipal meeting minutes, comprising 100 documents and 2,322 manually hand-written summaries, each corresponding to a distinct discussion subject. Leveraging this dataset, we establish baseline results for automatic summarization in this domain, employing state-of-the-art generative models (e.g., BART, PRIMERA) as well as large language models (LLMs), evaluated with both lexical and semantic metrics such as ROUGE, BLEU, METEOR, and BERTScore. CitiLink-Summ provides the first benchmark for municipal-domain summarization in European Portuguese, offering a valuable resource for advancing NLP research on complex administrative texts."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T17:03:07Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    17,
                    3,
                    7,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Miguel Marques"
                    },
                    {
                        "name": "Ana Luísa Fernandes"
                    },
                    {
                        "name": "Ana Filipa Pacheco"
                    },
                    {
                        "name": "Rute Rebouças"
                    },
                    {
                        "name": "Inês Cantante"
                    },
                    {
                        "name": "José Isidro"
                    },
                    {
                        "name": "Luís Filipe Cunha"
                    },
                    {
                        "name": "Alípio Jorge"
                    },
                    {
                        "name": "Nuno Guimarães"
                    },
                    {
                        "name": "Sérgio Nunes"
                    },
                    {
                        "name": "António Leal"
                    },
                    {
                        "name": "Purificação Silvano"
                    },
                    {
                        "name": "Ricardo Campos"
                    }
                ],
                "author_detail": {
                    "name": "Ricardo Campos"
                },
                "author": "Ricardo Campos"
            },
            {
                "id": "http://arxiv.org/abs/2602.16603v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16603v1",
                "title": "FlowPrefill: Decoupling Preemption from Prefill Scheduling Granularity to Mitigate Head-of-Line Blocking in LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlowPrefill: Decoupling Preemption from Prefill Scheduling Granularity to Mitigate Head-of-Line Blocking in LLM Serving"
                },
                "updated": "2026-02-18T16:57:45Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    16,
                    57,
                    45,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16603v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16603v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The growing demand for large language models (LLMs) requires serving systems to handle many concurrent requests with diverse service level objectives (SLOs). This exacerbates head-of-line (HoL) blocking during the compute-intensive prefill phase, where long-running requests monopolize resources and delay higher-priority ones, leading to widespread time-to-first-token (TTFT) SLO violations. While chunked prefill enables interruptibility, it introduces an inherent trade-off between responsiveness and throughput: reducing chunk size improves response latency but degrades computational efficiency, whereas increasing chunk size maximizes throughput but exacerbates blocking. This necessitates an adaptive preemption mechanism. However, dynamically balancing execution granularity against scheduling overheads remains a key challenge.\n  In this paper, we propose FlowPrefill, a TTFT-goodput-optimized serving system that resolves this conflict by decoupling preemption granularity from scheduling frequency. To achieve adaptive prefill scheduling, FlowPrefill introduces two key innovations: 1) Operator-Level Preemption, which leverages operator boundaries to enable fine-grained execution interruption without the efficiency loss associated with fixed small chunking; and 2) Event-Driven Scheduling, which triggers scheduling decisions only upon request arrival or completion events, thereby supporting efficient preemption responsiveness while minimizing control-plane overhead. Evaluation on real-world production traces shows that FlowPrefill improves maximum goodput by up to 5.6$\\times$ compared to state-of-the-art systems while satisfying heterogeneous SLOs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing demand for large language models (LLMs) requires serving systems to handle many concurrent requests with diverse service level objectives (SLOs). This exacerbates head-of-line (HoL) blocking during the compute-intensive prefill phase, where long-running requests monopolize resources and delay higher-priority ones, leading to widespread time-to-first-token (TTFT) SLO violations. While chunked prefill enables interruptibility, it introduces an inherent trade-off between responsiveness and throughput: reducing chunk size improves response latency but degrades computational efficiency, whereas increasing chunk size maximizes throughput but exacerbates blocking. This necessitates an adaptive preemption mechanism. However, dynamically balancing execution granularity against scheduling overheads remains a key challenge.\n  In this paper, we propose FlowPrefill, a TTFT-goodput-optimized serving system that resolves this conflict by decoupling preemption granularity from scheduling frequency. To achieve adaptive prefill scheduling, FlowPrefill introduces two key innovations: 1) Operator-Level Preemption, which leverages operator boundaries to enable fine-grained execution interruption without the efficiency loss associated with fixed small chunking; and 2) Event-Driven Scheduling, which triggers scheduling decisions only upon request arrival or completion events, thereby supporting efficient preemption responsiveness while minimizing control-plane overhead. Evaluation on real-world production traces shows that FlowPrefill improves maximum goodput by up to 5.6$\\times$ compared to state-of-the-art systems while satisfying heterogeneous SLOs."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T16:57:45Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    16,
                    57,
                    45,
                    2,
                    49,
                    0
                ],
                "arxiv_comment": "13 pages",
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Chia-chi Hsieh"
                    },
                    {
                        "name": "Zan Zong"
                    },
                    {
                        "name": "Xinyang Chen"
                    },
                    {
                        "name": "Jianjiang Li"
                    },
                    {
                        "name": "Jidong Zhai"
                    },
                    {
                        "name": "Lijie Wen"
                    }
                ],
                "author_detail": {
                    "name": "Lijie Wen"
                },
                "author": "Lijie Wen"
            },
            {
                "id": "http://arxiv.org/abs/2403.00553v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2403.00553v3",
                "title": "Standardizing the Measurement of Text Diversity: A Tool and a Comparative Analysis of Scores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Standardizing the Measurement of Text Diversity: A Tool and a Comparative Analysis of Scores"
                },
                "updated": "2026-02-18T16:51:27Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    16,
                    51,
                    27,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2403.00553v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2403.00553v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The diversity across outputs generated by LLMs shapes perception of their quality and utility. High lexical diversity is often desirable, but there is no standard method to measure this property. Templated answer structures and ``canned'' responses across different documents are readily noticeable, but difficult to visualize across large corpora. This work aims to standardize measurement of text diversity. Specifically, we empirically investigate the convergent validity of existing scores across English texts, and we release diversity, an open-source Python package for measuring and extracting repetition in text. We also build a platform based on diversity for users to interactively explore repetition in text. We find that fast compression algorithms capture information similar to what is measured by slow-to-compute $n$-gram overlap homogeneity scores. Further, a combination of measures -- compression ratios, self-repetition of long $n$-grams, and Self-BLEU and BERTScore -- are sufficient to report, as they have low mutual correlation with each other.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The diversity across outputs generated by LLMs shapes perception of their quality and utility. High lexical diversity is often desirable, but there is no standard method to measure this property. Templated answer structures and ``canned'' responses across different documents are readily noticeable, but difficult to visualize across large corpora. This work aims to standardize measurement of text diversity. Specifically, we empirically investigate the convergent validity of existing scores across English texts, and we release diversity, an open-source Python package for measuring and extracting repetition in text. We also build a platform based on diversity for users to interactively explore repetition in text. We find that fast compression algorithms capture information similar to what is measured by slow-to-compute $n$-gram overlap homogeneity scores. Further, a combination of measures -- compression ratios, self-repetition of long $n$-grams, and Self-BLEU and BERTScore -- are sufficient to report, as they have low mutual correlation with each other."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-03-01T14:23:12Z",
                "published_parsed": [
                    2024,
                    3,
                    1,
                    14,
                    23,
                    12,
                    4,
                    61,
                    0
                ],
                "arxiv_comment": "AACL 2025",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Chantal Shaib"
                    },
                    {
                        "name": "Venkata S. Govindarajan"
                    },
                    {
                        "name": "Joe Barrow"
                    },
                    {
                        "name": "Jiuding Sun"
                    },
                    {
                        "name": "Alexa F. Siu"
                    },
                    {
                        "name": "Byron C. Wallace"
                    },
                    {
                        "name": "Ani Nenkova"
                    }
                ],
                "author_detail": {
                    "name": "Ani Nenkova"
                },
                "author": "Ani Nenkova"
            },
            {
                "id": "http://arxiv.org/abs/2602.16596v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16596v1",
                "title": "Sequential Membership Inference Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential Membership Inference Attacks"
                },
                "updated": "2026-02-18T16:51:13Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    16,
                    51,
                    13,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16596v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16596v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Modern AI models are not static. They go through multiple updates in their lifecycles. Thus, exploiting the model dynamics to create stronger Membership Inference (MI) attacks and tighter privacy audits are timely questions. Though the literature empirically shows that using a sequence of model updates can increase the power of MI attacks, rigorous analysis of the `optimal' MI attacks is limited to static models with infinite samples. Hence, we develop an `optimal' MI attack, SeMI*, that uses the sequence of model updates to identify the presence of a target inserted at a certain update step. For the empirical mean computation, we derive the optimal power of SeMI*, while accessing a finite number of samples with or without privacy. Our results retrieve the existing asymptotic analysis. We observe that having access to the model sequence avoids the dilution of MI signals unlike the existing attacks on the final model, where the MI signal vanishes as training data accumulates. Furthermore, an adversary can use SeMI* to tune both the insertion time and the canary to yield tighter privacy audits. Finally, we conduct experiments across data distributions and models trained or fine-tuned with DP-SGD demonstrating that practical variants of SeMI* lead to tighter privacy audits than the baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern AI models are not static. They go through multiple updates in their lifecycles. Thus, exploiting the model dynamics to create stronger Membership Inference (MI) attacks and tighter privacy audits are timely questions. Though the literature empirically shows that using a sequence of model updates can increase the power of MI attacks, rigorous analysis of the `optimal' MI attacks is limited to static models with infinite samples. Hence, we develop an `optimal' MI attack, SeMI*, that uses the sequence of model updates to identify the presence of a target inserted at a certain update step. For the empirical mean computation, we derive the optimal power of SeMI*, while accessing a finite number of samples with or without privacy. Our results retrieve the existing asymptotic analysis. We observe that having access to the model sequence avoids the dilution of MI signals unlike the existing attacks on the final model, where the MI signal vanishes as training data accumulates. Furthermore, an adversary can use SeMI* to tune both the insertion time and the canary to yield tighter privacy audits. Finally, we conduct experiments across data distributions and models trained or fine-tuned with DP-SGD demonstrating that practical variants of SeMI* lead to tighter privacy audits than the baselines."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T16:51:13Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    16,
                    51,
                    13,
                    2,
                    49,
                    0
                ],
                "arxiv_comment": "27 pages, 10 figures",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Thomas Michel"
                    },
                    {
                        "name": "Debabrota Basu"
                    },
                    {
                        "name": "Emilie Kaufmann"
                    }
                ],
                "author_detail": {
                    "name": "Emilie Kaufmann"
                },
                "author": "Emilie Kaufmann"
            },
            {
                "id": "http://arxiv.org/abs/2602.08755v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.08755v3",
                "title": "Align and Adapt: Multimodal Multiview Human Activity Recognition under Arbitrary View Combinations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Align and Adapt: Multimodal Multiview Human Activity Recognition under Arbitrary View Combinations"
                },
                "updated": "2026-02-18T16:47:35Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    16,
                    47,
                    35,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.08755v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.08755v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Multimodal multiview learning seeks to integrate information from diverse sources to enhance task performance. Existing approaches often struggle with flexible view configurations, including arbitrary view combinations, numbers of views, and heterogeneous modalities. Focusing on the context of human activity recognition, we propose AliAd, a model that combines multiview contrastive learning with a mixture-of-experts module to support arbitrary view availability during both training and inference. Instead of trying to reconstruct missing views, an adjusted center contrastive loss is used for self-supervised representation learning and view alignment, mitigating the impact of missing views on multiview fusion. This loss formulation allows for the integration of view weights to account for view quality. Additionally, it reduces computational complexity from $O(V^2)$ to $O(V)$, where $V$ is the number of views. To address residual discrepancies not captured by contrastive learning, we employ a mixture-of-experts module with a specialized load balancing strategy, tasked with adapting to arbitrary view combinations. We highlight the geometric relationship among components in our model and how they combine well in the latent space. AliAd is validated on four datasets encompassing inertial and human pose modalities, with the number of views ranging from three to nine, demonstrating its performance and flexibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal multiview learning seeks to integrate information from diverse sources to enhance task performance. Existing approaches often struggle with flexible view configurations, including arbitrary view combinations, numbers of views, and heterogeneous modalities. Focusing on the context of human activity recognition, we propose AliAd, a model that combines multiview contrastive learning with a mixture-of-experts module to support arbitrary view availability during both training and inference. Instead of trying to reconstruct missing views, an adjusted center contrastive loss is used for self-supervised representation learning and view alignment, mitigating the impact of missing views on multiview fusion. This loss formulation allows for the integration of view weights to account for view quality. Additionally, it reduces computational complexity from $O(V^2)$ to $O(V)$, where $V$ is the number of views. To address residual discrepancies not captured by contrastive learning, we employ a mixture-of-experts module with a specialized load balancing strategy, tasked with adapting to arbitrary view combinations. We highlight the geometric relationship among components in our model and how they combine well in the latent space. AliAd is validated on four datasets encompassing inertial and human pose modalities, with the number of views ranging from three to nine, demonstrating its performance and flexibility."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-09T14:58:08Z",
                "published_parsed": [
                    2026,
                    2,
                    9,
                    14,
                    58,
                    8,
                    0,
                    40,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Duc-Anh Nguyen"
                    },
                    {
                        "name": "Nhien-An Le-Khac"
                    }
                ],
                "author_detail": {
                    "name": "Nhien-An Le-Khac"
                },
                "author": "Nhien-An Le-Khac"
            },
            {
                "id": "http://arxiv.org/abs/2602.16594v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16594v1",
                "title": "Decentralized and Fully Onboard: Range-Aided Cooperative Localization and Navigation on Micro Aerial Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized and Fully Onboard: Range-Aided Cooperative Localization and Navigation on Micro Aerial Vehicles"
                },
                "updated": "2026-02-18T16:46:15Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    16,
                    46,
                    15,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16594v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Controlling a team of robots in a coordinated manner is challenging because centralized approaches (where all computation is performed on a central machine) scale poorly, and globally referenced external localization systems may not always be available. In this work, we consider the problem of range-aided decentralized localization and formation control. In such a setting, each robot estimates its relative pose by combining data only from onboard odometry sensors and distance measurements to other robots in the team. Additionally, each robot calculates the control inputs necessary to collaboratively navigate an environment to accomplish a specific task, for example, moving in a desired formation while monitoring an area. We present a block coordinate descent approach to localization that does not require strict coordination between the robots. We present a novel formulation for formation control as inference on factor graphs that takes into account the state estimation uncertainty and can be solved efficiently. Our approach to range-aided localization and formation-based navigation is completely decentralized, does not require specialized trajectories to maintain formation, and achieves decimeter-level positioning and formation control accuracy. We demonstrate our approach through multiple real experiments involving formation flights in diverse indoor and outdoor environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controlling a team of robots in a coordinated manner is challenging because centralized approaches (where all computation is performed on a central machine) scale poorly, and globally referenced external localization systems may not always be available. In this work, we consider the problem of range-aided decentralized localization and formation control. In such a setting, each robot estimates its relative pose by combining data only from onboard odometry sensors and distance measurements to other robots in the team. Additionally, each robot calculates the control inputs necessary to collaboratively navigate an environment to accomplish a specific task, for example, moving in a desired formation while monitoring an area. We present a block coordinate descent approach to localization that does not require strict coordination between the robots. We present a novel formulation for formation control as inference on factor graphs that takes into account the state estimation uncertainty and can be solved efficiently. Our approach to range-aided localization and formation-based navigation is completely decentralized, does not require specialized trajectories to maintain formation, and achieves decimeter-level positioning and formation control accuracy. We demonstrate our approach through multiple real experiments involving formation flights in diverse indoor and outdoor environments."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T16:46:15Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    16,
                    46,
                    15,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Abhishek Goudar"
                    },
                    {
                        "name": "Angela P. Schoellig"
                    }
                ],
                "author_detail": {
                    "name": "Angela P. Schoellig"
                },
                "author": "Angela P. Schoellig"
            },
            {
                "id": "http://arxiv.org/abs/2505.10992v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.10992v2",
                "title": "ReaCritic: Reasoning Transformer-based DRL Critic-model Scaling For Wireless Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReaCritic: Reasoning Transformer-based DRL Critic-model Scaling For Wireless Networks"
                },
                "updated": "2026-02-18T16:45:13Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    16,
                    45,
                    13,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.10992v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.10992v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Heterogeneous Networks (HetNets) pose critical challenges for intelligent management due to the diverse user requirements and time-varying wireless conditions. These factors introduce significant decision complexity, which limits the adaptability of existing Deep Reinforcement Learning (DRL) methods. In many DRL algorithms, especially those involving value-based or actor-critic structures, the critic component plays a key role in guiding policy learning by estimating value functions. However, conventional critic models often use shallow architectures that map observations directly to scalar estimates, limiting their ability to handle multi-task complexity. In contrast, recent progress in inference-time scaling of Large Language Models (LLMs) has shown that generating intermediate reasoning steps can significantly improve decision quality. Motivated by this, we propose ReaCritic, a reasoning transformer-based critic-model scaling scheme that brings reasoning-like ability into DRL. ReaCritic performs horizontal reasoning over parallel state-action inputs and vertical reasoning through deep transformer stacks. It is compatible with a broad range of value-based and actor-critic DRL algorithms and enhances generalization in dynamic wireless environments. Extensive experiments demonstrate that ReaCritic improves convergence speed and final performance across various HetNet settings and standard OpenAI Gym control tasks. The code of ReaCritic is available at https://github.com/NICE-HKU/ReaCritic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous Networks (HetNets) pose critical challenges for intelligent management due to the diverse user requirements and time-varying wireless conditions. These factors introduce significant decision complexity, which limits the adaptability of existing Deep Reinforcement Learning (DRL) methods. In many DRL algorithms, especially those involving value-based or actor-critic structures, the critic component plays a key role in guiding policy learning by estimating value functions. However, conventional critic models often use shallow architectures that map observations directly to scalar estimates, limiting their ability to handle multi-task complexity. In contrast, recent progress in inference-time scaling of Large Language Models (LLMs) has shown that generating intermediate reasoning steps can significantly improve decision quality. Motivated by this, we propose ReaCritic, a reasoning transformer-based critic-model scaling scheme that brings reasoning-like ability into DRL. ReaCritic performs horizontal reasoning over parallel state-action inputs and vertical reasoning through deep transformer stacks. It is compatible with a broad range of value-based and actor-critic DRL algorithms and enhances generalization in dynamic wireless environments. Extensive experiments demonstrate that ReaCritic improves convergence speed and final performance across various HetNet settings and standard OpenAI Gym control tasks. The code of ReaCritic is available at https://github.com/NICE-HKU/ReaCritic."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-16T08:42:08Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    8,
                    42,
                    8,
                    4,
                    136,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Feiran You"
                    },
                    {
                        "name": "Hongyang Du"
                    }
                ],
                "author_detail": {
                    "name": "Hongyang Du"
                },
                "author": "Hongyang Du"
            },
            {
                "id": "http://arxiv.org/abs/2602.15689v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.15689v2",
                "title": "A Content-Based Framework for Cybersecurity Refusal Decisions in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Content-Based Framework for Cybersecurity Refusal Decisions in Large Language Models"
                },
                "updated": "2026-02-18T16:42:07Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    16,
                    42,
                    7,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.15689v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.15689v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models and LLM-based agents are increasingly used for cybersecurity tasks that are inherently dual-use. Existing approaches to refusal, spanning academic policy frameworks and commercially deployed systems, often rely on broad topic-based bans or offensive-focused taxonomies. As a result, they can yield inconsistent decisions, over-restrict legitimate defenders, and behave brittlely under obfuscation or request segmentation. We argue that effective refusal requires explicitly modeling the trade-off between offensive risk and defensive benefit, rather than relying solely on intent or offensive classification. In this paper, we introduce a content-based framework for designing and auditing cyber refusal policies that makes offense-defense tradeoffs explicit. The framework characterizes requests along five dimensions: Offensive Action Contribution, Offensive Risk, Technical Complexity, Defensive Benefit, and Expected Frequency for Legitimate Users, grounded in the technical substance of the request rather than stated intent. We demonstrate that this content-grounded approach resolves inconsistencies in current frontier model behavior and allows organizations to construct tunable, risk-aware refusal policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models and LLM-based agents are increasingly used for cybersecurity tasks that are inherently dual-use. Existing approaches to refusal, spanning academic policy frameworks and commercially deployed systems, often rely on broad topic-based bans or offensive-focused taxonomies. As a result, they can yield inconsistent decisions, over-restrict legitimate defenders, and behave brittlely under obfuscation or request segmentation. We argue that effective refusal requires explicitly modeling the trade-off between offensive risk and defensive benefit, rather than relying solely on intent or offensive classification. In this paper, we introduce a content-based framework for designing and auditing cyber refusal policies that makes offense-defense tradeoffs explicit. The framework characterizes requests along five dimensions: Offensive Action Contribution, Offensive Risk, Technical Complexity, Defensive Benefit, and Expected Frequency for Legitimate Users, grounded in the technical substance of the request rather than stated intent. We demonstrate that this content-grounded approach resolves inconsistencies in current frontier model behavior and allows organizations to construct tunable, risk-aware refusal policies."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-17T16:12:21Z",
                "published_parsed": [
                    2026,
                    2,
                    17,
                    16,
                    12,
                    21,
                    1,
                    48,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Noa Linder"
                    },
                    {
                        "name": "Meirav Segal"
                    },
                    {
                        "name": "Omer Antverg"
                    },
                    {
                        "name": "Gil Gekker"
                    },
                    {
                        "name": "Tomer Fichman"
                    },
                    {
                        "name": "Omri Bodenheimer"
                    },
                    {
                        "name": "Edan Maor"
                    },
                    {
                        "name": "Omer Nevo"
                    }
                ],
                "author_detail": {
                    "name": "Omer Nevo"
                },
                "author": "Omer Nevo"
            },
            {
                "id": "http://arxiv.org/abs/2602.16589v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16589v1",
                "title": "Isospin dependence of nuclear EMC effect from global QCD analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Isospin dependence of nuclear EMC effect from global QCD analysis"
                },
                "updated": "2026-02-18T16:40:36Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    16,
                    40,
                    36,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16589v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16589v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We perform a new global QCD analysis of unpolarized parton distribution functions (PDFs) in the nucleon from proton, deuteron and $A=3$ data, including recent measurements of $^3$He/$D$ and $^3$H/$D$ cross section ratios from the MARATHON experiment at Jefferson Lab. Simultaneously inferring the PDFs and nucleon off-shell corrections allows both to be determined consistently, without theoretical assumptions about the isospin dependence of nuclear effects. The analysis provides strong evidence for the need of nucleon off-shell corrections to describe the $A=3$ data, and suggests the presence of both isoscalar and isovector nuclear effects in $A \\leq 3$ nuclei. We find that the extracted EMC ratios of nuclear to nucleon structure functions for $A=2$ and 3 differ from those naively extrapolated from heavy nuclei down to low $A$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We perform a new global QCD analysis of unpolarized parton distribution functions (PDFs) in the nucleon from proton, deuteron and $A=3$ data, including recent measurements of $^3$He/$D$ and $^3$H/$D$ cross section ratios from the MARATHON experiment at Jefferson Lab. Simultaneously inferring the PDFs and nucleon off-shell corrections allows both to be determined consistently, without theoretical assumptions about the isospin dependence of nuclear effects. The analysis provides strong evidence for the need of nucleon off-shell corrections to describe the $A=3$ data, and suggests the presence of both isoscalar and isovector nuclear effects in $A \\leq 3$ nuclei. We find that the extracted EMC ratios of nuclear to nucleon structure functions for $A=2$ and 3 differ from those naively extrapolated from heavy nuclei down to low $A$."
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T16:40:36Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    16,
                    40,
                    36,
                    2,
                    49,
                    0
                ],
                "arxiv_comment": "32 pages, 10 figures",
                "arxiv_primary_category": {
                    "term": "hep-ph"
                },
                "authors": [
                    {
                        "name": "C. Cocuzza"
                    },
                    {
                        "name": "T. J. Hague"
                    },
                    {
                        "name": "W. Melnitchouk"
                    },
                    {
                        "name": "N. Sato"
                    },
                    {
                        "name": "A. W. Thomas"
                    }
                ],
                "author_detail": {
                    "name": "A. W. Thomas"
                },
                "author": "A. W. Thomas"
            },
            {
                "id": "http://arxiv.org/abs/2503.12286v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2503.12286v2",
                "title": "Integrating Chain-of-Thought and Retrieval Augmented Generation Enhances Rare Disease Diagnosis from Clinical Notes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Chain-of-Thought and Retrieval Augmented Generation Enhances Rare Disease Diagnosis from Clinical Notes"
                },
                "updated": "2026-02-18T16:38:37Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    16,
                    38,
                    37,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2503.12286v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2503.12286v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Background: Several studies show that large language models (LLMs) struggle with phenotype-driven gene prioritization for rare diseases. These studies typically use Human Phenotype Ontology (HPO) terms to prompt foundation models like GPT and LLaMA to predict candidate genes. However, in real-world settings, foundation models are not optimized for domain-specific tasks like clinical diagnosis, yet inputs are unstructured clinical notes rather than standardized terms. How LLMs can be instructed to predict candidate genes or disease diagnosis from unstructured clinical notes remains a major challenge. Methods: We introduce RAG-driven CoT and CoT-driven RAG, two methods that combine Chain-of-Thought (CoT) and Retrieval Augmented Generation (RAG) to analyze clinical notes. A five-question CoT protocol mimics expert reasoning, while RAG retrieves data from sources like HPO and OMIM (Online Mendelian Inheritance in Man). We evaluated these approaches on rare disease datasets, including 5,980 Phenopacket-derived notes, 255 literature-based narratives, and 220 in-house clinical notes from Childrens Hospital of Philadelphia. Results: We found that recent foundations models, including Llama 3.3-70B-Instruct and DeepSeek-R1-Distill-Llama-70B, outperformed earlier versions such as Llama 2 and GPT-3.5. We also showed that RAG-driven CoT and CoT-driven RAG both outperform foundation models in candidate gene prioritization from clinical notes; in particular, both methods with DeepSeek backbone resulted in a top-10 gene accuracy of over 40% on Phenopacket-derived clinical notes. RAG-driven CoT works better for high-quality notes, where early retrieval can anchor the subsequent reasoning steps in domain-specific evidence, while CoT-driven RAG has advantage when processing lengthy and noisy notes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Several studies show that large language models (LLMs) struggle with phenotype-driven gene prioritization for rare diseases. These studies typically use Human Phenotype Ontology (HPO) terms to prompt foundation models like GPT and LLaMA to predict candidate genes. However, in real-world settings, foundation models are not optimized for domain-specific tasks like clinical diagnosis, yet inputs are unstructured clinical notes rather than standardized terms. How LLMs can be instructed to predict candidate genes or disease diagnosis from unstructured clinical notes remains a major challenge. Methods: We introduce RAG-driven CoT and CoT-driven RAG, two methods that combine Chain-of-Thought (CoT) and Retrieval Augmented Generation (RAG) to analyze clinical notes. A five-question CoT protocol mimics expert reasoning, while RAG retrieves data from sources like HPO and OMIM (Online Mendelian Inheritance in Man). We evaluated these approaches on rare disease datasets, including 5,980 Phenopacket-derived notes, 255 literature-based narratives, and 220 in-house clinical notes from Childrens Hospital of Philadelphia. Results: We found that recent foundations models, including Llama 3.3-70B-Instruct and DeepSeek-R1-Distill-Llama-70B, outperformed earlier versions such as Llama 2 and GPT-3.5. We also showed that RAG-driven CoT and CoT-driven RAG both outperform foundation models in candidate gene prioritization from clinical notes; in particular, both methods with DeepSeek backbone resulted in a top-10 gene accuracy of over 40% on Phenopacket-derived clinical notes. RAG-driven CoT works better for high-quality notes, where early retrieval can anchor the subsequent reasoning steps in domain-specific evidence, while CoT-driven RAG has advantage when processing lengthy and noisy notes."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-03-15T22:57:31Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    22,
                    57,
                    31,
                    5,
                    74,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Zhanliang Wang"
                    },
                    {
                        "name": "Da Wu"
                    },
                    {
                        "name": "Quan Nguyen"
                    },
                    {
                        "name": "Kai Wang"
                    }
                ],
                "author_detail": {
                    "name": "Kai Wang"
                },
                "author": "Kai Wang"
            },
            {
                "id": "http://arxiv.org/abs/2602.16587v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16587v1",
                "title": "Why Thinking Hurts? Diagnosing and Rectifying the Reasoning Shift in Foundation Recommender Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why Thinking Hurts? Diagnosing and Rectifying the Reasoning Shift in Foundation Recommender Models"
                },
                "updated": "2026-02-18T16:38:21Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    16,
                    38,
                    21,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16587v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16587v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Integrating Chain-of-Thought (CoT) reasoning into Semantic ID-based recommendation foundation models (such as OpenOneRec) often paradoxically degrades recommendation performance. We identify the root cause as textual inertia from the General Subspace, where verbose reasoning dominates inference and causes the model to neglect critical Semantic ID. To address this, we propose a training-free Inference-Time Subspace Alignment framework. By compressing reasoning chains and applying bias-subtracted contrastive decoding, our approach mitigates ungrounded textual drift. Experiments show this effectively calibrates inference, allowing foundation models to leverage reasoning without sacrificing ID-grounded accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Chain-of-Thought (CoT) reasoning into Semantic ID-based recommendation foundation models (such as OpenOneRec) often paradoxically degrades recommendation performance. We identify the root cause as textual inertia from the General Subspace, where verbose reasoning dominates inference and causes the model to neglect critical Semantic ID. To address this, we propose a training-free Inference-Time Subspace Alignment framework. By compressing reasoning chains and applying bias-subtracted contrastive decoding, our approach mitigates ungrounded textual drift. Experiments show this effectively calibrates inference, allowing foundation models to leverage reasoning without sacrificing ID-grounded accuracy."
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T16:38:21Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    16,
                    38,
                    21,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR"
                },
                "authors": [
                    {
                        "name": "Luankang Zhang"
                    },
                    {
                        "name": "Yonghao Huang"
                    },
                    {
                        "name": "Hang Lv"
                    },
                    {
                        "name": "Mingjia Yin"
                    },
                    {
                        "name": "Liangyue Li"
                    },
                    {
                        "name": "Zulong Chen"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Enhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Enhong Chen"
                },
                "author": "Enhong Chen"
            },
            {
                "id": "http://arxiv.org/abs/2602.16571v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16571v1",
                "title": "Utility-Preserving De-Identification for Math Tutoring: Investigating Numeric Ambiguity in the MathEd-PII Benchmark Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Utility-Preserving De-Identification for Math Tutoring: Investigating Numeric Ambiguity in the MathEd-PII Benchmark Dataset"
                },
                "updated": "2026-02-18T16:12:46Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    16,
                    12,
                    46,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16571v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16571v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large-scale sharing of dialogue-based data is instrumental for advancing the science of teaching and learning, yet rigorous de-identification remains a major barrier. In mathematics tutoring transcripts, numeric expressions frequently resemble structured identifiers (e.g., dates or IDs), leading generic Personally Identifiable Information (PII) detection systems to over-redact core instructional content and reduce dataset utility. This work asks how PII can be detected in math tutoring transcripts while preserving their educational utility. To address this challenge, we investigate the \"numeric ambiguity\" problem and introduce MathEd-PII, the first benchmark dataset for PII detection in math tutoring dialogues, created through a human-in-the-loop LLM workflow that audits upstream redactions and generates privacy-preserving surrogates. The dataset contains 1,000 tutoring sessions (115,620 messages; 769,628 tokens) with validated PII annotations. Using a density-based segmentation method, we show that false PII redactions are disproportionately concentrated in math-dense regions, confirming numeric ambiguity as a key failure mode. We then compare four detection strategies: a Presidio baseline and LLM-based approaches with basic, math-aware, and segment-aware prompting. Math-aware prompting substantially improves performance over the baseline (F1: 0.821 vs. 0.379) while reducing numeric false positives, demonstrating that de-identification must incorporate domain context to preserve analytic utility. This work provides both a new benchmark and evidence that utility-preserving de-identification for tutoring data requires domain-aware modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale sharing of dialogue-based data is instrumental for advancing the science of teaching and learning, yet rigorous de-identification remains a major barrier. In mathematics tutoring transcripts, numeric expressions frequently resemble structured identifiers (e.g., dates or IDs), leading generic Personally Identifiable Information (PII) detection systems to over-redact core instructional content and reduce dataset utility. This work asks how PII can be detected in math tutoring transcripts while preserving their educational utility. To address this challenge, we investigate the \"numeric ambiguity\" problem and introduce MathEd-PII, the first benchmark dataset for PII detection in math tutoring dialogues, created through a human-in-the-loop LLM workflow that audits upstream redactions and generates privacy-preserving surrogates. The dataset contains 1,000 tutoring sessions (115,620 messages; 769,628 tokens) with validated PII annotations. Using a density-based segmentation method, we show that false PII redactions are disproportionately concentrated in math-dense regions, confirming numeric ambiguity as a key failure mode. We then compare four detection strategies: a Presidio baseline and LLM-based approaches with basic, math-aware, and segment-aware prompting. Math-aware prompting substantially improves performance over the baseline (F1: 0.821 vs. 0.379) while reducing numeric false positives, demonstrating that de-identification must incorporate domain context to preserve analytic utility. This work provides both a new benchmark and evidence that utility-preserving de-identification for tutoring data requires domain-aware modeling."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T16:12:46Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    16,
                    12,
                    46,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Zhuqian Zhou"
                    },
                    {
                        "name": "Kirk Vanacore"
                    },
                    {
                        "name": "Bakhtawar Ahtisham"
                    },
                    {
                        "name": "Jinsook Lee"
                    },
                    {
                        "name": "Doug Pietrzak"
                    },
                    {
                        "name": "Daryl Hedley"
                    },
                    {
                        "name": "Jorge Dias"
                    },
                    {
                        "name": "Chris Shaw"
                    },
                    {
                        "name": "Ruth Schäfer"
                    },
                    {
                        "name": "René F. Kizilcec"
                    }
                ],
                "author_detail": {
                    "name": "René F. Kizilcec"
                },
                "author": "René F. Kizilcec"
            },
            {
                "id": "http://arxiv.org/abs/2602.16570v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16570v1",
                "title": "Steering diffusion models with quadratic rewards: a fine-grained analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steering diffusion models with quadratic rewards: a fine-grained analysis"
                },
                "updated": "2026-02-18T16:11:17Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    16,
                    11,
                    17,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16570v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Inference-time algorithms are an emerging paradigm in which pre-trained models are used as subroutines to solve downstream tasks. Such algorithms have been proposed for tasks ranging from inverse problems and guided image generation to reasoning. However, the methods currently deployed in practice are heuristics with a variety of failure modes -- and we have very little understanding of when these heuristics can be efficiently improved.\n  In this paper, we consider the task of sampling from a reward-tilted diffusion model -- that is, sampling from $p^{\\star}(x) \\propto p(x) \\exp(r(x))$ -- given a reward function $r$ and pre-trained diffusion oracle for $p$. We provide a fine-grained analysis of the computational tractability of this task for quadratic rewards $r(x) = x^\\top A x + b^\\top x$. We show that linear-reward tilts are always efficiently sampleable -- a simple result that seems to have gone unnoticed in the literature. We use this as a building block, along with a conceptually new ingredient -- the Hubbard-Stratonovich transform -- to provide an efficient algorithm for sampling from low-rank positive-definite quadratic tilts, i.e. $r(x) = x^\\top A x$ where $A$ is positive-definite and of rank $O(1)$. For negative-definite tilts, i.e. $r(x) = - x^\\top A x$ where $A$ is positive-definite, we prove that the problem is intractable even if $A$ is of rank 1 (albeit with exponentially-large entries).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-time algorithms are an emerging paradigm in which pre-trained models are used as subroutines to solve downstream tasks. Such algorithms have been proposed for tasks ranging from inverse problems and guided image generation to reasoning. However, the methods currently deployed in practice are heuristics with a variety of failure modes -- and we have very little understanding of when these heuristics can be efficiently improved.\n  In this paper, we consider the task of sampling from a reward-tilted diffusion model -- that is, sampling from $p^{\\star}(x) \\propto p(x) \\exp(r(x))$ -- given a reward function $r$ and pre-trained diffusion oracle for $p$. We provide a fine-grained analysis of the computational tractability of this task for quadratic rewards $r(x) = x^\\top A x + b^\\top x$. We show that linear-reward tilts are always efficiently sampleable -- a simple result that seems to have gone unnoticed in the literature. We use this as a building block, along with a conceptually new ingredient -- the Hubbard-Stratonovich transform -- to provide an efficient algorithm for sampling from low-rank positive-definite quadratic tilts, i.e. $r(x) = x^\\top A x$ where $A$ is positive-definite and of rank $O(1)$. For negative-definite tilts, i.e. $r(x) = - x^\\top A x$ where $A$ is positive-definite, we prove that the problem is intractable even if $A$ is of rank 1 (albeit with exponentially-large entries)."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T16:11:17Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    16,
                    11,
                    17,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Ankur Moitra"
                    },
                    {
                        "name": "Andrej Risteski"
                    },
                    {
                        "name": "Dhruv Rohatgi"
                    }
                ],
                "author_detail": {
                    "name": "Dhruv Rohatgi"
                },
                "author": "Dhruv Rohatgi"
            },
            {
                "id": "http://arxiv.org/abs/2601.07611v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.07611v2",
                "title": "DIAGPaper: Diagnosing Valid and Specific Weaknesses in Scientific Papers via Multi-Agent Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DIAGPaper: Diagnosing Valid and Specific Weaknesses in Scientific Papers via Multi-Agent Reasoning"
                },
                "updated": "2026-02-18T16:09:49Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    16,
                    9,
                    49,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.07611v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.07611v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Paper weakness identification using single-agent or multi-agent LLMs has attracted increasing attention, yet existing approaches exhibit key limitations. Many multi-agent systems simulate human roles at a surface level, missing the underlying criteria that lead experts to assess complementary intellectual aspects of a paper. Moreover, prior methods implicitly assume identified weaknesses are valid, ignoring reviewer bias, misunderstanding, and the critical role of author rebuttals in validating review quality. Finally, most systems output unranked weakness lists, rather than prioritizing the most consequential issues for users. In this work, we propose DIAGPaper, a novel multi-agent framework that addresses these challenges through three tightly integrated modules. The customizer module simulates human-defined review criteria and instantiates multiple reviewer agents with criterion-specific expertise. The rebuttal module introduces author agents that engage in structured debate with reviewer agents to validate and refine proposed weaknesses. The prioritizer module learns from large-scale human review practices to assess the severity of validated weaknesses and surfaces the top-K severest ones to users. Experiments on two benchmarks, AAAR and ReviewCritique, demonstrate that DIAGPaper substantially outperforms existing methods by producing more valid and more paper-specific weaknesses, while presenting them in a user-oriented, prioritized manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Paper weakness identification using single-agent or multi-agent LLMs has attracted increasing attention, yet existing approaches exhibit key limitations. Many multi-agent systems simulate human roles at a surface level, missing the underlying criteria that lead experts to assess complementary intellectual aspects of a paper. Moreover, prior methods implicitly assume identified weaknesses are valid, ignoring reviewer bias, misunderstanding, and the critical role of author rebuttals in validating review quality. Finally, most systems output unranked weakness lists, rather than prioritizing the most consequential issues for users. In this work, we propose DIAGPaper, a novel multi-agent framework that addresses these challenges through three tightly integrated modules. The customizer module simulates human-defined review criteria and instantiates multiple reviewer agents with criterion-specific expertise. The rebuttal module introduces author agents that engage in structured debate with reviewer agents to validate and refine proposed weaknesses. The prioritizer module learns from large-scale human review practices to assess the severity of validated weaknesses and surfaces the top-K severest ones to users. Experiments on two benchmarks, AAAR and ReviewCritique, demonstrate that DIAGPaper substantially outperforms existing methods by producing more valid and more paper-specific weaknesses, while presenting them in a user-oriented, prioritized manner."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-12T14:59:00Z",
                "published_parsed": [
                    2026,
                    1,
                    12,
                    14,
                    59,
                    0,
                    0,
                    12,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Zhuoyang Zou"
                    },
                    {
                        "name": "Abolfazl Ansari"
                    },
                    {
                        "name": "Delvin Ce Zhang"
                    },
                    {
                        "name": "Dongwon Lee"
                    },
                    {
                        "name": "Wenpeng Yin"
                    }
                ],
                "author_detail": {
                    "name": "Wenpeng Yin"
                },
                "author": "Wenpeng Yin"
            },
            {
                "id": "http://arxiv.org/abs/2602.16567v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16567v1",
                "title": "Scattering and sputtering on the lunar surface; Insights from negative ions observed at the surface",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scattering and sputtering on the lunar surface; Insights from negative ions observed at the surface"
                },
                "updated": "2026-02-18T16:09:02Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    16,
                    9,
                    2,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16567v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16567v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Context. Airless planetary bodies are directly exposed to solar wind ions, which can scatter or become implanted upon impact with the regolith-covered surface, while also sputtering surface atoms.\n  Aims. We construct a semi-analytical model for the scattering of ions of hundreds of eV and the sputtering of surface atoms, both resulting in the emission of negative ions from the lunar surface. Our model contains a novel description of the scattering process that is physics-based and constrained by observations.\n  Methods. We use data from the Negative Ions at the Lunar Surface (NILS) instrument on the Chang'e-6 lander to update prior knowledge of ion scattering and sputtering from lunar regolith through Bayesian inference.\n  Results. Our model shows good agreement with the NILS data. A precipitating solar wind proton has roughly a 22% chance of scattering from the lunar surface in any charge state, and about an 8% chance of sputtering a surface hydrogen atom. The resulting ratio of scattered to sputtered hydrogen flux is eta_sc / eta_sp = 1.5 for a proton speed of 300 km/s. We find a high probability (7-20%) that a hydrogen atom leaves the surface negatively charged. The angular emission distributions at near-grazing angles for both scattered and sputtered fluxes are controlled by surface roughness. Our model also indicates significant inelastic energy losses for hydrogen interacting with the regolith, suggesting a longer effective path length than previously assumed. Finally, we estimate a surface binding energy of 5.5 eV, consistent with the observations.\n  Conclusions. Our model describes the scattering and sputtering of particles of any charge state from any homogeneous, multi-species surface. Using NILS data, we successfully applied the model to update our understanding of solar wind interacting with lunar regolith, and the emission of negative hydrogen ions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context. Airless planetary bodies are directly exposed to solar wind ions, which can scatter or become implanted upon impact with the regolith-covered surface, while also sputtering surface atoms.\n  Aims. We construct a semi-analytical model for the scattering of ions of hundreds of eV and the sputtering of surface atoms, both resulting in the emission of negative ions from the lunar surface. Our model contains a novel description of the scattering process that is physics-based and constrained by observations.\n  Methods. We use data from the Negative Ions at the Lunar Surface (NILS) instrument on the Chang'e-6 lander to update prior knowledge of ion scattering and sputtering from lunar regolith through Bayesian inference.\n  Results. Our model shows good agreement with the NILS data. A precipitating solar wind proton has roughly a 22% chance of scattering from the lunar surface in any charge state, and about an 8% chance of sputtering a surface hydrogen atom. The resulting ratio of scattered to sputtered hydrogen flux is eta_sc / eta_sp = 1.5 for a proton speed of 300 km/s. We find a high probability (7-20%) that a hydrogen atom leaves the surface negatively charged. The angular emission distributions at near-grazing angles for both scattered and sputtered fluxes are controlled by surface roughness. Our model also indicates significant inelastic energy losses for hydrogen interacting with the regolith, suggesting a longer effective path length than previously assumed. Finally, we estimate a surface binding energy of 5.5 eV, consistent with the observations.\n  Conclusions. Our model describes the scattering and sputtering of particles of any charge state from any homogeneous, multi-species surface. Using NILS data, we successfully applied the model to update our understanding of solar wind interacting with lunar regolith, and the emission of negative hydrogen ions."
                },
                "tags": [
                    {
                        "term": "physics.space-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T16:09:02Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    16,
                    9,
                    2,
                    2,
                    49,
                    0
                ],
                "arxiv_comment": "27 pages, 15 figures",
                "arxiv_primary_category": {
                    "term": "physics.space-ph"
                },
                "authors": [
                    {
                        "name": "Romain Canu-Blot"
                    },
                    {
                        "name": "Martin Wieser"
                    },
                    {
                        "name": "Umberto Rollero"
                    },
                    {
                        "name": "Thomas Maynadié"
                    },
                    {
                        "name": "Stas Barabash"
                    },
                    {
                        "name": "Gabriella Stenberg Wieser"
                    },
                    {
                        "name": "Aibing Zhang"
                    },
                    {
                        "name": "Wenjing Wang"
                    },
                    {
                        "name": "Chi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Chi Wang"
                },
                "author": "Chi Wang"
            },
            {
                "id": "http://arxiv.org/abs/2510.23350v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.23350v2",
                "title": "Validating Formal Specifications with LLM-generated Test Cases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Validating Formal Specifications with LLM-generated Test Cases"
                },
                "updated": "2026-02-18T16:04:20Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    16,
                    4,
                    20,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.23350v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.23350v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Validation is a central activity when developing formal specifications. Similarly to coding, a possible validation technique is to define upfront test cases or scenarios that a future specification should satisfy or not. Unfortunately, specifying such test cases is burdensome and error prone, which could cause users to skip this validation task. This paper reports the results of an empirical evaluation of using pre-trained large language models (LLMs) to automate the generation of test cases from natural language requirements. In particular, we focus on test cases for structural requirements of simple domain models formalized in the Alloy specification language. Our evaluation focuses on the state-of-the-art GPT-5 model, but results from other closed- and open-source LLMs are also reported. The results show that, in this context, GPT-5 is already quite effective at generating positive (and negative) test cases that are syntactically correct and that satisfy (or not) the given requirement, and that can detect many wrong specifications written by humans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Validation is a central activity when developing formal specifications. Similarly to coding, a possible validation technique is to define upfront test cases or scenarios that a future specification should satisfy or not. Unfortunately, specifying such test cases is burdensome and error prone, which could cause users to skip this validation task. This paper reports the results of an empirical evaluation of using pre-trained large language models (LLMs) to automate the generation of test cases from natural language requirements. In particular, we focus on test cases for structural requirements of simple domain models formalized in the Alloy specification language. Our evaluation focuses on the state-of-the-art GPT-5 model, but results from other closed- and open-source LLMs are also reported. The results show that, in this context, GPT-5 is already quite effective at generating positive (and negative) test cases that are syntactically correct and that satisfy (or not) the given requirement, and that can detect many wrong specifications written by humans."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-27T14:02:20Z",
                "published_parsed": [
                    2025,
                    10,
                    27,
                    14,
                    2,
                    20,
                    0,
                    300,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Alcino Cunha"
                    },
                    {
                        "name": "Nuno Macedo"
                    }
                ],
                "author_detail": {
                    "name": "Nuno Macedo"
                },
                "author": "Nuno Macedo"
            },
            {
                "id": "http://arxiv.org/abs/2508.12907v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.12907v4",
                "title": "SNAP-UQ: Self-supervised Next-Activation Prediction for Single-Pass Uncertainty in TinyML",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SNAP-UQ: Self-supervised Next-Activation Prediction for Single-Pass Uncertainty in TinyML"
                },
                "updated": "2026-02-18T15:56:15Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    15,
                    56,
                    15,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.12907v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.12907v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Reliable uncertainty estimation is a key missing piece for on-device monitoring in TinyML: microcontrollers must detect failures, distribution shift, or accuracy drops under strict flash/latency budgets, yet common uncertainty approaches (deep ensembles, MC dropout, early exits, temporal buffering) typically require multiple passes, extra branches, or state that is impractical on milliwatt hardware. This paper proposes a novel and practical method, SNAP-UQ, for single-pass, label-free uncertainty estimation based on depth-wise next-activation prediction. SNAP-UQ taps a small set of backbone layers and uses tiny int8 heads to predict the mean and scale of the next activation from a low-rank projection of the previous one; the resulting standardized prediction error forms a depth-wise surprisal signal that is aggregated and mapped through a lightweight monotone calibrator into an actionable uncertainty score. The design introduces no temporal buffers or auxiliary exits and preserves state-free inference, while increasing deployment footprint by only a few tens of kilobytes. Across vision and audio backbones, SNAP-UQ reduces flash and latency relative to early-exit and deep-ensemble baselines (typically $\\sim$40--60% smaller and $\\sim$25--35% faster), with several competing methods at similar accuracy often exceeding MCU memory limits. On corrupted streams, it improves accuracy-drop event detection by multiple AUPRC points and maintains strong failure detection (AUROC $\\approx 0.9$) in a single forward pass. By grounding uncertainty in layer-to-layer dynamics rather than solely in output confidence, SNAP-UQ offers a novel, resource-efficient basis for robust TinyML monitoring. Our code is available at: https://github.com/Ism-ail11/SNAP-UQ",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable uncertainty estimation is a key missing piece for on-device monitoring in TinyML: microcontrollers must detect failures, distribution shift, or accuracy drops under strict flash/latency budgets, yet common uncertainty approaches (deep ensembles, MC dropout, early exits, temporal buffering) typically require multiple passes, extra branches, or state that is impractical on milliwatt hardware. This paper proposes a novel and practical method, SNAP-UQ, for single-pass, label-free uncertainty estimation based on depth-wise next-activation prediction. SNAP-UQ taps a small set of backbone layers and uses tiny int8 heads to predict the mean and scale of the next activation from a low-rank projection of the previous one; the resulting standardized prediction error forms a depth-wise surprisal signal that is aggregated and mapped through a lightweight monotone calibrator into an actionable uncertainty score. The design introduces no temporal buffers or auxiliary exits and preserves state-free inference, while increasing deployment footprint by only a few tens of kilobytes. Across vision and audio backbones, SNAP-UQ reduces flash and latency relative to early-exit and deep-ensemble baselines (typically $\\sim$40--60% smaller and $\\sim$25--35% faster), with several competing methods at similar accuracy often exceeding MCU memory limits. On corrupted streams, it improves accuracy-drop event detection by multiple AUPRC points and maintains strong failure detection (AUROC $\\approx 0.9$) in a single forward pass. By grounding uncertainty in layer-to-layer dynamics rather than solely in output confidence, SNAP-UQ offers a novel, resource-efficient basis for robust TinyML monitoring. Our code is available at: https://github.com/Ism-ail11/SNAP-UQ"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-18T13:14:20Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    13,
                    14,
                    20,
                    0,
                    230,
                    0
                ],
                "arxiv_comment": "Published as a conference paper at ICLR 2026",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Ismail Lamaakal"
                    },
                    {
                        "name": "Chaymae Yahyati"
                    },
                    {
                        "name": "Khalid El Makkaoui"
                    },
                    {
                        "name": "Ibrahim Ouahbi"
                    },
                    {
                        "name": "Yassine Maleh"
                    }
                ],
                "author_detail": {
                    "name": "Yassine Maleh"
                },
                "author": "Yassine Maleh"
            },
            {
                "id": "http://arxiv.org/abs/2602.04375v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.04375v2",
                "title": "Angle dependent dose transformer algorithm for fast proton therapy dose calculations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Angle dependent dose transformer algorithm for fast proton therapy dose calculations"
                },
                "updated": "2026-02-18T15:53:42Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    15,
                    53,
                    42,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.04375v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.04375v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Accurate 3D dose calculation for Pencil Beam Scanning Proton Therapy (PBSPT) is typically performed with Monte Carlo (MC) engines, but their runtimes limit adaptive workflows and repeated evaluations. Current deep-learning proton dose engines often require orthogonality between proton rays and the CT grid, forcing computationally expensive beamlet-wise 3D reinterpolation. We propose the Angle-dependent Dose Transformer Algorithm (ADoTA), which eliminates grid rotation by augmenting the model input with a fast analytical beamlet-shape projection that explicitly encodes beam direction. The model was trained on CT data from 108 patients to predict beamlet dose distributions for initial energies of $70$--$270\\,\\mathrm{MeV}$ over an $80\\times110\\,\\mathrm{mm}^2$ field, and tested on an independent cohort of 50 patients. On the test set, gamma pass rates $(1\\%,3\\,\\mathrm{mm})$ were $99.40\\pm0.86\\%$ (thorax) and $99.87\\pm0.23\\%$ (abdomen/pelvis). Single-beamlet inference took $1.72\\pm0.8\\,\\mathrm{ms}$. By avoiding reinterpolation, end-to-end 3D dose computation was reduced by $\\approx86\\%$ relative to the fastest published reinterpolation-based methods. For full treatment plans, gamma pass rates $Γ(2\\%,2\\,\\mathrm{mm})$ with a 10\\% dose cut-off reached $98.4\\%$ (lung) and $98.9\\%$ (prostate). ADoTA provides an angle-aware deep-learning proton dose engine that preserves MC-level accuracy across heterogeneous anatomies while substantially reducing computational overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate 3D dose calculation for Pencil Beam Scanning Proton Therapy (PBSPT) is typically performed with Monte Carlo (MC) engines, but their runtimes limit adaptive workflows and repeated evaluations. Current deep-learning proton dose engines often require orthogonality between proton rays and the CT grid, forcing computationally expensive beamlet-wise 3D reinterpolation. We propose the Angle-dependent Dose Transformer Algorithm (ADoTA), which eliminates grid rotation by augmenting the model input with a fast analytical beamlet-shape projection that explicitly encodes beam direction. The model was trained on CT data from 108 patients to predict beamlet dose distributions for initial energies of $70$--$270\\,\\mathrm{MeV}$ over an $80\\times110\\,\\mathrm{mm}^2$ field, and tested on an independent cohort of 50 patients. On the test set, gamma pass rates $(1\\%,3\\,\\mathrm{mm})$ were $99.40\\pm0.86\\%$ (thorax) and $99.87\\pm0.23\\%$ (abdomen/pelvis). Single-beamlet inference took $1.72\\pm0.8\\,\\mathrm{ms}$. By avoiding reinterpolation, end-to-end 3D dose computation was reduced by $\\approx86\\%$ relative to the fastest published reinterpolation-based methods. For full treatment plans, gamma pass rates $Γ(2\\%,2\\,\\mathrm{mm})$ with a 10\\% dose cut-off reached $98.4\\%$ (lung) and $98.9\\%$ (prostate). ADoTA provides an angle-aware deep-learning proton dose engine that preserves MC-level accuracy across heterogeneous anatomies while substantially reducing computational overhead."
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-04T09:52:00Z",
                "published_parsed": [
                    2026,
                    2,
                    4,
                    9,
                    52,
                    0,
                    2,
                    35,
                    0
                ],
                "arxiv_comment": "27 pages, 15 Figures, 7 Tables Code can be found: https://github.com/Medical-Physics-Technology/adota",
                "arxiv_primary_category": {
                    "term": "physics.med-ph"
                },
                "authors": [
                    {
                        "name": "Mikołaj Stryja"
                    },
                    {
                        "name": "Danny Lathouwers"
                    },
                    {
                        "name": "Zoltán Perkó"
                    }
                ],
                "author_detail": {
                    "name": "Zoltán Perkó"
                },
                "author": "Zoltán Perkó"
            },
            {
                "id": "http://arxiv.org/abs/2602.16551v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16551v1",
                "title": "Automated Extraction of Mechanical Constitutive Models from Scientific Literature using Large Language Models: Applications in Cultural Heritage Conservation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Extraction of Mechanical Constitutive Models from Scientific Literature using Large Language Models: Applications in Cultural Heritage Conservation"
                },
                "updated": "2026-02-18T15:53:15Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    15,
                    53,
                    15,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16551v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16551v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The preservation of cultural heritage is increasingly transitioning towards data-driven predictive maintenance and \"Digital Twin\" construction. However, the mechanical constitutive models required for high-fidelity simulations remain fragmented across decades of unstructured scientific literature, creating a \"Data Silo\" that hinders conservation engineering. To address this, we present an automated, two-stage agentic framework leveraging Large Language Models (LLMs) to extract mechanical constitutive equations, calibrated parameters, and metadata from PDF documents. The workflow employs a resource-efficient \"Gatekeeper\" agent for relevance filtering and a high-capability \"Analyst\" agent for fine-grained extraction, featuring a novel Context-Aware Symbolic Grounding mechanism to resolve mathematical ambiguities. Applied to a corpus of over 2,000 research papers, the system successfully isolated 113 core documents and constructed a structured database containing 185 constitutive model instances and over 450 calibrated parameters. The extraction precision reached 80.4\\%, establishing a highly efficient \"Human-in-the-loop\" workflow that reduces manual data curation time by approximately 90\\%. We demonstrate the system's utility through a web-based Knowledge Retrieval Platform, which enables rapid parameter discovery for computational modeling. This work transforms scattered literature into a queryable digital asset, laying the data foundation for the \"Digital Material Twin\" of built heritage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The preservation of cultural heritage is increasingly transitioning towards data-driven predictive maintenance and \"Digital Twin\" construction. However, the mechanical constitutive models required for high-fidelity simulations remain fragmented across decades of unstructured scientific literature, creating a \"Data Silo\" that hinders conservation engineering. To address this, we present an automated, two-stage agentic framework leveraging Large Language Models (LLMs) to extract mechanical constitutive equations, calibrated parameters, and metadata from PDF documents. The workflow employs a resource-efficient \"Gatekeeper\" agent for relevance filtering and a high-capability \"Analyst\" agent for fine-grained extraction, featuring a novel Context-Aware Symbolic Grounding mechanism to resolve mathematical ambiguities. Applied to a corpus of over 2,000 research papers, the system successfully isolated 113 core documents and constructed a structured database containing 185 constitutive model instances and over 450 calibrated parameters. The extraction precision reached 80.4\\%, establishing a highly efficient \"Human-in-the-loop\" workflow that reduces manual data curation time by approximately 90\\%. We demonstrate the system's utility through a web-based Knowledge Retrieval Platform, which enables rapid parameter discovery for computational modeling. This work transforms scattered literature into a queryable digital asset, laying the data foundation for the \"Digital Material Twin\" of built heritage."
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T15:53:15Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    15,
                    53,
                    15,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB"
                },
                "authors": [
                    {
                        "name": "Rui Hu"
                    },
                    {
                        "name": "Yue Wu"
                    },
                    {
                        "name": "Tianhao Su"
                    },
                    {
                        "name": "Yin Wang"
                    },
                    {
                        "name": "Shunbo Hu"
                    },
                    {
                        "name": "Jizhong Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jizhong Huang"
                },
                "author": "Jizhong Huang"
            },
            {
                "id": "http://arxiv.org/abs/2509.22237v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.22237v2",
                "title": "FeatBench: Towards More Realistic Evaluation of Feature-level Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FeatBench: Towards More Realistic Evaluation of Feature-level Code Generation"
                },
                "updated": "2026-02-18T15:49:02Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    15,
                    49,
                    2,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.22237v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.22237v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Evaluating Large Language Models (LLMs) on repository-level feature implementation is a critical frontier in software engineering. However, establishing a benchmark that faithfully mirrors realistic development scenarios remains a significant challenge. Existing feature-level benchmarks generally suffer from two primary limitations: unrealistic task inputs enriched with code hints and significant data leakage risks due to their static nature. To address these limitations, we propose a new benchmark - FeatBench, which introduces the following advances: (1) Realistic Task Inputs. Task inputs consist solely of natural language requirements, strictly devoid of code hints (e.g., function signatures). This format mirrors realistic software development by requiring agents to independently bridge the gap between abstract user intent and concrete code changes. (2) Evolving Data. FeatBench employs a fully automated pipeline to construct new benchmark versions from the latest repositories, effectively mitigating data contamination. The initial release comprises 157 tasks sourced from 27 actively maintained repositories. We evaluate two state-of-the-art agent frameworks with four leading LLMs on FeatBench. The results reveal that FeatBench poses a significant challenge, with the highest resolved rate reaching only 29.94%. Crucially, our analysis uncovers a prevalent behavioral pattern of aggressive implementation, which leads to \"scope creep\" and widespread regressions where agents break existing features by diverging from the user's explicit intent. We release FeatBench, our automated pipeline, and all experimental results to facilitate further community research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Large Language Models (LLMs) on repository-level feature implementation is a critical frontier in software engineering. However, establishing a benchmark that faithfully mirrors realistic development scenarios remains a significant challenge. Existing feature-level benchmarks generally suffer from two primary limitations: unrealistic task inputs enriched with code hints and significant data leakage risks due to their static nature. To address these limitations, we propose a new benchmark - FeatBench, which introduces the following advances: (1) Realistic Task Inputs. Task inputs consist solely of natural language requirements, strictly devoid of code hints (e.g., function signatures). This format mirrors realistic software development by requiring agents to independently bridge the gap between abstract user intent and concrete code changes. (2) Evolving Data. FeatBench employs a fully automated pipeline to construct new benchmark versions from the latest repositories, effectively mitigating data contamination. The initial release comprises 157 tasks sourced from 27 actively maintained repositories. We evaluate two state-of-the-art agent frameworks with four leading LLMs on FeatBench. The results reveal that FeatBench poses a significant challenge, with the highest resolved rate reaching only 29.94%. Crucially, our analysis uncovers a prevalent behavioral pattern of aggressive implementation, which leads to \"scope creep\" and widespread regressions where agents break existing features by diverging from the user's explicit intent. We release FeatBench, our automated pipeline, and all experimental results to facilitate further community research."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-26T11:47:50Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    11,
                    47,
                    50,
                    4,
                    269,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Haorui Chen"
                    },
                    {
                        "name": "Chengze Li"
                    },
                    {
                        "name": "Jia Li"
                    }
                ],
                "author_detail": {
                    "name": "Jia Li"
                },
                "author": "Jia Li"
            },
            {
                "id": "http://arxiv.org/abs/2409.12019v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2409.12019v3",
                "title": "Asymptotics for conformal inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asymptotics for conformal inference"
                },
                "updated": "2026-02-18T15:42:26Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    15,
                    42,
                    26,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2409.12019v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2409.12019v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Conformal inference is a versatile tool for building prediction sets in regression or classification. We study the false coverage proportion (FCP) in a simultaneous inference setting with a calibration sample of $n$ points and a test sample of $m$ points. We identify the exact, distribution-free, asymptotic distribution of the FCP when both $n$ and $m$ tend to infinity. This shows in particular that FCP control can be achieved by using the well-known Kolmogorov distribution, and puts forward that the asymptotic variance is decreasing in the ratio $n/m$. We then provide a number of extensions by considering the problems of novelty detection, weighted conformal inference or distribution shift between the calibration sample and the test sample. In particular, our asymptotic results allow to accurately quantify the asymptotic behavior of the errors (a miscovering interval or declaring a false novelty) when weighted conformal inference is used.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conformal inference is a versatile tool for building prediction sets in regression or classification. We study the false coverage proportion (FCP) in a simultaneous inference setting with a calibration sample of $n$ points and a test sample of $m$ points. We identify the exact, distribution-free, asymptotic distribution of the FCP when both $n$ and $m$ tend to infinity. This shows in particular that FCP control can be achieved by using the well-known Kolmogorov distribution, and puts forward that the asymptotic variance is decreasing in the ratio $n/m$. We then provide a number of extensions by considering the problems of novelty detection, weighted conformal inference or distribution shift between the calibration sample and the test sample. In particular, our asymptotic results allow to accurately quantify the asymptotic behavior of the errors (a miscovering interval or declaring a false novelty) when weighted conformal inference is used."
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-09-18T14:30:06Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    14,
                    30,
                    6,
                    2,
                    262,
                    0
                ],
                "arxiv_comment": "39 pages, 3 figures, 2 tables",
                "arxiv_primary_category": {
                    "term": "math.ST"
                },
                "authors": [
                    {
                        "name": "Ulysse Gazin"
                    }
                ],
                "author_detail": {
                    "name": "Ulysse Gazin"
                },
                "author": "Ulysse Gazin"
            },
            {
                "id": "http://arxiv.org/abs/2602.16540v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16540v1",
                "title": "Generalised Linear Models Driven by Latent Processes: Asymptotic Theory and Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalised Linear Models Driven by Latent Processes: Asymptotic Theory and Applications"
                },
                "updated": "2026-02-18T15:39:25Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    15,
                    39,
                    25,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16540v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This paper introduces a class of generalised linear models (GLMs) driven by latent processes for modelling count, real-valued, binary, and positive continuous time series. Extending earlier latent-process regression frameworks based on Poisson or one-parameter exponential family assumptions, we allow the conditional distribution of the response to belong to a bi-parameter exponential family, with the latent process entering the conditional mean multiplicatively. This formulation substantially broadens the scope of latent-process GLMs, for instance, it naturally accommodates gamma responses for positive continuous data, enables estimation of an unknown dispersion parameter via method of moments, and avoids restrictive conditions on link functions that arise under existing formulations. We establish the asymptotic normality of the GLM estimators obtained from the GLM likelihood that ignores the latent process, and we derive the correct information matrix for valid inference. In addition, we provide a principled approach to prediction and forecasting in GLMs driven by latent processes, a topic not previously addressed in the literature. We present two real data applications on measles infections in North Rhine-Westphalia (Germany) and paleoclimatic glacial varves, which highlight the practical advantages and enhanced flexibility of the proposed modelling framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a class of generalised linear models (GLMs) driven by latent processes for modelling count, real-valued, binary, and positive continuous time series. Extending earlier latent-process regression frameworks based on Poisson or one-parameter exponential family assumptions, we allow the conditional distribution of the response to belong to a bi-parameter exponential family, with the latent process entering the conditional mean multiplicatively. This formulation substantially broadens the scope of latent-process GLMs, for instance, it naturally accommodates gamma responses for positive continuous data, enables estimation of an unknown dispersion parameter via method of moments, and avoids restrictive conditions on link functions that arise under existing formulations. We establish the asymptotic normality of the GLM estimators obtained from the GLM likelihood that ignores the latent process, and we derive the correct information matrix for valid inference. In addition, we provide a principled approach to prediction and forecasting in GLMs driven by latent processes, a topic not previously addressed in the literature. We present two real data applications on measles infections in North Rhine-Westphalia (Germany) and paleoclimatic glacial varves, which highlight the practical advantages and enhanced flexibility of the proposed modelling framework."
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T15:39:25Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    15,
                    39,
                    25,
                    2,
                    49,
                    0
                ],
                "arxiv_comment": "Paper submitted for publication",
                "arxiv_primary_category": {
                    "term": "stat.ME"
                },
                "authors": [
                    {
                        "name": "Wagner Barreto-Souza"
                    },
                    {
                        "name": "Ngai Hang Chan"
                    }
                ],
                "author_detail": {
                    "name": "Ngai Hang Chan"
                },
                "author": "Ngai Hang Chan"
            },
            {
                "id": "http://arxiv.org/abs/2504.09103v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2504.09103v4",
                "title": "IMPACT: Behavioral Intention-aware Multimodal Trajectory Prediction with Adaptive Context Trimming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IMPACT: Behavioral Intention-aware Multimodal Trajectory Prediction with Adaptive Context Trimming"
                },
                "updated": "2026-02-18T15:15:54Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    15,
                    15,
                    54,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2504.09103v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2504.09103v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "While most prior research has focused on improving the precision of multimodal trajectory predictions, the explicit modeling of multimodal behavioral intentions (e.g., yielding, overtaking) remains relatively underexplored. This paper proposes a unified framework that jointly predicts both behavioral intentions and trajectories to enhance prediction accuracy, interpretability, and efficiency. Specifically, we employ a shared context encoder for both intention and trajectory predictions, thereby reducing structural redundancy and information loss. Moreover, we address the lack of ground-truth behavioral intention labels in mainstream datasets (Waymo, Argoverse) by auto-labeling these datasets, thus advancing the community's efforts in this direction. We further introduce a vectorized occupancy prediction module that infers the probability of each map polyline being occupied by the target vehicle's future trajectory. By leveraging these intention and occupancy prediction priors, our method conducts dynamic, modality-dependent pruning of irrelevant agents and map polylines in the decoding stage, effectively reducing computational overhead and mitigating noise from non-critical elements. Our approach ranks first among LiDAR-free methods on the Waymo Motion Dataset and achieves first place on the Waymo Interactive Prediction Dataset. Remarkably, even without model ensembling, our single-model framework improves the soft mean average precision (softmAP) by 10 percent compared to the second-best method in the Waymo Interactive Prediction Leaderboard. Furthermore, the proposed framework has been successfully deployed on real vehicles, demonstrating its practical effectiveness in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While most prior research has focused on improving the precision of multimodal trajectory predictions, the explicit modeling of multimodal behavioral intentions (e.g., yielding, overtaking) remains relatively underexplored. This paper proposes a unified framework that jointly predicts both behavioral intentions and trajectories to enhance prediction accuracy, interpretability, and efficiency. Specifically, we employ a shared context encoder for both intention and trajectory predictions, thereby reducing structural redundancy and information loss. Moreover, we address the lack of ground-truth behavioral intention labels in mainstream datasets (Waymo, Argoverse) by auto-labeling these datasets, thus advancing the community's efforts in this direction. We further introduce a vectorized occupancy prediction module that infers the probability of each map polyline being occupied by the target vehicle's future trajectory. By leveraging these intention and occupancy prediction priors, our method conducts dynamic, modality-dependent pruning of irrelevant agents and map polylines in the decoding stage, effectively reducing computational overhead and mitigating noise from non-critical elements. Our approach ranks first among LiDAR-free methods on the Waymo Motion Dataset and achieves first place on the Waymo Interactive Prediction Dataset. Remarkably, even without model ensembling, our single-model framework improves the soft mean average precision (softmAP) by 10 percent compared to the second-best method in the Waymo Interactive Prediction Leaderboard. Furthermore, the proposed framework has been successfully deployed on real vehicles, demonstrating its practical effectiveness in real-world applications."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-04-12T06:56:36Z",
                "published_parsed": [
                    2025,
                    4,
                    12,
                    6,
                    56,
                    36,
                    5,
                    102,
                    0
                ],
                "arxiv_comment": "accepted by IEEE Robotics and Automation Letters",
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Jiawei Sun"
                    },
                    {
                        "name": "Xibin Yue"
                    },
                    {
                        "name": "Jiahui Li"
                    },
                    {
                        "name": "Tianle Shen"
                    },
                    {
                        "name": "Chengran Yuan"
                    },
                    {
                        "name": "Shuo Sun"
                    },
                    {
                        "name": "Sheng Guo"
                    },
                    {
                        "name": "Quanyun Zhou"
                    },
                    {
                        "name": "Marcelo H Ang"
                    }
                ],
                "author_detail": {
                    "name": "Marcelo H Ang"
                },
                "author": "Marcelo H Ang"
            },
            {
                "id": "http://arxiv.org/abs/2602.16520v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16520v1",
                "title": "Recursive language models for jailbreak detection: a procedural defense for tool-augmented agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recursive language models for jailbreak detection: a procedural defense for tool-augmented agents"
                },
                "updated": "2026-02-18T15:07:09Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    15,
                    7,
                    9,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16520v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16520v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Jailbreak prompts are a practical and evolving threat to large language models (LLMs), particularly in agentic systems that execute tools over untrusted content. Many attacks exploit long-context hiding, semantic camouflage, and lightweight obfuscations that can evade single-pass guardrails. We present RLM-JB, an end-to-end jailbreak detection framework built on Recursive Language Models (RLMs), in which a root model orchestrates a bounded analysis program that transforms the input, queries worker models over covered segments, and aggregates evidence into an auditable decision. RLM-JB treats detection as a procedure rather than a one-shot classification: it normalizes and de-obfuscates suspicious inputs, chunks text to reduce context dilution and guarantee coverage, performs parallel chunk screening, and composes cross-chunk signals to recover split-payload attacks. On AutoDAN-style adversarial inputs, RLM-JB achieves high detection effectiveness across three LLM backends (ASR/Recall 92.5-98.0%) while maintaining very high precision (98.99-100%) and low false positive rates (0.0-2.0%), highlighting a practical sensitivity-specificity trade-off as the screening backend changes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreak prompts are a practical and evolving threat to large language models (LLMs), particularly in agentic systems that execute tools over untrusted content. Many attacks exploit long-context hiding, semantic camouflage, and lightweight obfuscations that can evade single-pass guardrails. We present RLM-JB, an end-to-end jailbreak detection framework built on Recursive Language Models (RLMs), in which a root model orchestrates a bounded analysis program that transforms the input, queries worker models over covered segments, and aggregates evidence into an auditable decision. RLM-JB treats detection as a procedure rather than a one-shot classification: it normalizes and de-obfuscates suspicious inputs, chunks text to reduce context dilution and guarantee coverage, performs parallel chunk screening, and composes cross-chunk signals to recover split-payload attacks. On AutoDAN-style adversarial inputs, RLM-JB achieves high detection effectiveness across three LLM backends (ASR/Recall 92.5-98.0%) while maintaining very high precision (98.99-100%) and low false positive rates (0.0-2.0%), highlighting a practical sensitivity-specificity trade-off as the screening backend changes."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T15:07:09Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    15,
                    7,
                    9,
                    2,
                    49,
                    0
                ],
                "arxiv_comment": "5 pages and 1 figure. Appendix: an additional 5 pages",
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Doron Shavit"
                    }
                ],
                "author_detail": {
                    "name": "Doron Shavit"
                },
                "author": "Doron Shavit"
            },
            {
                "id": "http://arxiv.org/abs/2602.16519v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16519v1",
                "title": "The role of radiative torques in the molecular cloud core L43",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The role of radiative torques in the molecular cloud core L43"
                },
                "updated": "2026-02-18T15:05:33Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    15,
                    5,
                    33,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16519v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16519v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1051/0004-6361/202557363",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Polarized emission from interstellar dust grains is commonly used to infer information about the underlying magnetic field from the diffuse interstellar medium to molecular cloud cores. Therefore, the ability to accurately determine properties of the magnetic field requires a thorough understanding of the dust alignment mechanism. We investigate the influence of anisotropic radiation fields on the alignment of dust particles by magnetic fields, known as radiative torque (RAT) alignment. Specifically, we take advantage of the unique spatial configuration of the molecular cloud core L43, which contains an embedded yet optically visible star acting as a local source of anisotropic illumination. Based on polarization maps obtained at wavelengths of $154 μ\\mathrm{m}$ (SOFIA/HAWC+), as well as $450 μ\\mathrm{m}$ and $850 μ\\mathrm{m}$ (JCMT/SCUBA-2), which show variations in the degree and angle of polarized emission across all wavelengths, we applied the differential measure analysis method to infer magnetic field strengths and analyze the global polarization spectrum of this source. We derived plane-of-sky magnetic field strengths ranging from approximately 13 to 60 $μ\\mathrm{G}$, varying with wavelength, and find a negative slope of the polarization spectrum. Compared to 3D radiative transfer simulations, this finding can be attributed, at least partially, to variations in dust properties and temperatures along the line of sight. However, the additional influence of variations in the magnetic field orientation along the line of sight cannot be ruled out. Our results favor radiative torques as the primary alignment mechanism, as they indicate that the degree of polarization is dependent on temperature and hence the strength of the local radiation field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Polarized emission from interstellar dust grains is commonly used to infer information about the underlying magnetic field from the diffuse interstellar medium to molecular cloud cores. Therefore, the ability to accurately determine properties of the magnetic field requires a thorough understanding of the dust alignment mechanism. We investigate the influence of anisotropic radiation fields on the alignment of dust particles by magnetic fields, known as radiative torque (RAT) alignment. Specifically, we take advantage of the unique spatial configuration of the molecular cloud core L43, which contains an embedded yet optically visible star acting as a local source of anisotropic illumination. Based on polarization maps obtained at wavelengths of $154 μ\\mathrm{m}$ (SOFIA/HAWC+), as well as $450 μ\\mathrm{m}$ and $850 μ\\mathrm{m}$ (JCMT/SCUBA-2), which show variations in the degree and angle of polarized emission across all wavelengths, we applied the differential measure analysis method to infer magnetic field strengths and analyze the global polarization spectrum of this source. We derived plane-of-sky magnetic field strengths ranging from approximately 13 to 60 $μ\\mathrm{G}$, varying with wavelength, and find a negative slope of the polarization spectrum. Compared to 3D radiative transfer simulations, this finding can be attributed, at least partially, to variations in dust properties and temperatures along the line of sight. However, the additional influence of variations in the magnetic field orientation along the line of sight cannot be ruled out. Our results favor radiative torques as the primary alignment mechanism, as they indicate that the degree of polarization is dependent on temperature and hence the strength of the local radiation field."
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T15:05:33Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    15,
                    5,
                    33,
                    2,
                    49,
                    0
                ],
                "arxiv_comment": "15 pages, 12 figures",
                "arxiv_primary_category": {
                    "term": "astro-ph.SR"
                },
                "arxiv_journal_ref": "Astronomy & Astrophysics, 705, A207 (2026)",
                "authors": [
                    {
                        "name": "Marco Leon Scheiter"
                    },
                    {
                        "name": "Sebastian Wolf"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Wolf"
                },
                "author": "Sebastian Wolf",
                "arxiv_doi": "10.1051/0004-6361/202557363"
            },
            {
                "id": "http://arxiv.org/abs/2602.16516v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16516v1",
                "title": "Supercharging Agenda Setting Research: The ParlaCAP Dataset of 28 European Parliaments and a Scalable Multilingual LLM-Based Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supercharging Agenda Setting Research: The ParlaCAP Dataset of 28 European Parliaments and a Scalable Multilingual LLM-Based Classification"
                },
                "updated": "2026-02-18T15:04:30Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    15,
                    4,
                    30,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16516v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16516v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This paper introduces ParlaCAP, a large-scale dataset for analyzing parliamentary agenda setting across Europe, and proposes a cost-effective method for building domain-specific policy topic classifiers. Applying the Comparative Agendas Project (CAP) schema to the multilingual ParlaMint corpus of over 8 million speeches from 28 parliaments of European countries and autonomous regions, we follow a teacher-student framework in which a high-performing large language model (LLM) annotates in-domain training data and a multilingual encoder model is fine-tuned on these annotations for scalable data annotation. We show that this approach produces a classifier tailored to the target domain. Agreement between the LLM and human annotators is comparable to inter-annotator agreement among humans, and the resulting model outperforms existing CAP classifiers trained on manually-annotated but out-of-domain data. In addition to the CAP annotations, the ParlaCAP dataset offers rich speaker and party metadata, as well as sentiment predictions coming from the ParlaSent multilingual transformer model, enabling comparative research on political attention and representation across countries. We illustrate the analytical potential of the dataset with three use cases, examining the distribution of parliamentary attention across policy topics, sentiment patterns in parliamentary speech, and gender differences in policy attention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces ParlaCAP, a large-scale dataset for analyzing parliamentary agenda setting across Europe, and proposes a cost-effective method for building domain-specific policy topic classifiers. Applying the Comparative Agendas Project (CAP) schema to the multilingual ParlaMint corpus of over 8 million speeches from 28 parliaments of European countries and autonomous regions, we follow a teacher-student framework in which a high-performing large language model (LLM) annotates in-domain training data and a multilingual encoder model is fine-tuned on these annotations for scalable data annotation. We show that this approach produces a classifier tailored to the target domain. Agreement between the LLM and human annotators is comparable to inter-annotator agreement among humans, and the resulting model outperforms existing CAP classifiers trained on manually-annotated but out-of-domain data. In addition to the CAP annotations, the ParlaCAP dataset offers rich speaker and party metadata, as well as sentiment predictions coming from the ParlaSent multilingual transformer model, enabling comparative research on political attention and representation across countries. We illustrate the analytical potential of the dataset with three use cases, examining the distribution of parliamentary attention across policy topics, sentiment patterns in parliamentary speech, and gender differences in policy attention."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T15:04:30Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    15,
                    4,
                    30,
                    2,
                    49,
                    0
                ],
                "arxiv_comment": "17 pages, 7 figures, 7 tables. Submitted to the PoliticalNLP 2026 workshop, co-located with LREC 2026 conference",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Taja Kuzman Pungeršek"
                    },
                    {
                        "name": "Peter Rupnik"
                    },
                    {
                        "name": "Daniela Širinić"
                    },
                    {
                        "name": "Nikola Ljubešić"
                    }
                ],
                "author_detail": {
                    "name": "Nikola Ljubešić"
                },
                "author": "Nikola Ljubešić"
            },
            {
                "id": "http://arxiv.org/abs/2512.12850v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.12850v2",
                "title": "KANELÉ: Kolmogorov-Arnold Networks for Efficient LUT-based Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KANELÉ: Kolmogorov-Arnold Networks for Efficient LUT-based Evaluation"
                },
                "updated": "2026-02-18T15:04:02Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    15,
                    4,
                    2,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.12850v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.12850v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1145/3748173.3779202",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Low-latency, resource-efficient neural network inference on FPGAs is essential for applications demanding real-time capability and low power. Lookup table (LUT)-based neural networks are a common solution, combining strong representational power with efficient FPGA implementation. In this work, we introduce KANELÉ, a framework that exploits the unique properties of Kolmogorov-Arnold Networks (KANs) for FPGA deployment. Unlike traditional multilayer perceptrons (MLPs), KANs employ learnable one-dimensional splines with fixed domains as edge activations, a structure naturally suited to discretization and efficient LUT mapping. We present the first systematic design flow for implementing KANs on FPGAs, co-optimizing training with quantization and pruning to enable compact, high-throughput, and low-latency KAN architectures. Our results demonstrate up to a 2700x speedup and orders of magnitude resource savings compared to prior KAN-on-FPGA approaches. Moreover, KANELÉ matches or surpasses other LUT-based architectures on widely used benchmarks, particularly for tasks involving symbolic or physical formulas, while balancing resource usage across FPGA hardware. Finally, we showcase the versatility of the framework by extending it to real-time, power-efficient control systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-latency, resource-efficient neural network inference on FPGAs is essential for applications demanding real-time capability and low power. Lookup table (LUT)-based neural networks are a common solution, combining strong representational power with efficient FPGA implementation. In this work, we introduce KANELÉ, a framework that exploits the unique properties of Kolmogorov-Arnold Networks (KANs) for FPGA deployment. Unlike traditional multilayer perceptrons (MLPs), KANs employ learnable one-dimensional splines with fixed domains as edge activations, a structure naturally suited to discretization and efficient LUT mapping. We present the first systematic design flow for implementing KANs on FPGAs, co-optimizing training with quantization and pruning to enable compact, high-throughput, and low-latency KAN architectures. Our results demonstrate up to a 2700x speedup and orders of magnitude resource savings compared to prior KAN-on-FPGA approaches. Moreover, KANELÉ matches or surpasses other LUT-based architectures on widely used benchmarks, particularly for tasks involving symbolic or physical formulas, while balancing resource usage across FPGA hardware. Finally, we showcase the versatility of the framework by extending it to real-time, power-efficient control systems."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-14T21:29:10Z",
                "published_parsed": [
                    2025,
                    12,
                    14,
                    21,
                    29,
                    10,
                    6,
                    348,
                    0
                ],
                "arxiv_comment": "International Symposium on Field-Programmable Gate Arrays 2026 (ISFPGA'2026)",
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Duc Hoang"
                    },
                    {
                        "name": "Aarush Gupta"
                    },
                    {
                        "name": "Philip Harris"
                    }
                ],
                "author_detail": {
                    "name": "Philip Harris"
                },
                "author": "Philip Harris",
                "arxiv_doi": "10.1145/3748173.3779202"
            },
            {
                "id": "http://arxiv.org/abs/2602.16515v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16515v1",
                "title": "Generative deep learning improves reconstruction of global historical climate records",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative deep learning improves reconstruction of global historical climate records"
                },
                "updated": "2026-02-18T15:03:26Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    15,
                    3,
                    26,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16515v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16515v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Accurate assessment of anthropogenic climate change relies on historical instrumental data, yet observations from the early 20th century are sparse, fragmented, and uncertain. Conventional reconstructions rely on disparate statistical interpolation, which excessively smooths local features and creates unphysical artifacts, leading to systematic underestimation of intrinsic variability and extremes. Here, we present a unified, probabilistic generative deep learning framework that overcomes these limitations and reveals previously unresolved historical climate variability back to 1850. Leveraging a learned generative prior of Earth system dynamics, our model performs probabilistic inference to recover spatiotemporally consistent historical temperature and precipitation fields from sparse observations. Our approach preserves the higher-order statistics of climate dynamics, transforming reconstruction into a robust uncertainty-aware assessment. We demonstrate that our reconstruction overcomes pronounced biases in widely used historical reference products, including those underlying IPCC assessments, especially regarding extreme weather events. Notably, we uncover higher early 20th-century global warming levels compared to existing reconstructions, primarily driven by more pronounced polar warming, with mean Arctic warming trends exceeding established benchmarks by 0.15--0.29°C per decade for 1900--1980. Conversely, for the modern era, our reconstruction indicates that the broad Arctic warming trend is likely overestimated in recent assessments, yet explicitly resolves previously unrecognized intense, localized hotspots in the Barents Sea and Northeastern Greenland. Furthermore, based on our seamless global reconstruction that recovers precipitation variability across the oceans and under-monitored regions, we uncover an intensification of the global hydrological cycle.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate assessment of anthropogenic climate change relies on historical instrumental data, yet observations from the early 20th century are sparse, fragmented, and uncertain. Conventional reconstructions rely on disparate statistical interpolation, which excessively smooths local features and creates unphysical artifacts, leading to systematic underestimation of intrinsic variability and extremes. Here, we present a unified, probabilistic generative deep learning framework that overcomes these limitations and reveals previously unresolved historical climate variability back to 1850. Leveraging a learned generative prior of Earth system dynamics, our model performs probabilistic inference to recover spatiotemporally consistent historical temperature and precipitation fields from sparse observations. Our approach preserves the higher-order statistics of climate dynamics, transforming reconstruction into a robust uncertainty-aware assessment. We demonstrate that our reconstruction overcomes pronounced biases in widely used historical reference products, including those underlying IPCC assessments, especially regarding extreme weather events. Notably, we uncover higher early 20th-century global warming levels compared to existing reconstructions, primarily driven by more pronounced polar warming, with mean Arctic warming trends exceeding established benchmarks by 0.15--0.29°C per decade for 1900--1980. Conversely, for the modern era, our reconstruction indicates that the broad Arctic warming trend is likely overestimated in recent assessments, yet explicitly resolves previously unrecognized intense, localized hotspots in the Barents Sea and Northeastern Greenland. Furthermore, based on our seamless global reconstruction that recovers precipitation variability across the oceans and under-monitored regions, we uncover an intensification of the global hydrological cycle."
                },
                "tags": [
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T15:03:26Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    15,
                    3,
                    26,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "physics.geo-ph"
                },
                "authors": [
                    {
                        "name": "Zhen Qian"
                    },
                    {
                        "name": "Teng Liu"
                    },
                    {
                        "name": "Sebastian Bathiany"
                    },
                    {
                        "name": "Shangshang Yang"
                    },
                    {
                        "name": "Philipp Hess"
                    },
                    {
                        "name": "Nils Bochow"
                    },
                    {
                        "name": "Christian Burmester"
                    },
                    {
                        "name": "Maximilian Gelbrecht"
                    },
                    {
                        "name": "Brian Groenke"
                    },
                    {
                        "name": "Niklas Boers"
                    }
                ],
                "author_detail": {
                    "name": "Niklas Boers"
                },
                "author": "Niklas Boers"
            },
            {
                "id": "http://arxiv.org/abs/2602.16511v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16511v1",
                "title": "VIGOR: Visual Goal-In-Context Inference for Unified Humanoid Fall Safety",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VIGOR: Visual Goal-In-Context Inference for Unified Humanoid Fall Safety"
                },
                "updated": "2026-02-18T14:57:33Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    14,
                    57,
                    33,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16511v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16511v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Reliable fall recovery is critical for humanoids operating in cluttered environments. Unlike quadrupeds or wheeled robots, humanoids experience high-energy impacts, complex whole-body contact, and large viewpoint changes during a fall, making recovery essential for continued operation. Existing methods fragment fall safety into separate problems such as fall avoidance, impact mitigation, and stand-up recovery, or rely on end-to-end policies trained without vision through reinforcement learning or imitation learning, often on flat terrain. At a deeper level, fall safety is treated as monolithic data complexity, coupling pose, dynamics, and terrain and requiring exhaustive coverage, limiting scalability and generalization. We present a unified fall safety approach that spans all phases of fall recovery. It builds on two insights: 1) Natural human fall and recovery poses are highly constrained and transferable from flat to complex terrain through alignment, and 2) Fast whole-body reactions require integrated perceptual-motor representations. We train a privileged teacher using sparse human demonstrations on flat terrain and simulated complex terrains, and distill it into a deployable student that relies only on egocentric depth and proprioception. The student learns how to react by matching the teacher's goal-in-context latent representation, which combines the next target pose with the local terrain, rather than separately encoding what it must perceive and how it must act. Results in simulation and on a real Unitree G1 humanoid demonstrate robust, zero-shot fall safety across diverse non-flat environments without real-world fine-tuning. The project page is available at https://vigor2026.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable fall recovery is critical for humanoids operating in cluttered environments. Unlike quadrupeds or wheeled robots, humanoids experience high-energy impacts, complex whole-body contact, and large viewpoint changes during a fall, making recovery essential for continued operation. Existing methods fragment fall safety into separate problems such as fall avoidance, impact mitigation, and stand-up recovery, or rely on end-to-end policies trained without vision through reinforcement learning or imitation learning, often on flat terrain. At a deeper level, fall safety is treated as monolithic data complexity, coupling pose, dynamics, and terrain and requiring exhaustive coverage, limiting scalability and generalization. We present a unified fall safety approach that spans all phases of fall recovery. It builds on two insights: 1) Natural human fall and recovery poses are highly constrained and transferable from flat to complex terrain through alignment, and 2) Fast whole-body reactions require integrated perceptual-motor representations. We train a privileged teacher using sparse human demonstrations on flat terrain and simulated complex terrains, and distill it into a deployable student that relies only on egocentric depth and proprioception. The student learns how to react by matching the teacher's goal-in-context latent representation, which combines the next target pose with the local terrain, rather than separately encoding what it must perceive and how it must act. Results in simulation and on a real Unitree G1 humanoid demonstrate robust, zero-shot fall safety across diverse non-flat environments without real-world fine-tuning. The project page is available at https://vigor2026.github.io/"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T14:57:33Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    14,
                    57,
                    33,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Osher Azulay"
                    },
                    {
                        "name": "Zhengjie Xu"
                    },
                    {
                        "name": "Andrew Scheffer"
                    },
                    {
                        "name": "Stella X. Yu"
                    }
                ],
                "author_detail": {
                    "name": "Stella X. Yu"
                },
                "author": "Stella X. Yu"
            },
            {
                "id": "http://arxiv.org/abs/2602.16504v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16504v1",
                "title": "GRIMM: Genetic stRatification for Inference in Molecular Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GRIMM: Genetic stRatification for Inference in Molecular Modeling"
                },
                "updated": "2026-02-18T14:46:18Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    14,
                    46,
                    18,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16504v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16504v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The vast majority of biological sequences encode unknown functions and bear little resemblance to experimentally characterized proteins, limiting both our understanding of biology and our ability to harness functional potential for the bioeconomy. Predicting enzyme function from sequence remains a central challenge in computational biology, complicated by low sequence diversity and imbalanced label support in publicly available datasets. Models trained on these data can overestimate performance and fail to generalize. To address this, we introduce GRIMM (Genetic stRatification for Inference in Molecular Modeling), a benchmark for enzyme function prediction that employs genetic stratification: sequences are clustered by similarity and clusters are assigned exclusively to training, validation, or test sets. This ensures that sequences from the same cluster do not appear in multiple partitions. GRIMM produces multiple test sets: a closed-set test with the same label distribution as training (Test-1) and an open-set test containing novel labels (Test-2), serving as a realistic out-of-distribution proxy for discovering novel enzyme functions. While demonstrated on enzymes, this approach is generalizable to any sequence-based classification task where inputs can be clustered by similarity. By formalizing a splitting strategy often used implicitly, GRIMM provides a unified and reproducible framework for closed- and open-set evaluation. The method is lightweight, requiring only sequence clustering and label annotations, and can be adapted to different similarity thresholds, data scales, and biological tasks. GRIMM enables more realistic evaluation of functional prediction models on both familiar and unseen classes and establishes a benchmark that more faithfully assesses model performance and generalizability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The vast majority of biological sequences encode unknown functions and bear little resemblance to experimentally characterized proteins, limiting both our understanding of biology and our ability to harness functional potential for the bioeconomy. Predicting enzyme function from sequence remains a central challenge in computational biology, complicated by low sequence diversity and imbalanced label support in publicly available datasets. Models trained on these data can overestimate performance and fail to generalize. To address this, we introduce GRIMM (Genetic stRatification for Inference in Molecular Modeling), a benchmark for enzyme function prediction that employs genetic stratification: sequences are clustered by similarity and clusters are assigned exclusively to training, validation, or test sets. This ensures that sequences from the same cluster do not appear in multiple partitions. GRIMM produces multiple test sets: a closed-set test with the same label distribution as training (Test-1) and an open-set test containing novel labels (Test-2), serving as a realistic out-of-distribution proxy for discovering novel enzyme functions. While demonstrated on enzymes, this approach is generalizable to any sequence-based classification task where inputs can be clustered by similarity. By formalizing a splitting strategy often used implicitly, GRIMM provides a unified and reproducible framework for closed- and open-set evaluation. The method is lightweight, requiring only sequence clustering and label annotations, and can be adapted to different similarity thresholds, data scales, and biological tasks. GRIMM enables more realistic evaluation of functional prediction models on both familiar and unseen classes and establishes a benchmark that more faithfully assesses model performance and generalizability."
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T14:46:18Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    14,
                    46,
                    18,
                    2,
                    49,
                    0
                ],
                "arxiv_comment": "9 pages, 1 figure, 2 tables, submitted to ISMB main conference proceedings 2026",
                "arxiv_primary_category": {
                    "term": "q-bio.QM"
                },
                "authors": [
                    {
                        "name": "Ashley Babjac"
                    },
                    {
                        "name": "Adrienne Hoarfrost"
                    }
                ],
                "author_detail": {
                    "name": "Adrienne Hoarfrost"
                },
                "author": "Adrienne Hoarfrost"
            },
            {
                "id": "http://arxiv.org/abs/2602.16500v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16500v1",
                "title": "Optimizing Soft Prompt Tuning via Structural Evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Soft Prompt Tuning via Structural Evolution"
                },
                "updated": "2026-02-18T14:43:20Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    14,
                    43,
                    20,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16500v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16500v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Soft prompt tuning leverages continuous embeddings to capture task-specific information in large pre-trained language models (LLMs), achieving competitive performance in few-shot settings. However, soft prompts rely on high-dimensional, implicit representations and lack explicit semantics and traceable training behaviors, which limits their interpretability. To address this limitation, we propose a soft prompt tuning optimization method based on topological morphological evolution. Specifically, we employ persistent homology from topological data analysis (TDA) to quantify the structural representations of soft prompts in continuous parameter space and their training process evolution. Quantitative analysis shows that topologically stable and compact soft prompts achieve better downstream performance. Based on this empirical observation, we construct a loss function for optimizing soft prompt tuning, termed Topological Soft Prompt Loss (TSLoss). TSLoss guides the model to learn structurally stable adaptations by quantifying inter-parameter connectivity and redundancy. Extensive experiments show that training with TSLoss accelerates convergence and improves tuning performance, providing an interpretable method to understand and optimize soft prompt tuning from structural and topological perspectives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soft prompt tuning leverages continuous embeddings to capture task-specific information in large pre-trained language models (LLMs), achieving competitive performance in few-shot settings. However, soft prompts rely on high-dimensional, implicit representations and lack explicit semantics and traceable training behaviors, which limits their interpretability. To address this limitation, we propose a soft prompt tuning optimization method based on topological morphological evolution. Specifically, we employ persistent homology from topological data analysis (TDA) to quantify the structural representations of soft prompts in continuous parameter space and their training process evolution. Quantitative analysis shows that topologically stable and compact soft prompts achieve better downstream performance. Based on this empirical observation, we construct a loss function for optimizing soft prompt tuning, termed Topological Soft Prompt Loss (TSLoss). TSLoss guides the model to learn structurally stable adaptations by quantifying inter-parameter connectivity and redundancy. Extensive experiments show that training with TSLoss accelerates convergence and improves tuning performance, providing an interpretable method to understand and optimize soft prompt tuning from structural and topological perspectives."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T14:43:20Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    14,
                    43,
                    20,
                    2,
                    49,
                    0
                ],
                "arxiv_comment": "This manuscript has been submitted to IEEE Transactions on Knowledge and Data Engineering (TKDE) for peer review",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Zhenzhen Huang"
                    },
                    {
                        "name": "Chaoning Zhang"
                    },
                    {
                        "name": "Haoyu Bian"
                    },
                    {
                        "name": "Songbo Zhang"
                    },
                    {
                        "name": "Chi-lok Andy Tai"
                    },
                    {
                        "name": "Jiaquan Zhang"
                    },
                    {
                        "name": "Caiyan Qin"
                    },
                    {
                        "name": "Jingjing Qu"
                    },
                    {
                        "name": "Yalan Ye"
                    },
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Heng Tao Shen"
                    }
                ],
                "author_detail": {
                    "name": "Heng Tao Shen"
                },
                "author": "Heng Tao Shen"
            },
            {
                "id": "http://arxiv.org/abs/2602.16498v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16498v1",
                "title": "Fast and Scalable Analytical Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and Scalable Analytical Diffusion"
                },
                "updated": "2026-02-18T14:41:09Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    14,
                    41,
                    9,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16498v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16498v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Analytical diffusion models offer a mathematically transparent path to generative modeling by formulating the denoising score as an empirical-Bayes posterior mean. However, this interpretability comes at a prohibitive cost: the standard formulation necessitates a full-dataset scan at every timestep, scaling linearly with dataset size. In this work, we present the first systematic study addressing this scalability bottleneck. We challenge the prevailing assumption that the entire training data is necessary, uncovering the phenomenon of Posterior Progressive Concentration: the effective golden support of the denoising score is not static but shrinks asymptotically from the global manifold to a local neighborhood as the signal-to-noise ratio increases. Capitalizing on this, we propose Dynamic Time-Aware Golden Subset Diffusion (GoldDiff), a training-free framework that decouples inference complexity from dataset size. Instead of static retrieval, GoldDiff uses a coarse-to-fine mechanism to dynamically pinpoint the ''Golden Subset'' for inference. Theoretically, we derive rigorous bounds guaranteeing that our sparse approximation converges to the exact score. Empirically, GoldDiff achieves a $\\bf 71 \\times$ speedup on AFHQ while matching or achieving even better performance than full-scan baselines. Most notably, we demonstrate the first successful scaling of analytical diffusion to ImageNet-1K, unlocking a scalable, training-free paradigm for large-scale generative modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analytical diffusion models offer a mathematically transparent path to generative modeling by formulating the denoising score as an empirical-Bayes posterior mean. However, this interpretability comes at a prohibitive cost: the standard formulation necessitates a full-dataset scan at every timestep, scaling linearly with dataset size. In this work, we present the first systematic study addressing this scalability bottleneck. We challenge the prevailing assumption that the entire training data is necessary, uncovering the phenomenon of Posterior Progressive Concentration: the effective golden support of the denoising score is not static but shrinks asymptotically from the global manifold to a local neighborhood as the signal-to-noise ratio increases. Capitalizing on this, we propose Dynamic Time-Aware Golden Subset Diffusion (GoldDiff), a training-free framework that decouples inference complexity from dataset size. Instead of static retrieval, GoldDiff uses a coarse-to-fine mechanism to dynamically pinpoint the ''Golden Subset'' for inference. Theoretically, we derive rigorous bounds guaranteeing that our sparse approximation converges to the exact score. Empirically, GoldDiff achieves a $\\bf 71 \\times$ speedup on AFHQ while matching or achieving even better performance than full-scan baselines. Most notably, we demonstrate the first successful scaling of analytical diffusion to ImageNet-1K, unlocking a scalable, training-free paradigm for large-scale generative modeling."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T14:41:09Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    14,
                    41,
                    9,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Xinyi Shang"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Jingyu Lin"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Shen"
                },
                "author": "Zhiqiang Shen"
            },
            {
                "id": "http://arxiv.org/abs/2602.16497v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16497v1",
                "title": "Factor-Adjusted Multiple Testing for High-Dimensional Individual Mediation Effects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Factor-Adjusted Multiple Testing for High-Dimensional Individual Mediation Effects"
                },
                "updated": "2026-02-18T14:40:42Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    14,
                    40,
                    42,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16497v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16497v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Identifying individual mediators is a central goal of high-dimensional mediation analysis, yet pervasive dependence among mediators can invalidate standard debiased inference and lead to substantial false discovery rate (FDR) inflation. We propose a Factor-Adjusted Debiased Mediation Testing (FADMT) framework that enables large-scale inference for individual mediation effects with FDR control under complex dependence structures. Our approach posits an approximate factor structure on the unobserved errors of the mediator model, extracts common latent factors, and constructs decorrelated pseudo-mediators for the subsequent inferential procedure. We establish the asymptotic normality of the debiased estimator and develop a multiple testing procedure with theoretical FDR control under mild high-dimensional conditions. By adjusting for latent factor induced dependence, FADMT also improves robustness to spurious associations driven by shared latent variation in observational studies. Extensive simulations demonstrate the superior finite-sample performance across a wide range of correlation structures. Applications to TCGA-BRCA multi-omics data and to China's stock connect study further illustrate the practical utility of the proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying individual mediators is a central goal of high-dimensional mediation analysis, yet pervasive dependence among mediators can invalidate standard debiased inference and lead to substantial false discovery rate (FDR) inflation. We propose a Factor-Adjusted Debiased Mediation Testing (FADMT) framework that enables large-scale inference for individual mediation effects with FDR control under complex dependence structures. Our approach posits an approximate factor structure on the unobserved errors of the mediator model, extracts common latent factors, and constructs decorrelated pseudo-mediators for the subsequent inferential procedure. We establish the asymptotic normality of the debiased estimator and develop a multiple testing procedure with theoretical FDR control under mild high-dimensional conditions. By adjusting for latent factor induced dependence, FADMT also improves robustness to spurious associations driven by shared latent variation in observational studies. Extensive simulations demonstrate the superior finite-sample performance across a wide range of correlation structures. Applications to TCGA-BRCA multi-omics data and to China's stock connect study further illustrate the practical utility of the proposed method."
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T14:40:42Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    14,
                    40,
                    42,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME"
                },
                "authors": [
                    {
                        "name": "Chen Shi"
                    },
                    {
                        "name": "Zhao Chen"
                    },
                    {
                        "name": "Christina Dan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Christina Dan Wang"
                },
                "author": "Christina Dan Wang"
            },
            {
                "id": "http://arxiv.org/abs/2602.15001v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.15001v2",
                "title": "Boundary Point Jailbreaking of Black-Box LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boundary Point Jailbreaking of Black-Box LLMs"
                },
                "updated": "2026-02-18T14:35:19Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    14,
                    35,
                    19,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.15001v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.15001v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Frontier LLMs are safeguarded against attempts to extract harmful information via adversarial prompts known as \"jailbreaks\". Recently, defenders have developed classifier-based systems that have survived thousands of hours of human red teaming. We introduce Boundary Point Jailbreaking (BPJ), a new class of automated jailbreak attacks that evade the strongest industry-deployed safeguards. Unlike previous attacks that rely on white/grey-box assumptions (such as classifier scores or gradients) or libraries of existing jailbreaks, BPJ is fully black-box and uses only a single bit of information per query: whether or not the classifier flags the interaction. To achieve this, BPJ addresses the core difficulty in optimising attacks against robust real-world defences: evaluating whether a proposed modification to an attack is an improvement. Instead of directly trying to learn an attack for a target harmful string, BPJ converts the string into a curriculum of intermediate attack targets and then actively selects evaluation points that best detect small changes in attack strength (\"boundary points\"). We believe BPJ is the first fully automated attack algorithm that succeeds in developing universal jailbreaks against Constitutional Classifiers, as well as the first automated attack algorithm that succeeds against GPT-5's input classifier without relying on human attack seeds. BPJ is difficult to defend against in individual interactions but incurs many flags during optimisation, suggesting that effective defence requires supplementing single-interaction methods with batch-level monitoring.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Frontier LLMs are safeguarded against attempts to extract harmful information via adversarial prompts known as \"jailbreaks\". Recently, defenders have developed classifier-based systems that have survived thousands of hours of human red teaming. We introduce Boundary Point Jailbreaking (BPJ), a new class of automated jailbreak attacks that evade the strongest industry-deployed safeguards. Unlike previous attacks that rely on white/grey-box assumptions (such as classifier scores or gradients) or libraries of existing jailbreaks, BPJ is fully black-box and uses only a single bit of information per query: whether or not the classifier flags the interaction. To achieve this, BPJ addresses the core difficulty in optimising attacks against robust real-world defences: evaluating whether a proposed modification to an attack is an improvement. Instead of directly trying to learn an attack for a target harmful string, BPJ converts the string into a curriculum of intermediate attack targets and then actively selects evaluation points that best detect small changes in attack strength (\"boundary points\"). We believe BPJ is the first fully automated attack algorithm that succeeds in developing universal jailbreaks against Constitutional Classifiers, as well as the first automated attack algorithm that succeeds against GPT-5's input classifier without relying on human attack seeds. BPJ is difficult to defend against in individual interactions but incurs many flags during optimisation, suggesting that effective defence requires supplementing single-interaction methods with batch-level monitoring."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-16T18:29:09Z",
                "published_parsed": [
                    2026,
                    2,
                    16,
                    18,
                    29,
                    9,
                    0,
                    47,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Xander Davies"
                    },
                    {
                        "name": "Giorgi Giglemiani"
                    },
                    {
                        "name": "Edmund Lau"
                    },
                    {
                        "name": "Eric Winsor"
                    },
                    {
                        "name": "Geoffrey Irving"
                    },
                    {
                        "name": "Yarin Gal"
                    }
                ],
                "author_detail": {
                    "name": "Yarin Gal"
                },
                "author": "Yarin Gal"
            },
            {
                "id": "http://arxiv.org/abs/2602.16490v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16490v1",
                "title": "From Growing to Looping: A Unified View of Iterative Computation in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Growing to Looping: A Unified View of Iterative Computation in LLMs"
                },
                "updated": "2026-02-18T14:25:16Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    14,
                    25,
                    16,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16490v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16490v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Looping, reusing a block of layers across depth, and depth growing, training shallow-to-deep models by duplicating middle layers, have both been linked to stronger reasoning, but their relationship remains unclear. We provide a mechanistic unification: looped and depth-grown models exhibit convergent depth-wise signatures, including increased reliance on late layers and recurring patterns aligned with the looped or grown block. These shared signatures support the view that their gains stem from a common form of iterative computation. Building on this connection, we show that the two techniques are adaptable and composable: applying inference-time looping to the middle blocks of a depth-grown model improves accuracy on some reasoning primitives by up to $2\\times$, despite the model never being trained to loop. Both approaches also adapt better than the baseline when given more in-context examples or additional supervised fine-tuning data. Additionally, depth-grown models achieve the largest reasoning gains when using higher-quality, math-heavy cooldown mixtures, which can be further boosted by adapting a middle block to loop. Overall, our results position depth growth and looping as complementary, practical methods for inducing and scaling iterative computation to improve reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Looping, reusing a block of layers across depth, and depth growing, training shallow-to-deep models by duplicating middle layers, have both been linked to stronger reasoning, but their relationship remains unclear. We provide a mechanistic unification: looped and depth-grown models exhibit convergent depth-wise signatures, including increased reliance on late layers and recurring patterns aligned with the looped or grown block. These shared signatures support the view that their gains stem from a common form of iterative computation. Building on this connection, we show that the two techniques are adaptable and composable: applying inference-time looping to the middle blocks of a depth-grown model improves accuracy on some reasoning primitives by up to $2\\times$, despite the model never being trained to loop. Both approaches also adapt better than the baseline when given more in-context examples or additional supervised fine-tuning data. Additionally, depth-grown models achieve the largest reasoning gains when using higher-quality, math-heavy cooldown mixtures, which can be further boosted by adapting a middle block to loop. Overall, our results position depth growth and looping as complementary, practical methods for inducing and scaling iterative computation to improve reasoning."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T14:25:16Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    14,
                    25,
                    16,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Ferdinand Kapl"
                    },
                    {
                        "name": "Emmanouil Angelis"
                    },
                    {
                        "name": "Kaitlin Maile"
                    },
                    {
                        "name": "Johannes von Oswald"
                    },
                    {
                        "name": "Stefan Bauer"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Bauer"
                },
                "author": "Stefan Bauer"
            },
            {
                "id": "http://arxiv.org/abs/2602.16488v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16488v1",
                "title": "Learning to Learn from Language Feedback with Social Meta-Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Learn from Language Feedback with Social Meta-Learning"
                },
                "updated": "2026-02-18T14:22:13Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    14,
                    22,
                    13,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16488v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) often struggle to learn from corrective feedback within a conversational context. They are rarely proactive in soliciting this feedback, even when faced with ambiguity, which can make their dialogues feel static, one-sided, and lacking the adaptive qualities of human conversation. To address these limitations, we draw inspiration from social meta-learning (SML) in humans - the process of learning how to learn from others. We formulate SML as a finetuning methodology, training LLMs to solicit and learn from language feedback in simulated pedagogical dialogues, where static tasks are converted into interactive social learning problems. SML effectively teaches models to use conversation to solve problems they are unable to solve in a single turn. This capability generalises across domains; SML on math problems produces models that better use feedback to solve coding problems and vice versa. Furthermore, despite being trained only on fully-specified problems, these models are better able to solve underspecified tasks where critical information is revealed over multiple turns. When faced with this ambiguity, SML-trained models make fewer premature answer attempts and are more likely to ask for the information they need. This work presents a scalable approach to developing AI systems that effectively learn from language feedback.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often struggle to learn from corrective feedback within a conversational context. They are rarely proactive in soliciting this feedback, even when faced with ambiguity, which can make their dialogues feel static, one-sided, and lacking the adaptive qualities of human conversation. To address these limitations, we draw inspiration from social meta-learning (SML) in humans - the process of learning how to learn from others. We formulate SML as a finetuning methodology, training LLMs to solicit and learn from language feedback in simulated pedagogical dialogues, where static tasks are converted into interactive social learning problems. SML effectively teaches models to use conversation to solve problems they are unable to solve in a single turn. This capability generalises across domains; SML on math problems produces models that better use feedback to solve coding problems and vice versa. Furthermore, despite being trained only on fully-specified problems, these models are better able to solve underspecified tasks where critical information is revealed over multiple turns. When faced with this ambiguity, SML-trained models make fewer premature answer attempts and are more likely to ask for the information they need. This work presents a scalable approach to developing AI systems that effectively learn from language feedback."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T14:22:13Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    14,
                    22,
                    13,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Jonathan Cook"
                    },
                    {
                        "name": "Diego Antognini"
                    },
                    {
                        "name": "Martin Klissarov"
                    },
                    {
                        "name": "Claudiu Musat"
                    },
                    {
                        "name": "Edward Grefenstette"
                    }
                ],
                "author_detail": {
                    "name": "Edward Grefenstette"
                },
                "author": "Edward Grefenstette"
            },
            {
                "id": "http://arxiv.org/abs/2602.16485v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16485v1",
                "title": "Team of Thoughts: Efficient Test-time Scaling of Agentic Systems through Orchestrated Tool Calling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Team of Thoughts: Efficient Test-time Scaling of Agentic Systems through Orchestrated Tool Calling"
                },
                "updated": "2026-02-18T14:19:01Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    14,
                    19,
                    1,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16485v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16485v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Existing Multi-Agent Systems (MAS) typically rely on static, homogeneous model configurations, limiting their ability to exploit the distinct strengths of differently post-trained models. To address this, we introduce Team-of-Thoughts, a novel MAS architecture that leverages the complementary capabilities of heterogeneous agents via an orchestrator-tool paradigm. Our framework introduces two key mechanisms to optimize performance: (1) an orchestrator calibration scheme that identifies models with superior coordination capabilities, and (2) a self-assessment protocol where tool agents profile their own domain expertise to account for variations in post-training skills. During inference, the orchestrator dynamically activates the most suitable tool agents based on these proficiency profiles. Experiments on five reasoning and code generation benchmarks show that Team-of-Thoughts delivers consistently superior task performance. Notably, on AIME24 and LiveCodeBench, our approach achieves accuracies of 96.67% and 72.53%, respectively, substantially outperforming homogeneous role-play baselines, which score 80% and 65.93%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing Multi-Agent Systems (MAS) typically rely on static, homogeneous model configurations, limiting their ability to exploit the distinct strengths of differently post-trained models. To address this, we introduce Team-of-Thoughts, a novel MAS architecture that leverages the complementary capabilities of heterogeneous agents via an orchestrator-tool paradigm. Our framework introduces two key mechanisms to optimize performance: (1) an orchestrator calibration scheme that identifies models with superior coordination capabilities, and (2) a self-assessment protocol where tool agents profile their own domain expertise to account for variations in post-training skills. During inference, the orchestrator dynamically activates the most suitable tool agents based on these proficiency profiles. Experiments on five reasoning and code generation benchmarks show that Team-of-Thoughts delivers consistently superior task performance. Notably, on AIME24 and LiveCodeBench, our approach achieves accuracies of 96.67% and 72.53%, respectively, substantially outperforming homogeneous role-play baselines, which score 80% and 65.93%."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T14:19:01Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    14,
                    19,
                    1,
                    2,
                    49,
                    0
                ],
                "arxiv_comment": "8 pages",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Jeffrey T. H. Wong"
                    },
                    {
                        "name": "Zixi Zhang"
                    },
                    {
                        "name": "Junyi Liu"
                    },
                    {
                        "name": "Yiren Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yiren Zhao"
                },
                "author": "Yiren Zhao"
            },
            {
                "id": "http://arxiv.org/abs/2602.16481v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16481v1",
                "title": "Leveraging Large Language Models for Causal Discovery: a Constraint-based, Argumentation-driven Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models for Causal Discovery: a Constraint-based, Argumentation-driven Approach"
                },
                "updated": "2026-02-18T14:15:21Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    14,
                    15,
                    21,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16481v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16481v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Causal discovery seeks to uncover causal relations from data, typically represented as causal graphs, and is essential for predicting the effects of interventions. While expert knowledge is required to construct principled causal graphs, many statistical methods have been proposed to leverage observational data with varying formal guarantees. Causal Assumption-based Argumentation (ABA) is a framework that uses symbolic reasoning to ensure correspondence between input constraints and output graphs, while offering a principled way to combine data and expertise. We explore the use of large language models (LLMs) as imperfect experts for Causal ABA, eliciting semantic structural priors from variable names and descriptions and integrating them with conditional-independence evidence. Experiments on standard benchmarks and semantically grounded synthetic graphs demonstrate state-of-the-art performance, and we additionally introduce an evaluation protocol to mitigate memorisation bias when assessing LLMs for causal discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal discovery seeks to uncover causal relations from data, typically represented as causal graphs, and is essential for predicting the effects of interventions. While expert knowledge is required to construct principled causal graphs, many statistical methods have been proposed to leverage observational data with varying formal guarantees. Causal Assumption-based Argumentation (ABA) is a framework that uses symbolic reasoning to ensure correspondence between input constraints and output graphs, while offering a principled way to combine data and expertise. We explore the use of large language models (LLMs) as imperfect experts for Causal ABA, eliciting semantic structural priors from variable names and descriptions and integrating them with conditional-independence evidence. Experiments on standard benchmarks and semantically grounded synthetic graphs demonstrate state-of-the-art performance, and we additionally introduce an evaluation protocol to mitigate memorisation bias when assessing LLMs for causal discovery."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T14:15:21Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    14,
                    15,
                    21,
                    2,
                    49,
                    0
                ],
                "arxiv_comment": "26 pages, including appendix",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Zihao Li"
                    },
                    {
                        "name": "Fabrizio Russo"
                    }
                ],
                "author_detail": {
                    "name": "Fabrizio Russo"
                },
                "author": "Fabrizio Russo"
            },
            {
                "id": "http://arxiv.org/abs/2602.16480v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16480v1",
                "title": "SRFed: Mitigating Poisoning Attacks in Privacy-Preserving Federated Learning with Heterogeneous Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SRFed: Mitigating Poisoning Attacks in Privacy-Preserving Federated Learning with Heterogeneous Data"
                },
                "updated": "2026-02-18T14:14:38Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    14,
                    14,
                    38,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16480v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Federated Learning (FL) enables collaborative model training without exposing clients' private data, and has been widely adopted in privacy-sensitive scenarios. However, FL faces two critical security threats: curious servers that may launch inference attacks to reconstruct clients' private data, and compromised clients that can launch poisoning attacks to disrupt model aggregation. Existing solutions mitigate these attacks by combining mainstream privacy-preserving techniques with defensive aggregation strategies. However, they either incur high computation and communication overhead or perform poorly under non-independent and identically distributed (Non-IID) data settings. To tackle these challenges, we propose SRFed, an efficient Byzantine-robust and privacy-preserving FL framework for Non-IID scenarios. First, we design a decentralized efficient functional encryption (DEFE) scheme to support efficient model encryption and non-interactive decryption. DEFE also eliminates third-party reliance and defends against server-side inference attacks. Second, we develop a privacy-preserving defensive model aggregation mechanism based on DEFE. This mechanism filters poisonous models under Non-IID data by layer-wise projection and clustering-based analysis. Theoretical analysis and extensive experiments show that SRFed outperforms state-of-the-art baselines in privacy protection, Byzantine robustness, and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) enables collaborative model training without exposing clients' private data, and has been widely adopted in privacy-sensitive scenarios. However, FL faces two critical security threats: curious servers that may launch inference attacks to reconstruct clients' private data, and compromised clients that can launch poisoning attacks to disrupt model aggregation. Existing solutions mitigate these attacks by combining mainstream privacy-preserving techniques with defensive aggregation strategies. However, they either incur high computation and communication overhead or perform poorly under non-independent and identically distributed (Non-IID) data settings. To tackle these challenges, we propose SRFed, an efficient Byzantine-robust and privacy-preserving FL framework for Non-IID scenarios. First, we design a decentralized efficient functional encryption (DEFE) scheme to support efficient model encryption and non-interactive decryption. DEFE also eliminates third-party reliance and defends against server-side inference attacks. Second, we develop a privacy-preserving defensive model aggregation mechanism based on DEFE. This mechanism filters poisonous models under Non-IID data by layer-wise projection and clustering-based analysis. Theoretical analysis and extensive experiments show that SRFed outperforms state-of-the-art baselines in privacy protection, Byzantine robustness, and efficiency."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T14:14:38Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    14,
                    14,
                    38,
                    2,
                    49,
                    0
                ],
                "arxiv_comment": "Federated learning, functional encryption, privacy-preserving machine learning, neural networks",
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Yiwen Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yiwen Lu"
                },
                "author": "Yiwen Lu"
            },
            {
                "id": "http://arxiv.org/abs/2602.15620v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.15620v2",
                "title": "STAPO: Stabilizing Reinforcement Learning for LLMs by Silencing Rare Spurious Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STAPO: Stabilizing Reinforcement Learning for LLMs by Silencing Rare Spurious Tokens"
                },
                "updated": "2026-02-18T14:13:03Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    14,
                    13,
                    3,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.15620v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.15620v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Reinforcement Learning (RL) has significantly improved large language model reasoning, but existing RL fine-tuning methods rely heavily on heuristic techniques such as entropy regularization and reweighting to maintain stability. In practice, they often suffer from late-stage performance collapse, leading to degraded reasoning quality and unstable training. Our analysis shows that the magnitude of token-wise policy gradients in RL is negatively correlated with token probability and local policy entropy. We find that training instability can be caused by a tiny fraction of tokens, approximately 0.01\\%, which we term \\emph{spurious tokens}. When such tokens appear in correct responses, they contribute little to the reasoning outcome but inherit the full sequence-level reward, leading to abnormally amplified gradient updates. To mitigate this instability, we design S2T (silencing spurious tokens) mechanism to efficiently identify spurious tokens through characteristic signals with low probability, low entropy, and positive advantage, and then to suppress their gradient perturbations during optimization. Incorporating this mechanism into a group-based objective, we propose Spurious-Token-Aware Policy Optimization (STAPO), which promotes stable and effective large-scale model refinement. Across six mathematical reasoning benchmarks using Qwen 1.7B, 8B, and 14B base models, STAPO consistently demonstrates superior entropy stability and achieves an average performance improvement of 7.13\\% ($ρ_{\\mathrm{T}}$=1.0, top-p=1.0) and 3.69\\% ($ρ_{\\mathrm{T}}$=0.7, top-p=0.9) over GRPO, 20-Entropy and JustRL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning (RL) has significantly improved large language model reasoning, but existing RL fine-tuning methods rely heavily on heuristic techniques such as entropy regularization and reweighting to maintain stability. In practice, they often suffer from late-stage performance collapse, leading to degraded reasoning quality and unstable training. Our analysis shows that the magnitude of token-wise policy gradients in RL is negatively correlated with token probability and local policy entropy. We find that training instability can be caused by a tiny fraction of tokens, approximately 0.01\\%, which we term \\emph{spurious tokens}. When such tokens appear in correct responses, they contribute little to the reasoning outcome but inherit the full sequence-level reward, leading to abnormally amplified gradient updates. To mitigate this instability, we design S2T (silencing spurious tokens) mechanism to efficiently identify spurious tokens through characteristic signals with low probability, low entropy, and positive advantage, and then to suppress their gradient perturbations during optimization. Incorporating this mechanism into a group-based objective, we propose Spurious-Token-Aware Policy Optimization (STAPO), which promotes stable and effective large-scale model refinement. Across six mathematical reasoning benchmarks using Qwen 1.7B, 8B, and 14B base models, STAPO consistently demonstrates superior entropy stability and achieves an average performance improvement of 7.13\\% ($ρ_{\\mathrm{T}}$=1.0, top-p=1.0) and 3.69\\% ($ρ_{\\mathrm{T}}$=0.7, top-p=0.9) over GRPO, 20-Entropy and JustRL."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-17T14:46:48Z",
                "published_parsed": [
                    2026,
                    2,
                    17,
                    14,
                    46,
                    48,
                    1,
                    48,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Shiqi Liu"
                    },
                    {
                        "name": "Zeyu He"
                    },
                    {
                        "name": "Guojian Zhan"
                    },
                    {
                        "name": "Letian Tao"
                    },
                    {
                        "name": "Zhilong Zheng"
                    },
                    {
                        "name": "Jiang Wu"
                    },
                    {
                        "name": "Yinuo Wang"
                    },
                    {
                        "name": "Yang Guan"
                    },
                    {
                        "name": "Kehua Sheng"
                    },
                    {
                        "name": "Bo Zhang"
                    },
                    {
                        "name": "Keqiang Li"
                    },
                    {
                        "name": "Jingliang Duan"
                    },
                    {
                        "name": "Shengbo Eben Li"
                    }
                ],
                "author_detail": {
                    "name": "Shengbo Eben Li"
                },
                "author": "Shengbo Eben Li"
            },
            {
                "id": "http://arxiv.org/abs/2508.10480v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.10480v2",
                "title": "Pinet: Optimizing hard-constrained neural networks with orthogonal projection layers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pinet: Optimizing hard-constrained neural networks with orthogonal projection layers"
                },
                "updated": "2026-02-18T14:10:20Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    14,
                    10,
                    20,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.10480v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.10480v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce an output layer for neural networks that ensures satisfaction of convex constraints. Our approach, $Π$net, leverages operator splitting for rapid and reliable projections in the forward pass, and the implicit function theorem for backpropagation. We deploy $Π$net as a feasible-by-design optimization proxy for parametric constrained optimization problems and obtain modest-accuracy solutions faster than traditional solvers when solving a single problem, and significantly faster for a batch of problems. We surpass state-of-the-art learning approaches by orders of magnitude in terms of training time, solution quality, and robustness to hyperparameter tuning, while maintaining similar inference times. Finally, we tackle multi-vehicle motion planning with non-convex trajectory preferences and provide $Π$net as a GPU-ready package implemented in JAX.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce an output layer for neural networks that ensures satisfaction of convex constraints. Our approach, $Π$net, leverages operator splitting for rapid and reliable projections in the forward pass, and the implicit function theorem for backpropagation. We deploy $Π$net as a feasible-by-design optimization proxy for parametric constrained optimization problems and obtain modest-accuracy solutions faster than traditional solvers when solving a single problem, and significantly faster for a batch of problems. We surpass state-of-the-art learning approaches by orders of magnitude in terms of training time, solution quality, and robustness to hyperparameter tuning, while maintaining similar inference times. Finally, we tackle multi-vehicle motion planning with non-convex trajectory preferences and provide $Π$net as a GPU-ready package implemented in JAX."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-14T09:32:09Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    9,
                    32,
                    9,
                    3,
                    226,
                    0
                ],
                "arxiv_comment": "Accepted for presentation at, and publication in the proceedings of, the Fourteenth International Conference on Learning Representations (ICLR 2026, oral)",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Panagiotis D. Grontas"
                    },
                    {
                        "name": "Antonio Terpin"
                    },
                    {
                        "name": "Efe C. Balta"
                    },
                    {
                        "name": "Raffaello D'Andrea"
                    },
                    {
                        "name": "John Lygeros"
                    }
                ],
                "author_detail": {
                    "name": "John Lygeros"
                },
                "author": "John Lygeros"
            },
            {
                "id": "http://arxiv.org/abs/2509.00074v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.00074v2",
                "title": "Language and Experience: A Computational Model of Social Learning in Complex Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language and Experience: A Computational Model of Social Learning in Complex Tasks"
                },
                "updated": "2026-02-18T14:02:18Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    14,
                    2,
                    18,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.00074v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.00074v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The ability to combine linguistic guidance from others with direct experience is central to human development, enabling safe and rapid learning in new environments. How do people integrate these two sources of knowledge, and how might AI systems? We present a computational framework that models social learning as joint probabilistic inference over structured, executable world models given sensorimotor and linguistic data. We make this possible by turning a pretrained language model into a probabilistic model of how humans share advice conditioned on their beliefs, allowing our agents both to generate advice for others and to interpret linguistic input as evidence during Bayesian inference. Using behavioral experiments and simulations across 10 video games, we show how linguistic guidance can shape exploration and accelerate learning by reducing risky interactions and speeding up key discoveries in both humans and models. We further explore how knowledge can accumulate across generations through iterated learning experiments and demonstrate successful knowledge transfer between humans and models -- revealing how structured, language-compatible representations might enable human-machine collaborative learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability to combine linguistic guidance from others with direct experience is central to human development, enabling safe and rapid learning in new environments. How do people integrate these two sources of knowledge, and how might AI systems? We present a computational framework that models social learning as joint probabilistic inference over structured, executable world models given sensorimotor and linguistic data. We make this possible by turning a pretrained language model into a probabilistic model of how humans share advice conditioned on their beliefs, allowing our agents both to generate advice for others and to interpret linguistic input as evidence during Bayesian inference. Using behavioral experiments and simulations across 10 video games, we show how linguistic guidance can shape exploration and accelerate learning by reducing risky interactions and speeding up key discoveries in both humans and models. We further explore how knowledge can accumulate across generations through iterated learning experiments and demonstrate successful knowledge transfer between humans and models -- revealing how structured, language-compatible representations might enable human-machine collaborative learning."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-26T18:01:22Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    18,
                    1,
                    22,
                    1,
                    238,
                    0
                ],
                "arxiv_comment": "Code: github.com/ccolas/language_and_experience Demo: cedriccolas.com/demos/language_and_experience",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "arxiv_journal_ref": "ICLR 2026; CogSci 2025",
                "authors": [
                    {
                        "name": "Cédric Colas"
                    },
                    {
                        "name": "Tracey Mills"
                    },
                    {
                        "name": "Ben Prystawski"
                    },
                    {
                        "name": "Michael Henry Tessler"
                    },
                    {
                        "name": "Noah Goodman"
                    },
                    {
                        "name": "Jacob Andreas"
                    },
                    {
                        "name": "Joshua Tenenbaum"
                    }
                ],
                "author_detail": {
                    "name": "Joshua Tenenbaum"
                },
                "author": "Joshua Tenenbaum"
            },
            {
                "id": "http://arxiv.org/abs/2602.16467v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16467v1",
                "title": "IndicEval: A Bilingual Indian Educational Evaluation Framework for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IndicEval: A Bilingual Indian Educational Evaluation Framework for Large Language Models"
                },
                "updated": "2026-02-18T13:55:57Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    13,
                    55,
                    57,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16467v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The rapid advancement of large language models (LLMs) necessitates evaluation frameworks that reflect real-world academic rigor and multilingual complexity. This paper introduces IndicEval, a scalable benchmarking platform designed to assess LLM performance using authentic high-stakes examination questions from UPSC, JEE, and NEET across STEM and humanities domains in both English and Hindi. Unlike synthetic benchmarks, IndicEval grounds evaluation in real examination standards, enabling realistic measurement of reasoning, domain knowledge, and bilingual adaptability. The framework automates assessment using Zero-Shot, Few-Shot, and Chain-of-Thought (CoT) prompting strategies and supports modular integration of new models and languages. Experiments conducted on Gemini 2.0 Flash, GPT-4, Claude, and LLaMA 3-70B reveal three major findings. First, CoT prompting consistently improves reasoning accuracy, with substantial gains across subjects and languages. Second, significant cross-model performance disparities persist, particularly in high-complexity examinations. Third, multilingual degradation remains a critical challenge, with marked accuracy drops in Hindi compared to English, especially under Zero-Shot conditions. These results highlight persistent gaps in bilingual reasoning and domain transfer. Overall, IndicEval provides a practice-oriented, extensible foundation for rigorous, equitable evaluation of LLMs in multilingual educational settings and offers actionable insights for improving reasoning robustness and language adaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) necessitates evaluation frameworks that reflect real-world academic rigor and multilingual complexity. This paper introduces IndicEval, a scalable benchmarking platform designed to assess LLM performance using authentic high-stakes examination questions from UPSC, JEE, and NEET across STEM and humanities domains in both English and Hindi. Unlike synthetic benchmarks, IndicEval grounds evaluation in real examination standards, enabling realistic measurement of reasoning, domain knowledge, and bilingual adaptability. The framework automates assessment using Zero-Shot, Few-Shot, and Chain-of-Thought (CoT) prompting strategies and supports modular integration of new models and languages. Experiments conducted on Gemini 2.0 Flash, GPT-4, Claude, and LLaMA 3-70B reveal three major findings. First, CoT prompting consistently improves reasoning accuracy, with substantial gains across subjects and languages. Second, significant cross-model performance disparities persist, particularly in high-complexity examinations. Third, multilingual degradation remains a critical challenge, with marked accuracy drops in Hindi compared to English, especially under Zero-Shot conditions. These results highlight persistent gaps in bilingual reasoning and domain transfer. Overall, IndicEval provides a practice-oriented, extensible foundation for rigorous, equitable evaluation of LLMs in multilingual educational settings and offers actionable insights for improving reasoning robustness and language adaptability."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T13:55:57Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    13,
                    55,
                    57,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Saurabh Bharti"
                    },
                    {
                        "name": "Gaurav Azad"
                    },
                    {
                        "name": "Abhinaw Jagtap"
                    },
                    {
                        "name": "Nachiket Tapas"
                    }
                ],
                "author_detail": {
                    "name": "Nachiket Tapas"
                },
                "author": "Nachiket Tapas"
            },
            {
                "id": "http://arxiv.org/abs/2506.08822v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.08822v2",
                "title": "FreqPolicy: Efficient Flow-based Visuomotor Policy via Frequency Consistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreqPolicy: Efficient Flow-based Visuomotor Policy via Frequency Consistency"
                },
                "updated": "2026-02-18T13:54:21Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    13,
                    54,
                    21,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.08822v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.08822v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Generative modeling-based visuomotor policies have been widely adopted in robotic manipulation, attributed to their ability to model multimodal action distributions. However, the high inference cost of multi-step sampling limits its applicability in real-time robotic systems. Existing approaches accelerate sampling in generative modeling-based visuomotor policies by adapting techniques originally developed to speed up image generation. However, a major distinction exists: image generation typically produces independent samples without temporal dependencies, while robotic manipulation requires generating action trajectories with continuity and temporal coherence. To this end, we propose FreqPolicy, a novel approach that first imposes frequency consistency constraints on flow-based visuomotor policies. Our work enables the action model to capture temporal structure effectively while supporting efficient, high-quality one-step action generation. Concretely, we introduce a frequency consistency constraint objective that enforces alignment of frequency-domain action features across different timesteps along the flow, thereby promoting convergence of one-step action generation toward the target distribution. In addition, we design an adaptive consistency loss to capture structural temporal variations inherent in robotic manipulation tasks. We assess FreqPolicy on 53 tasks across 3 simulation benchmarks, proving its superiority over existing one-step action generators. We further integrate FreqPolicy into the vision-language-action (VLA) model and achieve acceleration without performance degradation on 40 tasks of LIBERO. Besides, we show efficiency and effectiveness in real-world robotic scenarios with an inference frequency of 93.5 Hz.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative modeling-based visuomotor policies have been widely adopted in robotic manipulation, attributed to their ability to model multimodal action distributions. However, the high inference cost of multi-step sampling limits its applicability in real-time robotic systems. Existing approaches accelerate sampling in generative modeling-based visuomotor policies by adapting techniques originally developed to speed up image generation. However, a major distinction exists: image generation typically produces independent samples without temporal dependencies, while robotic manipulation requires generating action trajectories with continuity and temporal coherence. To this end, we propose FreqPolicy, a novel approach that first imposes frequency consistency constraints on flow-based visuomotor policies. Our work enables the action model to capture temporal structure effectively while supporting efficient, high-quality one-step action generation. Concretely, we introduce a frequency consistency constraint objective that enforces alignment of frequency-domain action features across different timesteps along the flow, thereby promoting convergence of one-step action generation toward the target distribution. In addition, we design an adaptive consistency loss to capture structural temporal variations inherent in robotic manipulation tasks. We assess FreqPolicy on 53 tasks across 3 simulation benchmarks, proving its superiority over existing one-step action generators. We further integrate FreqPolicy into the vision-language-action (VLA) model and achieve acceleration without performance degradation on 40 tasks of LIBERO. Besides, we show efficiency and effectiveness in real-world robotic scenarios with an inference frequency of 93.5 Hz."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-10T14:12:53Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    14,
                    12,
                    53,
                    1,
                    161,
                    0
                ],
                "arxiv_comment": "NeurIPS 2025",
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Yifei Su"
                    },
                    {
                        "name": "Ning Liu"
                    },
                    {
                        "name": "Dong Chen"
                    },
                    {
                        "name": "Zhen Zhao"
                    },
                    {
                        "name": "Kun Wu"
                    },
                    {
                        "name": "Meng Li"
                    },
                    {
                        "name": "Zhiyuan Xu"
                    },
                    {
                        "name": "Zhengping Che"
                    },
                    {
                        "name": "Jian Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Tang"
                },
                "author": "Jian Tang"
            },
            {
                "id": "http://arxiv.org/abs/2602.06051v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.06051v3",
                "title": "CAST: Character-and-Scene Episodic Memory for Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAST: Character-and-Scene Episodic Memory for Agents"
                },
                "updated": "2026-02-18T13:48:31Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    13,
                    48,
                    31,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.06051v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.06051v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Episodic memory is a central component of human memory, which refers to the ability to recall coherent events grounded in who, when, and where. However, most agent memory systems only emphasize semantic recall and treat experience as structures such as key-value, vector, or graph, which makes them struggle to represent and retrieve coherent events. To address this challenge, we propose a Character-and-Scene based memory architecture(CAST) inspired by dramatic theory. Specifically, CAST constructs 3D scenes (time/place/topic) and organizes them into character profiles that summarize the events of a character to represent episodic memory. Moreover, CAST complements this episodic memory with a graph-based semantic memory, which yields a robust dual memory design. Experiments demonstrate that CAST has averagely improved 8.11% F1 and 10.21% J(LLM-as-a-Judge) than baselines on various datasets, especially on open and time-sensitive conversational questions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Episodic memory is a central component of human memory, which refers to the ability to recall coherent events grounded in who, when, and where. However, most agent memory systems only emphasize semantic recall and treat experience as structures such as key-value, vector, or graph, which makes them struggle to represent and retrieve coherent events. To address this challenge, we propose a Character-and-Scene based memory architecture(CAST) inspired by dramatic theory. Specifically, CAST constructs 3D scenes (time/place/topic) and organizes them into character profiles that summarize the events of a character to represent episodic memory. Moreover, CAST complements this episodic memory with a graph-based semantic memory, which yields a robust dual memory design. Experiments demonstrate that CAST has averagely improved 8.11% F1 and 10.21% J(LLM-as-a-Judge) than baselines on various datasets, especially on open and time-sensitive conversational questions."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-14T05:06:16Z",
                "published_parsed": [
                    2026,
                    1,
                    14,
                    5,
                    6,
                    16,
                    2,
                    14,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Kexin Ma"
                    },
                    {
                        "name": "Bojun Li"
                    },
                    {
                        "name": "Yuhua Tang"
                    },
                    {
                        "name": "Liting Sun"
                    },
                    {
                        "name": "Ruochun Jin"
                    }
                ],
                "author_detail": {
                    "name": "Ruochun Jin"
                },
                "author": "Ruochun Jin"
            },
            {
                "id": "http://arxiv.org/abs/2508.02515v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.02515v2",
                "title": "PoeTone: A Framework for Constrained Generation of Structured Chinese Songci with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PoeTone: A Framework for Constrained Generation of Structured Chinese Songci with LLMs"
                },
                "updated": "2026-02-18T13:33:12Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    13,
                    33,
                    12,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.02515v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.02515v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This paper presents a systematic investigation into the constrained generation capabilities of large language models (LLMs) in producing Songci, a classical Chinese poetry form characterized by strict structural, tonal, and rhyme constraints defined by Cipai templates. We first develop a comprehensive, multi-faceted evaluation framework that includes: (i) a formal conformity score, (ii) automated quality assessment using LLMs, (iii) human evaluation, and (iv) classification-based probing tasks. Using this framework, we evaluate the generative performance of 18 LLMs, including 3 proprietary models and 15 open-source models across 4 families, under five prompting strategies: zero-shot, one-shot, completion-based, instruction-based, and chain-of-thought. Finally, we propose a Generate-Critic architecture in which the evaluation framework functions as an automated critic. Leveraging the critic's feedback as a scoring function for best-of-N selection, we fine-tune 3 lightweight open-source LLMs via supervised fine-tuning (SFT), resulting in improvements of up to 5.88% in formal conformity. Our findings offer new insights into the generative strengths and limitations of LLMs in producing culturally significant and formally constrained literary texts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a systematic investigation into the constrained generation capabilities of large language models (LLMs) in producing Songci, a classical Chinese poetry form characterized by strict structural, tonal, and rhyme constraints defined by Cipai templates. We first develop a comprehensive, multi-faceted evaluation framework that includes: (i) a formal conformity score, (ii) automated quality assessment using LLMs, (iii) human evaluation, and (iv) classification-based probing tasks. Using this framework, we evaluate the generative performance of 18 LLMs, including 3 proprietary models and 15 open-source models across 4 families, under five prompting strategies: zero-shot, one-shot, completion-based, instruction-based, and chain-of-thought. Finally, we propose a Generate-Critic architecture in which the evaluation framework functions as an automated critic. Leveraging the critic's feedback as a scoring function for best-of-N selection, we fine-tune 3 lightweight open-source LLMs via supervised fine-tuning (SFT), resulting in improvements of up to 5.88% in formal conformity. Our findings offer new insights into the generative strengths and limitations of LLMs in producing culturally significant and formally constrained literary texts."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-04T15:19:22Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    19,
                    22,
                    0,
                    216,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Zhan Qu"
                    },
                    {
                        "name": "Shuzhou Yuan"
                    },
                    {
                        "name": "Michael Färber"
                    }
                ],
                "author_detail": {
                    "name": "Michael Färber"
                },
                "author": "Michael Färber"
            },
            {
                "id": "http://arxiv.org/abs/2602.16438v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16438v1",
                "title": "Intra-Fairness Dynamics: The Bias Spillover Effect in Targeted LLM Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intra-Fairness Dynamics: The Bias Spillover Effect in Targeted LLM Alignment"
                },
                "updated": "2026-02-18T13:19:11Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    13,
                    19,
                    11,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16438v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16438v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Conventional large language model (LLM) fairness alignment largely focuses on mitigating bias along single sensitive attributes, overlooking fairness as an inherently multidimensional and context-specific value. This approach risks creating systems that achieve narrow fairness metrics while exacerbating disparities along untargeted attributes, a phenomenon known as bias spillover. While extensively studied in machine learning, bias spillover remains critically underexplored in LLM alignment. In this work, we investigate how targeted gender alignment affects fairness across nine sensitive attributes in three state-of-the-art LLMs (Mistral 7B, Llama 3.1 8B, Qwen 2.5 7B). Using Direct Preference Optimization and the BBQ benchmark, we evaluate fairness under ambiguous and disambiguous contexts. Our findings reveal noticeable bias spillover: while aggregate results show improvements, context-aware analysis exposes significant degradations in ambiguous contexts, particularly for physical appearance ($p< 0.001$ across all models), sexual orientation, and disability status. We demonstrate that improving fairness along one attribute can inadvertently worsen disparities in others under uncertainty, highlighting the necessity of context-aware, multi-attribute fairness evaluation frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional large language model (LLM) fairness alignment largely focuses on mitigating bias along single sensitive attributes, overlooking fairness as an inherently multidimensional and context-specific value. This approach risks creating systems that achieve narrow fairness metrics while exacerbating disparities along untargeted attributes, a phenomenon known as bias spillover. While extensively studied in machine learning, bias spillover remains critically underexplored in LLM alignment. In this work, we investigate how targeted gender alignment affects fairness across nine sensitive attributes in three state-of-the-art LLMs (Mistral 7B, Llama 3.1 8B, Qwen 2.5 7B). Using Direct Preference Optimization and the BBQ benchmark, we evaluate fairness under ambiguous and disambiguous contexts. Our findings reveal noticeable bias spillover: while aggregate results show improvements, context-aware analysis exposes significant degradations in ambiguous contexts, particularly for physical appearance ($p< 0.001$ across all models), sexual orientation, and disability status. We demonstrate that improving fairness along one attribute can inadvertently worsen disparities in others under uncertainty, highlighting the necessity of context-aware, multi-attribute fairness evaluation frameworks."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T13:19:11Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    13,
                    19,
                    11,
                    2,
                    49,
                    0
                ],
                "arxiv_comment": "Submitted to the BiAlign CHI Workshop 2026",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Eva Paraschou"
                    },
                    {
                        "name": "Line Harder Clemmensen"
                    },
                    {
                        "name": "Sneha Das"
                    }
                ],
                "author_detail": {
                    "name": "Sneha Das"
                },
                "author": "Sneha Das"
            },
            {
                "id": "http://arxiv.org/abs/2602.16437v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16437v1",
                "title": "Mapping tuberculosis fatalities by region and age group in South Korea: A dataset for targeted health policy optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping tuberculosis fatalities by region and age group in South Korea: A dataset for targeted health policy optimization"
                },
                "updated": "2026-02-18T13:15:56Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    13,
                    15,
                    56,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16437v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16437v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In South Korea, age-disaggregated tuberculosis (TB) data at the district level are not publicly available due to privacy constraints, limiting fine-scale analyses of healthcare accessibility. To address this limitation, we present a high-resolution, district-level dataset on tuberculosis (TB) fatalities and hospital accessibility in South Korea, covering the years 2014 to 2022 across 228 districts. The dataset is constructed using a reconstruction method that infers age-disaggregated TB cases and fatalities at the district level by integrating province-level age-specific statistics with district-level spatial and demographic data, enabling analyses that account for both spatial heterogeneity and age structure. Building on an existing hospital allocation framework, we extend the objective function to an age-weighted formulation and apply it to the reconstructed dataset to minimize TB fatalities under different age-weighting schemes. We demonstrate that incorporating age structure can give rise to distinct optimized hospital allocation patterns, even when the total number of minimized fatalities is similar, revealing trade-offs between efficiency and demographic targeting. In addition, the dataset supports temporal analyses of TB burden, hospital availability, and demographic variation over time, and provides a testbed for spatial epidemiology and optimization studies that require high-resolution demographic and healthcare data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In South Korea, age-disaggregated tuberculosis (TB) data at the district level are not publicly available due to privacy constraints, limiting fine-scale analyses of healthcare accessibility. To address this limitation, we present a high-resolution, district-level dataset on tuberculosis (TB) fatalities and hospital accessibility in South Korea, covering the years 2014 to 2022 across 228 districts. The dataset is constructed using a reconstruction method that infers age-disaggregated TB cases and fatalities at the district level by integrating province-level age-specific statistics with district-level spatial and demographic data, enabling analyses that account for both spatial heterogeneity and age structure. Building on an existing hospital allocation framework, we extend the objective function to an age-weighted formulation and apply it to the reconstructed dataset to minimize TB fatalities under different age-weighting schemes. We demonstrate that incorporating age structure can give rise to distinct optimized hospital allocation patterns, even when the total number of minimized fatalities is similar, revealing trade-offs between efficiency and demographic targeting. In addition, the dataset supports temporal analyses of TB burden, hospital availability, and demographic variation over time, and provides a testbed for spatial epidemiology and optimization studies that require high-resolution demographic and healthcare data."
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T13:15:56Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    13,
                    15,
                    56,
                    2,
                    49,
                    0
                ],
                "arxiv_comment": "15 pages, 9 figures",
                "arxiv_primary_category": {
                    "term": "physics.soc-ph"
                },
                "authors": [
                    {
                        "name": "Yongsung Kwon"
                    },
                    {
                        "name": "Deok-Sun Lee"
                    },
                    {
                        "name": "Mi Jin Lee"
                    },
                    {
                        "name": "Seung-Woo Son"
                    }
                ],
                "author_detail": {
                    "name": "Seung-Woo Son"
                },
                "author": "Seung-Woo Son"
            },
            {
                "id": "http://arxiv.org/abs/2502.20063v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2502.20063v2",
                "title": "Strategic Hiring under Algorithmic Monoculture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strategic Hiring under Algorithmic Monoculture"
                },
                "updated": "2026-02-18T13:04:45Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    13,
                    4,
                    45,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2502.20063v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2502.20063v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We study the impact of strategic behavior in labor markets characterized by algorithmic monoculture, where firms compete for a shared pool of applicants using a common algorithmic evaluation. In this setting, \"naive\" hiring strategies lead to severe congestion, as firms collectively target the same high-scoring candidates. We model this competition as a game with capacity-constrained firms and fully characterize the set of Nash equilibria. We demonstrate that equilibrium strategies, which naturally diversify firms' interview targets, significantly outperform naive selection, increasing social welfare for both firms and applicants. Specifically, the Price of Naive Selection (welfare gain from strategy) grows linearly with the number of firms, while the Price of Anarchy (efficiency loss from decentralization) approaches 1, implying that the decentralized equilibrium is nearly socially optimal. Finally, we analyze convergence, and we show that a simple sequential best-response process converges to the desired equilibrium. However, we show that firms generally cannot infer the key input needed to compute best responses, namely congestion for specific candidates, from their own historical data alone. Consequently, to realize the welfare gains of strategic differentiation, algorithmic platforms must explicitly reveal congestion information to participating firms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the impact of strategic behavior in labor markets characterized by algorithmic monoculture, where firms compete for a shared pool of applicants using a common algorithmic evaluation. In this setting, \"naive\" hiring strategies lead to severe congestion, as firms collectively target the same high-scoring candidates. We model this competition as a game with capacity-constrained firms and fully characterize the set of Nash equilibria. We demonstrate that equilibrium strategies, which naturally diversify firms' interview targets, significantly outperform naive selection, increasing social welfare for both firms and applicants. Specifically, the Price of Naive Selection (welfare gain from strategy) grows linearly with the number of firms, while the Price of Anarchy (efficiency loss from decentralization) approaches 1, implying that the decentralized equilibrium is nearly socially optimal. Finally, we analyze convergence, and we show that a simple sequential best-response process converges to the desired equilibrium. However, we show that firms generally cannot infer the key input needed to compute best responses, namely congestion for specific candidates, from their own historical data alone. Consequently, to realize the welfare gains of strategic differentiation, algorithmic platforms must explicitly reveal congestion information to participating firms."
                },
                "tags": [
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-02-27T13:11:11Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    13,
                    11,
                    11,
                    3,
                    58,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.GT"
                },
                "authors": [
                    {
                        "name": "Jackie Baek"
                    },
                    {
                        "name": "Hamsa Bastani"
                    },
                    {
                        "name": "Shihan Chen"
                    }
                ],
                "author_detail": {
                    "name": "Shihan Chen"
                },
                "author": "Shihan Chen"
            },
            {
                "id": "http://arxiv.org/abs/2602.16430v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16430v1",
                "title": "Designing Production-Scale OCR for India: Multilingual and Domain-Specific Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing Production-Scale OCR for India: Multilingual and Domain-Specific Systems"
                },
                "updated": "2026-02-18T13:03:05Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    13,
                    3,
                    5,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16430v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16430v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Designing Optical Character Recognition (OCR) systems for India requires balancing linguistic diversity, document heterogeneity, and deployment constraints. In this paper, we study two training strategies for building multilingual OCR systems with Vision-Language Models through the Chitrapathak series. We first follow a popular multimodal approach, pairing a generic vision encoder with a strong multilingual language model and training the system end-to-end for OCR. Alternatively, we explore fine-tuning an existing OCR model, despite not being trained for the target languages. Through extensive evaluation on multilingual Indic OCR benchmarks and deployment-oriented metrics, we find that the second strategy consistently achieves better accuracy-latency trade-offs. Chitrapathak-2 achieves 3-6x speedup over its predecessor with being state-of-the-art (SOTA) in Telugu (6.69 char ANLS) and second best in the rest. In addition, we present Parichay, an independent OCR model series designed specifically for 9 Indian government documents to extract structured key fields, achieving 89.8% Exact Match score with a faster inference. Together, these systems achieve SOTA performance and provide practical guidance for building production-scale OCR pipelines in the Indian context.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing Optical Character Recognition (OCR) systems for India requires balancing linguistic diversity, document heterogeneity, and deployment constraints. In this paper, we study two training strategies for building multilingual OCR systems with Vision-Language Models through the Chitrapathak series. We first follow a popular multimodal approach, pairing a generic vision encoder with a strong multilingual language model and training the system end-to-end for OCR. Alternatively, we explore fine-tuning an existing OCR model, despite not being trained for the target languages. Through extensive evaluation on multilingual Indic OCR benchmarks and deployment-oriented metrics, we find that the second strategy consistently achieves better accuracy-latency trade-offs. Chitrapathak-2 achieves 3-6x speedup over its predecessor with being state-of-the-art (SOTA) in Telugu (6.69 char ANLS) and second best in the rest. In addition, we present Parichay, an independent OCR model series designed specifically for 9 Indian government documents to extract structured key fields, achieving 89.8% Exact Match score with a faster inference. Together, these systems achieve SOTA performance and provide practical guidance for building production-scale OCR pipelines in the Indian context."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T13:03:05Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    13,
                    3,
                    5,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Ali Faraz"
                    },
                    {
                        "name": "Raja Kolla"
                    },
                    {
                        "name": "Ashish Kulkarni"
                    },
                    {
                        "name": "Shubham Agarwal"
                    }
                ],
                "author_detail": {
                    "name": "Shubham Agarwal"
                },
                "author": "Shubham Agarwal"
            },
            {
                "id": "http://arxiv.org/abs/2602.16429v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16429v1",
                "title": "TabAgent: A Framework for Replacing Agentic Generative Components with Tabular-Textual Classifiers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TabAgent: A Framework for Replacing Agentic Generative Components with Tabular-Textual Classifiers"
                },
                "updated": "2026-02-18T13:01:17Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    13,
                    1,
                    17,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16429v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16429v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Agentic systems, AI architectures that autonomously execute multi-step workflows to achieve complex goals, are often built using repeated large language model (LLM) calls for closed-set decision tasks such as routing, shortlisting, gating, and verification. While convenient, this design makes deployments slow and expensive due to cumulative latency and token usage. We propose TabAgent, a framework for replacing generative decision components in closed-set selection tasks with a compact textual-tabular classifier trained on execution traces. TabAgent (i) extracts structured schema, state, and dependency features from trajectories (TabSchema), (ii) augments coverage with schema-aligned synthetic supervision (TabSynth), and (iii) scores candidates with a lightweight classifier (TabHead). On the long-horizon AppWorld benchmark, TabAgent maintains task-level success while eliminating shortlist-time LLM calls, reducing latency by approximately 95% and inference cost by 85-91%. Beyond tool shortlisting, TabAgent generalizes to other agentic decision heads, establishing a paradigm for learned discriminative replacements of generative bottlenecks in production agent architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic systems, AI architectures that autonomously execute multi-step workflows to achieve complex goals, are often built using repeated large language model (LLM) calls for closed-set decision tasks such as routing, shortlisting, gating, and verification. While convenient, this design makes deployments slow and expensive due to cumulative latency and token usage. We propose TabAgent, a framework for replacing generative decision components in closed-set selection tasks with a compact textual-tabular classifier trained on execution traces. TabAgent (i) extracts structured schema, state, and dependency features from trajectories (TabSchema), (ii) augments coverage with schema-aligned synthetic supervision (TabSynth), and (iii) scores candidates with a lightweight classifier (TabHead). On the long-horizon AppWorld benchmark, TabAgent maintains task-level success while eliminating shortlist-time LLM calls, reducing latency by approximately 95% and inference cost by 85-91%. Beyond tool shortlisting, TabAgent generalizes to other agentic decision heads, establishing a paradigm for learned discriminative replacements of generative bottlenecks in production agent architectures."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T13:01:17Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    13,
                    1,
                    17,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Ido Levy"
                    },
                    {
                        "name": "Eilam Shapira"
                    },
                    {
                        "name": "Yinon Goldshtein"
                    },
                    {
                        "name": "Avi Yaeli"
                    },
                    {
                        "name": "Nir Mashkif"
                    },
                    {
                        "name": "Segev Shlomov"
                    }
                ],
                "author_detail": {
                    "name": "Segev Shlomov"
                },
                "author": "Segev Shlomov"
            },
            {
                "id": "http://arxiv.org/abs/2510.12976v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.12976v3",
                "title": "Likelihood-free inference of phylogenetic tree posterior distributions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Likelihood-free inference of phylogenetic tree posterior distributions"
                },
                "updated": "2026-02-18T12:10:59Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    12,
                    10,
                    59,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.12976v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.12976v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Phylogenetic inference, the task of reconstructing how related sequences evolved from common ancestors, is a central objective in evolutionary genomics. The current state-of-the-art methods exploit probabilistic models of sequence evolution along phylogenetic trees, by searching for the tree maximizing the likelihood of observed sequences, or by estimating the posterior of the tree given the sequences in a Bayesian framework. Both approaches typically require to compute likelihoods, which is only feasible under simplifying assumptions such as independence of the evolution at the different positions of the sequence, and even then remains a costly operation. Here we present the first likelihood-free inference method for posterior distributions over phylogenies. It exploits a novel expressive encoding for pairs of sequences, and a parameterized probability distribution factorized over a succession of subtree merges. The resulting network provides well-calibrated estimates of the posterior distribution leading to more accurate tree topologies than existing methods, even under models amenable to likelihood computation. We further show that its edge against likelihood-based methods dramatically increases under models of sequence evolution with intractable likelihoods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phylogenetic inference, the task of reconstructing how related sequences evolved from common ancestors, is a central objective in evolutionary genomics. The current state-of-the-art methods exploit probabilistic models of sequence evolution along phylogenetic trees, by searching for the tree maximizing the likelihood of observed sequences, or by estimating the posterior of the tree given the sequences in a Bayesian framework. Both approaches typically require to compute likelihoods, which is only feasible under simplifying assumptions such as independence of the evolution at the different positions of the sequence, and even then remains a costly operation. Here we present the first likelihood-free inference method for posterior distributions over phylogenies. It exploits a novel expressive encoding for pairs of sequences, and a parameterized probability distribution factorized over a succession of subtree merges. The resulting network provides well-calibrated estimates of the posterior distribution leading to more accurate tree topologies than existing methods, even under models amenable to likelihood computation. We further show that its edge against likelihood-based methods dramatically increases under models of sequence evolution with intractable likelihoods."
                },
                "tags": [
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-14T20:38:44Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    20,
                    38,
                    44,
                    1,
                    287,
                    0
                ],
                "arxiv_comment": "12 Pages, 3 figures",
                "arxiv_primary_category": {
                    "term": "q-bio.PE"
                },
                "authors": [
                    {
                        "name": "Luc Blassel"
                    },
                    {
                        "name": "Noémie Sauvage"
                    },
                    {
                        "name": "Pierre Barrat-Charlaix"
                    },
                    {
                        "name": "Bastien Boussau"
                    },
                    {
                        "name": "Nicolas Lartillot"
                    },
                    {
                        "name": "Laurent Jacob"
                    }
                ],
                "author_detail": {
                    "name": "Laurent Jacob"
                },
                "author": "Laurent Jacob"
            },
            {
                "id": "http://arxiv.org/abs/2508.10088v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.10088v2",
                "title": "Can GW231123 have a stellar origin?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can GW231123 have a stellar origin?"
                },
                "updated": "2026-02-18T11:56:54Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    11,
                    56,
                    54,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.10088v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.10088v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1093/mnras/stag073",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "The gravitational wave event GW231123 detected by the LIGO interferometers during their fourth observing run features two black holes with source-frame masses of $137^{+23}_{-18} M_\\odot$ and $101^{+22}_{-50} M_\\odot$ -- in the range of the pair-instability black hole mass gap predicted by standard stellar evolution theory. Both black holes are also inferred to be rapidly spinning ($χ_1 \\simeq 0.9$, $χ_2 \\simeq 0.8$). The primary object in GW231123 is the heaviest stellar mass black hole detected to date, which, together with its extreme rotation, raises questions about its astrophysical origin. Accounting for the unusually large spin of $\\sim 0.9$ with hierarchical mergers requires some degree of fine tuning. We investigate whether such a massive, highly spinning object could plausibly form from the collapse of a single rotating massive star. We simulate stars with an initial core mass of $160\\,M_\\odot$ -- sufficient to produce BH masses at the upper edge of the 90\\% credible interval for $m_1$ in GW231123 -- across a range of rotation rates and $^{12}\\mathrm{C}(α,γ)^{16}\\mathrm{O}$ reaction rates. We allow for differential rotation to explore the high-spin regime. In this limit of weak angular momentum transport, we find that: (i) rotation shifts the pair-instability mass gap to higher masses, introducing an important correlation between masses and spins in gravitational wave predictions; and (ii) highly spinning BHs with masses $\\gtrsim 150 \\rm M_\\odot$ can form above the mass gap. Our results suggest that the primary object of GW231123 may be the first directly observed black hole that formed via direct core collapse following the photodisintegration instability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The gravitational wave event GW231123 detected by the LIGO interferometers during their fourth observing run features two black holes with source-frame masses of $137^{+23}_{-18} M_\\odot$ and $101^{+22}_{-50} M_\\odot$ -- in the range of the pair-instability black hole mass gap predicted by standard stellar evolution theory. Both black holes are also inferred to be rapidly spinning ($χ_1 \\simeq 0.9$, $χ_2 \\simeq 0.8$). The primary object in GW231123 is the heaviest stellar mass black hole detected to date, which, together with its extreme rotation, raises questions about its astrophysical origin. Accounting for the unusually large spin of $\\sim 0.9$ with hierarchical mergers requires some degree of fine tuning. We investigate whether such a massive, highly spinning object could plausibly form from the collapse of a single rotating massive star. We simulate stars with an initial core mass of $160\\,M_\\odot$ -- sufficient to produce BH masses at the upper edge of the 90\\% credible interval for $m_1$ in GW231123 -- across a range of rotation rates and $^{12}\\mathrm{C}(α,γ)^{16}\\mathrm{O}$ reaction rates. We allow for differential rotation to explore the high-spin regime. In this limit of weak angular momentum transport, we find that: (i) rotation shifts the pair-instability mass gap to higher masses, introducing an important correlation between masses and spins in gravitational wave predictions; and (ii) highly spinning BHs with masses $\\gtrsim 150 \\rm M_\\odot$ can form above the mass gap. Our results suggest that the primary object of GW231123 may be the first directly observed black hole that formed via direct core collapse following the photodisintegration instability."
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-13T18:00:00Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    18,
                    0,
                    0,
                    2,
                    225,
                    0
                ],
                "arxiv_comment": "5 pages, 2 figures. Updated to version published in MNRAS",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE"
                },
                "arxiv_journal_ref": "Mon Not R Astron Soc (2026)",
                "authors": [
                    {
                        "name": "Djuna Croon"
                    },
                    {
                        "name": "Davide Gerosa"
                    },
                    {
                        "name": "Jeremy Sakstein"
                    }
                ],
                "author_detail": {
                    "name": "Jeremy Sakstein"
                },
                "author": "Jeremy Sakstein",
                "arxiv_doi": "10.1093/mnras/stag073"
            },
            {
                "id": "http://arxiv.org/abs/2602.03672v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.03672v2",
                "title": "JWST imaging of the Pleiades: anisotropy of turbulence in the cold neutral medium",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JWST imaging of the Pleiades: anisotropy of turbulence in the cold neutral medium"
                },
                "updated": "2026-02-18T11:52:59Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    11,
                    52,
                    59,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.03672v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.03672v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Interstellar medium studies rely on magnetohydrodynamic (MHD) turbulence as a framework for interpretation. In this context, the statistical characterization of interstellar observations is of prime importance. We open a new perspective on diffuse interstellar matter by analyzing James Webb Space Telescope (JWST) observations of the Pleiades nebula with NIRCam. These observations are remarkable in that they provide a microscope view at the cold neutral medium (CNM) with a spatial resolution of 0.2 mpc (40 au). A two-dimensional Fourier analysis is used to characterize the structure of PAH emission in regions near and far from the Pleiades star Merope. To produce maps of the interstellar emission, stars and galaxies are filtered out. The final step in the data cleaning involves subtracting a component, in Fourier space, which we infer to be a residual of the near-infrared cosmic background. The PAH emission power spectra are highly anisotropic. They are well fitted with a break-free power-law, suggesting that we do not observe a specific scale for energy dissipation. Power-law indices are -3.5 near Merope and -3 in the more distant field. The magnetic field orientation, as derived from the Planck dust polarization data, aligns with the PAH anisotropy. The power anisotropy is constant across scales. These findings are discussed in relation to interstellar turbulence that may be driven by the Pleiades stars. The JWST observations of the Pleiades offer a new viewpoint for comparing observations and theoretical models, as they examine physical scales at which turbulence in the CNM is subsonic and decoupled from the thermal instability. The observations may indicate that the turbulent energy cascade in the CNM is anisotropic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interstellar medium studies rely on magnetohydrodynamic (MHD) turbulence as a framework for interpretation. In this context, the statistical characterization of interstellar observations is of prime importance. We open a new perspective on diffuse interstellar matter by analyzing James Webb Space Telescope (JWST) observations of the Pleiades nebula with NIRCam. These observations are remarkable in that they provide a microscope view at the cold neutral medium (CNM) with a spatial resolution of 0.2 mpc (40 au). A two-dimensional Fourier analysis is used to characterize the structure of PAH emission in regions near and far from the Pleiades star Merope. To produce maps of the interstellar emission, stars and galaxies are filtered out. The final step in the data cleaning involves subtracting a component, in Fourier space, which we infer to be a residual of the near-infrared cosmic background. The PAH emission power spectra are highly anisotropic. They are well fitted with a break-free power-law, suggesting that we do not observe a specific scale for energy dissipation. Power-law indices are -3.5 near Merope and -3 in the more distant field. The magnetic field orientation, as derived from the Planck dust polarization data, aligns with the PAH anisotropy. The power anisotropy is constant across scales. These findings are discussed in relation to interstellar turbulence that may be driven by the Pleiades stars. The JWST observations of the Pleiades offer a new viewpoint for comparing observations and theoretical models, as they examine physical scales at which turbulence in the CNM is subsonic and decoupled from the thermal instability. The observations may indicate that the turbulent energy cascade in the CNM is anisotropic."
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-03T15:52:45Z",
                "published_parsed": [
                    2026,
                    2,
                    3,
                    15,
                    52,
                    45,
                    1,
                    34,
                    0
                ],
                "arxiv_comment": "Accepted by A&A",
                "arxiv_primary_category": {
                    "term": "astro-ph.GA"
                },
                "authors": [
                    {
                        "name": "G. Vigoureux"
                    },
                    {
                        "name": "N. Flagey"
                    },
                    {
                        "name": "F. Boulanger"
                    },
                    {
                        "name": "A. Noriega-Crespo"
                    },
                    {
                        "name": "V. Guillet"
                    },
                    {
                        "name": "A. J. Alvarez-Castro"
                    },
                    {
                        "name": "N. deJesus-Rivera"
                    },
                    {
                        "name": "E. Allys"
                    },
                    {
                        "name": "J. M. Delouis"
                    },
                    {
                        "name": "E. Falgarone"
                    },
                    {
                        "name": "B. Godard"
                    },
                    {
                        "name": "P. Guillard"
                    },
                    {
                        "name": "F. Levrier"
                    },
                    {
                        "name": "P. Lesaffre"
                    },
                    {
                        "name": "A. Marcowith"
                    },
                    {
                        "name": "M. A. Miville-Deschênes"
                    },
                    {
                        "name": "G. Pineau des Forêts"
                    }
                ],
                "author_detail": {
                    "name": "G. Pineau des Forêts"
                },
                "author": "G. Pineau des Forêts"
            },
            {
                "id": "http://arxiv.org/abs/2602.16379v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16379v1",
                "title": "Label-Consistent Data Generation for Aspect-Based Sentiment Analysis Using LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Label-Consistent Data Generation for Aspect-Based Sentiment Analysis Using LLM Agents"
                },
                "updated": "2026-02-18T11:38:11Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    11,
                    38,
                    11,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16379v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16379v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We propose an agentic data augmentation method for Aspect-Based Sentiment Analysis (ABSA) that uses iterative generation and verification to produce high quality synthetic training examples. To isolate the effect of agentic structure, we also develop a closely matched prompting-based baseline using the same model and instructions. Both methods are evaluated across three ABSA subtasks (Aspect Term Extraction (ATE), Aspect Sentiment Classification (ATSC), and Aspect Sentiment Pair Extraction (ASPE)), four SemEval datasets, and two encoder-decoder models: T5-Base and Tk-Instruct. Our results show that the agentic augmentation outperforms raw prompting in label preservation of the augmented data, especially when the tasks require aspect term generation. In addition, when combined with real data, agentic augmentation provides higher gains, consistently outperforming prompting-based generation. These benefits are most pronounced for T5-Base, while the more heavily pretrained Tk-Instruct exhibits smaller improvements. As a result, augmented data helps T5-Base achieve comparable performance with its counterpart.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose an agentic data augmentation method for Aspect-Based Sentiment Analysis (ABSA) that uses iterative generation and verification to produce high quality synthetic training examples. To isolate the effect of agentic structure, we also develop a closely matched prompting-based baseline using the same model and instructions. Both methods are evaluated across three ABSA subtasks (Aspect Term Extraction (ATE), Aspect Sentiment Classification (ATSC), and Aspect Sentiment Pair Extraction (ASPE)), four SemEval datasets, and two encoder-decoder models: T5-Base and Tk-Instruct. Our results show that the agentic augmentation outperforms raw prompting in label preservation of the augmented data, especially when the tasks require aspect term generation. In addition, when combined with real data, agentic augmentation provides higher gains, consistently outperforming prompting-based generation. These benefits are most pronounced for T5-Base, while the more heavily pretrained Tk-Instruct exhibits smaller improvements. As a result, augmented data helps T5-Base achieve comparable performance with its counterpart."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T11:38:11Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    11,
                    38,
                    11,
                    2,
                    49,
                    0
                ],
                "arxiv_comment": "Accepted to WASSA Workshop at EACL 2026",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Mohammad H. A. Monfared"
                    },
                    {
                        "name": "Lucie Flek"
                    },
                    {
                        "name": "Akbar Karimi"
                    }
                ],
                "author_detail": {
                    "name": "Akbar Karimi"
                },
                "author": "Akbar Karimi"
            },
            {
                "id": "http://arxiv.org/abs/2602.16376v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16376v1",
                "title": "Two-way Clustering Robust Variance Estimator in Quantile Regression Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-way Clustering Robust Variance Estimator in Quantile Regression Models"
                },
                "updated": "2026-02-18T11:35:18Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    11,
                    35,
                    18,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16376v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16376v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We study inference for linear quantile regression with two-way clustered data. Using a separately exchangeable array framework and a projection decomposition of the quantile score, we characterize regime-dependent convergence rates and establish a self-normalized Gaussian approximation. We propose a two-way cluster-robust sandwich variance estimator with a kernel-based density ``bread'' and a projection-matched ``meat'', and prove consistency and validity of inference in Gaussian regimes. We also show an impossibility result for uniform inference in a non-Gaussian interaction regime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study inference for linear quantile regression with two-way clustered data. Using a separately exchangeable array framework and a projection decomposition of the quantile score, we characterize regime-dependent convergence rates and establish a self-normalized Gaussian approximation. We propose a two-way cluster-robust sandwich variance estimator with a kernel-based density ``bread'' and a projection-matched ``meat'', and prove consistency and validity of inference in Gaussian regimes. We also show an impossibility result for uniform inference in a non-Gaussian interaction regime."
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T11:35:18Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    11,
                    35,
                    18,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM"
                },
                "authors": [
                    {
                        "name": "Ulrich Hounyo"
                    },
                    {
                        "name": "Jiahao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Jiahao Lin"
                },
                "author": "Jiahao Lin"
            },
            {
                "id": "http://arxiv.org/abs/2602.11047v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.11047v3",
                "title": "Embedding Inversion via Conditional Masked Diffusion Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embedding Inversion via Conditional Masked Diffusion Language Models"
                },
                "updated": "2026-02-18T11:21:28Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    11,
                    21,
                    28,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.11047v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.11047v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We frame embedding inversion as conditional masked diffusion, recovering all tokens in parallel through iterative denoising rather than sequential autoregressive generation. A masked diffusion language model is conditioned on the target embedding via adaptive layer normalization, requiring only 8 forward passes with no access to the target encoder at inference time. On 32-token sequences across three embedding models, the method achieves token recovery through parallel denoising without requiring encoder access, iterative correction, or architecture-specific alignment. Source code and live demo are available at https://github.com/jina-ai/embedding-inversion-demo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We frame embedding inversion as conditional masked diffusion, recovering all tokens in parallel through iterative denoising rather than sequential autoregressive generation. A masked diffusion language model is conditioned on the target embedding via adaptive layer normalization, requiring only 8 forward passes with no access to the target encoder at inference time. On 32-token sequences across three embedding models, the method achieves token recovery through parallel denoising without requiring encoder access, iterative correction, or architecture-specific alignment. Source code and live demo are available at https://github.com/jina-ai/embedding-inversion-demo."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-11T17:17:13Z",
                "published_parsed": [
                    2026,
                    2,
                    11,
                    17,
                    17,
                    13,
                    2,
                    42,
                    0
                ],
                "arxiv_comment": "8 pages, 3 figures, 4 tables. Code and demo: https://github.com/jina-ai/embedding-inversion-demo",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Han Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Han Xiao"
                },
                "author": "Han Xiao"
            },
            {
                "id": "http://arxiv.org/abs/2511.17178v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17178v2",
                "title": "Efficient Robot Design with Multi-Objective Black-Box Optimization and Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Robot Design with Multi-Objective Black-Box Optimization and Large Language Models"
                },
                "updated": "2026-02-18T11:18:33Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    11,
                    18,
                    33,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17178v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17178v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1109/ACCESS.2026.3664844",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Various methods for robot design optimization have been developed so far. These methods are diverse, ranging from numerical optimization to black-box optimization. While numerical optimization is fast, it is not suitable for cases involving complex structures or discrete values, leading to frequent use of black-box optimization instead. However, black-box optimization suffers from low sampling efficiency and takes considerable sampling iterations to obtain good solutions. In this study, we propose a method to enhance the efficiency of robot body design based on black-box optimization by utilizing large language models (LLMs). In parallel with the sampling process based on black-box optimization, sampling is performed using LLMs, which are provided with problem settings and extensive feedback. We demonstrate that this method enables more efficient exploration of design solutions and discuss its characteristics and limitations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Various methods for robot design optimization have been developed so far. These methods are diverse, ranging from numerical optimization to black-box optimization. While numerical optimization is fast, it is not suitable for cases involving complex structures or discrete values, leading to frequent use of black-box optimization instead. However, black-box optimization suffers from low sampling efficiency and takes considerable sampling iterations to obtain good solutions. In this study, we propose a method to enhance the efficiency of robot body design based on black-box optimization by utilizing large language models (LLMs). In parallel with the sampling process based on black-box optimization, sampling is performed using LLMs, which are provided with problem settings and extensive feedback. We demonstrate that this method enables more efficient exploration of design solutions and discuss its characteristics and limitations."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T11:56:52Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    11,
                    56,
                    52,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "Accepted to IEEE Access, website: https://haraduka.github.io/urdf-llm-opt/ , video: https://www.youtube.com/watch?v=N9iMjx7of1w",
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Kento Kawaharazuka"
                    },
                    {
                        "name": "Yoshiki Obinata"
                    },
                    {
                        "name": "Naoaki Kanazawa"
                    },
                    {
                        "name": "Haoyu Jia"
                    },
                    {
                        "name": "Kei Okada"
                    }
                ],
                "author_detail": {
                    "name": "Kei Okada"
                },
                "author": "Kei Okada",
                "arxiv_doi": "10.1109/ACCESS.2026.3664844"
            },
            {
                "id": "http://arxiv.org/abs/2602.16372v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16372v1",
                "title": "AI-Driven Structure Refinement of X-ray Diffraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-Driven Structure Refinement of X-ray Diffraction"
                },
                "updated": "2026-02-18T11:14:35Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    11,
                    14,
                    35,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16372v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16372v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Artificial intelligence can rapidly propose candidate phases and structures from X-ray diffraction (XRD), but these hypotheses often fail in downstream refinement because peak intensities cannot be stably assigned under severe overlap and diffraction consistency is enforced only weakly. Here we introduce WPEM, a physics-constrained whole-pattern decomposition and refinement workflow that turns Bragg's law into an explicit constraint within a batch expectation--maximization framework. WPEM models the full profile as a probabilistic mixture density and iteratively infers component-resolved intensities while keeping peak centres Bragg-consistent, producing a continuous, physically admissible intensity representation that remains stable in heavily overlapped regions and in the presence of mixed radiation or multiple phases. We benchmark WPEM on standard reference patterns (\\ce{PbSO4} and \\ce{Tb2BaCoO5}), where it yields lower $R_{\\mathrm{p}}$/$R_{\\mathrm{wp}}$ than widely used packages (FullProf and TOPAS) under matched refinement conditions. We further demonstrate generality across realistic experimental scenarios, including phase-resolved decomposition of a multiphase Ti--15Nb thin film, quantitative recovery of \\ce{NaCl}--\\ce{Li2CO3} mixture compositions, separation of crystalline peaks from amorphous halos in semicrystalline polymers, high-throughput operando lattice tracking in layered cathodes, automated refinement of a compositionally disordered Ru--Mn oxide solid solution (CCDC 2530452), and quantitative phase-resolved deciphering of an ancient Egyptian make-up sample from synchrotron powder XRD. By providing Bragg-consistent, uncertainty-aware intensity partitioning as a refinement-ready interface, WPEM closes the gap between AI-generated hypotheses and diffraction-admissible structure refinement on challenging XRD data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence can rapidly propose candidate phases and structures from X-ray diffraction (XRD), but these hypotheses often fail in downstream refinement because peak intensities cannot be stably assigned under severe overlap and diffraction consistency is enforced only weakly. Here we introduce WPEM, a physics-constrained whole-pattern decomposition and refinement workflow that turns Bragg's law into an explicit constraint within a batch expectation--maximization framework. WPEM models the full profile as a probabilistic mixture density and iteratively infers component-resolved intensities while keeping peak centres Bragg-consistent, producing a continuous, physically admissible intensity representation that remains stable in heavily overlapped regions and in the presence of mixed radiation or multiple phases. We benchmark WPEM on standard reference patterns (\\ce{PbSO4} and \\ce{Tb2BaCoO5}), where it yields lower $R_{\\mathrm{p}}$/$R_{\\mathrm{wp}}$ than widely used packages (FullProf and TOPAS) under matched refinement conditions. We further demonstrate generality across realistic experimental scenarios, including phase-resolved decomposition of a multiphase Ti--15Nb thin film, quantitative recovery of \\ce{NaCl}--\\ce{Li2CO3} mixture compositions, separation of crystalline peaks from amorphous halos in semicrystalline polymers, high-throughput operando lattice tracking in layered cathodes, automated refinement of a compositionally disordered Ru--Mn oxide solid solution (CCDC 2530452), and quantitative phase-resolved deciphering of an ancient Egyptian make-up sample from synchrotron powder XRD. By providing Bragg-consistent, uncertainty-aware intensity partitioning as a refinement-ready interface, WPEM closes the gap between AI-generated hypotheses and diffraction-admissible structure refinement on challenging XRD data."
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T11:14:35Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    11,
                    14,
                    35,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci"
                },
                "authors": [
                    {
                        "name": "Bin Cao"
                    },
                    {
                        "name": "Qian Zhang"
                    },
                    {
                        "name": "Zhenjie Feng"
                    },
                    {
                        "name": "Taolue Zhang"
                    },
                    {
                        "name": "Jiaqiang Huang"
                    },
                    {
                        "name": "Lu-Tao Weng"
                    },
                    {
                        "name": "Tong-Yi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong-Yi Zhang"
                },
                "author": "Tong-Yi Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2602.16362v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16362v1",
                "title": "How Reliable is Your Service at the Extreme Edge? Analytical Modeling of Computational Reliability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Reliable is Your Service at the Extreme Edge? Analytical Modeling of Computational Reliability"
                },
                "updated": "2026-02-18T11:03:07Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    11,
                    3,
                    7,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16362v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16362v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Extreme Edge Computing (XEC) distributes streaming workloads across consumer-owned devices, exploiting their proximity to users and ubiquitous availability. Many such workloads are AI-driven, requiring continuous neural network inference for tasks like object detection and video analytics. Distributed Inference (DI), which partitions model execution across multiple edge devices, enables these streaming services to meet strict throughput and latency requirements. Yet consumer devices exhibit volatile computational availability due to competing applications and unpredictable usage patterns. This volatility poses a fundamental challenge: how can we quantify the probability that a device, or ensemble of devices, will maintain the processing rate required by a streaming service? This paper presents an analytical framework for computational reliability in XEC, defined as the probability that instantaneous capacity meets demand at a specified Quality of Service (QoS) threshold. We derive closed-form reliability expressions under two information regimes: Minimal Information (MI), requiring only declared operational bounds, and historical data, which refines estimates via Maximum Likelihood Estimation from past observations. The framework extends to multi-device deployments, providing reliability expressions for series, parallel, and partitioned workload configurations. We derive optimal workload allocation rules and analytical bounds for device selection, equipping orchestrators with tractable tools to evaluate deployment feasibility and configure distributed streaming systems. We validate the framework using real-time object detection with YOLO11m model as a representative DI streaming workload; experiments on emulated XED environments demonstrate close agreement between analytical predictions, Monte Carlo sampling, and empirical measurements across diverse capacity and demand configurations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extreme Edge Computing (XEC) distributes streaming workloads across consumer-owned devices, exploiting their proximity to users and ubiquitous availability. Many such workloads are AI-driven, requiring continuous neural network inference for tasks like object detection and video analytics. Distributed Inference (DI), which partitions model execution across multiple edge devices, enables these streaming services to meet strict throughput and latency requirements. Yet consumer devices exhibit volatile computational availability due to competing applications and unpredictable usage patterns. This volatility poses a fundamental challenge: how can we quantify the probability that a device, or ensemble of devices, will maintain the processing rate required by a streaming service? This paper presents an analytical framework for computational reliability in XEC, defined as the probability that instantaneous capacity meets demand at a specified Quality of Service (QoS) threshold. We derive closed-form reliability expressions under two information regimes: Minimal Information (MI), requiring only declared operational bounds, and historical data, which refines estimates via Maximum Likelihood Estimation from past observations. The framework extends to multi-device deployments, providing reliability expressions for series, parallel, and partitioned workload configurations. We derive optimal workload allocation rules and analytical bounds for device selection, equipping orchestrators with tractable tools to evaluate deployment feasibility and configure distributed streaming systems. We validate the framework using real-time object detection with YOLO11m model as a representative DI streaming workload; experiments on emulated XED environments demonstrate close agreement between analytical predictions, Monte Carlo sampling, and empirical measurements across diverse capacity and demand configurations."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T11:03:07Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    11,
                    3,
                    7,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "MHD Saria Allahham"
                    },
                    {
                        "name": "Hossam S. Hassanein"
                    }
                ],
                "author_detail": {
                    "name": "Hossam S. Hassanein"
                },
                "author": "Hossam S. Hassanein"
            },
            {
                "id": "http://arxiv.org/abs/2602.00663v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.00663v2",
                "title": "SEISMO: Increasing Sample Efficiency in Molecular Optimization with a Trajectory-Aware LLM Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEISMO: Increasing Sample Efficiency in Molecular Optimization with a Trajectory-Aware LLM Agent"
                },
                "updated": "2026-02-18T10:50:04Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    10,
                    50,
                    4,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.00663v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.00663v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Optimizing the structure of molecules to achieve desired properties is a central bottleneck across the chemical sciences, particularly in the pharmaceutical industry where it underlies the discovery of new drugs. Since molecular property evaluation often relies on costly and rate-limited oracles, such as experimental assays, molecular optimization must be highly sample-efficient. To address this, we introduce SEISMO, an LLM agent that performs strictly online, inference-time molecular optimization, updating after every oracle call without the need for population-based or batched learning. SEISMO conditions each proposal on the full optimization trajectory, combining natural-language task descriptions with scalar scores and, when available, structured explanatory feedback. Across the Practical Molecular Optimization benchmark of 23 tasks, SEISMO achieves a 2-3 times higher area under the optimisation curve than prior methods, often reaching near-maximal task scores within 50 oracle calls. Our additional medicinal-chemistry tasks show that providing explanatory feedback further improves efficiency, demonstrating that leveraging domain knowledge and structured information is key to sample-efficient molecular optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing the structure of molecules to achieve desired properties is a central bottleneck across the chemical sciences, particularly in the pharmaceutical industry where it underlies the discovery of new drugs. Since molecular property evaluation often relies on costly and rate-limited oracles, such as experimental assays, molecular optimization must be highly sample-efficient. To address this, we introduce SEISMO, an LLM agent that performs strictly online, inference-time molecular optimization, updating after every oracle call without the need for population-based or batched learning. SEISMO conditions each proposal on the full optimization trajectory, combining natural-language task descriptions with scalar scores and, when available, structured explanatory feedback. Across the Practical Molecular Optimization benchmark of 23 tasks, SEISMO achieves a 2-3 times higher area under the optimisation curve than prior methods, often reaching near-maximal task scores within 50 oracle calls. Our additional medicinal-chemistry tasks show that providing explanatory feedback further improves efficiency, demonstrating that leveraging domain knowledge and structured information is key to sample-efficient molecular optimization."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-31T11:23:48Z",
                "published_parsed": [
                    2026,
                    1,
                    31,
                    11,
                    23,
                    48,
                    5,
                    31,
                    0
                ],
                "arxiv_comment": "Fabian P. Krüger and Andrea Hunklinger contributed equally to this work",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Fabian P. Krüger"
                    },
                    {
                        "name": "Andrea Hunklinger"
                    },
                    {
                        "name": "Adrian Wolny"
                    },
                    {
                        "name": "Tim J. Adler"
                    },
                    {
                        "name": "Igor Tetko"
                    },
                    {
                        "name": "Santiago David Villalba"
                    }
                ],
                "author_detail": {
                    "name": "Santiago David Villalba"
                },
                "author": "Santiago David Villalba"
            },
            {
                "id": "http://arxiv.org/abs/2602.14981v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.14981v2",
                "title": "Block Empirical Likelihood Inference for Longitudinal Generalized Partially Linear Single-Index Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block Empirical Likelihood Inference for Longitudinal Generalized Partially Linear Single-Index Models"
                },
                "updated": "2026-02-18T10:47:59Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    10,
                    47,
                    59,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.14981v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.14981v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Generalized partially linear single-index models (GPLSIMs) provide a flexible and interpretable semiparametric framework for longitudinal outcomes by combining a low-dimensional parametric component with a nonparametric index component. For repeated measurements, valid inference is challenging because within-subject correlation induces nuisance parameters and variance estimation can be unstable in semiparametric settings. We propose a profile estimating-equation approach based on spline approximation of the unknown link function and construct a subject-level block empirical likelihood (BEL) for joint inference on the parametric coefficients and the single-index direction. The resulting BEL ratio statistic enjoys a Wilks-type chi-square limit, yielding likelihood-free confidence regions without explicit sandwich variance estimation. We also discuss practical implementation, including constrained optimization for the index parameter, working-correlation choices, and bootstrap-based confidence bands for the nonparametric component. Simulation studies and an application to the epilepsy longitudinal study illustrate the finite-sample performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalized partially linear single-index models (GPLSIMs) provide a flexible and interpretable semiparametric framework for longitudinal outcomes by combining a low-dimensional parametric component with a nonparametric index component. For repeated measurements, valid inference is challenging because within-subject correlation induces nuisance parameters and variance estimation can be unstable in semiparametric settings. We propose a profile estimating-equation approach based on spline approximation of the unknown link function and construct a subject-level block empirical likelihood (BEL) for joint inference on the parametric coefficients and the single-index direction. The resulting BEL ratio statistic enjoys a Wilks-type chi-square limit, yielding likelihood-free confidence regions without explicit sandwich variance estimation. We also discuss practical implementation, including constrained optimization for the index parameter, working-correlation choices, and bootstrap-based confidence bands for the nonparametric component. Simulation studies and an application to the epilepsy longitudinal study illustrate the finite-sample performance."
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-16T18:03:59Z",
                "published_parsed": [
                    2026,
                    2,
                    16,
                    18,
                    3,
                    59,
                    0,
                    47,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME"
                },
                "authors": [
                    {
                        "name": "Tianni Zhang"
                    },
                    {
                        "name": "Yuyao Wang"
                    },
                    {
                        "name": "Yu Lu"
                    },
                    {
                        "name": "Mengfei Ran"
                    }
                ],
                "author_detail": {
                    "name": "Mengfei Ran"
                },
                "author": "Mengfei Ran"
            },
            {
                "id": "http://arxiv.org/abs/2602.16356v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16356v1",
                "title": "Articulated 3D Scene Graphs for Open-World Mobile Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Articulated 3D Scene Graphs for Open-World Mobile Manipulation"
                },
                "updated": "2026-02-18T10:40:35Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    10,
                    40,
                    35,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16356v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16356v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Semantics has enabled 3D scene understanding and affordance-driven object interaction. However, robots operating in real-world environments face a critical limitation: they cannot anticipate how objects move. Long-horizon mobile manipulation requires closing the gap between semantics, geometry, and kinematics. In this work, we present MoMa-SG, a novel framework for building semantic-kinematic 3D scene graphs of articulated scenes containing a myriad of interactable objects. Given RGB-D sequences containing multiple object articulations, we temporally segment object interactions and infer object motion using occlusion-robust point tracking. We then lift point trajectories into 3D and estimate articulation models using a novel unified twist estimation formulation that robustly estimates revolute and prismatic joint parameters in a single optimization pass. Next, we associate objects with estimated articulations and detect contained objects by reasoning over parent-child relations at identified opening states. We also introduce the novel Arti4D-Semantic dataset, which uniquely combines hierarchical object semantics including parent-child relation labels with object axis annotations across 62 in-the-wild RGB-D sequences containing 600 object interactions and three distinct observation paradigms. We extensively evaluate the performance of MoMa-SG on two datasets and ablate key design choices of our approach. In addition, real-world experiments on both a quadruped and a mobile manipulator demonstrate that our semantic-kinematic scene graphs enable robust manipulation of articulated objects in everyday home environments. We provide code and data at: https://momasg.cs.uni-freiburg.de.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantics has enabled 3D scene understanding and affordance-driven object interaction. However, robots operating in real-world environments face a critical limitation: they cannot anticipate how objects move. Long-horizon mobile manipulation requires closing the gap between semantics, geometry, and kinematics. In this work, we present MoMa-SG, a novel framework for building semantic-kinematic 3D scene graphs of articulated scenes containing a myriad of interactable objects. Given RGB-D sequences containing multiple object articulations, we temporally segment object interactions and infer object motion using occlusion-robust point tracking. We then lift point trajectories into 3D and estimate articulation models using a novel unified twist estimation formulation that robustly estimates revolute and prismatic joint parameters in a single optimization pass. Next, we associate objects with estimated articulations and detect contained objects by reasoning over parent-child relations at identified opening states. We also introduce the novel Arti4D-Semantic dataset, which uniquely combines hierarchical object semantics including parent-child relation labels with object axis annotations across 62 in-the-wild RGB-D sequences containing 600 object interactions and three distinct observation paradigms. We extensively evaluate the performance of MoMa-SG on two datasets and ablate key design choices of our approach. In addition, real-world experiments on both a quadruped and a mobile manipulator demonstrate that our semantic-kinematic scene graphs enable robust manipulation of articulated objects in everyday home environments. We provide code and data at: https://momasg.cs.uni-freiburg.de."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T10:40:35Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    10,
                    40,
                    35,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Martin Büchner"
                    },
                    {
                        "name": "Adrian Röfer"
                    },
                    {
                        "name": "Tim Engelbracht"
                    },
                    {
                        "name": "Tim Welschehold"
                    },
                    {
                        "name": "Zuria Bauer"
                    },
                    {
                        "name": "Hermann Blum"
                    },
                    {
                        "name": "Marc Pollefeys"
                    },
                    {
                        "name": "Abhinav Valada"
                    }
                ],
                "author_detail": {
                    "name": "Abhinav Valada"
                },
                "author": "Abhinav Valada"
            },
            {
                "id": "http://arxiv.org/abs/2602.16346v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16346v1",
                "title": "Helpful to a Fault: Measuring Illicit Assistance in Multi-Turn, Multilingual LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Helpful to a Fault: Measuring Illicit Assistance in Multi-Turn, Multilingual LLM Agents"
                },
                "updated": "2026-02-18T10:31:19Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    10,
                    31,
                    19,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16346v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16346v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "LLM-based agents execute real-world workflows via tools and memory. These affordances enable ill-intended adversaries to also use these agents to carry out complex misuse scenarios. Existing agent misuse benchmarks largely test single-prompt instructions, leaving a gap in measuring how agents end up helping with harmful or illegal tasks over multiple turns. We introduce STING (Sequential Testing of Illicit N-step Goal execution), an automated red-teaming framework that constructs a step-by-step illicit plan grounded in a benign persona and iteratively probes a target agent with adaptive follow-ups, using judge agents to track phase completion. We further introduce an analysis framework that models multi-turn red-teaming as a time-to-first-jailbreak random variable, enabling analysis tools like discovery curves, hazard-ratio attribution by attack language, and a new metric: Restricted Mean Jailbreak Discovery. Across AgentHarm scenarios, STING yields substantially higher illicit-task completion than single-turn prompting and chat-oriented multi-turn baselines adapted to tool-using agents. In multilingual evaluations across six non-English settings, we find that attack success and illicit-task completion do not consistently increase in lower-resource languages, diverging from common chatbot findings. Overall, STING provides a practical way to evaluate and stress-test agent misuse in realistic deployment settings, where interactions are inherently multi-turn and often multilingual.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based agents execute real-world workflows via tools and memory. These affordances enable ill-intended adversaries to also use these agents to carry out complex misuse scenarios. Existing agent misuse benchmarks largely test single-prompt instructions, leaving a gap in measuring how agents end up helping with harmful or illegal tasks over multiple turns. We introduce STING (Sequential Testing of Illicit N-step Goal execution), an automated red-teaming framework that constructs a step-by-step illicit plan grounded in a benign persona and iteratively probes a target agent with adaptive follow-ups, using judge agents to track phase completion. We further introduce an analysis framework that models multi-turn red-teaming as a time-to-first-jailbreak random variable, enabling analysis tools like discovery curves, hazard-ratio attribution by attack language, and a new metric: Restricted Mean Jailbreak Discovery. Across AgentHarm scenarios, STING yields substantially higher illicit-task completion than single-turn prompting and chat-oriented multi-turn baselines adapted to tool-using agents. In multilingual evaluations across six non-English settings, we find that attack success and illicit-task completion do not consistently increase in lower-resource languages, diverging from common chatbot findings. Overall, STING provides a practical way to evaluate and stress-test agent misuse in realistic deployment settings, where interactions are inherently multi-turn and often multilingual."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T10:31:19Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    10,
                    31,
                    19,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Nivya Talokar"
                    },
                    {
                        "name": "Ayush K Tarun"
                    },
                    {
                        "name": "Murari Mandal"
                    },
                    {
                        "name": "Maksym Andriushchenko"
                    },
                    {
                        "name": "Antoine Bosselut"
                    }
                ],
                "author_detail": {
                    "name": "Antoine Bosselut"
                },
                "author": "Antoine Bosselut"
            },
            {
                "id": "http://arxiv.org/abs/2601.16800v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.16800v2",
                "title": "Large Language Models as Automatic Annotators and Annotation Adjudicators for Fine-Grained Opinion Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models as Automatic Annotators and Annotation Adjudicators for Fine-Grained Opinion Analysis"
                },
                "updated": "2026-02-18T10:27:00Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    10,
                    27,
                    0,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.16800v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.16800v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Fine-grained opinion analysis of text provides a detailed understanding of expressed sentiments, including the addressed entity. Although this level of detail is sound, it requires considerable human effort and substantial cost to annotate opinions in datasets for training models, especially across diverse domains and real-world applications. We explore the feasibility of LLMs as automatic annotators for fine-grained opinion analysis, addressing the shortage of domain-specific labelled datasets. In this work, we use a declarative annotation pipeline. This approach reduces the variability of manual prompt engineering when using LLMs to identify fine-grained opinion spans in text. We also present a novel methodology for an LLM to adjudicate multiple labels and produce final annotations. After trialling the pipeline with models of different sizes for the Aspect Sentiment Triplet Extraction (ASTE) and Aspect-Category-Opinion-Sentiment (ACOS) analysis tasks, we show that LLMs can serve as automatic annotators and adjudicators, achieving high Inter-Annotator Agreement across individual LLM-based annotators. This reduces the cost and human effort needed to create these fine-grained opinion-annotated datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-grained opinion analysis of text provides a detailed understanding of expressed sentiments, including the addressed entity. Although this level of detail is sound, it requires considerable human effort and substantial cost to annotate opinions in datasets for training models, especially across diverse domains and real-world applications. We explore the feasibility of LLMs as automatic annotators for fine-grained opinion analysis, addressing the shortage of domain-specific labelled datasets. In this work, we use a declarative annotation pipeline. This approach reduces the variability of manual prompt engineering when using LLMs to identify fine-grained opinion spans in text. We also present a novel methodology for an LLM to adjudicate multiple labels and produce final annotations. After trialling the pipeline with models of different sizes for the Aspect Sentiment Triplet Extraction (ASTE) and Aspect-Category-Opinion-Sentiment (ACOS) analysis tasks, we show that LLMs can serve as automatic annotators and adjudicators, achieving high Inter-Annotator Agreement across individual LLM-based annotators. This reduces the cost and human effort needed to create these fine-grained opinion-annotated datasets."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-23T14:52:56Z",
                "published_parsed": [
                    2026,
                    1,
                    23,
                    14,
                    52,
                    56,
                    4,
                    23,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Gaurav Negi"
                    },
                    {
                        "name": "MA Waskow"
                    },
                    {
                        "name": "John McCrae"
                    },
                    {
                        "name": "Paul Buitelaar"
                    }
                ],
                "author_detail": {
                    "name": "Paul Buitelaar"
                },
                "author": "Paul Buitelaar"
            },
            {
                "id": "http://arxiv.org/abs/2602.16334v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16334v1",
                "title": "Spatial Audio Question Answering and Reasoning on Dynamic Source Movements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatial Audio Question Answering and Reasoning on Dynamic Source Movements"
                },
                "updated": "2026-02-18T10:16:30Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    10,
                    16,
                    30,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16334v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16334v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Spatial audio understanding aims to enable machines to interpret complex auditory scenes, particularly when sound sources move over time. In this work, we study Spatial Audio Question Answering (Spatial AQA) with a focus on movement reasoning, where a model must infer object motion, position, and directional changes directly from stereo audio. First, we introduce a movement-centric spatial audio augmentation framework that synthesizes diverse motion patterns from isolated mono audio events, enabling controlled and scalable training data generation. Second, we propose an end-to-end multimodal finetuning approach with a thinking mode, which allows audio-language models to produce explicit intermediate reasoning steps before predicting an answer. Third, we investigate the impact of query-conditioned source separation as a preprocessing stage and compare three inference regimes: no masking, an audio grounding model (AGM), and ground-truth masks. Our results show that reasoning amplifies the benefits of source separation, with thinking mode showing significant improvement of +5.1% when a single event is present in the question. These findings highlight the interplay between movement modeling, reasoning, and separation quality, offering new insights for advancing spatial audio understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatial audio understanding aims to enable machines to interpret complex auditory scenes, particularly when sound sources move over time. In this work, we study Spatial Audio Question Answering (Spatial AQA) with a focus on movement reasoning, where a model must infer object motion, position, and directional changes directly from stereo audio. First, we introduce a movement-centric spatial audio augmentation framework that synthesizes diverse motion patterns from isolated mono audio events, enabling controlled and scalable training data generation. Second, we propose an end-to-end multimodal finetuning approach with a thinking mode, which allows audio-language models to produce explicit intermediate reasoning steps before predicting an answer. Third, we investigate the impact of query-conditioned source separation as a preprocessing stage and compare three inference regimes: no masking, an audio grounding model (AGM), and ground-truth masks. Our results show that reasoning amplifies the benefits of source separation, with thinking mode showing significant improvement of +5.1% when a single event is present in the question. These findings highlight the interplay between movement modeling, reasoning, and separation quality, offering new insights for advancing spatial audio understanding."
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T10:16:30Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    10,
                    16,
                    30,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD"
                },
                "authors": [
                    {
                        "name": "Arvind Krishna Sridhar"
                    },
                    {
                        "name": "Yinyi Guo"
                    },
                    {
                        "name": "Erik Visser"
                    }
                ],
                "author_detail": {
                    "name": "Erik Visser"
                },
                "author": "Erik Visser"
            },
            {
                "id": "http://arxiv.org/abs/2602.16320v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16320v1",
                "title": "RefineFormer3D: Efficient 3D Medical Image Segmentation via Adaptive Multi-Scale Transformer with Cross Attention Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RefineFormer3D: Efficient 3D Medical Image Segmentation via Adaptive Multi-Scale Transformer with Cross Attention Fusion"
                },
                "updated": "2026-02-18T09:58:59Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    9,
                    58,
                    59,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16320v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16320v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Accurate and computationally efficient 3D medical image segmentation remains a critical challenge in clinical workflows. Transformer-based architectures often demonstrate superior global contextual modeling but at the expense of excessive parameter counts and memory demands, restricting their clinical deployment. We propose RefineFormer3D, a lightweight hierarchical transformer architecture that balances segmentation accuracy and computational efficiency for volumetric medical imaging. The architecture integrates three key components: (i) GhostConv3D-based patch embedding for efficient feature extraction with minimal redundancy, (ii) MixFFN3D module with low-rank projections and depthwise convolutions for parameter-efficient feature extraction, and (iii) a cross-attention fusion decoder enabling adaptive multi-scale skip connection integration. RefineFormer3D contains only 2.94M parameters, substantially fewer than contemporary transformer-based methods. Extensive experiments on ACDC and BraTS benchmarks demonstrate that RefineFormer3D achieves 93.44\\% and 85.9\\% average Dice scores respectively, outperforming or matching state-of-the-art methods while requiring significantly fewer parameters. Furthermore, the model achieves fast inference (8.35 ms per volume on GPU) with low memory requirements, supporting deployment in resource-constrained clinical environments. These results establish RefineFormer3D as an effective and scalable solution for practical 3D medical image segmentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate and computationally efficient 3D medical image segmentation remains a critical challenge in clinical workflows. Transformer-based architectures often demonstrate superior global contextual modeling but at the expense of excessive parameter counts and memory demands, restricting their clinical deployment. We propose RefineFormer3D, a lightweight hierarchical transformer architecture that balances segmentation accuracy and computational efficiency for volumetric medical imaging. The architecture integrates three key components: (i) GhostConv3D-based patch embedding for efficient feature extraction with minimal redundancy, (ii) MixFFN3D module with low-rank projections and depthwise convolutions for parameter-efficient feature extraction, and (iii) a cross-attention fusion decoder enabling adaptive multi-scale skip connection integration. RefineFormer3D contains only 2.94M parameters, substantially fewer than contemporary transformer-based methods. Extensive experiments on ACDC and BraTS benchmarks demonstrate that RefineFormer3D achieves 93.44\\% and 85.9\\% average Dice scores respectively, outperforming or matching state-of-the-art methods while requiring significantly fewer parameters. Furthermore, the model achieves fast inference (8.35 ms per volume on GPU) with low memory requirements, supporting deployment in resource-constrained clinical environments. These results establish RefineFormer3D as an effective and scalable solution for practical 3D medical image segmentation."
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T09:58:59Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    9,
                    58,
                    59,
                    2,
                    49,
                    0
                ],
                "arxiv_comment": "13 pages, 5 figures, 7 tables",
                "arxiv_primary_category": {
                    "term": "eess.IV"
                },
                "authors": [
                    {
                        "name": "Kavyansh Tyagi"
                    },
                    {
                        "name": "Vishwas Rathi"
                    },
                    {
                        "name": "Puneet Goyal"
                    }
                ],
                "author_detail": {
                    "name": "Puneet Goyal"
                },
                "author": "Puneet Goyal"
            },
            {
                "id": "http://arxiv.org/abs/2601.20568v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.20568v2",
                "title": "Reinforcement Unlearning via Group Relative Policy Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Unlearning via Group Relative Policy Optimization"
                },
                "updated": "2026-02-18T09:58:17Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    9,
                    58,
                    17,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.20568v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.20568v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "During pretraining, LLMs inadvertently memorize sensitive or copyrighted data, posing significant compliance challenges under legal frameworks like the GDPR and the EU AI Act. Fulfilling these mandates demands techniques that can remove information from a deployed model without retraining from scratch. Existing unlearning approaches attempt to address this need, but often leak the very data they aim to erase, sacrifice fluency and robustness, or depend on costly external reward models. We introduce PURGE (Policy Unlearning through Relative Group Erasure), a novel method grounded in the Group Relative Policy Optimization framework that formulates unlearning as a verifiable problem. PURGE uses an intrinsic reward signal that penalizes any mention of forbidden concepts, allowing safe and consistent unlearning. Our approach achieves up to x46 lower token usage per target than state-of-the-art methods, while improving fluency by +5.48% and adversarial robustness by +12.02% over the base model. Extensive evaluation on the Real World Knowledge Unlearning (RWKU) benchmark shows that PURGE reaches 11% unlearning effectiveness while preserving 98% of original utility. PURGE shows that framing LLM unlearning as a verifiable task enables more reliable, efficient, and scalable forgetting, suggesting a promising new direction for unlearning research that combines theoretical guarantees, improved safety, and practical deployment efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "During pretraining, LLMs inadvertently memorize sensitive or copyrighted data, posing significant compliance challenges under legal frameworks like the GDPR and the EU AI Act. Fulfilling these mandates demands techniques that can remove information from a deployed model without retraining from scratch. Existing unlearning approaches attempt to address this need, but often leak the very data they aim to erase, sacrifice fluency and robustness, or depend on costly external reward models. We introduce PURGE (Policy Unlearning through Relative Group Erasure), a novel method grounded in the Group Relative Policy Optimization framework that formulates unlearning as a verifiable problem. PURGE uses an intrinsic reward signal that penalizes any mention of forbidden concepts, allowing safe and consistent unlearning. Our approach achieves up to x46 lower token usage per target than state-of-the-art methods, while improving fluency by +5.48% and adversarial robustness by +12.02% over the base model. Extensive evaluation on the Real World Knowledge Unlearning (RWKU) benchmark shows that PURGE reaches 11% unlearning effectiveness while preserving 98% of original utility. PURGE shows that framing LLM unlearning as a verifiable task enables more reliable, efficient, and scalable forgetting, suggesting a promising new direction for unlearning research that combines theoretical guarantees, improved safety, and practical deployment efficiency."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-28T13:07:58Z",
                "published_parsed": [
                    2026,
                    1,
                    28,
                    13,
                    7,
                    58,
                    2,
                    28,
                    0
                ],
                "arxiv_comment": "Accepted to ICLR 2026",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Efstratios Zaradoukas"
                    },
                    {
                        "name": "Bardh Prenkaj"
                    },
                    {
                        "name": "Gjergji Kasneci"
                    }
                ],
                "author_detail": {
                    "name": "Gjergji Kasneci"
                },
                "author": "Gjergji Kasneci"
            },
            {
                "id": "http://arxiv.org/abs/2510.21190v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.21190v2",
                "title": "The Trojan Example: Jailbreaking LLMs through Template Filling and Unsafety Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Trojan Example: Jailbreaking LLMs through Template Filling and Unsafety Reasoning"
                },
                "updated": "2026-02-18T09:55:23Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    9,
                    55,
                    23,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.21190v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.21190v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As Large Language Models (LLMs) become integral to computing infrastructure, safety alignment serves as the primary security control preventing the generation of harmful payloads. However, this defense remains brittle. Existing jailbreak attacks typically bifurcate into white-box methods, which are inapplicable to commercial APIs due to lack of gradient access, and black-box optimization techniques, which often yield unnatural (e.g., syntactically rigid) or non-transferable (e.g., lacking cross-model generalization) prompts. In this work, we introduce TrojFill, a black-box exploitation framework that bypasses safety filters by targeting a fundamental logic flaw in current alignment paradigms: the decoupling of unsafety reasoning from content generation. TrojFill structurally reframes malicious instructions as a template-filling task required for safety analysis. By embedding obfuscated payloads (e.g., via placeholder substitution) into a \"Trojan\" structure, the attack induces the model to generate prohibited content as a \"demonstrative example\" ostensibly required for a subsequent sentence-by-sentence safety critique. This approach effectively masks the malicious intent from standard intent classifiers. We evaluate TrojFill against representative commercial systems, including GPT-4o, Gemini-2.5, DeepSeek-3.1, and Qwen-Max. Our results demonstrate that TrojFill achieves near-universal bypass rates: reaching 100% Attack Success Rate (ASR) on Gemini-flash-2.5 and DeepSeek-3.1, and 97% on GPT-4o, significantly outperforming existing black-box baselines. Furthermore, unlike optimization-based adversarial prompts, TrojFill generates highly interpretable and transferable attack vectors, exposing a systematic vulnerability inaligned LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) become integral to computing infrastructure, safety alignment serves as the primary security control preventing the generation of harmful payloads. However, this defense remains brittle. Existing jailbreak attacks typically bifurcate into white-box methods, which are inapplicable to commercial APIs due to lack of gradient access, and black-box optimization techniques, which often yield unnatural (e.g., syntactically rigid) or non-transferable (e.g., lacking cross-model generalization) prompts. In this work, we introduce TrojFill, a black-box exploitation framework that bypasses safety filters by targeting a fundamental logic flaw in current alignment paradigms: the decoupling of unsafety reasoning from content generation. TrojFill structurally reframes malicious instructions as a template-filling task required for safety analysis. By embedding obfuscated payloads (e.g., via placeholder substitution) into a \"Trojan\" structure, the attack induces the model to generate prohibited content as a \"demonstrative example\" ostensibly required for a subsequent sentence-by-sentence safety critique. This approach effectively masks the malicious intent from standard intent classifiers. We evaluate TrojFill against representative commercial systems, including GPT-4o, Gemini-2.5, DeepSeek-3.1, and Qwen-Max. Our results demonstrate that TrojFill achieves near-universal bypass rates: reaching 100% Attack Success Rate (ASR) on Gemini-flash-2.5 and DeepSeek-3.1, and 97% on GPT-4o, significantly outperforming existing black-box baselines. Furthermore, unlike optimization-based adversarial prompts, TrojFill generates highly interpretable and transferable attack vectors, exposing a systematic vulnerability inaligned LLMs."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-24T06:43:10Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    6,
                    43,
                    10,
                    4,
                    297,
                    0
                ],
                "arxiv_comment": "under review",
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Mingrui Liu"
                    },
                    {
                        "name": "Sixiao Zhang"
                    },
                    {
                        "name": "Cheng Long"
                    },
                    {
                        "name": "Kwok Yan Lam"
                    }
                ],
                "author_detail": {
                    "name": "Kwok Yan Lam"
                },
                "author": "Kwok Yan Lam"
            },
            {
                "id": "http://arxiv.org/abs/2602.02050v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.02050v2",
                "title": "Rethinking the Role of Entropy in Optimizing Tool-Use Behaviors for Large Language Model Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking the Role of Entropy in Optimizing Tool-Use Behaviors for Large Language Model Agents"
                },
                "updated": "2026-02-18T09:53:43Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    9,
                    53,
                    43,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.02050v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.02050v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Tool-using agents based on Large Language Models (LLMs) excel in tasks such as mathematical reasoning and multi-hop question answering. However, in long trajectories, agents often trigger excessive and low-quality tool calls, increasing latency and degrading inference performance, making managing tool-use behavior challenging. In this work, we conduct entropy-based pilot experiments and observe a strong positive correlation between entropy reduction and high-quality tool calls. Building on this finding, we propose using entropy reduction as a supervisory signal and design two reward strategies to address the differing needs of optimizing tool-use behavior. Sparse outcome rewards provide coarse, trajectory-level guidance to improve efficiency, while dense process rewards offer fine-grained supervision to enhance performance. Experiments across diverse domains show that both reward designs improve tool-use behavior: the former reduces tool calls by 72.07% compared to the average of baselines, while the latter improves performance by 22.27%. These results position entropy reduction as a key mechanism for enhancing tool-use behavior, enabling agents to be more adaptive in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool-using agents based on Large Language Models (LLMs) excel in tasks such as mathematical reasoning and multi-hop question answering. However, in long trajectories, agents often trigger excessive and low-quality tool calls, increasing latency and degrading inference performance, making managing tool-use behavior challenging. In this work, we conduct entropy-based pilot experiments and observe a strong positive correlation between entropy reduction and high-quality tool calls. Building on this finding, we propose using entropy reduction as a supervisory signal and design two reward strategies to address the differing needs of optimizing tool-use behavior. Sparse outcome rewards provide coarse, trajectory-level guidance to improve efficiency, while dense process rewards offer fine-grained supervision to enhance performance. Experiments across diverse domains show that both reward designs improve tool-use behavior: the former reduces tool calls by 72.07% compared to the average of baselines, while the latter improves performance by 22.27%. These results position entropy reduction as a key mechanism for enhancing tool-use behavior, enabling agents to be more adaptive in real-world applications."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-02T12:52:14Z",
                "published_parsed": [
                    2026,
                    2,
                    2,
                    12,
                    52,
                    14,
                    0,
                    33,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Zeping Li"
                    },
                    {
                        "name": "Hongru Wang"
                    },
                    {
                        "name": "Yiwen Zhao"
                    },
                    {
                        "name": "Guanhua Chen"
                    },
                    {
                        "name": "Yixia Li"
                    },
                    {
                        "name": "Keyang Chen"
                    },
                    {
                        "name": "Yixin Cao"
                    },
                    {
                        "name": "Guangnan Ye"
                    },
                    {
                        "name": "Hongfeng Chai"
                    },
                    {
                        "name": "Zhenfei Yin"
                    }
                ],
                "author_detail": {
                    "name": "Zhenfei Yin"
                },
                "author": "Zhenfei Yin"
            },
            {
                "id": "http://arxiv.org/abs/2509.23863v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.23863v3",
                "title": "SPELL: Self-Play Reinforcement Learning for Evolving Long-Context Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPELL: Self-Play Reinforcement Learning for Evolving Long-Context Language Models"
                },
                "updated": "2026-02-18T09:50:21Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    9,
                    50,
                    21,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.23863v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.23863v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Progress in long-context reasoning for large language models (LLMs) has lagged behind other recent advances. This gap arises not only from the intrinsic difficulty of processing long texts, but also from the scarcity of reliable human annotations and programmatically verifiable reward signals. In this paper, we propose SPELL, a multi-role self-play reinforcement learning framework that enables scalable, label-free optimization for long-context reasoning. SPELL integrates three cyclical roles-questioner, responder, and verifier-within a single model to enable continual self-improvement. The questioner generates questions from raw documents paired with reference answers; the responder learns to solve these questions based on the documents; and the verifier evaluates semantic equivalence between the responder's output and the questioner's reference answer, producing reward signals to guide continual training. To stabilize training, we introduce an automated curriculum that gradually increases document length and a reward function that adapts question difficulty to the model's evolving capabilities. Extensive experiments on six long-context benchmarks show that SPELL consistently improves performance across diverse LLMs and outperforms equally sized models fine-tuned on large-scale annotated data. Notably, SPELL achieves an average 7.6-point gain in pass@8 on the strong reasoning model Qwen3-30B-A3B-Thinking, raising its performance ceiling and showing promise for scaling to even more capable models. Our code is available at https://github.com/Tongyi-Zhiwen/Qwen-Doc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Progress in long-context reasoning for large language models (LLMs) has lagged behind other recent advances. This gap arises not only from the intrinsic difficulty of processing long texts, but also from the scarcity of reliable human annotations and programmatically verifiable reward signals. In this paper, we propose SPELL, a multi-role self-play reinforcement learning framework that enables scalable, label-free optimization for long-context reasoning. SPELL integrates three cyclical roles-questioner, responder, and verifier-within a single model to enable continual self-improvement. The questioner generates questions from raw documents paired with reference answers; the responder learns to solve these questions based on the documents; and the verifier evaluates semantic equivalence between the responder's output and the questioner's reference answer, producing reward signals to guide continual training. To stabilize training, we introduce an automated curriculum that gradually increases document length and a reward function that adapts question difficulty to the model's evolving capabilities. Extensive experiments on six long-context benchmarks show that SPELL consistently improves performance across diverse LLMs and outperforms equally sized models fine-tuned on large-scale annotated data. Notably, SPELL achieves an average 7.6-point gain in pass@8 on the strong reasoning model Qwen3-30B-A3B-Thinking, raising its performance ceiling and showing promise for scaling to even more capable models. Our code is available at https://github.com/Tongyi-Zhiwen/Qwen-Doc."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-28T13:08:10Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    13,
                    8,
                    10,
                    6,
                    271,
                    0
                ],
                "arxiv_comment": "Accepted to ICLR 2026",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Ziyi Yang"
                    },
                    {
                        "name": "Weizhou Shen"
                    },
                    {
                        "name": "Chenliang Li"
                    },
                    {
                        "name": "Ruijun Chen"
                    },
                    {
                        "name": "Fanqi Wan"
                    },
                    {
                        "name": "Ming Yan"
                    },
                    {
                        "name": "Xiaojun Quan"
                    },
                    {
                        "name": "Fei Huang"
                    }
                ],
                "author_detail": {
                    "name": "Fei Huang"
                },
                "author": "Fei Huang"
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2602.13194v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.13194v2",
                "title": "Semantic Chunking and the Entropy of Natural Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Chunking and the Entropy of Natural Language"
                },
                "updated": "2026-02-18T18:59:22Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    18,
                    59,
                    22,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.13194v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.13194v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The entropy rate of printed English is famously estimated to be about one bit per character, a benchmark that modern large language models (LLMs) have only recently approached. This entropy rate implies that English contains nearly 80 percent redundancy relative to the five bits per character expected for random text. We introduce a statistical model that attempts to capture the intricate multi-scale structure of natural language, providing a first-principles account of this redundancy level. Our model describes a procedure of self-similarly segmenting text into semantically coherent chunks down to the single-word level. The semantic structure of the text can then be hierarchically decomposed, allowing for analytical treatment. Numerical experiments with modern LLMs and open datasets suggest that our model quantitatively captures the structure of real texts at different levels of the semantic hierarchy. The entropy rate predicted by our model agrees with the estimated entropy rate of printed English. Moreover, our theory further reveals that the entropy rate of natural language is not fixed but should increase systematically with the semantic complexity of corpora, which are captured by the only free parameter in our model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The entropy rate of printed English is famously estimated to be about one bit per character, a benchmark that modern large language models (LLMs) have only recently approached. This entropy rate implies that English contains nearly 80 percent redundancy relative to the five bits per character expected for random text. We introduce a statistical model that attempts to capture the intricate multi-scale structure of natural language, providing a first-principles account of this redundancy level. Our model describes a procedure of self-similarly segmenting text into semantically coherent chunks down to the single-word level. The semantic structure of the text can then be hierarchically decomposed, allowing for analytical treatment. Numerical experiments with modern LLMs and open datasets suggest that our model quantitatively captures the structure of real texts at different levels of the semantic hierarchy. The entropy rate predicted by our model agrees with the estimated entropy rate of printed English. Moreover, our theory further reveals that the entropy rate of natural language is not fixed but should increase systematically with the semantic complexity of corpora, which are captured by the only free parameter in our model."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-13T18:58:10Z",
                "published_parsed": [
                    2026,
                    2,
                    13,
                    18,
                    58,
                    10,
                    4,
                    44,
                    0
                ],
                "arxiv_comment": "29 pages, 9 figures; typos fixed",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Weishun Zhong"
                    },
                    {
                        "name": "Doron Sivan"
                    },
                    {
                        "name": "Tankut Can"
                    },
                    {
                        "name": "Mikhail Katkov"
                    },
                    {
                        "name": "Misha Tsodyks"
                    }
                ],
                "author_detail": {
                    "name": "Misha Tsodyks"
                },
                "author": "Misha Tsodyks"
            },
            {
                "id": "http://arxiv.org/abs/2602.16708v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16708v1",
                "title": "Policy Compiler for Secure Agentic Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Policy Compiler for Secure Agentic Systems"
                },
                "updated": "2026-02-18T18:57:12Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    18,
                    57,
                    12,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16708v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16708v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "LLM-based agents are increasingly being deployed in contexts requiring complex authorization policies: customer service protocols, approval workflows, data access restrictions, and regulatory compliance. Embedding these policies in prompts provides no enforcement guarantees. We present PCAS, a Policy Compiler for Agentic Systems that provides deterministic policy enforcement.\n  Enforcing such policies requires tracking information flow across agents, which linear message histories cannot capture. Instead, PCAS models the agentic system state as a dependency graph capturing causal relationships among events such as tool calls, tool results, and messages. Policies are expressed in a Datalog-derived language, as declarative rules that account for transitive information flow and cross-agent provenance. A reference monitor intercepts all actions and blocks violations before execution, providing deterministic enforcement independent of model reasoning.\n  PCAS takes an existing agent implementation and a policy specification, and compiles them into an instrumented system that is policy-compliant by construction, with no security-specific restructuring required. We evaluate PCAS on three case studies: information flow policies for prompt injection defense, approval workflows in a multi-agent pharmacovigilance system, and organizational policies for customer service. On customer service tasks, PCAS improves policy compliance from 48% to 93% across frontier models, with zero policy violations in instrumented runs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based agents are increasingly being deployed in contexts requiring complex authorization policies: customer service protocols, approval workflows, data access restrictions, and regulatory compliance. Embedding these policies in prompts provides no enforcement guarantees. We present PCAS, a Policy Compiler for Agentic Systems that provides deterministic policy enforcement.\n  Enforcing such policies requires tracking information flow across agents, which linear message histories cannot capture. Instead, PCAS models the agentic system state as a dependency graph capturing causal relationships among events such as tool calls, tool results, and messages. Policies are expressed in a Datalog-derived language, as declarative rules that account for transitive information flow and cross-agent provenance. A reference monitor intercepts all actions and blocks violations before execution, providing deterministic enforcement independent of model reasoning.\n  PCAS takes an existing agent implementation and a policy specification, and compiles them into an instrumented system that is policy-compliant by construction, with no security-specific restructuring required. We evaluate PCAS on three case studies: information flow policies for prompt injection defense, approval workflows in a multi-agent pharmacovigilance system, and organizational policies for customer service. On customer service tasks, PCAS improves policy compliance from 48% to 93% across frontier models, with zero policy violations in instrumented runs."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T18:57:12Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    18,
                    57,
                    12,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Nils Palumbo"
                    },
                    {
                        "name": "Sarthak Choudhary"
                    },
                    {
                        "name": "Jihye Choi"
                    },
                    {
                        "name": "Prasad Chalasani"
                    },
                    {
                        "name": "Mihai Christodorescu"
                    },
                    {
                        "name": "Somesh Jha"
                    }
                ],
                "author_detail": {
                    "name": "Somesh Jha"
                },
                "author": "Somesh Jha"
            },
            {
                "id": "http://arxiv.org/abs/2602.16703v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16703v1",
                "title": "Measuring Mid-2025 LLM-Assistance on Novice Performance in Biology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring Mid-2025 LLM-Assistance on Novice Performance in Biology"
                },
                "updated": "2026-02-18T18:51:28Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    18,
                    51,
                    28,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16703v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16703v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) perform strongly on biological benchmarks, raising concerns that they may help novice actors acquire dual-use laboratory skills. Yet, whether this translates to improved human performance in the physical laboratory remains unclear. To address this, we conducted a pre-registered, investigator-blinded, randomized controlled trial (June-August 2025; n = 153) evaluating whether LLMs improve novice performance in tasks that collectively model a viral reverse genetics workflow. We observed no significant difference in the primary endpoint of workflow completion (5.2% LLM vs. 6.6% Internet; P = 0.759), nor in the success rate of individual tasks. However, the LLM arm had numerically higher success rates in four of the five tasks, most notably for the cell culture task (68.8% LLM vs. 55.3% Internet; P = 0.059). Post-hoc Bayesian modeling of pooled data estimates an approximate 1.4-fold increase (95% CrI 0.74-2.62) in success for a \"typical\" reverse genetics task under LLM assistance. Ordinal regression modelling suggests that participants in the LLM arm were more likely to progress through intermediate steps across all tasks (posterior probability of a positive effect: 81%-96%). Overall, mid-2025 LLMs did not substantially increase novice completion of complex laboratory procedures but were associated with a modest performance benefit. These results reveal a gap between in silico benchmarks and real-world utility, underscoring the need for physical-world validation of AI biosecurity assessments as model capabilities and user proficiency evolve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) perform strongly on biological benchmarks, raising concerns that they may help novice actors acquire dual-use laboratory skills. Yet, whether this translates to improved human performance in the physical laboratory remains unclear. To address this, we conducted a pre-registered, investigator-blinded, randomized controlled trial (June-August 2025; n = 153) evaluating whether LLMs improve novice performance in tasks that collectively model a viral reverse genetics workflow. We observed no significant difference in the primary endpoint of workflow completion (5.2% LLM vs. 6.6% Internet; P = 0.759), nor in the success rate of individual tasks. However, the LLM arm had numerically higher success rates in four of the five tasks, most notably for the cell culture task (68.8% LLM vs. 55.3% Internet; P = 0.059). Post-hoc Bayesian modeling of pooled data estimates an approximate 1.4-fold increase (95% CrI 0.74-2.62) in success for a \"typical\" reverse genetics task under LLM assistance. Ordinal regression modelling suggests that participants in the LLM arm were more likely to progress through intermediate steps across all tasks (posterior probability of a positive effect: 81%-96%). Overall, mid-2025 LLMs did not substantially increase novice completion of complex laboratory procedures but were associated with a modest performance benefit. These results reveal a gap between in silico benchmarks and real-world utility, underscoring the need for physical-world validation of AI biosecurity assessments as model capabilities and user proficiency evolve."
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T18:51:28Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    18,
                    51,
                    28,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY"
                },
                "authors": [
                    {
                        "name": "Shen Zhou Hong"
                    },
                    {
                        "name": "Alex Kleinman"
                    },
                    {
                        "name": "Alyssa Mathiowetz"
                    },
                    {
                        "name": "Adam Howes"
                    },
                    {
                        "name": "Julian Cohen"
                    },
                    {
                        "name": "Suveer Ganta"
                    },
                    {
                        "name": "Alex Letizia"
                    },
                    {
                        "name": "Dora Liao"
                    },
                    {
                        "name": "Deepika Pahari"
                    },
                    {
                        "name": "Xavier Roberts-Gaal"
                    },
                    {
                        "name": "Luca Righetti"
                    },
                    {
                        "name": "Joe Torres"
                    }
                ],
                "author_detail": {
                    "name": "Joe Torres"
                },
                "author": "Joe Torres"
            },
            {
                "id": "http://arxiv.org/abs/2402.18060v6",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2402.18060v6",
                "title": "Benchmarking Large Language Models on Answering and Explaining Challenging Medical Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Large Language Models on Answering and Explaining Challenging Medical Questions"
                },
                "updated": "2026-02-18T18:50:32Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    18,
                    50,
                    32,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2402.18060v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2402.18060v6",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "LLMs have demonstrated impressive performance in answering medical questions, such as achieving passing scores on medical licensing examinations. However, medical board exams or general clinical questions do not capture the complexity of realistic clinical cases. Moreover, the lack of reference explanations means we cannot easily evaluate the reasoning of model decisions, a crucial component of supporting doctors in making complex medical decisions. To address these challenges, we construct two new datasets: JAMA Clinical Challenge and Medbullets. Datasets and code are available at https://github.com/HanjieChen/ChallengeClinicalQA. JAMA Clinical Challenge consists of questions based on challenging clinical cases, while Medbullets comprises simulated clinical questions. Both datasets are structured as multiple-choice question-answering tasks, accompanied by expert-written explanations. We evaluate seven LLMs on the two datasets using various prompts. Experiments demonstrate that our datasets are harder than previous benchmarks. In-depth automatic and human evaluations of model-generated explanations provide insights into the promise and deficiency of LLMs for explainable medical QA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs have demonstrated impressive performance in answering medical questions, such as achieving passing scores on medical licensing examinations. However, medical board exams or general clinical questions do not capture the complexity of realistic clinical cases. Moreover, the lack of reference explanations means we cannot easily evaluate the reasoning of model decisions, a crucial component of supporting doctors in making complex medical decisions. To address these challenges, we construct two new datasets: JAMA Clinical Challenge and Medbullets. Datasets and code are available at https://github.com/HanjieChen/ChallengeClinicalQA. JAMA Clinical Challenge consists of questions based on challenging clinical cases, while Medbullets comprises simulated clinical questions. Both datasets are structured as multiple-choice question-answering tasks, accompanied by expert-written explanations. We evaluate seven LLMs on the two datasets using various prompts. Experiments demonstrate that our datasets are harder than previous benchmarks. In-depth automatic and human evaluations of model-generated explanations provide insights into the promise and deficiency of LLMs for explainable medical QA."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-02-28T05:44:41Z",
                "published_parsed": [
                    2024,
                    2,
                    28,
                    5,
                    44,
                    41,
                    2,
                    59,
                    0
                ],
                "arxiv_comment": "NAACL 2025",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Hanjie Chen"
                    },
                    {
                        "name": "Zhouxiang Fang"
                    },
                    {
                        "name": "Yash Singla"
                    },
                    {
                        "name": "Mark Dredze"
                    }
                ],
                "author_detail": {
                    "name": "Mark Dredze"
                },
                "author": "Mark Dredze"
            },
            {
                "id": "http://arxiv.org/abs/2602.16702v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16702v1",
                "title": "Saliency-Aware Multi-Route Thinking: Revisiting Vision-Language Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Saliency-Aware Multi-Route Thinking: Revisiting Vision-Language Reasoning"
                },
                "updated": "2026-02-18T18:49:56Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    18,
                    49,
                    56,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16702v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16702v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Vision-language models (VLMs) aim to reason by jointly leveraging visual and textual modalities. While allocating additional inference-time computation has proven effective for large language models (LLMs), achieving similar scaling in VLMs remains challenging. A key obstacle is that visual inputs are typically provided only once at the start of generation, while textual reasoning (e.g., early visual summaries) is generated autoregressively, causing reasoning to become increasingly text-dominated and allowing early visual grounding errors to accumulate. Moreover, vanilla guidance for visual grounding during inference is often coarse and noisy, making it difficult to steer reasoning over long texts. To address these challenges, we propose \\emph{Saliency-Aware Principle} (SAP) selection. SAP operates on high-level reasoning principles rather than token-level trajectories, which enable stable control over discrete generation under noisy feedback while allowing later reasoning steps to re-consult visual evidence when renewed grounding is required. In addition, SAP supports multi-route inference, enabling parallel exploration of diverse reasoning behaviors. SAP is model-agnostic and data-free, requiring no additional training. Empirical results show that SAP achieves competitive performance, especially in reducing object hallucination, under comparable token-generation budgets while yielding more stable reasoning and lower response latency than CoT-style long sequential reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) aim to reason by jointly leveraging visual and textual modalities. While allocating additional inference-time computation has proven effective for large language models (LLMs), achieving similar scaling in VLMs remains challenging. A key obstacle is that visual inputs are typically provided only once at the start of generation, while textual reasoning (e.g., early visual summaries) is generated autoregressively, causing reasoning to become increasingly text-dominated and allowing early visual grounding errors to accumulate. Moreover, vanilla guidance for visual grounding during inference is often coarse and noisy, making it difficult to steer reasoning over long texts. To address these challenges, we propose \\emph{Saliency-Aware Principle} (SAP) selection. SAP operates on high-level reasoning principles rather than token-level trajectories, which enable stable control over discrete generation under noisy feedback while allowing later reasoning steps to re-consult visual evidence when renewed grounding is required. In addition, SAP supports multi-route inference, enabling parallel exploration of diverse reasoning behaviors. SAP is model-agnostic and data-free, requiring no additional training. Empirical results show that SAP achieves competitive performance, especially in reducing object hallucination, under comparable token-generation budgets while yielding more stable reasoning and lower response latency than CoT-style long sequential reasoning."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T18:49:56Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    18,
                    49,
                    56,
                    2,
                    49,
                    0
                ],
                "arxiv_comment": "preprint 10 pages, 4 figures",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Mingjia Shi"
                    },
                    {
                        "name": "Yinhan He"
                    },
                    {
                        "name": "Yaochen Zhu"
                    },
                    {
                        "name": "Jundong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jundong Li"
                },
                "author": "Jundong Li"
            },
            {
                "id": "http://arxiv.org/abs/2602.16699v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16699v1",
                "title": "Calibrate-Then-Act: Cost-Aware Exploration in LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Calibrate-Then-Act: Cost-Aware Exploration in LLM Agents"
                },
                "updated": "2026-02-18T18:46:14Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    18,
                    46,
                    14,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16699v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16699v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "LLMs are increasingly being used for complex problems which are not necessarily resolved in a single response, but require interacting with an environment to acquire information. In these scenarios, LLMs must reason about inherent cost-uncertainty tradeoffs in when to stop exploring and commit to an answer. For instance, on a programming task, an LLM should test a generated code snippet if it is uncertain about the correctness of that code; the cost of writing a test is nonzero, but typically lower than the cost of making a mistake. In this work, we show that we can induce LLMs to explicitly reason about balancing these cost-uncertainty tradeoffs, then perform more optimal environment exploration. We formalize multiple tasks, including information retrieval and coding, as sequential decision-making problems under uncertainty. Each problem has latent environment state that can be reasoned about via a prior which is passed to the LLM agent. We introduce a framework called Calibrate-Then-Act (CTA), where we feed the LLM this additional context to enable it to act more optimally. This improvement is preserved even under RL training of both the baseline and CTA. Our results on information-seeking QA and on a simplified coding task show that making cost-benefit tradeoffs explicit with CTA can help agents discover more optimal decision-making strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are increasingly being used for complex problems which are not necessarily resolved in a single response, but require interacting with an environment to acquire information. In these scenarios, LLMs must reason about inherent cost-uncertainty tradeoffs in when to stop exploring and commit to an answer. For instance, on a programming task, an LLM should test a generated code snippet if it is uncertain about the correctness of that code; the cost of writing a test is nonzero, but typically lower than the cost of making a mistake. In this work, we show that we can induce LLMs to explicitly reason about balancing these cost-uncertainty tradeoffs, then perform more optimal environment exploration. We formalize multiple tasks, including information retrieval and coding, as sequential decision-making problems under uncertainty. Each problem has latent environment state that can be reasoned about via a prior which is passed to the LLM agent. We introduce a framework called Calibrate-Then-Act (CTA), where we feed the LLM this additional context to enable it to act more optimally. This improvement is preserved even under RL training of both the baseline and CTA. Our results on information-seeking QA and on a simplified coding task show that making cost-benefit tradeoffs explicit with CTA can help agents discover more optimal decision-making strategies."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T18:46:14Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    18,
                    46,
                    14,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Wenxuan Ding"
                    },
                    {
                        "name": "Nicholas Tomlin"
                    },
                    {
                        "name": "Greg Durrett"
                    }
                ],
                "author_detail": {
                    "name": "Greg Durrett"
                },
                "author": "Greg Durrett"
            },
            {
                "id": "http://arxiv.org/abs/2602.16698v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16698v1",
                "title": "Causality is Key for Interpretability Claims to Generalise",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causality is Key for Interpretability Claims to Generalise"
                },
                "updated": "2026-02-18T18:45:04Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    18,
                    45,
                    4,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16698v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16698v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Interpretability research on large language models (LLMs) has yielded important insights into model behaviour, yet recurring pitfalls persist: findings that do not generalise, and causal interpretations that outrun the evidence. Our position is that causal inference specifies what constitutes a valid mapping from model activations to invariant high-level structures, the data or assumptions needed to achieve it, and the inferences it can support. Specifically, Pearl's causal hierarchy clarifies what an interpretability study can justify. Observations establish associations between model behaviour and internal components. Interventions (e.g., ablations or activation patching) support claims how these edits affect a behavioural metric (\\eg, average change in token probabilities) over a set of prompts. However, counterfactual claims -- i.e., asking what the model output would have been for the same prompt under an unobserved intervention -- remain largely unverifiable without controlled supervision. We show how causal representation learning (CRL) operationalises this hierarchy, specifying which variables are recoverable from activations and under what assumptions. Together, these motivate a diagnostic framework that helps practitioners select methods and evaluations matching claims to evidence such that findings generalise.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpretability research on large language models (LLMs) has yielded important insights into model behaviour, yet recurring pitfalls persist: findings that do not generalise, and causal interpretations that outrun the evidence. Our position is that causal inference specifies what constitutes a valid mapping from model activations to invariant high-level structures, the data or assumptions needed to achieve it, and the inferences it can support. Specifically, Pearl's causal hierarchy clarifies what an interpretability study can justify. Observations establish associations between model behaviour and internal components. Interventions (e.g., ablations or activation patching) support claims how these edits affect a behavioural metric (\\eg, average change in token probabilities) over a set of prompts. However, counterfactual claims -- i.e., asking what the model output would have been for the same prompt under an unobserved intervention -- remain largely unverifiable without controlled supervision. We show how causal representation learning (CRL) operationalises this hierarchy, specifying which variables are recoverable from activations and under what assumptions. Together, these motivate a diagnostic framework that helps practitioners select methods and evaluations matching claims to evidence such that findings generalise."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T18:45:04Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    18,
                    45,
                    4,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Shruti Joshi"
                    },
                    {
                        "name": "Aaron Mueller"
                    },
                    {
                        "name": "David Klindt"
                    },
                    {
                        "name": "Wieland Brendel"
                    },
                    {
                        "name": "Patrik Reizinger"
                    },
                    {
                        "name": "Dhanya Sridhar"
                    }
                ],
                "author_detail": {
                    "name": "Dhanya Sridhar"
                },
                "author": "Dhanya Sridhar"
            },
            {
                "id": "http://arxiv.org/abs/2512.20773v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20773v3",
                "title": "DIAL: Direct Iterative Adversarial Learning for Realistic Multi-Turn Dialogue Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DIAL: Direct Iterative Adversarial Learning for Realistic Multi-Turn Dialogue Simulation"
                },
                "updated": "2026-02-18T18:41:36Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    18,
                    41,
                    36,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20773v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20773v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Realistic user simulation is crucial for training and evaluating multi-turn dialogue systems, yet creating simulators that accurately replicate human behavior remains a significant challenge. An effective simulator must expose the failure modes of the systems under evaluation. This work introduces Direct Iterative Adversarial Learning (DIAL), a DPO-based adversarial training framework that iteratively enhances user simulator realism through a competitive dynamic between a generator (user simulator) and a discriminator. When applied to mental health support, a domain characterized by diverse failure types and a critical dependence on realistic user behavior for failure detection, DIAL restores lexical diversity diminished by supervised fine-tuning and reduces discriminator accuracy from near-perfect to near-random levels. The resulting simulator exhibits a strong correlation between simulated and real failure occurrence rates while maintaining low distributional divergence of failure modes. These findings indicate that DIAL is a promising method for developing realistic user simulators in multi-turn dialogue, facilitating rapid, reliable, and cost-effective system evaluation prior to deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Realistic user simulation is crucial for training and evaluating multi-turn dialogue systems, yet creating simulators that accurately replicate human behavior remains a significant challenge. An effective simulator must expose the failure modes of the systems under evaluation. This work introduces Direct Iterative Adversarial Learning (DIAL), a DPO-based adversarial training framework that iteratively enhances user simulator realism through a competitive dynamic between a generator (user simulator) and a discriminator. When applied to mental health support, a domain characterized by diverse failure types and a critical dependence on realistic user behavior for failure detection, DIAL restores lexical diversity diminished by supervised fine-tuning and reduces discriminator accuracy from near-perfect to near-random levels. The resulting simulator exhibits a strong correlation between simulated and real failure occurrence rates while maintaining low distributional divergence of failure modes. These findings indicate that DIAL is a promising method for developing realistic user simulators in multi-turn dialogue, facilitating rapid, reliable, and cost-effective system evaluation prior to deployment."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T21:21:08Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    21,
                    21,
                    8,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Ziyi Zhu"
                    },
                    {
                        "name": "Olivier Tieleman"
                    },
                    {
                        "name": "Caitlin A. Stamatis"
                    },
                    {
                        "name": "Luka Smyth"
                    },
                    {
                        "name": "Thomas D. Hull"
                    },
                    {
                        "name": "Daniel R. Cahn"
                    },
                    {
                        "name": "Matteo Malgaroli"
                    }
                ],
                "author_detail": {
                    "name": "Matteo Malgaroli"
                },
                "author": "Matteo Malgaroli"
            },
            {
                "id": "http://arxiv.org/abs/2503.18825v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2503.18825v4",
                "title": "EconEvals: Benchmarks and Litmus Tests for Economic Decision-Making by LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EconEvals: Benchmarks and Litmus Tests for Economic Decision-Making by LLM Agents"
                },
                "updated": "2026-02-18T18:37:52Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    18,
                    37,
                    52,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2503.18825v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2503.18825v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We develop evaluation methods for measuring the economic decision-making capabilities and tendencies of LLMs. First, we develop benchmarks derived from key problems in economics -- procurement, scheduling, and pricing -- that test an LLM's ability to learn from the environment in context. Second, we develop the framework of litmus tests, evaluations that quantify an LLM's choice behavior on a stylized decision-making task with multiple conflicting objectives. Each litmus test outputs a litmus score, which quantifies an LLM's tradeoff response, a reliability score, which measures the coherence of an LLM's choice behavior, and a competency score, which measures an LLM's capability at the same task when the conflicting objectives are replaced by a single, well-specified objective. Evaluating a broad array of frontier LLMs, we (1) investigate changes in LLM capabilities and tendencies over time, (2) derive economically meaningful insights from the LLMs' choice behavior and chain-of-thought, (3) validate our litmus test framework by testing self-consistency, robustness, and generalizability. Overall, this work provides a foundation for evaluating LLM agents as they are further integrated into economic decision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop evaluation methods for measuring the economic decision-making capabilities and tendencies of LLMs. First, we develop benchmarks derived from key problems in economics -- procurement, scheduling, and pricing -- that test an LLM's ability to learn from the environment in context. Second, we develop the framework of litmus tests, evaluations that quantify an LLM's choice behavior on a stylized decision-making task with multiple conflicting objectives. Each litmus test outputs a litmus score, which quantifies an LLM's tradeoff response, a reliability score, which measures the coherence of an LLM's choice behavior, and a competency score, which measures an LLM's capability at the same task when the conflicting objectives are replaced by a single, well-specified objective. Evaluating a broad array of frontier LLMs, we (1) investigate changes in LLM capabilities and tendencies over time, (2) derive economically meaningful insights from the LLMs' choice behavior and chain-of-thought, (3) validate our litmus test framework by testing self-consistency, robustness, and generalizability. Overall, this work provides a foundation for evaluating LLM agents as they are further integrated into economic decision-making."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-03-24T16:06:04Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    6,
                    4,
                    0,
                    83,
                    0
                ],
                "arxiv_comment": "v3 was a major revision with updated experiments and analysis; v4 consists of minor edits",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Sara Fish"
                    },
                    {
                        "name": "Julia Shephard"
                    },
                    {
                        "name": "Minkai Li"
                    },
                    {
                        "name": "Ran I. Shorrer"
                    },
                    {
                        "name": "Yannai A. Gonczarowski"
                    }
                ],
                "author_detail": {
                    "name": "Yannai A. Gonczarowski"
                },
                "author": "Yannai A. Gonczarowski"
            },
            {
                "id": "http://arxiv.org/abs/2602.16687v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16687v1",
                "title": "Scaling Open Discrete Audio Foundation Models with Interleaved Semantic, Acoustic, and Text Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Open Discrete Audio Foundation Models with Interleaved Semantic, Acoustic, and Text Tokens"
                },
                "updated": "2026-02-18T18:32:46Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    18,
                    32,
                    46,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16687v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16687v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Current audio language models are predominantly text-first, either extending pre-trained text LLM backbones or relying on semantic-only audio tokens, limiting general audio modeling. This paper presents a systematic empirical study of native audio foundation models that apply next-token prediction to audio at scale, jointly modeling semantic content, acoustic details, and text to support both general audio generation and cross-modal capabilities. We provide comprehensive empirical insights for building such models: (1) We systematically investigate design choices -- data sources, text mixture ratios, and token composition -- establishing a validated training recipe. (2) We conduct the first scaling law study for discrete audio models via IsoFLOP analysis on 64 models spanning $3{\\times}10^{18}$ to $3{\\times}10^{20}$ FLOPs, finding that optimal data grows 1.6$\\times$ faster than optimal model size. (3) We apply these lessons to train SODA (Scaling Open Discrete Audio), a suite of models from 135M to 4B parameters on 500B tokens, comparing against our scaling predictions and existing models. SODA serves as a flexible backbone for diverse audio/text tasks -- we demonstrate this by fine-tuning for voice-preserving speech-to-speech translation, using the same unified architecture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current audio language models are predominantly text-first, either extending pre-trained text LLM backbones or relying on semantic-only audio tokens, limiting general audio modeling. This paper presents a systematic empirical study of native audio foundation models that apply next-token prediction to audio at scale, jointly modeling semantic content, acoustic details, and text to support both general audio generation and cross-modal capabilities. We provide comprehensive empirical insights for building such models: (1) We systematically investigate design choices -- data sources, text mixture ratios, and token composition -- establishing a validated training recipe. (2) We conduct the first scaling law study for discrete audio models via IsoFLOP analysis on 64 models spanning $3{\\times}10^{18}$ to $3{\\times}10^{20}$ FLOPs, finding that optimal data grows 1.6$\\times$ faster than optimal model size. (3) We apply these lessons to train SODA (Scaling Open Discrete Audio), a suite of models from 135M to 4B parameters on 500B tokens, comparing against our scaling predictions and existing models. SODA serves as a flexible backbone for diverse audio/text tasks -- we demonstrate this by fine-tuning for voice-preserving speech-to-speech translation, using the same unified architecture."
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T18:32:46Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    18,
                    32,
                    46,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD"
                },
                "authors": [
                    {
                        "name": "Potsawee Manakul"
                    },
                    {
                        "name": "Woody Haosheng Gan"
                    },
                    {
                        "name": "Martijn Bartelds"
                    },
                    {
                        "name": "Guangzhi Sun"
                    },
                    {
                        "name": "William Held"
                    },
                    {
                        "name": "Diyi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Diyi Yang"
                },
                "author": "Diyi Yang"
            },
            {
                "id": "http://arxiv.org/abs/2602.16675v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16675v1",
                "title": "Learning to unfold cloth: Scaling up world models to deformable object manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to unfold cloth: Scaling up world models to deformable object manipulation"
                },
                "updated": "2026-02-18T18:14:41Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    18,
                    14,
                    41,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16675v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16675v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Learning to manipulate cloth is both a paradigmatic problem for robotic research and a problem of immediate relevance to a variety of applications ranging from assistive care to the service industry. The complex physics of the deformable object makes this problem of cloth manipulation nontrivial. In order to create a general manipulation strategy that addresses a variety of shapes, sizes, fold and wrinkle patterns, in addition to the usual problems of appearance variations, it becomes important to carefully consider model structure and their implications for generalisation performance. In this paper, we present an approach to in-air cloth manipulation that uses a variation of a recently proposed reinforcement learning architecture, DreamerV2. Our implementation modifies this architecture to utilise surface normals input, in addition to modiying the replay buffer and data augmentation procedures. Taken together these modifications represent an enhancement to the world model used by the robot, addressing the physical complexity of the object being manipulated by the robot. We present evaluations both in simulation and in a zero-shot deployment of the trained policies in a physical robot setup, performing in-air unfolding of a variety of different cloth types, demonstrating the generalisation benefits of our proposed architecture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to manipulate cloth is both a paradigmatic problem for robotic research and a problem of immediate relevance to a variety of applications ranging from assistive care to the service industry. The complex physics of the deformable object makes this problem of cloth manipulation nontrivial. In order to create a general manipulation strategy that addresses a variety of shapes, sizes, fold and wrinkle patterns, in addition to the usual problems of appearance variations, it becomes important to carefully consider model structure and their implications for generalisation performance. In this paper, we present an approach to in-air cloth manipulation that uses a variation of a recently proposed reinforcement learning architecture, DreamerV2. Our implementation modifies this architecture to utilise surface normals input, in addition to modiying the replay buffer and data augmentation procedures. Taken together these modifications represent an enhancement to the world model used by the robot, addressing the physical complexity of the object being manipulated by the robot. We present evaluations both in simulation and in a zero-shot deployment of the trained policies in a physical robot setup, performing in-air unfolding of a variety of different cloth types, demonstrating the generalisation benefits of our proposed architecture."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T18:14:41Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    18,
                    14,
                    41,
                    2,
                    49,
                    0
                ],
                "arxiv_comment": "8 pages, 5 figures, 3 tables",
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Jack Rome"
                    },
                    {
                        "name": "Stephen James"
                    },
                    {
                        "name": "Subramanian Ramamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Subramanian Ramamoorthy"
                },
                "author": "Subramanian Ramamoorthy"
            },
            {
                "id": "http://arxiv.org/abs/2602.16671v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16671v1",
                "title": "SPARC: Scenario Planning and Reasoning for Automated C Unit Test Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPARC: Scenario Planning and Reasoning for Automated C Unit Test Generation"
                },
                "updated": "2026-02-18T18:09:03Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    18,
                    9,
                    3,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16671v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16671v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Automated unit test generation for C remains a formidable challenge due to the semantic gap between high-level program intent and the rigid syntactic constraints of pointer arithmetic and manual memory management. While Large Language Models (LLMs) exhibit strong generative capabilities, direct intent-to-code synthesis frequently suffers from the leap-to-code failure mode, where models prematurely emit code without grounding in program structure, constraints, and semantics. This will result in non-compilable tests, hallucinated function signatures, low branch coverage, and semantically irrelevant assertions that cannot properly capture bugs. We introduce SPARC, a neuro-symbolic, scenario-based framework that bridges this gap through four stages: (1) Control Flow Graph (CFG) analysis, (2) an Operation Map that grounds LLM reasoning in validated utility helpers, (3) Path-targeted test synthesis, and (4) an iterative, self-correction validation loop using compiler and runtime feedback. We evaluate SPARC on 59 real-world and algorithmic subjects, where it outperforms the vanilla prompt generation baseline by 31.36% in line coverage, 26.01% in branch coverage, and 20.78% in mutation score, matching or exceeding the symbolic execution tool KLEE on complex subjects. SPARC retains 94.3% of tests through iterative repair and produces code with significantly higher developer-rated readability and maintainability. By aligning LLM reasoning with program structure, SPARC provides a scalable path for industrial-grade testing of legacy C codebases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated unit test generation for C remains a formidable challenge due to the semantic gap between high-level program intent and the rigid syntactic constraints of pointer arithmetic and manual memory management. While Large Language Models (LLMs) exhibit strong generative capabilities, direct intent-to-code synthesis frequently suffers from the leap-to-code failure mode, where models prematurely emit code without grounding in program structure, constraints, and semantics. This will result in non-compilable tests, hallucinated function signatures, low branch coverage, and semantically irrelevant assertions that cannot properly capture bugs. We introduce SPARC, a neuro-symbolic, scenario-based framework that bridges this gap through four stages: (1) Control Flow Graph (CFG) analysis, (2) an Operation Map that grounds LLM reasoning in validated utility helpers, (3) Path-targeted test synthesis, and (4) an iterative, self-correction validation loop using compiler and runtime feedback. We evaluate SPARC on 59 real-world and algorithmic subjects, where it outperforms the vanilla prompt generation baseline by 31.36% in line coverage, 26.01% in branch coverage, and 20.78% in mutation score, matching or exceeding the symbolic execution tool KLEE on complex subjects. SPARC retains 94.3% of tests through iterative repair and produces code with significantly higher developer-rated readability and maintainability. By aligning LLM reasoning with program structure, SPARC provides a scalable path for industrial-grade testing of legacy C codebases."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T18:09:03Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    18,
                    9,
                    3,
                    2,
                    49,
                    0
                ],
                "arxiv_comment": "9 pages, 6 figures, 4 tables",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Jaid Monwar Chowdhury"
                    },
                    {
                        "name": "Chi-An Fu"
                    },
                    {
                        "name": "Reyhaneh Jabbarvand"
                    }
                ],
                "author_detail": {
                    "name": "Reyhaneh Jabbarvand"
                },
                "author": "Reyhaneh Jabbarvand"
            },
            {
                "id": "http://arxiv.org/abs/2602.16662v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16662v1",
                "title": "Evaluating Collective Behaviour of Hundreds of LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Collective Behaviour of Hundreds of LLM Agents"
                },
                "updated": "2026-02-18T18:02:51Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    18,
                    2,
                    51,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16662v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16662v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As autonomous agents powered by LLM are increasingly deployed in society, understanding their collective behaviour in social dilemmas becomes critical. We introduce an evaluation framework where LLMs generate strategies encoded as algorithms, enabling inspection prior to deployment and scaling to populations of hundreds of agents -- substantially larger than in previous work. We find that more recent models tend to produce worse societal outcomes compared to older models when agents prioritise individual gain over collective benefits. Using cultural evolution to model user selection of agents, our simulations reveal a significant risk of convergence to poor societal equilibria, particularly when the relative benefit of cooperation diminishes and population sizes increase. We release our code as an evaluation suite for developers to assess the emergent collective behaviour of their models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As autonomous agents powered by LLM are increasingly deployed in society, understanding their collective behaviour in social dilemmas becomes critical. We introduce an evaluation framework where LLMs generate strategies encoded as algorithms, enabling inspection prior to deployment and scaling to populations of hundreds of agents -- substantially larger than in previous work. We find that more recent models tend to produce worse societal outcomes compared to older models when agents prioritise individual gain over collective benefits. Using cultural evolution to model user selection of agents, our simulations reveal a significant risk of convergence to poor societal equilibria, particularly when the relative benefit of cooperation diminishes and population sizes increase. We release our code as an evaluation suite for developers to assess the emergent collective behaviour of their models."
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T18:02:51Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    18,
                    2,
                    51,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA"
                },
                "authors": [
                    {
                        "name": "Richard Willis"
                    },
                    {
                        "name": "Jianing Zhao"
                    },
                    {
                        "name": "Yali Du"
                    },
                    {
                        "name": "Joel Z. Leibo"
                    }
                ],
                "author_detail": {
                    "name": "Joel Z. Leibo"
                },
                "author": "Joel Z. Leibo"
            },
            {
                "id": "http://arxiv.org/abs/2602.16660v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16660v1",
                "title": "Align Once, Benefit Multilingually: Enforcing Multilingual Consistency for LLM Safety Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Align Once, Benefit Multilingually: Enforcing Multilingual Consistency for LLM Safety Alignment"
                },
                "updated": "2026-02-18T18:01:23Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    18,
                    1,
                    23,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16660v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16660v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The widespread deployment of large language models (LLMs) across linguistic communities necessitates reliable multilingual safety alignment. However, recent efforts to extend alignment to other languages often require substantial resources, either through large-scale, high-quality supervision in the target language or through pairwise alignment with high-resource languages, which limits scalability. In this work, we propose a resource-efficient method for improving multilingual safety alignment. We introduce a plug-and-play Multi-Lingual Consistency (MLC) loss that can be integrated into existing monolingual alignment pipelines. By improving collinearity between multilingual representation vectors, our method encourages directional consistency at the multilingual semantic level in a single update. This allows simultaneous alignment across multiple languages using only multilingual prompt variants without requiring additional response-level supervision in low-resource languages. We validate the proposed method across different model architectures and alignment paradigms, and demonstrate its effectiveness in enhancing multilingual safety with limited impact on general model utility. Further evaluation across languages and tasks indicates improved cross-lingual generalization, suggesting the proposed approach as a practical solution for multilingual consistency alignment under limited supervision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread deployment of large language models (LLMs) across linguistic communities necessitates reliable multilingual safety alignment. However, recent efforts to extend alignment to other languages often require substantial resources, either through large-scale, high-quality supervision in the target language or through pairwise alignment with high-resource languages, which limits scalability. In this work, we propose a resource-efficient method for improving multilingual safety alignment. We introduce a plug-and-play Multi-Lingual Consistency (MLC) loss that can be integrated into existing monolingual alignment pipelines. By improving collinearity between multilingual representation vectors, our method encourages directional consistency at the multilingual semantic level in a single update. This allows simultaneous alignment across multiple languages using only multilingual prompt variants without requiring additional response-level supervision in low-resource languages. We validate the proposed method across different model architectures and alignment paradigms, and demonstrate its effectiveness in enhancing multilingual safety with limited impact on general model utility. Further evaluation across languages and tasks indicates improved cross-lingual generalization, suggesting the proposed approach as a practical solution for multilingual consistency alignment under limited supervision."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T18:01:23Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    18,
                    1,
                    23,
                    2,
                    49,
                    0
                ],
                "arxiv_comment": "Accepted by ICLR 2026",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yuyan Bu"
                    },
                    {
                        "name": "Xiaohao Liu"
                    },
                    {
                        "name": "ZhaoXing Ren"
                    },
                    {
                        "name": "Yaodong Yang"
                    },
                    {
                        "name": "Juntao Dai"
                    }
                ],
                "author_detail": {
                    "name": "Juntao Dai"
                },
                "author": "Juntao Dai"
            },
            {
                "id": "http://arxiv.org/abs/2602.15238v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.15238v2",
                "title": "Closing the Distribution Gap in Adversarial Training for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Closing the Distribution Gap in Adversarial Training for LLMs"
                },
                "updated": "2026-02-18T17:57:10Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    17,
                    57,
                    10,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.15238v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.15238v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Adversarial training for LLMs is one of the most promising methods to reliably improve robustness against adversaries. However, despite significant progress, models remain vulnerable to simple in-distribution exploits, such as rewriting prompts in the past tense or translating them into other languages. We argue that this persistent fragility stems from a fundamental limitation in current adversarial training algorithms: they minimize adversarial loss on their training set but inadequately cover the data distribution, resulting in vulnerability to seemingly simple attacks. To bridge this gap, we propose Distributional Adversarial Training, DAT. We leverage Diffusion LLMs to approximate the true joint distribution of prompts and responses, enabling generation of diverse, high-likelihood samples that address generalization failures. By combining optimization over the data distribution provided by the diffusion model with continuous adversarial training, DAT achieves substantially higher adversarial robustness than previous methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial training for LLMs is one of the most promising methods to reliably improve robustness against adversaries. However, despite significant progress, models remain vulnerable to simple in-distribution exploits, such as rewriting prompts in the past tense or translating them into other languages. We argue that this persistent fragility stems from a fundamental limitation in current adversarial training algorithms: they minimize adversarial loss on their training set but inadequately cover the data distribution, resulting in vulnerability to seemingly simple attacks. To bridge this gap, we propose Distributional Adversarial Training, DAT. We leverage Diffusion LLMs to approximate the true joint distribution of prompts and responses, enabling generation of diverse, high-likelihood samples that address generalization failures. By combining optimization over the data distribution provided by the diffusion model with continuous adversarial training, DAT achieves substantially higher adversarial robustness than previous methods."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-16T22:34:52Z",
                "published_parsed": [
                    2026,
                    2,
                    16,
                    22,
                    34,
                    52,
                    0,
                    47,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Chengzhi Hu"
                    },
                    {
                        "name": "Jonas Dornbusch"
                    },
                    {
                        "name": "David Lüdke"
                    },
                    {
                        "name": "Stephan Günnemann"
                    },
                    {
                        "name": "Leo Schwinn"
                    }
                ],
                "author_detail": {
                    "name": "Leo Schwinn"
                },
                "author": "Leo Schwinn"
            },
            {
                "id": "http://arxiv.org/abs/2602.16653v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16653v1",
                "title": "Agent Skill Framework: Perspectives on the Potential of Small Language Models in Industrial Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent Skill Framework: Perspectives on the Potential of Small Language Models in Industrial Environments"
                },
                "updated": "2026-02-18T17:52:17Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    17,
                    52,
                    17,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16653v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16653v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Agent Skill framework, now widely and officially supported by major players such as GitHub Copilot, LangChain, and OpenAI, performs especially well with proprietary models by improving context engineering, reducing hallucinations, and boosting task accuracy. Based on these observations, an investigation is conducted to determine whether the Agent Skill paradigm provides similar benefits to small language models (SLMs). This question matters in industrial scenarios where continuous reliance on public APIs is infeasible due to data-security and budget constraints requirements, and where SLMs often show limited generalization in highly customized scenarios. This work introduces a formal mathematical definition of the Agent Skill process, followed by a systematic evaluation of language models of varying sizes across multiple use cases. The evaluation encompasses two open-source tasks and a real-world insurance claims data set. The results show that tiny models struggle with reliable skill selection, while moderately sized SLMs (approximately 12B - 30B) parameters) benefit substantially from the Agent Skill approach. Moreover, code-specialized variants at around 80B parameters achieve performance comparable to closed-source baselines while improving GPU efficiency. Collectively, these findings provide a comprehensive and nuanced characterization of the capabilities and constraints of the framework, while providing actionable insights for the effective deployment of Agent Skills in SLM-centered environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent Skill framework, now widely and officially supported by major players such as GitHub Copilot, LangChain, and OpenAI, performs especially well with proprietary models by improving context engineering, reducing hallucinations, and boosting task accuracy. Based on these observations, an investigation is conducted to determine whether the Agent Skill paradigm provides similar benefits to small language models (SLMs). This question matters in industrial scenarios where continuous reliance on public APIs is infeasible due to data-security and budget constraints requirements, and where SLMs often show limited generalization in highly customized scenarios. This work introduces a formal mathematical definition of the Agent Skill process, followed by a systematic evaluation of language models of varying sizes across multiple use cases. The evaluation encompasses two open-source tasks and a real-world insurance claims data set. The results show that tiny models struggle with reliable skill selection, while moderately sized SLMs (approximately 12B - 30B) parameters) benefit substantially from the Agent Skill approach. Moreover, code-specialized variants at around 80B parameters achieve performance comparable to closed-source baselines while improving GPU efficiency. Collectively, these findings provide a comprehensive and nuanced characterization of the capabilities and constraints of the framework, while providing actionable insights for the effective deployment of Agent Skills in SLM-centered environments."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T17:52:17Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    17,
                    52,
                    17,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Yangjie Xu"
                    },
                    {
                        "name": "Lujun Li"
                    },
                    {
                        "name": "Lama Sleem"
                    },
                    {
                        "name": "Niccolo Gentile"
                    },
                    {
                        "name": "Yewei Song"
                    },
                    {
                        "name": "Yiqun Wang"
                    },
                    {
                        "name": "Siming Ji"
                    },
                    {
                        "name": "Wenbo Wu"
                    },
                    {
                        "name": "Radu State"
                    }
                ],
                "author_detail": {
                    "name": "Radu State"
                },
                "author": "Radu State"
            },
            {
                "id": "http://arxiv.org/abs/2602.16650v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16650v1",
                "title": "Retrieval Augmented Generation of Literature-derived Polymer Knowledge: The Example of a Biodegradable Polymer Expert System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation of Literature-derived Polymer Knowledge: The Example of a Biodegradable Polymer Expert System"
                },
                "updated": "2026-02-18T17:46:09Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    17,
                    46,
                    9,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16650v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16650v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Polymer literature contains a large and growing body of experimental knowledge, yet much of it is buried in unstructured text and inconsistent terminology, making systematic retrieval and reasoning difficult. Existing tools typically extract narrow, study-specific facts in isolation, failing to preserve the cross-study context required to answer broader scientific questions. Retrieval-augmented generation (RAG) offers a promising way to overcome this limitation by combining large language models (LLMs) with external retrieval, but its effectiveness depends strongly on how domain knowledge is represented. In this work, we develop two retrieval pipelines: a dense semantic vector-based approach (VectorRAG) and a graph-based approach (GraphRAG). Using over 1,000 polyhydroxyalkanoate (PHA) papers, we construct context-preserving paragraph embeddings and a canonicalized structured knowledge graph supporting entity disambiguation and multi-hop reasoning. We evaluate these pipelines through standard retrieval metrics, comparisons with general state-of-the-art systems such as GPT and Gemini, and qualitative validation by a domain chemist. The results show that GraphRAG achieves higher precision and interpretability, while VectorRAG provides broader recall, highlighting complementary trade-offs. Expert validation further confirms that the tailored pipelines, particularly GraphRAG, produce well-grounded, citation-reliable responses with strong domain relevance. By grounding every statement in evidence, these systems enable researchers to navigate the literature, compare findings across studies, and uncover patterns that are difficult to extract manually. More broadly, this work establishes a practical framework for building materials science assistants using curated corpora and retrieval design, reducing reliance on proprietary models while enabling trustworthy literature analysis at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Polymer literature contains a large and growing body of experimental knowledge, yet much of it is buried in unstructured text and inconsistent terminology, making systematic retrieval and reasoning difficult. Existing tools typically extract narrow, study-specific facts in isolation, failing to preserve the cross-study context required to answer broader scientific questions. Retrieval-augmented generation (RAG) offers a promising way to overcome this limitation by combining large language models (LLMs) with external retrieval, but its effectiveness depends strongly on how domain knowledge is represented. In this work, we develop two retrieval pipelines: a dense semantic vector-based approach (VectorRAG) and a graph-based approach (GraphRAG). Using over 1,000 polyhydroxyalkanoate (PHA) papers, we construct context-preserving paragraph embeddings and a canonicalized structured knowledge graph supporting entity disambiguation and multi-hop reasoning. We evaluate these pipelines through standard retrieval metrics, comparisons with general state-of-the-art systems such as GPT and Gemini, and qualitative validation by a domain chemist. The results show that GraphRAG achieves higher precision and interpretability, while VectorRAG provides broader recall, highlighting complementary trade-offs. Expert validation further confirms that the tailored pipelines, particularly GraphRAG, produce well-grounded, citation-reliable responses with strong domain relevance. By grounding every statement in evidence, these systems enable researchers to navigate the literature, compare findings across studies, and uncover patterns that are difficult to extract manually. More broadly, this work establishes a practical framework for building materials science assistants using curated corpora and retrieval design, reducing reliance on proprietary models while enabling trustworthy literature analysis at scale."
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T17:46:09Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    17,
                    46,
                    9,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE"
                },
                "authors": [
                    {
                        "name": "Sonakshi Gupta"
                    },
                    {
                        "name": "Akhlak Mahmood"
                    },
                    {
                        "name": "Wei Xiong"
                    },
                    {
                        "name": "Rampi Ramprasad"
                    }
                ],
                "author_detail": {
                    "name": "Rampi Ramprasad"
                },
                "author": "Rampi Ramprasad"
            },
            {
                "id": "http://arxiv.org/abs/2602.16640v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16640v1",
                "title": "Quecto-V1: Empirical Analysis of 8-bit Quantized Small Language Models for On-Device Legal Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quecto-V1: Empirical Analysis of 8-bit Quantized Small Language Models for On-Device Legal Retrieval"
                },
                "updated": "2026-02-18T17:29:43Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    17,
                    29,
                    43,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16640v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16640v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The rapid proliferation of Large Language Models (LLMs) has revolutionized Natural Language Processing (NLP) but has simultaneously created a \"resource divide.\" State-of-the-art legal intelligence systems typically rely on massive parameter counts (7B+) and cloud-based inference, rendering them inaccessible to practitioners in resource-constrained environments and posing significant data sovereignty risks. This paper introduces Quecto-V1, a domain-specific Small Language Model (SLM) engineered to democratize access to Indian legal intelligence. Built upon a custom configuration of the GPT-2 architecture (124 million parameters), Quecto-V1 was trained from scratch exclusively on a corpus of Indian statutes, including the Indian Penal Code (IPC), the Code of Criminal Procedure (CrPC), and the Constitution of India. Unlike generalist models, which prioritize broad world knowledge, our approach maximizes \"lexical density\" within the legal domain. Furthermore, we address the deployment bottleneck by applying post-training 8-bit quantization (GGUF format), compressing the model to a memory footprint of under 150 MB. Our empirical analysis demonstrates that Quecto-V1 achieves high fidelity in retrieving statutory definitions and penal provisions, outperforming general-purpose SLMs in domain-specific exact match tasks while running entirely offline on consumer-grade CPUs. We further present an ablation study showing that 8-bit quantization yields a 74% reduction in model size with less than 3.5% degradation in retrieval accuracy compared to full-precision baselines. These findings suggest that for specialized, high-stakes domains like law, domain-specific training coupled with aggressive quantization offers a viable, privacy-preserving alternative to monolithic cloud models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid proliferation of Large Language Models (LLMs) has revolutionized Natural Language Processing (NLP) but has simultaneously created a \"resource divide.\" State-of-the-art legal intelligence systems typically rely on massive parameter counts (7B+) and cloud-based inference, rendering them inaccessible to practitioners in resource-constrained environments and posing significant data sovereignty risks. This paper introduces Quecto-V1, a domain-specific Small Language Model (SLM) engineered to democratize access to Indian legal intelligence. Built upon a custom configuration of the GPT-2 architecture (124 million parameters), Quecto-V1 was trained from scratch exclusively on a corpus of Indian statutes, including the Indian Penal Code (IPC), the Code of Criminal Procedure (CrPC), and the Constitution of India. Unlike generalist models, which prioritize broad world knowledge, our approach maximizes \"lexical density\" within the legal domain. Furthermore, we address the deployment bottleneck by applying post-training 8-bit quantization (GGUF format), compressing the model to a memory footprint of under 150 MB. Our empirical analysis demonstrates that Quecto-V1 achieves high fidelity in retrieving statutory definitions and penal provisions, outperforming general-purpose SLMs in domain-specific exact match tasks while running entirely offline on consumer-grade CPUs. We further present an ablation study showing that 8-bit quantization yields a 74% reduction in model size with less than 3.5% degradation in retrieval accuracy compared to full-precision baselines. These findings suggest that for specialized, high-stakes domains like law, domain-specific training coupled with aggressive quantization offers a viable, privacy-preserving alternative to monolithic cloud models."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T17:29:43Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    17,
                    29,
                    43,
                    2,
                    49,
                    0
                ],
                "arxiv_comment": "5 pages, 2 tables",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Subrit Dikshit"
                    }
                ],
                "author_detail": {
                    "name": "Subrit Dikshit"
                },
                "author": "Subrit Dikshit"
            },
            {
                "id": "http://arxiv.org/abs/2602.16639v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16639v1",
                "title": "AREG: Adversarial Resource Extraction Game for Evaluating Persuasion and Resistance in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AREG: Adversarial Resource Extraction Game for Evaluating Persuasion and Resistance in Large Language Models"
                },
                "updated": "2026-02-18T17:28:28Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    17,
                    28,
                    28,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16639v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16639v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Evaluating the social intelligence of Large Language Models (LLMs) increasingly requires moving beyond static text generation toward dynamic, adversarial interaction. We introduce the Adversarial Resource Extraction Game (AREG), a benchmark that operationalizes persuasion and resistance as a multi-turn, zero-sum negotiation over financial resources. Using a round-robin tournament across frontier models, AREG enables joint evaluation of offensive (persuasion) and defensive (resistance) capabilities within a single interactional framework. Our analysis provides evidence that these capabilities are weakly correlated ($ρ= 0.33$) and empirically dissociated: strong persuasive performance does not reliably predict strong resistance, and vice versa. Across all evaluated models, resistance scores exceed persuasion scores, indicating a systematic defensive advantage in adversarial dialogue settings. Further linguistic analysis suggests that interaction structure plays a central role in these outcomes. Incremental commitment-seeking strategies are associated with higher extraction success, while verification-seeking responses are more prevalent in successful defenses than explicit refusal. Together, these findings indicate that social influence in LLMs is not a monolithic capability and that evaluation frameworks focusing on persuasion alone may overlook asymmetric behavioral vulnerabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the social intelligence of Large Language Models (LLMs) increasingly requires moving beyond static text generation toward dynamic, adversarial interaction. We introduce the Adversarial Resource Extraction Game (AREG), a benchmark that operationalizes persuasion and resistance as a multi-turn, zero-sum negotiation over financial resources. Using a round-robin tournament across frontier models, AREG enables joint evaluation of offensive (persuasion) and defensive (resistance) capabilities within a single interactional framework. Our analysis provides evidence that these capabilities are weakly correlated ($ρ= 0.33$) and empirically dissociated: strong persuasive performance does not reliably predict strong resistance, and vice versa. Across all evaluated models, resistance scores exceed persuasion scores, indicating a systematic defensive advantage in adversarial dialogue settings. Further linguistic analysis suggests that interaction structure plays a central role in these outcomes. Incremental commitment-seeking strategies are associated with higher extraction success, while verification-seeking responses are more prevalent in successful defenses than explicit refusal. Together, these findings indicate that social influence in LLMs is not a monolithic capability and that evaluation frameworks focusing on persuasion alone may overlook asymmetric behavioral vulnerabilities."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T17:28:28Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    17,
                    28,
                    28,
                    2,
                    49,
                    0
                ],
                "arxiv_comment": "15 pages, 5 figures, 11 tables. Includes appendix with detailed experimental results and prompts",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Adib Sakhawat"
                    },
                    {
                        "name": "Fardeen Sadab"
                    }
                ],
                "author_detail": {
                    "name": "Fardeen Sadab"
                },
                "author": "Fardeen Sadab"
            },
            {
                "id": "http://arxiv.org/abs/2602.16637v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16637v1",
                "title": "Active RIS-Assisted MIMO System for Vital Signs Extraction: ISAC Modeling, Deep Learning, and Prototype Measurements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active RIS-Assisted MIMO System for Vital Signs Extraction: ISAC Modeling, Deep Learning, and Prototype Measurements"
                },
                "updated": "2026-02-18T17:27:48Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    17,
                    27,
                    48,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16637v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16637v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present the RIS-VSign system, an active reconfigurable intelligent surface (RIS)-assisted multiple-input multiple-output orthogonal frequency division multiplexing (MIMO-OFDM) framework for vital signs extraction under an integrated sensing and communication (ISAC) model. The system consists of two stages: the phase selector of RIS and the extraction of respiration rate. To mitigate synchronization-induced common phase drifts, the difference of Möbius transformation (DMT) is integrated into the deep learning framework, named DMTNet, to jointly configure multiple active RIS elements. Notably, the training data are generated in simulation without collecting real-world measurements, and the resulting phase selector is validated experimentally. For sensing, multi-antenna measurements are fused by the DC-offset calibration and the DeepMining-MMV processing with CA-CFAR detection and Newton's refinements. Prototype experiments indicate that active RIS deployment improves respiration detectability while simultaneously enabling higher-order modulation; without RIS, respiration detection is unreliable and only lower-order modulation is supported.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the RIS-VSign system, an active reconfigurable intelligent surface (RIS)-assisted multiple-input multiple-output orthogonal frequency division multiplexing (MIMO-OFDM) framework for vital signs extraction under an integrated sensing and communication (ISAC) model. The system consists of two stages: the phase selector of RIS and the extraction of respiration rate. To mitigate synchronization-induced common phase drifts, the difference of Möbius transformation (DMT) is integrated into the deep learning framework, named DMTNet, to jointly configure multiple active RIS elements. Notably, the training data are generated in simulation without collecting real-world measurements, and the resulting phase selector is validated experimentally. For sensing, multi-antenna measurements are fused by the DC-offset calibration and the DeepMining-MMV processing with CA-CFAR detection and Newton's refinements. Prototype experiments indicate that active RIS deployment improves respiration detectability while simultaneously enabling higher-order modulation; without RIS, respiration detection is unreliable and only lower-order modulation is supported."
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T17:27:48Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    17,
                    27,
                    48,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP"
                },
                "authors": [
                    {
                        "name": "De-Ming Chian"
                    },
                    {
                        "name": "Chao-Kai Wen"
                    },
                    {
                        "name": "Feng-Ji Chen"
                    },
                    {
                        "name": "Yi-Jie Sun"
                    },
                    {
                        "name": "Fu-Kang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Fu-Kang Wang"
                },
                "author": "Fu-Kang Wang"
            },
            {
                "id": "http://arxiv.org/abs/2601.15145v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.15145v2",
                "title": "Weather Estimation for Integrated Sensing and Communication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weather Estimation for Integrated Sensing and Communication"
                },
                "updated": "2026-02-18T17:15:32Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    17,
                    15,
                    32,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.15145v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.15145v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "One of the key features of sixth-generation (6G) mobile communications will be integrated sensing and communication (ISAC). While the main goal of ISAC in standardization efforts is to detect objects, the byproducts of radar operations can be used to enable new services in 6G, such as weather sensing. Even though weather radars are the most prominent technology for weather detection and monitoring, they are expensive and usually neglect areas in close vicinity. To this end, we propose reusing the dense deployment of 6G base stations for weather sensing purposes by detecting and estimating weather conditions. We implement both a classifier and a regressor as a convolutional neural network trained across measurements with varying precipitation rates and wind speeds. We implement our approach in an ISAC proof-of-concept and conduct a multi-week experiment campaign. Experimental results show that we are able to jointly and accurately classify weather conditions with accuracies of 99.38% and 98.99% for precipitation rate and wind speed, respectively. For estimation, we obtain errors of 1.2 mm/h and 1.5 km/h, for precipitation rate and wind speed, respectively. These findings indicate that weather sensing services can be reliably deployed in 6G ISAC networks, broadening their service portfolio and boosting their market value.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One of the key features of sixth-generation (6G) mobile communications will be integrated sensing and communication (ISAC). While the main goal of ISAC in standardization efforts is to detect objects, the byproducts of radar operations can be used to enable new services in 6G, such as weather sensing. Even though weather radars are the most prominent technology for weather detection and monitoring, they are expensive and usually neglect areas in close vicinity. To this end, we propose reusing the dense deployment of 6G base stations for weather sensing purposes by detecting and estimating weather conditions. We implement both a classifier and a regressor as a convolutional neural network trained across measurements with varying precipitation rates and wind speeds. We implement our approach in an ISAC proof-of-concept and conduct a multi-week experiment campaign. Experimental results show that we are able to jointly and accurately classify weather conditions with accuracies of 99.38% and 98.99% for precipitation rate and wind speed, respectively. For estimation, we obtain errors of 1.2 mm/h and 1.5 km/h, for precipitation rate and wind speed, respectively. These findings indicate that weather sensing services can be reliably deployed in 6G ISAC networks, broadening their service portfolio and boosting their market value."
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-21T16:18:22Z",
                "published_parsed": [
                    2026,
                    1,
                    21,
                    16,
                    18,
                    22,
                    2,
                    21,
                    0
                ],
                "arxiv_comment": "This work has been submitted to IEEE for possible publication",
                "arxiv_primary_category": {
                    "term": "eess.SP"
                },
                "authors": [
                    {
                        "name": "Victoria Palhares"
                    },
                    {
                        "name": "Artjom Grudnitsky"
                    },
                    {
                        "name": "Silvio Mandelli"
                    }
                ],
                "author_detail": {
                    "name": "Silvio Mandelli"
                },
                "author": "Silvio Mandelli"
            },
            {
                "id": "http://arxiv.org/abs/2510.16161v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.16161v2",
                "title": "Still Competitive: Revisiting Recurrent Models for Irregular Time Series Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Still Competitive: Revisiting Recurrent Models for Irregular Time Series Prediction"
                },
                "updated": "2026-02-18T17:10:14Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    17,
                    10,
                    14,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.16161v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.16161v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Modeling irregularly sampled multivariate time series is a persistent challenge in domains like healthcare and sensor networks. While recent works have explored a variety of complex learning architectures to solve the prediction problems for irregularly sampled time series, it remains unclear what the true benefits of some of these architectures are, and whether clever modifications of simpler and more efficient RNN-based algorithms are still competitive, i.e. they are on par with or even superior to these methods. In this work, we propose and study GRUwE: Gated Recurrent Unit with Exponential basis functions, that builds upon RNN-based architectures for observations made at irregular times. GRUwE supports both regression-based and event-based predictions in continuous time. GRUwE works by maintaining a Markov state representation of the time series that updates with the arrival of irregular observations. The Markov state update relies on two reset mechanisms: (i) observation-triggered reset to account for the new observation, and (ii) time-triggered reset that relies on learnable exponential decays, to support the predictions in continuous time. Our empirical evaluations across several real-world benchmarks on next-observation and next-event prediction tasks demonstrate that GRUwE can indeed achieve competitive or superior performance compared to the recent state-of-the-art (SOTA) methods. Thanks to its simplicity, GRUwE offers compelling advantages: it is easy to implement, requires minimal hyper-parameter tuning efforts, and significantly reduces the computational overhead in the online deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling irregularly sampled multivariate time series is a persistent challenge in domains like healthcare and sensor networks. While recent works have explored a variety of complex learning architectures to solve the prediction problems for irregularly sampled time series, it remains unclear what the true benefits of some of these architectures are, and whether clever modifications of simpler and more efficient RNN-based algorithms are still competitive, i.e. they are on par with or even superior to these methods. In this work, we propose and study GRUwE: Gated Recurrent Unit with Exponential basis functions, that builds upon RNN-based architectures for observations made at irregular times. GRUwE supports both regression-based and event-based predictions in continuous time. GRUwE works by maintaining a Markov state representation of the time series that updates with the arrival of irregular observations. The Markov state update relies on two reset mechanisms: (i) observation-triggered reset to account for the new observation, and (ii) time-triggered reset that relies on learnable exponential decays, to support the predictions in continuous time. Our empirical evaluations across several real-world benchmarks on next-observation and next-event prediction tasks demonstrate that GRUwE can indeed achieve competitive or superior performance compared to the recent state-of-the-art (SOTA) methods. Thanks to its simplicity, GRUwE offers compelling advantages: it is easy to implement, requires minimal hyper-parameter tuning efforts, and significantly reduces the computational overhead in the online deployment."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-17T19:04:16Z",
                "published_parsed": [
                    2025,
                    10,
                    17,
                    19,
                    4,
                    16,
                    4,
                    290,
                    0
                ],
                "arxiv_comment": "Published in Transactions on Machine Learning Research, 2026",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Ankitkumar Joshi"
                    },
                    {
                        "name": "Milos Hauskrecht"
                    }
                ],
                "author_detail": {
                    "name": "Milos Hauskrecht"
                },
                "author": "Milos Hauskrecht"
            },
            {
                "id": "http://arxiv.org/abs/2602.16610v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16610v1",
                "title": "Who can we trust? LLM-as-a-jury for Comparative Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Who can we trust? LLM-as-a-jury for Comparative Assessment"
                },
                "updated": "2026-02-18T17:04:02Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    17,
                    4,
                    2,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16610v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16610v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) are increasingly applied as automatic evaluators for natural language generation assessment often using pairwise comparative judgements. Existing approaches typically rely on single judges or aggregate multiple judges assuming equal reliability. In practice, LLM judges vary substantially in performance across tasks and aspects, and their judgment probabilities may be biased and inconsistent. Furthermore, human-labelled supervision for judge calibration may be unavailable. We first empirically demonstrate that inconsistencies in LLM comparison probabilities exist and show that it limits the effectiveness of direct probability-based ranking. To address this, we study the LLM-as-a-jury setting and propose BT-sigma, a judge-aware extension of the Bradley-Terry model that introduces a discriminator parameter for each judge to jointly infer item rankings and judge reliability from pairwise comparisons alone. Experiments on benchmark NLG evaluation datasets show that BT-sigma consistently outperforms averaging-based aggregation methods, and that the learned discriminator strongly correlates with independent measures of the cycle consistency of LLM judgments. Further analysis reveals that BT-sigma can be interpreted as an unsupervised calibration mechanism that improves aggregation by modelling judge reliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly applied as automatic evaluators for natural language generation assessment often using pairwise comparative judgements. Existing approaches typically rely on single judges or aggregate multiple judges assuming equal reliability. In practice, LLM judges vary substantially in performance across tasks and aspects, and their judgment probabilities may be biased and inconsistent. Furthermore, human-labelled supervision for judge calibration may be unavailable. We first empirically demonstrate that inconsistencies in LLM comparison probabilities exist and show that it limits the effectiveness of direct probability-based ranking. To address this, we study the LLM-as-a-jury setting and propose BT-sigma, a judge-aware extension of the Bradley-Terry model that introduces a discriminator parameter for each judge to jointly infer item rankings and judge reliability from pairwise comparisons alone. Experiments on benchmark NLG evaluation datasets show that BT-sigma consistently outperforms averaging-based aggregation methods, and that the learned discriminator strongly correlates with independent measures of the cycle consistency of LLM judgments. Further analysis reveals that BT-sigma can be interpreted as an unsupervised calibration mechanism that improves aggregation by modelling judge reliability."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T17:04:02Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    17,
                    4,
                    2,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Mengjie Qian"
                    },
                    {
                        "name": "Guangzhi Sun"
                    },
                    {
                        "name": "Mark J. F. Gales"
                    },
                    {
                        "name": "Kate M. Knill"
                    }
                ],
                "author_detail": {
                    "name": "Kate M. Knill"
                },
                "author": "Kate M. Knill"
            },
            {
                "id": "http://arxiv.org/abs/2602.16607v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16607v1",
                "title": "CitiLink-Summ: Summarization of Discussion Subjects in European Portuguese Municipal Meeting Minutes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CitiLink-Summ: Summarization of Discussion Subjects in European Portuguese Municipal Meeting Minutes"
                },
                "updated": "2026-02-18T17:03:07Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    17,
                    3,
                    7,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16607v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16607v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Municipal meeting minutes are formal records documenting the discussions and decisions of local government, yet their content is often lengthy, dense, and difficult for citizens to navigate. Automatic summarization can help address this challenge by producing concise summaries for each discussion subject. Despite its potential, research on summarizing discussion subjects in municipal meeting minutes remains largely unexplored, especially in low-resource languages, where the inherent complexity of these documents adds further challenges. A major bottleneck is the scarcity of datasets containing high-quality, manually crafted summaries, which limits the development and evaluation of effective summarization models for this domain. In this paper, we present CitiLink-Summ, a new corpus of European Portuguese municipal meeting minutes, comprising 100 documents and 2,322 manually hand-written summaries, each corresponding to a distinct discussion subject. Leveraging this dataset, we establish baseline results for automatic summarization in this domain, employing state-of-the-art generative models (e.g., BART, PRIMERA) as well as large language models (LLMs), evaluated with both lexical and semantic metrics such as ROUGE, BLEU, METEOR, and BERTScore. CitiLink-Summ provides the first benchmark for municipal-domain summarization in European Portuguese, offering a valuable resource for advancing NLP research on complex administrative texts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Municipal meeting minutes are formal records documenting the discussions and decisions of local government, yet their content is often lengthy, dense, and difficult for citizens to navigate. Automatic summarization can help address this challenge by producing concise summaries for each discussion subject. Despite its potential, research on summarizing discussion subjects in municipal meeting minutes remains largely unexplored, especially in low-resource languages, where the inherent complexity of these documents adds further challenges. A major bottleneck is the scarcity of datasets containing high-quality, manually crafted summaries, which limits the development and evaluation of effective summarization models for this domain. In this paper, we present CitiLink-Summ, a new corpus of European Portuguese municipal meeting minutes, comprising 100 documents and 2,322 manually hand-written summaries, each corresponding to a distinct discussion subject. Leveraging this dataset, we establish baseline results for automatic summarization in this domain, employing state-of-the-art generative models (e.g., BART, PRIMERA) as well as large language models (LLMs), evaluated with both lexical and semantic metrics such as ROUGE, BLEU, METEOR, and BERTScore. CitiLink-Summ provides the first benchmark for municipal-domain summarization in European Portuguese, offering a valuable resource for advancing NLP research on complex administrative texts."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T17:03:07Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    17,
                    3,
                    7,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Miguel Marques"
                    },
                    {
                        "name": "Ana Luísa Fernandes"
                    },
                    {
                        "name": "Ana Filipa Pacheco"
                    },
                    {
                        "name": "Rute Rebouças"
                    },
                    {
                        "name": "Inês Cantante"
                    },
                    {
                        "name": "José Isidro"
                    },
                    {
                        "name": "Luís Filipe Cunha"
                    },
                    {
                        "name": "Alípio Jorge"
                    },
                    {
                        "name": "Nuno Guimarães"
                    },
                    {
                        "name": "Sérgio Nunes"
                    },
                    {
                        "name": "António Leal"
                    },
                    {
                        "name": "Purificação Silvano"
                    },
                    {
                        "name": "Ricardo Campos"
                    }
                ],
                "author_detail": {
                    "name": "Ricardo Campos"
                },
                "author": "Ricardo Campos"
            },
            {
                "id": "http://arxiv.org/abs/2602.16603v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16603v1",
                "title": "FlowPrefill: Decoupling Preemption from Prefill Scheduling Granularity to Mitigate Head-of-Line Blocking in LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlowPrefill: Decoupling Preemption from Prefill Scheduling Granularity to Mitigate Head-of-Line Blocking in LLM Serving"
                },
                "updated": "2026-02-18T16:57:45Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    16,
                    57,
                    45,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16603v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16603v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The growing demand for large language models (LLMs) requires serving systems to handle many concurrent requests with diverse service level objectives (SLOs). This exacerbates head-of-line (HoL) blocking during the compute-intensive prefill phase, where long-running requests monopolize resources and delay higher-priority ones, leading to widespread time-to-first-token (TTFT) SLO violations. While chunked prefill enables interruptibility, it introduces an inherent trade-off between responsiveness and throughput: reducing chunk size improves response latency but degrades computational efficiency, whereas increasing chunk size maximizes throughput but exacerbates blocking. This necessitates an adaptive preemption mechanism. However, dynamically balancing execution granularity against scheduling overheads remains a key challenge.\n  In this paper, we propose FlowPrefill, a TTFT-goodput-optimized serving system that resolves this conflict by decoupling preemption granularity from scheduling frequency. To achieve adaptive prefill scheduling, FlowPrefill introduces two key innovations: 1) Operator-Level Preemption, which leverages operator boundaries to enable fine-grained execution interruption without the efficiency loss associated with fixed small chunking; and 2) Event-Driven Scheduling, which triggers scheduling decisions only upon request arrival or completion events, thereby supporting efficient preemption responsiveness while minimizing control-plane overhead. Evaluation on real-world production traces shows that FlowPrefill improves maximum goodput by up to 5.6$\\times$ compared to state-of-the-art systems while satisfying heterogeneous SLOs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing demand for large language models (LLMs) requires serving systems to handle many concurrent requests with diverse service level objectives (SLOs). This exacerbates head-of-line (HoL) blocking during the compute-intensive prefill phase, where long-running requests monopolize resources and delay higher-priority ones, leading to widespread time-to-first-token (TTFT) SLO violations. While chunked prefill enables interruptibility, it introduces an inherent trade-off between responsiveness and throughput: reducing chunk size improves response latency but degrades computational efficiency, whereas increasing chunk size maximizes throughput but exacerbates blocking. This necessitates an adaptive preemption mechanism. However, dynamically balancing execution granularity against scheduling overheads remains a key challenge.\n  In this paper, we propose FlowPrefill, a TTFT-goodput-optimized serving system that resolves this conflict by decoupling preemption granularity from scheduling frequency. To achieve adaptive prefill scheduling, FlowPrefill introduces two key innovations: 1) Operator-Level Preemption, which leverages operator boundaries to enable fine-grained execution interruption without the efficiency loss associated with fixed small chunking; and 2) Event-Driven Scheduling, which triggers scheduling decisions only upon request arrival or completion events, thereby supporting efficient preemption responsiveness while minimizing control-plane overhead. Evaluation on real-world production traces shows that FlowPrefill improves maximum goodput by up to 5.6$\\times$ compared to state-of-the-art systems while satisfying heterogeneous SLOs."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T16:57:45Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    16,
                    57,
                    45,
                    2,
                    49,
                    0
                ],
                "arxiv_comment": "13 pages",
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Chia-chi Hsieh"
                    },
                    {
                        "name": "Zan Zong"
                    },
                    {
                        "name": "Xinyang Chen"
                    },
                    {
                        "name": "Jianjiang Li"
                    },
                    {
                        "name": "Jidong Zhai"
                    },
                    {
                        "name": "Lijie Wen"
                    }
                ],
                "author_detail": {
                    "name": "Lijie Wen"
                },
                "author": "Lijie Wen"
            },
            {
                "id": "http://arxiv.org/abs/2403.00553v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2403.00553v3",
                "title": "Standardizing the Measurement of Text Diversity: A Tool and a Comparative Analysis of Scores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Standardizing the Measurement of Text Diversity: A Tool and a Comparative Analysis of Scores"
                },
                "updated": "2026-02-18T16:51:27Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    16,
                    51,
                    27,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2403.00553v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2403.00553v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The diversity across outputs generated by LLMs shapes perception of their quality and utility. High lexical diversity is often desirable, but there is no standard method to measure this property. Templated answer structures and ``canned'' responses across different documents are readily noticeable, but difficult to visualize across large corpora. This work aims to standardize measurement of text diversity. Specifically, we empirically investigate the convergent validity of existing scores across English texts, and we release diversity, an open-source Python package for measuring and extracting repetition in text. We also build a platform based on diversity for users to interactively explore repetition in text. We find that fast compression algorithms capture information similar to what is measured by slow-to-compute $n$-gram overlap homogeneity scores. Further, a combination of measures -- compression ratios, self-repetition of long $n$-grams, and Self-BLEU and BERTScore -- are sufficient to report, as they have low mutual correlation with each other.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The diversity across outputs generated by LLMs shapes perception of their quality and utility. High lexical diversity is often desirable, but there is no standard method to measure this property. Templated answer structures and ``canned'' responses across different documents are readily noticeable, but difficult to visualize across large corpora. This work aims to standardize measurement of text diversity. Specifically, we empirically investigate the convergent validity of existing scores across English texts, and we release diversity, an open-source Python package for measuring and extracting repetition in text. We also build a platform based on diversity for users to interactively explore repetition in text. We find that fast compression algorithms capture information similar to what is measured by slow-to-compute $n$-gram overlap homogeneity scores. Further, a combination of measures -- compression ratios, self-repetition of long $n$-grams, and Self-BLEU and BERTScore -- are sufficient to report, as they have low mutual correlation with each other."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-03-01T14:23:12Z",
                "published_parsed": [
                    2024,
                    3,
                    1,
                    14,
                    23,
                    12,
                    4,
                    61,
                    0
                ],
                "arxiv_comment": "AACL 2025",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Chantal Shaib"
                    },
                    {
                        "name": "Venkata S. Govindarajan"
                    },
                    {
                        "name": "Joe Barrow"
                    },
                    {
                        "name": "Jiuding Sun"
                    },
                    {
                        "name": "Alexa F. Siu"
                    },
                    {
                        "name": "Byron C. Wallace"
                    },
                    {
                        "name": "Ani Nenkova"
                    }
                ],
                "author_detail": {
                    "name": "Ani Nenkova"
                },
                "author": "Ani Nenkova"
            },
            {
                "id": "http://arxiv.org/abs/2505.10992v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.10992v2",
                "title": "ReaCritic: Reasoning Transformer-based DRL Critic-model Scaling For Wireless Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReaCritic: Reasoning Transformer-based DRL Critic-model Scaling For Wireless Networks"
                },
                "updated": "2026-02-18T16:45:13Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    16,
                    45,
                    13,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.10992v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.10992v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Heterogeneous Networks (HetNets) pose critical challenges for intelligent management due to the diverse user requirements and time-varying wireless conditions. These factors introduce significant decision complexity, which limits the adaptability of existing Deep Reinforcement Learning (DRL) methods. In many DRL algorithms, especially those involving value-based or actor-critic structures, the critic component plays a key role in guiding policy learning by estimating value functions. However, conventional critic models often use shallow architectures that map observations directly to scalar estimates, limiting their ability to handle multi-task complexity. In contrast, recent progress in inference-time scaling of Large Language Models (LLMs) has shown that generating intermediate reasoning steps can significantly improve decision quality. Motivated by this, we propose ReaCritic, a reasoning transformer-based critic-model scaling scheme that brings reasoning-like ability into DRL. ReaCritic performs horizontal reasoning over parallel state-action inputs and vertical reasoning through deep transformer stacks. It is compatible with a broad range of value-based and actor-critic DRL algorithms and enhances generalization in dynamic wireless environments. Extensive experiments demonstrate that ReaCritic improves convergence speed and final performance across various HetNet settings and standard OpenAI Gym control tasks. The code of ReaCritic is available at https://github.com/NICE-HKU/ReaCritic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous Networks (HetNets) pose critical challenges for intelligent management due to the diverse user requirements and time-varying wireless conditions. These factors introduce significant decision complexity, which limits the adaptability of existing Deep Reinforcement Learning (DRL) methods. In many DRL algorithms, especially those involving value-based or actor-critic structures, the critic component plays a key role in guiding policy learning by estimating value functions. However, conventional critic models often use shallow architectures that map observations directly to scalar estimates, limiting their ability to handle multi-task complexity. In contrast, recent progress in inference-time scaling of Large Language Models (LLMs) has shown that generating intermediate reasoning steps can significantly improve decision quality. Motivated by this, we propose ReaCritic, a reasoning transformer-based critic-model scaling scheme that brings reasoning-like ability into DRL. ReaCritic performs horizontal reasoning over parallel state-action inputs and vertical reasoning through deep transformer stacks. It is compatible with a broad range of value-based and actor-critic DRL algorithms and enhances generalization in dynamic wireless environments. Extensive experiments demonstrate that ReaCritic improves convergence speed and final performance across various HetNet settings and standard OpenAI Gym control tasks. The code of ReaCritic is available at https://github.com/NICE-HKU/ReaCritic."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-16T08:42:08Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    8,
                    42,
                    8,
                    4,
                    136,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Feiran You"
                    },
                    {
                        "name": "Hongyang Du"
                    }
                ],
                "author_detail": {
                    "name": "Hongyang Du"
                },
                "author": "Hongyang Du"
            },
            {
                "id": "http://arxiv.org/abs/2602.14803v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.14803v2",
                "title": "A physics inspired and efficient transform for optoacoustic systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A physics inspired and efficient transform for optoacoustic systems"
                },
                "updated": "2026-02-18T16:42:32Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    16,
                    42,
                    32,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.14803v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.14803v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Optoacoustic imaging technologies require fast and accurate signal pre-processing algorithms to enable widespread deployment in clinical and home-care settings. However, they still rely on the Discrete Fourier Transform (DFT) as the default tool for essential signal-conditioning operations, which imposes hard limits on both execution speed and signal-retrieval accuracy. Here, we present a new transform whose building blocks are directly inspired by the physics of optoacoustic signal generation. We compared its performance with the DFT and other classical transforms on common signal-processing tasks using both simulations and experimental datasets. Our results indicate that the proposed transform not only sets a new lower bound on computational complexity relative to the DFT, but also substantially outperforms classical transforms on basic signal-processing operations in terms of accuracy. We expect this transform to catalyze broader adoption of optoacoustic methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optoacoustic imaging technologies require fast and accurate signal pre-processing algorithms to enable widespread deployment in clinical and home-care settings. However, they still rely on the Discrete Fourier Transform (DFT) as the default tool for essential signal-conditioning operations, which imposes hard limits on both execution speed and signal-retrieval accuracy. Here, we present a new transform whose building blocks are directly inspired by the physics of optoacoustic signal generation. We compared its performance with the DFT and other classical transforms on common signal-processing tasks using both simulations and experimental datasets. Our results indicate that the proposed transform not only sets a new lower bound on computational complexity relative to the DFT, but also substantially outperforms classical transforms on basic signal-processing operations in terms of accuracy. We expect this transform to catalyze broader adoption of optoacoustic methods."
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-16T14:54:00Z",
                "published_parsed": [
                    2026,
                    2,
                    16,
                    14,
                    54,
                    0,
                    0,
                    47,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph"
                },
                "authors": [
                    {
                        "name": "Maria Rodriguez Saenz de Tejada"
                    },
                    {
                        "name": "Alvaro Jimenez"
                    },
                    {
                        "name": "Rodrigo Rojo"
                    },
                    {
                        "name": "Sergio Contador"
                    },
                    {
                        "name": "Juan Aguirre"
                    }
                ],
                "author_detail": {
                    "name": "Juan Aguirre"
                },
                "author": "Juan Aguirre"
            },
            {
                "id": "http://arxiv.org/abs/2602.15689v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.15689v2",
                "title": "A Content-Based Framework for Cybersecurity Refusal Decisions in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Content-Based Framework for Cybersecurity Refusal Decisions in Large Language Models"
                },
                "updated": "2026-02-18T16:42:07Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    16,
                    42,
                    7,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.15689v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.15689v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models and LLM-based agents are increasingly used for cybersecurity tasks that are inherently dual-use. Existing approaches to refusal, spanning academic policy frameworks and commercially deployed systems, often rely on broad topic-based bans or offensive-focused taxonomies. As a result, they can yield inconsistent decisions, over-restrict legitimate defenders, and behave brittlely under obfuscation or request segmentation. We argue that effective refusal requires explicitly modeling the trade-off between offensive risk and defensive benefit, rather than relying solely on intent or offensive classification. In this paper, we introduce a content-based framework for designing and auditing cyber refusal policies that makes offense-defense tradeoffs explicit. The framework characterizes requests along five dimensions: Offensive Action Contribution, Offensive Risk, Technical Complexity, Defensive Benefit, and Expected Frequency for Legitimate Users, grounded in the technical substance of the request rather than stated intent. We demonstrate that this content-grounded approach resolves inconsistencies in current frontier model behavior and allows organizations to construct tunable, risk-aware refusal policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models and LLM-based agents are increasingly used for cybersecurity tasks that are inherently dual-use. Existing approaches to refusal, spanning academic policy frameworks and commercially deployed systems, often rely on broad topic-based bans or offensive-focused taxonomies. As a result, they can yield inconsistent decisions, over-restrict legitimate defenders, and behave brittlely under obfuscation or request segmentation. We argue that effective refusal requires explicitly modeling the trade-off between offensive risk and defensive benefit, rather than relying solely on intent or offensive classification. In this paper, we introduce a content-based framework for designing and auditing cyber refusal policies that makes offense-defense tradeoffs explicit. The framework characterizes requests along five dimensions: Offensive Action Contribution, Offensive Risk, Technical Complexity, Defensive Benefit, and Expected Frequency for Legitimate Users, grounded in the technical substance of the request rather than stated intent. We demonstrate that this content-grounded approach resolves inconsistencies in current frontier model behavior and allows organizations to construct tunable, risk-aware refusal policies."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-17T16:12:21Z",
                "published_parsed": [
                    2026,
                    2,
                    17,
                    16,
                    12,
                    21,
                    1,
                    48,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Noa Linder"
                    },
                    {
                        "name": "Meirav Segal"
                    },
                    {
                        "name": "Omer Antverg"
                    },
                    {
                        "name": "Gil Gekker"
                    },
                    {
                        "name": "Tomer Fichman"
                    },
                    {
                        "name": "Omri Bodenheimer"
                    },
                    {
                        "name": "Edan Maor"
                    },
                    {
                        "name": "Omer Nevo"
                    }
                ],
                "author_detail": {
                    "name": "Omer Nevo"
                },
                "author": "Omer Nevo"
            },
            {
                "id": "http://arxiv.org/abs/2503.12286v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2503.12286v2",
                "title": "Integrating Chain-of-Thought and Retrieval Augmented Generation Enhances Rare Disease Diagnosis from Clinical Notes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Chain-of-Thought and Retrieval Augmented Generation Enhances Rare Disease Diagnosis from Clinical Notes"
                },
                "updated": "2026-02-18T16:38:37Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    16,
                    38,
                    37,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2503.12286v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2503.12286v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Background: Several studies show that large language models (LLMs) struggle with phenotype-driven gene prioritization for rare diseases. These studies typically use Human Phenotype Ontology (HPO) terms to prompt foundation models like GPT and LLaMA to predict candidate genes. However, in real-world settings, foundation models are not optimized for domain-specific tasks like clinical diagnosis, yet inputs are unstructured clinical notes rather than standardized terms. How LLMs can be instructed to predict candidate genes or disease diagnosis from unstructured clinical notes remains a major challenge. Methods: We introduce RAG-driven CoT and CoT-driven RAG, two methods that combine Chain-of-Thought (CoT) and Retrieval Augmented Generation (RAG) to analyze clinical notes. A five-question CoT protocol mimics expert reasoning, while RAG retrieves data from sources like HPO and OMIM (Online Mendelian Inheritance in Man). We evaluated these approaches on rare disease datasets, including 5,980 Phenopacket-derived notes, 255 literature-based narratives, and 220 in-house clinical notes from Childrens Hospital of Philadelphia. Results: We found that recent foundations models, including Llama 3.3-70B-Instruct and DeepSeek-R1-Distill-Llama-70B, outperformed earlier versions such as Llama 2 and GPT-3.5. We also showed that RAG-driven CoT and CoT-driven RAG both outperform foundation models in candidate gene prioritization from clinical notes; in particular, both methods with DeepSeek backbone resulted in a top-10 gene accuracy of over 40% on Phenopacket-derived clinical notes. RAG-driven CoT works better for high-quality notes, where early retrieval can anchor the subsequent reasoning steps in domain-specific evidence, while CoT-driven RAG has advantage when processing lengthy and noisy notes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Several studies show that large language models (LLMs) struggle with phenotype-driven gene prioritization for rare diseases. These studies typically use Human Phenotype Ontology (HPO) terms to prompt foundation models like GPT and LLaMA to predict candidate genes. However, in real-world settings, foundation models are not optimized for domain-specific tasks like clinical diagnosis, yet inputs are unstructured clinical notes rather than standardized terms. How LLMs can be instructed to predict candidate genes or disease diagnosis from unstructured clinical notes remains a major challenge. Methods: We introduce RAG-driven CoT and CoT-driven RAG, two methods that combine Chain-of-Thought (CoT) and Retrieval Augmented Generation (RAG) to analyze clinical notes. A five-question CoT protocol mimics expert reasoning, while RAG retrieves data from sources like HPO and OMIM (Online Mendelian Inheritance in Man). We evaluated these approaches on rare disease datasets, including 5,980 Phenopacket-derived notes, 255 literature-based narratives, and 220 in-house clinical notes from Childrens Hospital of Philadelphia. Results: We found that recent foundations models, including Llama 3.3-70B-Instruct and DeepSeek-R1-Distill-Llama-70B, outperformed earlier versions such as Llama 2 and GPT-3.5. We also showed that RAG-driven CoT and CoT-driven RAG both outperform foundation models in candidate gene prioritization from clinical notes; in particular, both methods with DeepSeek backbone resulted in a top-10 gene accuracy of over 40% on Phenopacket-derived clinical notes. RAG-driven CoT works better for high-quality notes, where early retrieval can anchor the subsequent reasoning steps in domain-specific evidence, while CoT-driven RAG has advantage when processing lengthy and noisy notes."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-03-15T22:57:31Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    22,
                    57,
                    31,
                    5,
                    74,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Zhanliang Wang"
                    },
                    {
                        "name": "Da Wu"
                    },
                    {
                        "name": "Quan Nguyen"
                    },
                    {
                        "name": "Kai Wang"
                    }
                ],
                "author_detail": {
                    "name": "Kai Wang"
                },
                "author": "Kai Wang"
            },
            {
                "id": "http://arxiv.org/abs/2602.16571v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16571v1",
                "title": "Utility-Preserving De-Identification for Math Tutoring: Investigating Numeric Ambiguity in the MathEd-PII Benchmark Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Utility-Preserving De-Identification for Math Tutoring: Investigating Numeric Ambiguity in the MathEd-PII Benchmark Dataset"
                },
                "updated": "2026-02-18T16:12:46Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    16,
                    12,
                    46,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16571v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16571v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large-scale sharing of dialogue-based data is instrumental for advancing the science of teaching and learning, yet rigorous de-identification remains a major barrier. In mathematics tutoring transcripts, numeric expressions frequently resemble structured identifiers (e.g., dates or IDs), leading generic Personally Identifiable Information (PII) detection systems to over-redact core instructional content and reduce dataset utility. This work asks how PII can be detected in math tutoring transcripts while preserving their educational utility. To address this challenge, we investigate the \"numeric ambiguity\" problem and introduce MathEd-PII, the first benchmark dataset for PII detection in math tutoring dialogues, created through a human-in-the-loop LLM workflow that audits upstream redactions and generates privacy-preserving surrogates. The dataset contains 1,000 tutoring sessions (115,620 messages; 769,628 tokens) with validated PII annotations. Using a density-based segmentation method, we show that false PII redactions are disproportionately concentrated in math-dense regions, confirming numeric ambiguity as a key failure mode. We then compare four detection strategies: a Presidio baseline and LLM-based approaches with basic, math-aware, and segment-aware prompting. Math-aware prompting substantially improves performance over the baseline (F1: 0.821 vs. 0.379) while reducing numeric false positives, demonstrating that de-identification must incorporate domain context to preserve analytic utility. This work provides both a new benchmark and evidence that utility-preserving de-identification for tutoring data requires domain-aware modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale sharing of dialogue-based data is instrumental for advancing the science of teaching and learning, yet rigorous de-identification remains a major barrier. In mathematics tutoring transcripts, numeric expressions frequently resemble structured identifiers (e.g., dates or IDs), leading generic Personally Identifiable Information (PII) detection systems to over-redact core instructional content and reduce dataset utility. This work asks how PII can be detected in math tutoring transcripts while preserving their educational utility. To address this challenge, we investigate the \"numeric ambiguity\" problem and introduce MathEd-PII, the first benchmark dataset for PII detection in math tutoring dialogues, created through a human-in-the-loop LLM workflow that audits upstream redactions and generates privacy-preserving surrogates. The dataset contains 1,000 tutoring sessions (115,620 messages; 769,628 tokens) with validated PII annotations. Using a density-based segmentation method, we show that false PII redactions are disproportionately concentrated in math-dense regions, confirming numeric ambiguity as a key failure mode. We then compare four detection strategies: a Presidio baseline and LLM-based approaches with basic, math-aware, and segment-aware prompting. Math-aware prompting substantially improves performance over the baseline (F1: 0.821 vs. 0.379) while reducing numeric false positives, demonstrating that de-identification must incorporate domain context to preserve analytic utility. This work provides both a new benchmark and evidence that utility-preserving de-identification for tutoring data requires domain-aware modeling."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T16:12:46Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    16,
                    12,
                    46,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Zhuqian Zhou"
                    },
                    {
                        "name": "Kirk Vanacore"
                    },
                    {
                        "name": "Bakhtawar Ahtisham"
                    },
                    {
                        "name": "Jinsook Lee"
                    },
                    {
                        "name": "Doug Pietrzak"
                    },
                    {
                        "name": "Daryl Hedley"
                    },
                    {
                        "name": "Jorge Dias"
                    },
                    {
                        "name": "Chris Shaw"
                    },
                    {
                        "name": "Ruth Schäfer"
                    },
                    {
                        "name": "René F. Kizilcec"
                    }
                ],
                "author_detail": {
                    "name": "René F. Kizilcec"
                },
                "author": "René F. Kizilcec"
            },
            {
                "id": "http://arxiv.org/abs/2601.07611v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.07611v2",
                "title": "DIAGPaper: Diagnosing Valid and Specific Weaknesses in Scientific Papers via Multi-Agent Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DIAGPaper: Diagnosing Valid and Specific Weaknesses in Scientific Papers via Multi-Agent Reasoning"
                },
                "updated": "2026-02-18T16:09:49Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    16,
                    9,
                    49,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.07611v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.07611v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Paper weakness identification using single-agent or multi-agent LLMs has attracted increasing attention, yet existing approaches exhibit key limitations. Many multi-agent systems simulate human roles at a surface level, missing the underlying criteria that lead experts to assess complementary intellectual aspects of a paper. Moreover, prior methods implicitly assume identified weaknesses are valid, ignoring reviewer bias, misunderstanding, and the critical role of author rebuttals in validating review quality. Finally, most systems output unranked weakness lists, rather than prioritizing the most consequential issues for users. In this work, we propose DIAGPaper, a novel multi-agent framework that addresses these challenges through three tightly integrated modules. The customizer module simulates human-defined review criteria and instantiates multiple reviewer agents with criterion-specific expertise. The rebuttal module introduces author agents that engage in structured debate with reviewer agents to validate and refine proposed weaknesses. The prioritizer module learns from large-scale human review practices to assess the severity of validated weaknesses and surfaces the top-K severest ones to users. Experiments on two benchmarks, AAAR and ReviewCritique, demonstrate that DIAGPaper substantially outperforms existing methods by producing more valid and more paper-specific weaknesses, while presenting them in a user-oriented, prioritized manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Paper weakness identification using single-agent or multi-agent LLMs has attracted increasing attention, yet existing approaches exhibit key limitations. Many multi-agent systems simulate human roles at a surface level, missing the underlying criteria that lead experts to assess complementary intellectual aspects of a paper. Moreover, prior methods implicitly assume identified weaknesses are valid, ignoring reviewer bias, misunderstanding, and the critical role of author rebuttals in validating review quality. Finally, most systems output unranked weakness lists, rather than prioritizing the most consequential issues for users. In this work, we propose DIAGPaper, a novel multi-agent framework that addresses these challenges through three tightly integrated modules. The customizer module simulates human-defined review criteria and instantiates multiple reviewer agents with criterion-specific expertise. The rebuttal module introduces author agents that engage in structured debate with reviewer agents to validate and refine proposed weaknesses. The prioritizer module learns from large-scale human review practices to assess the severity of validated weaknesses and surfaces the top-K severest ones to users. Experiments on two benchmarks, AAAR and ReviewCritique, demonstrate that DIAGPaper substantially outperforms existing methods by producing more valid and more paper-specific weaknesses, while presenting them in a user-oriented, prioritized manner."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-12T14:59:00Z",
                "published_parsed": [
                    2026,
                    1,
                    12,
                    14,
                    59,
                    0,
                    0,
                    12,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Zhuoyang Zou"
                    },
                    {
                        "name": "Abolfazl Ansari"
                    },
                    {
                        "name": "Delvin Ce Zhang"
                    },
                    {
                        "name": "Dongwon Lee"
                    },
                    {
                        "name": "Wenpeng Yin"
                    }
                ],
                "author_detail": {
                    "name": "Wenpeng Yin"
                },
                "author": "Wenpeng Yin"
            },
            {
                "id": "http://arxiv.org/abs/2510.23350v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.23350v2",
                "title": "Validating Formal Specifications with LLM-generated Test Cases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Validating Formal Specifications with LLM-generated Test Cases"
                },
                "updated": "2026-02-18T16:04:20Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    16,
                    4,
                    20,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.23350v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.23350v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Validation is a central activity when developing formal specifications. Similarly to coding, a possible validation technique is to define upfront test cases or scenarios that a future specification should satisfy or not. Unfortunately, specifying such test cases is burdensome and error prone, which could cause users to skip this validation task. This paper reports the results of an empirical evaluation of using pre-trained large language models (LLMs) to automate the generation of test cases from natural language requirements. In particular, we focus on test cases for structural requirements of simple domain models formalized in the Alloy specification language. Our evaluation focuses on the state-of-the-art GPT-5 model, but results from other closed- and open-source LLMs are also reported. The results show that, in this context, GPT-5 is already quite effective at generating positive (and negative) test cases that are syntactically correct and that satisfy (or not) the given requirement, and that can detect many wrong specifications written by humans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Validation is a central activity when developing formal specifications. Similarly to coding, a possible validation technique is to define upfront test cases or scenarios that a future specification should satisfy or not. Unfortunately, specifying such test cases is burdensome and error prone, which could cause users to skip this validation task. This paper reports the results of an empirical evaluation of using pre-trained large language models (LLMs) to automate the generation of test cases from natural language requirements. In particular, we focus on test cases for structural requirements of simple domain models formalized in the Alloy specification language. Our evaluation focuses on the state-of-the-art GPT-5 model, but results from other closed- and open-source LLMs are also reported. The results show that, in this context, GPT-5 is already quite effective at generating positive (and negative) test cases that are syntactically correct and that satisfy (or not) the given requirement, and that can detect many wrong specifications written by humans."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-27T14:02:20Z",
                "published_parsed": [
                    2025,
                    10,
                    27,
                    14,
                    2,
                    20,
                    0,
                    300,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Alcino Cunha"
                    },
                    {
                        "name": "Nuno Macedo"
                    }
                ],
                "author_detail": {
                    "name": "Nuno Macedo"
                },
                "author": "Nuno Macedo"
            },
            {
                "id": "http://arxiv.org/abs/2508.12907v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.12907v4",
                "title": "SNAP-UQ: Self-supervised Next-Activation Prediction for Single-Pass Uncertainty in TinyML",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SNAP-UQ: Self-supervised Next-Activation Prediction for Single-Pass Uncertainty in TinyML"
                },
                "updated": "2026-02-18T15:56:15Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    15,
                    56,
                    15,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.12907v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.12907v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Reliable uncertainty estimation is a key missing piece for on-device monitoring in TinyML: microcontrollers must detect failures, distribution shift, or accuracy drops under strict flash/latency budgets, yet common uncertainty approaches (deep ensembles, MC dropout, early exits, temporal buffering) typically require multiple passes, extra branches, or state that is impractical on milliwatt hardware. This paper proposes a novel and practical method, SNAP-UQ, for single-pass, label-free uncertainty estimation based on depth-wise next-activation prediction. SNAP-UQ taps a small set of backbone layers and uses tiny int8 heads to predict the mean and scale of the next activation from a low-rank projection of the previous one; the resulting standardized prediction error forms a depth-wise surprisal signal that is aggregated and mapped through a lightweight monotone calibrator into an actionable uncertainty score. The design introduces no temporal buffers or auxiliary exits and preserves state-free inference, while increasing deployment footprint by only a few tens of kilobytes. Across vision and audio backbones, SNAP-UQ reduces flash and latency relative to early-exit and deep-ensemble baselines (typically $\\sim$40--60% smaller and $\\sim$25--35% faster), with several competing methods at similar accuracy often exceeding MCU memory limits. On corrupted streams, it improves accuracy-drop event detection by multiple AUPRC points and maintains strong failure detection (AUROC $\\approx 0.9$) in a single forward pass. By grounding uncertainty in layer-to-layer dynamics rather than solely in output confidence, SNAP-UQ offers a novel, resource-efficient basis for robust TinyML monitoring. Our code is available at: https://github.com/Ism-ail11/SNAP-UQ",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable uncertainty estimation is a key missing piece for on-device monitoring in TinyML: microcontrollers must detect failures, distribution shift, or accuracy drops under strict flash/latency budgets, yet common uncertainty approaches (deep ensembles, MC dropout, early exits, temporal buffering) typically require multiple passes, extra branches, or state that is impractical on milliwatt hardware. This paper proposes a novel and practical method, SNAP-UQ, for single-pass, label-free uncertainty estimation based on depth-wise next-activation prediction. SNAP-UQ taps a small set of backbone layers and uses tiny int8 heads to predict the mean and scale of the next activation from a low-rank projection of the previous one; the resulting standardized prediction error forms a depth-wise surprisal signal that is aggregated and mapped through a lightweight monotone calibrator into an actionable uncertainty score. The design introduces no temporal buffers or auxiliary exits and preserves state-free inference, while increasing deployment footprint by only a few tens of kilobytes. Across vision and audio backbones, SNAP-UQ reduces flash and latency relative to early-exit and deep-ensemble baselines (typically $\\sim$40--60% smaller and $\\sim$25--35% faster), with several competing methods at similar accuracy often exceeding MCU memory limits. On corrupted streams, it improves accuracy-drop event detection by multiple AUPRC points and maintains strong failure detection (AUROC $\\approx 0.9$) in a single forward pass. By grounding uncertainty in layer-to-layer dynamics rather than solely in output confidence, SNAP-UQ offers a novel, resource-efficient basis for robust TinyML monitoring. Our code is available at: https://github.com/Ism-ail11/SNAP-UQ"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-18T13:14:20Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    13,
                    14,
                    20,
                    0,
                    230,
                    0
                ],
                "arxiv_comment": "Published as a conference paper at ICLR 2026",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Ismail Lamaakal"
                    },
                    {
                        "name": "Chaymae Yahyati"
                    },
                    {
                        "name": "Khalid El Makkaoui"
                    },
                    {
                        "name": "Ibrahim Ouahbi"
                    },
                    {
                        "name": "Yassine Maleh"
                    }
                ],
                "author_detail": {
                    "name": "Yassine Maleh"
                },
                "author": "Yassine Maleh"
            },
            {
                "id": "http://arxiv.org/abs/2602.16553v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16553v1",
                "title": "Agentic AI, Medical Morality, and the Transformation of the Patient-Physician Relationship",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic AI, Medical Morality, and the Transformation of the Patient-Physician Relationship"
                },
                "updated": "2026-02-18T15:54:17Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    15,
                    54,
                    17,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16553v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16553v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The emergence of agentic AI marks a new phase in the digital transformation of healthcare. Distinct from conventional generative AI, agentic AI systems are capable of autonomous, goal-directed actions and complex task coordination. They promise to support or even collaborate with clinicians and patients in increasingly independent ways. While agentic AI raises familiar moral concerns regarding safety, accountability, and bias, this article focuses on a less explored dimension: its capacity to transform the moral fabric of healthcare itself. Drawing on the framework of techno-moral change and the three domains of decision, relation and perception, we investigate how agentic AI might reshape the patient-physician relationship and reconfigure core concepts of medical morality. We argue that these shifts, while not fully predictable, demand ethical attention before widespread deployment. Ultimately, the paper calls for integrating ethical foresight into the design and use of agentic AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of agentic AI marks a new phase in the digital transformation of healthcare. Distinct from conventional generative AI, agentic AI systems are capable of autonomous, goal-directed actions and complex task coordination. They promise to support or even collaborate with clinicians and patients in increasingly independent ways. While agentic AI raises familiar moral concerns regarding safety, accountability, and bias, this article focuses on a less explored dimension: its capacity to transform the moral fabric of healthcare itself. Drawing on the framework of techno-moral change and the three domains of decision, relation and perception, we investigate how agentic AI might reshape the patient-physician relationship and reconfigure core concepts of medical morality. We argue that these shifts, while not fully predictable, demand ethical attention before widespread deployment. Ultimately, the paper calls for integrating ethical foresight into the design and use of agentic AI."
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T15:54:17Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    15,
                    54,
                    17,
                    2,
                    49,
                    0
                ],
                "arxiv_comment": "25 pages",
                "arxiv_primary_category": {
                    "term": "cs.CY"
                },
                "authors": [
                    {
                        "name": "Robert Ranisch"
                    },
                    {
                        "name": "Sabine Salloch"
                    }
                ],
                "author_detail": {
                    "name": "Sabine Salloch"
                },
                "author": "Sabine Salloch"
            },
            {
                "id": "http://arxiv.org/abs/2602.16551v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16551v1",
                "title": "Automated Extraction of Mechanical Constitutive Models from Scientific Literature using Large Language Models: Applications in Cultural Heritage Conservation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Extraction of Mechanical Constitutive Models from Scientific Literature using Large Language Models: Applications in Cultural Heritage Conservation"
                },
                "updated": "2026-02-18T15:53:15Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    15,
                    53,
                    15,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16551v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16551v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The preservation of cultural heritage is increasingly transitioning towards data-driven predictive maintenance and \"Digital Twin\" construction. However, the mechanical constitutive models required for high-fidelity simulations remain fragmented across decades of unstructured scientific literature, creating a \"Data Silo\" that hinders conservation engineering. To address this, we present an automated, two-stage agentic framework leveraging Large Language Models (LLMs) to extract mechanical constitutive equations, calibrated parameters, and metadata from PDF documents. The workflow employs a resource-efficient \"Gatekeeper\" agent for relevance filtering and a high-capability \"Analyst\" agent for fine-grained extraction, featuring a novel Context-Aware Symbolic Grounding mechanism to resolve mathematical ambiguities. Applied to a corpus of over 2,000 research papers, the system successfully isolated 113 core documents and constructed a structured database containing 185 constitutive model instances and over 450 calibrated parameters. The extraction precision reached 80.4\\%, establishing a highly efficient \"Human-in-the-loop\" workflow that reduces manual data curation time by approximately 90\\%. We demonstrate the system's utility through a web-based Knowledge Retrieval Platform, which enables rapid parameter discovery for computational modeling. This work transforms scattered literature into a queryable digital asset, laying the data foundation for the \"Digital Material Twin\" of built heritage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The preservation of cultural heritage is increasingly transitioning towards data-driven predictive maintenance and \"Digital Twin\" construction. However, the mechanical constitutive models required for high-fidelity simulations remain fragmented across decades of unstructured scientific literature, creating a \"Data Silo\" that hinders conservation engineering. To address this, we present an automated, two-stage agentic framework leveraging Large Language Models (LLMs) to extract mechanical constitutive equations, calibrated parameters, and metadata from PDF documents. The workflow employs a resource-efficient \"Gatekeeper\" agent for relevance filtering and a high-capability \"Analyst\" agent for fine-grained extraction, featuring a novel Context-Aware Symbolic Grounding mechanism to resolve mathematical ambiguities. Applied to a corpus of over 2,000 research papers, the system successfully isolated 113 core documents and constructed a structured database containing 185 constitutive model instances and over 450 calibrated parameters. The extraction precision reached 80.4\\%, establishing a highly efficient \"Human-in-the-loop\" workflow that reduces manual data curation time by approximately 90\\%. We demonstrate the system's utility through a web-based Knowledge Retrieval Platform, which enables rapid parameter discovery for computational modeling. This work transforms scattered literature into a queryable digital asset, laying the data foundation for the \"Digital Material Twin\" of built heritage."
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T15:53:15Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    15,
                    53,
                    15,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB"
                },
                "authors": [
                    {
                        "name": "Rui Hu"
                    },
                    {
                        "name": "Yue Wu"
                    },
                    {
                        "name": "Tianhao Su"
                    },
                    {
                        "name": "Yin Wang"
                    },
                    {
                        "name": "Shunbo Hu"
                    },
                    {
                        "name": "Jizhong Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jizhong Huang"
                },
                "author": "Jizhong Huang"
            },
            {
                "id": "http://arxiv.org/abs/2504.08603v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2504.08603v3",
                "title": "FindAnything: Open-Vocabulary and Object-Centric Mapping for Robot Exploration in Any Environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FindAnything: Open-Vocabulary and Object-Centric Mapping for Robot Exploration in Any Environment"
                },
                "updated": "2026-02-18T15:52:04Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    15,
                    52,
                    4,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2504.08603v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2504.08603v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Geometrically accurate and semantically expressive map representations have proven invaluable for robot deployment and task planning in unknown environments. Nevertheless, real-time, open-vocabulary semantic understanding of large-scale unknown environments still presents open challenges, mainly due to computational requirements. In this paper we present FindAnything, an open-world mapping framework that incorporates vision-language information into dense volumetric submaps. Thanks to the use of vision-language features, FindAnything combines pure geometric and open-vocabulary semantic information for a higher level of understanding. It proposes an efficient storage of open-vocabulary information through the aggregation of features at the object level. Pixelwise vision-language features are aggregated based on eSAM segments, which are in turn integrated into object-centric volumetric submaps, providing a mapping from open-vocabulary queries to 3D geometry that is scalable also in terms of memory usage. We demonstrate that FindAnything performs on par with the state-of-the-art in terms of semantic accuracy while being substantially faster and more memory-efficient, allowing its deployment in large-scale environments and on resourceconstrained devices, such as MAVs. We show that the real-time capabilities of FindAnything make it useful for downstream tasks, such as autonomous MAV exploration in a simulated Search and Rescue scenario. Project Page: https://ethz-mrl.github.io/findanything/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geometrically accurate and semantically expressive map representations have proven invaluable for robot deployment and task planning in unknown environments. Nevertheless, real-time, open-vocabulary semantic understanding of large-scale unknown environments still presents open challenges, mainly due to computational requirements. In this paper we present FindAnything, an open-world mapping framework that incorporates vision-language information into dense volumetric submaps. Thanks to the use of vision-language features, FindAnything combines pure geometric and open-vocabulary semantic information for a higher level of understanding. It proposes an efficient storage of open-vocabulary information through the aggregation of features at the object level. Pixelwise vision-language features are aggregated based on eSAM segments, which are in turn integrated into object-centric volumetric submaps, providing a mapping from open-vocabulary queries to 3D geometry that is scalable also in terms of memory usage. We demonstrate that FindAnything performs on par with the state-of-the-art in terms of semantic accuracy while being substantially faster and more memory-efficient, allowing its deployment in large-scale environments and on resourceconstrained devices, such as MAVs. We show that the real-time capabilities of FindAnything make it useful for downstream tasks, such as autonomous MAV exploration in a simulated Search and Rescue scenario. Project Page: https://ethz-mrl.github.io/findanything/."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-04-11T15:12:05Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    15,
                    12,
                    5,
                    4,
                    101,
                    0
                ],
                "arxiv_comment": "11 pages, 5 figures",
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Sebastián Barbas Laina"
                    },
                    {
                        "name": "Simon Boche"
                    },
                    {
                        "name": "Sotiris Papatheodorou"
                    },
                    {
                        "name": "Simon Schaefer"
                    },
                    {
                        "name": "Jaehyung Jung"
                    },
                    {
                        "name": "Stefan Leutenegger"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Leutenegger"
                },
                "author": "Stefan Leutenegger"
            },
            {
                "id": "http://arxiv.org/abs/2509.22237v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.22237v2",
                "title": "FeatBench: Towards More Realistic Evaluation of Feature-level Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FeatBench: Towards More Realistic Evaluation of Feature-level Code Generation"
                },
                "updated": "2026-02-18T15:49:02Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    15,
                    49,
                    2,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.22237v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.22237v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Evaluating Large Language Models (LLMs) on repository-level feature implementation is a critical frontier in software engineering. However, establishing a benchmark that faithfully mirrors realistic development scenarios remains a significant challenge. Existing feature-level benchmarks generally suffer from two primary limitations: unrealistic task inputs enriched with code hints and significant data leakage risks due to their static nature. To address these limitations, we propose a new benchmark - FeatBench, which introduces the following advances: (1) Realistic Task Inputs. Task inputs consist solely of natural language requirements, strictly devoid of code hints (e.g., function signatures). This format mirrors realistic software development by requiring agents to independently bridge the gap between abstract user intent and concrete code changes. (2) Evolving Data. FeatBench employs a fully automated pipeline to construct new benchmark versions from the latest repositories, effectively mitigating data contamination. The initial release comprises 157 tasks sourced from 27 actively maintained repositories. We evaluate two state-of-the-art agent frameworks with four leading LLMs on FeatBench. The results reveal that FeatBench poses a significant challenge, with the highest resolved rate reaching only 29.94%. Crucially, our analysis uncovers a prevalent behavioral pattern of aggressive implementation, which leads to \"scope creep\" and widespread regressions where agents break existing features by diverging from the user's explicit intent. We release FeatBench, our automated pipeline, and all experimental results to facilitate further community research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Large Language Models (LLMs) on repository-level feature implementation is a critical frontier in software engineering. However, establishing a benchmark that faithfully mirrors realistic development scenarios remains a significant challenge. Existing feature-level benchmarks generally suffer from two primary limitations: unrealistic task inputs enriched with code hints and significant data leakage risks due to their static nature. To address these limitations, we propose a new benchmark - FeatBench, which introduces the following advances: (1) Realistic Task Inputs. Task inputs consist solely of natural language requirements, strictly devoid of code hints (e.g., function signatures). This format mirrors realistic software development by requiring agents to independently bridge the gap between abstract user intent and concrete code changes. (2) Evolving Data. FeatBench employs a fully automated pipeline to construct new benchmark versions from the latest repositories, effectively mitigating data contamination. The initial release comprises 157 tasks sourced from 27 actively maintained repositories. We evaluate two state-of-the-art agent frameworks with four leading LLMs on FeatBench. The results reveal that FeatBench poses a significant challenge, with the highest resolved rate reaching only 29.94%. Crucially, our analysis uncovers a prevalent behavioral pattern of aggressive implementation, which leads to \"scope creep\" and widespread regressions where agents break existing features by diverging from the user's explicit intent. We release FeatBench, our automated pipeline, and all experimental results to facilitate further community research."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-26T11:47:50Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    11,
                    47,
                    50,
                    4,
                    269,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Haorui Chen"
                    },
                    {
                        "name": "Chengze Li"
                    },
                    {
                        "name": "Jia Li"
                    }
                ],
                "author_detail": {
                    "name": "Jia Li"
                },
                "author": "Jia Li"
            },
            {
                "id": "http://arxiv.org/abs/2510.18478v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.18478v2",
                "title": "Safe But Not Sorry: Reducing Over-Conservatism in Safety Critics via Uncertainty-Aware Modulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safe But Not Sorry: Reducing Over-Conservatism in Safety Critics via Uncertainty-Aware Modulation"
                },
                "updated": "2026-02-18T15:47:54Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    15,
                    47,
                    54,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.18478v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.18478v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Ensuring the safe exploration of reinforcement learning (RL) agents is critical for deployment in real-world systems. Yet existing approaches struggle to strike the right balance: methods that tightly enforce safety often cripple task performance, while those that prioritize reward leave safety constraints frequently violated, producing diffuse cost landscapes that flatten gradients and stall policy improvement. We introduce the Uncertain Safety Critic (USC), a novel approach that integrates uncertainty-aware modulation and refinement into critic training. By concentrating conservatism in uncertain and costly regions while preserving sharp gradients in safe areas, USC enables policies to achieve effective reward-safety trade-offs. Extensive experiments show that USC reduces safety violations by approximately 40% while maintaining competitive or higher rewards, and reduces the error between predicted and true cost gradients by approximately 83%, breaking the prevailing trade-off between safety and performance and paving the way for scalable safe RL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring the safe exploration of reinforcement learning (RL) agents is critical for deployment in real-world systems. Yet existing approaches struggle to strike the right balance: methods that tightly enforce safety often cripple task performance, while those that prioritize reward leave safety constraints frequently violated, producing diffuse cost landscapes that flatten gradients and stall policy improvement. We introduce the Uncertain Safety Critic (USC), a novel approach that integrates uncertainty-aware modulation and refinement into critic training. By concentrating conservatism in uncertain and costly regions while preserving sharp gradients in safe areas, USC enables policies to achieve effective reward-safety trade-offs. Extensive experiments show that USC reduces safety violations by approximately 40% while maintaining competitive or higher rewards, and reduces the error between predicted and true cost gradients by approximately 83%, breaking the prevailing trade-off between safety and performance and paving the way for scalable safe RL."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-21T09:57:44Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    9,
                    57,
                    44,
                    1,
                    294,
                    0
                ],
                "arxiv_comment": "Accepted into AAMAS '26",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Daniel Bethell"
                    },
                    {
                        "name": "Simos Gerasimou"
                    },
                    {
                        "name": "Radu Calinescu"
                    },
                    {
                        "name": "Calum Imrie"
                    }
                ],
                "author_detail": {
                    "name": "Calum Imrie"
                },
                "author": "Calum Imrie"
            },
            {
                "id": "http://arxiv.org/abs/2602.16520v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16520v1",
                "title": "Recursive language models for jailbreak detection: a procedural defense for tool-augmented agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recursive language models for jailbreak detection: a procedural defense for tool-augmented agents"
                },
                "updated": "2026-02-18T15:07:09Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    15,
                    7,
                    9,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16520v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16520v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Jailbreak prompts are a practical and evolving threat to large language models (LLMs), particularly in agentic systems that execute tools over untrusted content. Many attacks exploit long-context hiding, semantic camouflage, and lightweight obfuscations that can evade single-pass guardrails. We present RLM-JB, an end-to-end jailbreak detection framework built on Recursive Language Models (RLMs), in which a root model orchestrates a bounded analysis program that transforms the input, queries worker models over covered segments, and aggregates evidence into an auditable decision. RLM-JB treats detection as a procedure rather than a one-shot classification: it normalizes and de-obfuscates suspicious inputs, chunks text to reduce context dilution and guarantee coverage, performs parallel chunk screening, and composes cross-chunk signals to recover split-payload attacks. On AutoDAN-style adversarial inputs, RLM-JB achieves high detection effectiveness across three LLM backends (ASR/Recall 92.5-98.0%) while maintaining very high precision (98.99-100%) and low false positive rates (0.0-2.0%), highlighting a practical sensitivity-specificity trade-off as the screening backend changes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreak prompts are a practical and evolving threat to large language models (LLMs), particularly in agentic systems that execute tools over untrusted content. Many attacks exploit long-context hiding, semantic camouflage, and lightweight obfuscations that can evade single-pass guardrails. We present RLM-JB, an end-to-end jailbreak detection framework built on Recursive Language Models (RLMs), in which a root model orchestrates a bounded analysis program that transforms the input, queries worker models over covered segments, and aggregates evidence into an auditable decision. RLM-JB treats detection as a procedure rather than a one-shot classification: it normalizes and de-obfuscates suspicious inputs, chunks text to reduce context dilution and guarantee coverage, performs parallel chunk screening, and composes cross-chunk signals to recover split-payload attacks. On AutoDAN-style adversarial inputs, RLM-JB achieves high detection effectiveness across three LLM backends (ASR/Recall 92.5-98.0%) while maintaining very high precision (98.99-100%) and low false positive rates (0.0-2.0%), highlighting a practical sensitivity-specificity trade-off as the screening backend changes."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T15:07:09Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    15,
                    7,
                    9,
                    2,
                    49,
                    0
                ],
                "arxiv_comment": "5 pages and 1 figure. Appendix: an additional 5 pages",
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Doron Shavit"
                    }
                ],
                "author_detail": {
                    "name": "Doron Shavit"
                },
                "author": "Doron Shavit"
            },
            {
                "id": "http://arxiv.org/abs/2602.16516v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16516v1",
                "title": "Supercharging Agenda Setting Research: The ParlaCAP Dataset of 28 European Parliaments and a Scalable Multilingual LLM-Based Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supercharging Agenda Setting Research: The ParlaCAP Dataset of 28 European Parliaments and a Scalable Multilingual LLM-Based Classification"
                },
                "updated": "2026-02-18T15:04:30Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    15,
                    4,
                    30,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16516v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16516v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This paper introduces ParlaCAP, a large-scale dataset for analyzing parliamentary agenda setting across Europe, and proposes a cost-effective method for building domain-specific policy topic classifiers. Applying the Comparative Agendas Project (CAP) schema to the multilingual ParlaMint corpus of over 8 million speeches from 28 parliaments of European countries and autonomous regions, we follow a teacher-student framework in which a high-performing large language model (LLM) annotates in-domain training data and a multilingual encoder model is fine-tuned on these annotations for scalable data annotation. We show that this approach produces a classifier tailored to the target domain. Agreement between the LLM and human annotators is comparable to inter-annotator agreement among humans, and the resulting model outperforms existing CAP classifiers trained on manually-annotated but out-of-domain data. In addition to the CAP annotations, the ParlaCAP dataset offers rich speaker and party metadata, as well as sentiment predictions coming from the ParlaSent multilingual transformer model, enabling comparative research on political attention and representation across countries. We illustrate the analytical potential of the dataset with three use cases, examining the distribution of parliamentary attention across policy topics, sentiment patterns in parliamentary speech, and gender differences in policy attention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces ParlaCAP, a large-scale dataset for analyzing parliamentary agenda setting across Europe, and proposes a cost-effective method for building domain-specific policy topic classifiers. Applying the Comparative Agendas Project (CAP) schema to the multilingual ParlaMint corpus of over 8 million speeches from 28 parliaments of European countries and autonomous regions, we follow a teacher-student framework in which a high-performing large language model (LLM) annotates in-domain training data and a multilingual encoder model is fine-tuned on these annotations for scalable data annotation. We show that this approach produces a classifier tailored to the target domain. Agreement between the LLM and human annotators is comparable to inter-annotator agreement among humans, and the resulting model outperforms existing CAP classifiers trained on manually-annotated but out-of-domain data. In addition to the CAP annotations, the ParlaCAP dataset offers rich speaker and party metadata, as well as sentiment predictions coming from the ParlaSent multilingual transformer model, enabling comparative research on political attention and representation across countries. We illustrate the analytical potential of the dataset with three use cases, examining the distribution of parliamentary attention across policy topics, sentiment patterns in parliamentary speech, and gender differences in policy attention."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T15:04:30Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    15,
                    4,
                    30,
                    2,
                    49,
                    0
                ],
                "arxiv_comment": "17 pages, 7 figures, 7 tables. Submitted to the PoliticalNLP 2026 workshop, co-located with LREC 2026 conference",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Taja Kuzman Pungeršek"
                    },
                    {
                        "name": "Peter Rupnik"
                    },
                    {
                        "name": "Daniela Širinić"
                    },
                    {
                        "name": "Nikola Ljubešić"
                    }
                ],
                "author_detail": {
                    "name": "Nikola Ljubešić"
                },
                "author": "Nikola Ljubešić"
            },
            {
                "id": "http://arxiv.org/abs/2512.12850v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.12850v2",
                "title": "KANELÉ: Kolmogorov-Arnold Networks for Efficient LUT-based Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KANELÉ: Kolmogorov-Arnold Networks for Efficient LUT-based Evaluation"
                },
                "updated": "2026-02-18T15:04:02Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    15,
                    4,
                    2,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.12850v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.12850v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1145/3748173.3779202",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Low-latency, resource-efficient neural network inference on FPGAs is essential for applications demanding real-time capability and low power. Lookup table (LUT)-based neural networks are a common solution, combining strong representational power with efficient FPGA implementation. In this work, we introduce KANELÉ, a framework that exploits the unique properties of Kolmogorov-Arnold Networks (KANs) for FPGA deployment. Unlike traditional multilayer perceptrons (MLPs), KANs employ learnable one-dimensional splines with fixed domains as edge activations, a structure naturally suited to discretization and efficient LUT mapping. We present the first systematic design flow for implementing KANs on FPGAs, co-optimizing training with quantization and pruning to enable compact, high-throughput, and low-latency KAN architectures. Our results demonstrate up to a 2700x speedup and orders of magnitude resource savings compared to prior KAN-on-FPGA approaches. Moreover, KANELÉ matches or surpasses other LUT-based architectures on widely used benchmarks, particularly for tasks involving symbolic or physical formulas, while balancing resource usage across FPGA hardware. Finally, we showcase the versatility of the framework by extending it to real-time, power-efficient control systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-latency, resource-efficient neural network inference on FPGAs is essential for applications demanding real-time capability and low power. Lookup table (LUT)-based neural networks are a common solution, combining strong representational power with efficient FPGA implementation. In this work, we introduce KANELÉ, a framework that exploits the unique properties of Kolmogorov-Arnold Networks (KANs) for FPGA deployment. Unlike traditional multilayer perceptrons (MLPs), KANs employ learnable one-dimensional splines with fixed domains as edge activations, a structure naturally suited to discretization and efficient LUT mapping. We present the first systematic design flow for implementing KANs on FPGAs, co-optimizing training with quantization and pruning to enable compact, high-throughput, and low-latency KAN architectures. Our results demonstrate up to a 2700x speedup and orders of magnitude resource savings compared to prior KAN-on-FPGA approaches. Moreover, KANELÉ matches or surpasses other LUT-based architectures on widely used benchmarks, particularly for tasks involving symbolic or physical formulas, while balancing resource usage across FPGA hardware. Finally, we showcase the versatility of the framework by extending it to real-time, power-efficient control systems."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-14T21:29:10Z",
                "published_parsed": [
                    2025,
                    12,
                    14,
                    21,
                    29,
                    10,
                    6,
                    348,
                    0
                ],
                "arxiv_comment": "International Symposium on Field-Programmable Gate Arrays 2026 (ISFPGA'2026)",
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Duc Hoang"
                    },
                    {
                        "name": "Aarush Gupta"
                    },
                    {
                        "name": "Philip Harris"
                    }
                ],
                "author_detail": {
                    "name": "Philip Harris"
                },
                "author": "Philip Harris",
                "arxiv_doi": "10.1145/3748173.3779202"
            },
            {
                "id": "http://arxiv.org/abs/2412.12427v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2412.12427v2",
                "title": "Ultra-wideband Time Difference of Arrival Indoor Localization: From Sensor Placement to System Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultra-wideband Time Difference of Arrival Indoor Localization: From Sensor Placement to System Evaluation"
                },
                "updated": "2026-02-18T14:57:37Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    14,
                    57,
                    37,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2412.12427v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2412.12427v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Wireless indoor localization has attracted significant research interest due to its high accuracy, low cost, lightweight design, and low power consumption. Specifically, ultra-wideband (UWB) time difference of arrival (TDOA)-based localization has emerged as a scalable positioning solution for mobile robots, consumer electronics, and wearable devices, featuring good accuracy and reliability. While UWB TDOA-based localization systems rely on the deployment of UWB radio sensors as positioning landmarks, existing works often assume these placements are predetermined or study the sensor placement problem alone without evaluating it in practical scenarios. In this article, we bridge this gap by approaching the UWB TDOA localization from a system-level perspective, integrating sensor placement as a key component and conducting practical evaluation in real-world scenarios. Through extensive real-world experiments, we demonstrate the accuracy and robustness of our localization system, comparing its performance to the theoretical lower bounds. Using a challenging multi-room environment as a case study, we illustrate the full system construction process, from sensor placement optimization to real-world deployment. Our evaluation, comprising a cumulative total of 39 minutes of real-world experiments involving up to five agents and covering 2608 meters across four distinct scenarios, provides valuable insights and guidelines for constructing UWB TDOA localization systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless indoor localization has attracted significant research interest due to its high accuracy, low cost, lightweight design, and low power consumption. Specifically, ultra-wideband (UWB) time difference of arrival (TDOA)-based localization has emerged as a scalable positioning solution for mobile robots, consumer electronics, and wearable devices, featuring good accuracy and reliability. While UWB TDOA-based localization systems rely on the deployment of UWB radio sensors as positioning landmarks, existing works often assume these placements are predetermined or study the sensor placement problem alone without evaluating it in practical scenarios. In this article, we bridge this gap by approaching the UWB TDOA localization from a system-level perspective, integrating sensor placement as a key component and conducting practical evaluation in real-world scenarios. Through extensive real-world experiments, we demonstrate the accuracy and robustness of our localization system, comparing its performance to the theoretical lower bounds. Using a challenging multi-room environment as a case study, we illustrate the full system construction process, from sensor placement optimization to real-world deployment. Our evaluation, comprising a cumulative total of 39 minutes of real-world experiments involving up to five agents and covering 2608 meters across four distinct scenarios, provides valuable insights and guidelines for constructing UWB TDOA localization systems."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-12-17T00:31:11Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    0,
                    31,
                    11,
                    1,
                    352,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Wenda Zhao"
                    },
                    {
                        "name": "Abhishek Goudar"
                    },
                    {
                        "name": "Mingliang Tang"
                    },
                    {
                        "name": "Angela P. Schoellig"
                    }
                ],
                "author_detail": {
                    "name": "Angela P. Schoellig"
                },
                "author": "Angela P. Schoellig"
            },
            {
                "id": "http://arxiv.org/abs/2602.16511v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16511v1",
                "title": "VIGOR: Visual Goal-In-Context Inference for Unified Humanoid Fall Safety",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VIGOR: Visual Goal-In-Context Inference for Unified Humanoid Fall Safety"
                },
                "updated": "2026-02-18T14:57:33Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    14,
                    57,
                    33,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16511v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16511v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Reliable fall recovery is critical for humanoids operating in cluttered environments. Unlike quadrupeds or wheeled robots, humanoids experience high-energy impacts, complex whole-body contact, and large viewpoint changes during a fall, making recovery essential for continued operation. Existing methods fragment fall safety into separate problems such as fall avoidance, impact mitigation, and stand-up recovery, or rely on end-to-end policies trained without vision through reinforcement learning or imitation learning, often on flat terrain. At a deeper level, fall safety is treated as monolithic data complexity, coupling pose, dynamics, and terrain and requiring exhaustive coverage, limiting scalability and generalization. We present a unified fall safety approach that spans all phases of fall recovery. It builds on two insights: 1) Natural human fall and recovery poses are highly constrained and transferable from flat to complex terrain through alignment, and 2) Fast whole-body reactions require integrated perceptual-motor representations. We train a privileged teacher using sparse human demonstrations on flat terrain and simulated complex terrains, and distill it into a deployable student that relies only on egocentric depth and proprioception. The student learns how to react by matching the teacher's goal-in-context latent representation, which combines the next target pose with the local terrain, rather than separately encoding what it must perceive and how it must act. Results in simulation and on a real Unitree G1 humanoid demonstrate robust, zero-shot fall safety across diverse non-flat environments without real-world fine-tuning. The project page is available at https://vigor2026.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable fall recovery is critical for humanoids operating in cluttered environments. Unlike quadrupeds or wheeled robots, humanoids experience high-energy impacts, complex whole-body contact, and large viewpoint changes during a fall, making recovery essential for continued operation. Existing methods fragment fall safety into separate problems such as fall avoidance, impact mitigation, and stand-up recovery, or rely on end-to-end policies trained without vision through reinforcement learning or imitation learning, often on flat terrain. At a deeper level, fall safety is treated as monolithic data complexity, coupling pose, dynamics, and terrain and requiring exhaustive coverage, limiting scalability and generalization. We present a unified fall safety approach that spans all phases of fall recovery. It builds on two insights: 1) Natural human fall and recovery poses are highly constrained and transferable from flat to complex terrain through alignment, and 2) Fast whole-body reactions require integrated perceptual-motor representations. We train a privileged teacher using sparse human demonstrations on flat terrain and simulated complex terrains, and distill it into a deployable student that relies only on egocentric depth and proprioception. The student learns how to react by matching the teacher's goal-in-context latent representation, which combines the next target pose with the local terrain, rather than separately encoding what it must perceive and how it must act. Results in simulation and on a real Unitree G1 humanoid demonstrate robust, zero-shot fall safety across diverse non-flat environments without real-world fine-tuning. The project page is available at https://vigor2026.github.io/"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T14:57:33Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    14,
                    57,
                    33,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Osher Azulay"
                    },
                    {
                        "name": "Zhengjie Xu"
                    },
                    {
                        "name": "Andrew Scheffer"
                    },
                    {
                        "name": "Stella X. Yu"
                    }
                ],
                "author_detail": {
                    "name": "Stella X. Yu"
                },
                "author": "Stella X. Yu"
            },
            {
                "id": "http://arxiv.org/abs/2602.16500v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16500v1",
                "title": "Optimizing Soft Prompt Tuning via Structural Evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Soft Prompt Tuning via Structural Evolution"
                },
                "updated": "2026-02-18T14:43:20Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    14,
                    43,
                    20,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16500v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16500v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Soft prompt tuning leverages continuous embeddings to capture task-specific information in large pre-trained language models (LLMs), achieving competitive performance in few-shot settings. However, soft prompts rely on high-dimensional, implicit representations and lack explicit semantics and traceable training behaviors, which limits their interpretability. To address this limitation, we propose a soft prompt tuning optimization method based on topological morphological evolution. Specifically, we employ persistent homology from topological data analysis (TDA) to quantify the structural representations of soft prompts in continuous parameter space and their training process evolution. Quantitative analysis shows that topologically stable and compact soft prompts achieve better downstream performance. Based on this empirical observation, we construct a loss function for optimizing soft prompt tuning, termed Topological Soft Prompt Loss (TSLoss). TSLoss guides the model to learn structurally stable adaptations by quantifying inter-parameter connectivity and redundancy. Extensive experiments show that training with TSLoss accelerates convergence and improves tuning performance, providing an interpretable method to understand and optimize soft prompt tuning from structural and topological perspectives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soft prompt tuning leverages continuous embeddings to capture task-specific information in large pre-trained language models (LLMs), achieving competitive performance in few-shot settings. However, soft prompts rely on high-dimensional, implicit representations and lack explicit semantics and traceable training behaviors, which limits their interpretability. To address this limitation, we propose a soft prompt tuning optimization method based on topological morphological evolution. Specifically, we employ persistent homology from topological data analysis (TDA) to quantify the structural representations of soft prompts in continuous parameter space and their training process evolution. Quantitative analysis shows that topologically stable and compact soft prompts achieve better downstream performance. Based on this empirical observation, we construct a loss function for optimizing soft prompt tuning, termed Topological Soft Prompt Loss (TSLoss). TSLoss guides the model to learn structurally stable adaptations by quantifying inter-parameter connectivity and redundancy. Extensive experiments show that training with TSLoss accelerates convergence and improves tuning performance, providing an interpretable method to understand and optimize soft prompt tuning from structural and topological perspectives."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T14:43:20Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    14,
                    43,
                    20,
                    2,
                    49,
                    0
                ],
                "arxiv_comment": "This manuscript has been submitted to IEEE Transactions on Knowledge and Data Engineering (TKDE) for peer review",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Zhenzhen Huang"
                    },
                    {
                        "name": "Chaoning Zhang"
                    },
                    {
                        "name": "Haoyu Bian"
                    },
                    {
                        "name": "Songbo Zhang"
                    },
                    {
                        "name": "Chi-lok Andy Tai"
                    },
                    {
                        "name": "Jiaquan Zhang"
                    },
                    {
                        "name": "Caiyan Qin"
                    },
                    {
                        "name": "Jingjing Qu"
                    },
                    {
                        "name": "Yalan Ye"
                    },
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Heng Tao Shen"
                    }
                ],
                "author_detail": {
                    "name": "Heng Tao Shen"
                },
                "author": "Heng Tao Shen"
            },
            {
                "id": "http://arxiv.org/abs/2602.15001v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.15001v2",
                "title": "Boundary Point Jailbreaking of Black-Box LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boundary Point Jailbreaking of Black-Box LLMs"
                },
                "updated": "2026-02-18T14:35:19Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    14,
                    35,
                    19,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.15001v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.15001v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Frontier LLMs are safeguarded against attempts to extract harmful information via adversarial prompts known as \"jailbreaks\". Recently, defenders have developed classifier-based systems that have survived thousands of hours of human red teaming. We introduce Boundary Point Jailbreaking (BPJ), a new class of automated jailbreak attacks that evade the strongest industry-deployed safeguards. Unlike previous attacks that rely on white/grey-box assumptions (such as classifier scores or gradients) or libraries of existing jailbreaks, BPJ is fully black-box and uses only a single bit of information per query: whether or not the classifier flags the interaction. To achieve this, BPJ addresses the core difficulty in optimising attacks against robust real-world defences: evaluating whether a proposed modification to an attack is an improvement. Instead of directly trying to learn an attack for a target harmful string, BPJ converts the string into a curriculum of intermediate attack targets and then actively selects evaluation points that best detect small changes in attack strength (\"boundary points\"). We believe BPJ is the first fully automated attack algorithm that succeeds in developing universal jailbreaks against Constitutional Classifiers, as well as the first automated attack algorithm that succeeds against GPT-5's input classifier without relying on human attack seeds. BPJ is difficult to defend against in individual interactions but incurs many flags during optimisation, suggesting that effective defence requires supplementing single-interaction methods with batch-level monitoring.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Frontier LLMs are safeguarded against attempts to extract harmful information via adversarial prompts known as \"jailbreaks\". Recently, defenders have developed classifier-based systems that have survived thousands of hours of human red teaming. We introduce Boundary Point Jailbreaking (BPJ), a new class of automated jailbreak attacks that evade the strongest industry-deployed safeguards. Unlike previous attacks that rely on white/grey-box assumptions (such as classifier scores or gradients) or libraries of existing jailbreaks, BPJ is fully black-box and uses only a single bit of information per query: whether or not the classifier flags the interaction. To achieve this, BPJ addresses the core difficulty in optimising attacks against robust real-world defences: evaluating whether a proposed modification to an attack is an improvement. Instead of directly trying to learn an attack for a target harmful string, BPJ converts the string into a curriculum of intermediate attack targets and then actively selects evaluation points that best detect small changes in attack strength (\"boundary points\"). We believe BPJ is the first fully automated attack algorithm that succeeds in developing universal jailbreaks against Constitutional Classifiers, as well as the first automated attack algorithm that succeeds against GPT-5's input classifier without relying on human attack seeds. BPJ is difficult to defend against in individual interactions but incurs many flags during optimisation, suggesting that effective defence requires supplementing single-interaction methods with batch-level monitoring."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-16T18:29:09Z",
                "published_parsed": [
                    2026,
                    2,
                    16,
                    18,
                    29,
                    9,
                    0,
                    47,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Xander Davies"
                    },
                    {
                        "name": "Giorgi Giglemiani"
                    },
                    {
                        "name": "Edmund Lau"
                    },
                    {
                        "name": "Eric Winsor"
                    },
                    {
                        "name": "Geoffrey Irving"
                    },
                    {
                        "name": "Yarin Gal"
                    }
                ],
                "author_detail": {
                    "name": "Yarin Gal"
                },
                "author": "Yarin Gal"
            },
            {
                "id": "http://arxiv.org/abs/2602.16490v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16490v1",
                "title": "From Growing to Looping: A Unified View of Iterative Computation in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Growing to Looping: A Unified View of Iterative Computation in LLMs"
                },
                "updated": "2026-02-18T14:25:16Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    14,
                    25,
                    16,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16490v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16490v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Looping, reusing a block of layers across depth, and depth growing, training shallow-to-deep models by duplicating middle layers, have both been linked to stronger reasoning, but their relationship remains unclear. We provide a mechanistic unification: looped and depth-grown models exhibit convergent depth-wise signatures, including increased reliance on late layers and recurring patterns aligned with the looped or grown block. These shared signatures support the view that their gains stem from a common form of iterative computation. Building on this connection, we show that the two techniques are adaptable and composable: applying inference-time looping to the middle blocks of a depth-grown model improves accuracy on some reasoning primitives by up to $2\\times$, despite the model never being trained to loop. Both approaches also adapt better than the baseline when given more in-context examples or additional supervised fine-tuning data. Additionally, depth-grown models achieve the largest reasoning gains when using higher-quality, math-heavy cooldown mixtures, which can be further boosted by adapting a middle block to loop. Overall, our results position depth growth and looping as complementary, practical methods for inducing and scaling iterative computation to improve reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Looping, reusing a block of layers across depth, and depth growing, training shallow-to-deep models by duplicating middle layers, have both been linked to stronger reasoning, but their relationship remains unclear. We provide a mechanistic unification: looped and depth-grown models exhibit convergent depth-wise signatures, including increased reliance on late layers and recurring patterns aligned with the looped or grown block. These shared signatures support the view that their gains stem from a common form of iterative computation. Building on this connection, we show that the two techniques are adaptable and composable: applying inference-time looping to the middle blocks of a depth-grown model improves accuracy on some reasoning primitives by up to $2\\times$, despite the model never being trained to loop. Both approaches also adapt better than the baseline when given more in-context examples or additional supervised fine-tuning data. Additionally, depth-grown models achieve the largest reasoning gains when using higher-quality, math-heavy cooldown mixtures, which can be further boosted by adapting a middle block to loop. Overall, our results position depth growth and looping as complementary, practical methods for inducing and scaling iterative computation to improve reasoning."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T14:25:16Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    14,
                    25,
                    16,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Ferdinand Kapl"
                    },
                    {
                        "name": "Emmanouil Angelis"
                    },
                    {
                        "name": "Kaitlin Maile"
                    },
                    {
                        "name": "Johannes von Oswald"
                    },
                    {
                        "name": "Stefan Bauer"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Bauer"
                },
                "author": "Stefan Bauer"
            },
            {
                "id": "http://arxiv.org/abs/2602.16488v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16488v1",
                "title": "Learning to Learn from Language Feedback with Social Meta-Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Learn from Language Feedback with Social Meta-Learning"
                },
                "updated": "2026-02-18T14:22:13Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    14,
                    22,
                    13,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16488v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) often struggle to learn from corrective feedback within a conversational context. They are rarely proactive in soliciting this feedback, even when faced with ambiguity, which can make their dialogues feel static, one-sided, and lacking the adaptive qualities of human conversation. To address these limitations, we draw inspiration from social meta-learning (SML) in humans - the process of learning how to learn from others. We formulate SML as a finetuning methodology, training LLMs to solicit and learn from language feedback in simulated pedagogical dialogues, where static tasks are converted into interactive social learning problems. SML effectively teaches models to use conversation to solve problems they are unable to solve in a single turn. This capability generalises across domains; SML on math problems produces models that better use feedback to solve coding problems and vice versa. Furthermore, despite being trained only on fully-specified problems, these models are better able to solve underspecified tasks where critical information is revealed over multiple turns. When faced with this ambiguity, SML-trained models make fewer premature answer attempts and are more likely to ask for the information they need. This work presents a scalable approach to developing AI systems that effectively learn from language feedback.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often struggle to learn from corrective feedback within a conversational context. They are rarely proactive in soliciting this feedback, even when faced with ambiguity, which can make their dialogues feel static, one-sided, and lacking the adaptive qualities of human conversation. To address these limitations, we draw inspiration from social meta-learning (SML) in humans - the process of learning how to learn from others. We formulate SML as a finetuning methodology, training LLMs to solicit and learn from language feedback in simulated pedagogical dialogues, where static tasks are converted into interactive social learning problems. SML effectively teaches models to use conversation to solve problems they are unable to solve in a single turn. This capability generalises across domains; SML on math problems produces models that better use feedback to solve coding problems and vice versa. Furthermore, despite being trained only on fully-specified problems, these models are better able to solve underspecified tasks where critical information is revealed over multiple turns. When faced with this ambiguity, SML-trained models make fewer premature answer attempts and are more likely to ask for the information they need. This work presents a scalable approach to developing AI systems that effectively learn from language feedback."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T14:22:13Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    14,
                    22,
                    13,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Jonathan Cook"
                    },
                    {
                        "name": "Diego Antognini"
                    },
                    {
                        "name": "Martin Klissarov"
                    },
                    {
                        "name": "Claudiu Musat"
                    },
                    {
                        "name": "Edward Grefenstette"
                    }
                ],
                "author_detail": {
                    "name": "Edward Grefenstette"
                },
                "author": "Edward Grefenstette"
            },
            {
                "id": "http://arxiv.org/abs/2602.16481v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16481v1",
                "title": "Leveraging Large Language Models for Causal Discovery: a Constraint-based, Argumentation-driven Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models for Causal Discovery: a Constraint-based, Argumentation-driven Approach"
                },
                "updated": "2026-02-18T14:15:21Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    14,
                    15,
                    21,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16481v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16481v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Causal discovery seeks to uncover causal relations from data, typically represented as causal graphs, and is essential for predicting the effects of interventions. While expert knowledge is required to construct principled causal graphs, many statistical methods have been proposed to leverage observational data with varying formal guarantees. Causal Assumption-based Argumentation (ABA) is a framework that uses symbolic reasoning to ensure correspondence between input constraints and output graphs, while offering a principled way to combine data and expertise. We explore the use of large language models (LLMs) as imperfect experts for Causal ABA, eliciting semantic structural priors from variable names and descriptions and integrating them with conditional-independence evidence. Experiments on standard benchmarks and semantically grounded synthetic graphs demonstrate state-of-the-art performance, and we additionally introduce an evaluation protocol to mitigate memorisation bias when assessing LLMs for causal discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal discovery seeks to uncover causal relations from data, typically represented as causal graphs, and is essential for predicting the effects of interventions. While expert knowledge is required to construct principled causal graphs, many statistical methods have been proposed to leverage observational data with varying formal guarantees. Causal Assumption-based Argumentation (ABA) is a framework that uses symbolic reasoning to ensure correspondence between input constraints and output graphs, while offering a principled way to combine data and expertise. We explore the use of large language models (LLMs) as imperfect experts for Causal ABA, eliciting semantic structural priors from variable names and descriptions and integrating them with conditional-independence evidence. Experiments on standard benchmarks and semantically grounded synthetic graphs demonstrate state-of-the-art performance, and we additionally introduce an evaluation protocol to mitigate memorisation bias when assessing LLMs for causal discovery."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T14:15:21Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    14,
                    15,
                    21,
                    2,
                    49,
                    0
                ],
                "arxiv_comment": "26 pages, including appendix",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Zihao Li"
                    },
                    {
                        "name": "Fabrizio Russo"
                    }
                ],
                "author_detail": {
                    "name": "Fabrizio Russo"
                },
                "author": "Fabrizio Russo"
            },
            {
                "id": "http://arxiv.org/abs/2602.15620v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.15620v2",
                "title": "STAPO: Stabilizing Reinforcement Learning for LLMs by Silencing Rare Spurious Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STAPO: Stabilizing Reinforcement Learning for LLMs by Silencing Rare Spurious Tokens"
                },
                "updated": "2026-02-18T14:13:03Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    14,
                    13,
                    3,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.15620v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.15620v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Reinforcement Learning (RL) has significantly improved large language model reasoning, but existing RL fine-tuning methods rely heavily on heuristic techniques such as entropy regularization and reweighting to maintain stability. In practice, they often suffer from late-stage performance collapse, leading to degraded reasoning quality and unstable training. Our analysis shows that the magnitude of token-wise policy gradients in RL is negatively correlated with token probability and local policy entropy. We find that training instability can be caused by a tiny fraction of tokens, approximately 0.01\\%, which we term \\emph{spurious tokens}. When such tokens appear in correct responses, they contribute little to the reasoning outcome but inherit the full sequence-level reward, leading to abnormally amplified gradient updates. To mitigate this instability, we design S2T (silencing spurious tokens) mechanism to efficiently identify spurious tokens through characteristic signals with low probability, low entropy, and positive advantage, and then to suppress their gradient perturbations during optimization. Incorporating this mechanism into a group-based objective, we propose Spurious-Token-Aware Policy Optimization (STAPO), which promotes stable and effective large-scale model refinement. Across six mathematical reasoning benchmarks using Qwen 1.7B, 8B, and 14B base models, STAPO consistently demonstrates superior entropy stability and achieves an average performance improvement of 7.13\\% ($ρ_{\\mathrm{T}}$=1.0, top-p=1.0) and 3.69\\% ($ρ_{\\mathrm{T}}$=0.7, top-p=0.9) over GRPO, 20-Entropy and JustRL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning (RL) has significantly improved large language model reasoning, but existing RL fine-tuning methods rely heavily on heuristic techniques such as entropy regularization and reweighting to maintain stability. In practice, they often suffer from late-stage performance collapse, leading to degraded reasoning quality and unstable training. Our analysis shows that the magnitude of token-wise policy gradients in RL is negatively correlated with token probability and local policy entropy. We find that training instability can be caused by a tiny fraction of tokens, approximately 0.01\\%, which we term \\emph{spurious tokens}. When such tokens appear in correct responses, they contribute little to the reasoning outcome but inherit the full sequence-level reward, leading to abnormally amplified gradient updates. To mitigate this instability, we design S2T (silencing spurious tokens) mechanism to efficiently identify spurious tokens through characteristic signals with low probability, low entropy, and positive advantage, and then to suppress their gradient perturbations during optimization. Incorporating this mechanism into a group-based objective, we propose Spurious-Token-Aware Policy Optimization (STAPO), which promotes stable and effective large-scale model refinement. Across six mathematical reasoning benchmarks using Qwen 1.7B, 8B, and 14B base models, STAPO consistently demonstrates superior entropy stability and achieves an average performance improvement of 7.13\\% ($ρ_{\\mathrm{T}}$=1.0, top-p=1.0) and 3.69\\% ($ρ_{\\mathrm{T}}$=0.7, top-p=0.9) over GRPO, 20-Entropy and JustRL."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-17T14:46:48Z",
                "published_parsed": [
                    2026,
                    2,
                    17,
                    14,
                    46,
                    48,
                    1,
                    48,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Shiqi Liu"
                    },
                    {
                        "name": "Zeyu He"
                    },
                    {
                        "name": "Guojian Zhan"
                    },
                    {
                        "name": "Letian Tao"
                    },
                    {
                        "name": "Zhilong Zheng"
                    },
                    {
                        "name": "Jiang Wu"
                    },
                    {
                        "name": "Yinuo Wang"
                    },
                    {
                        "name": "Yang Guan"
                    },
                    {
                        "name": "Kehua Sheng"
                    },
                    {
                        "name": "Bo Zhang"
                    },
                    {
                        "name": "Keqiang Li"
                    },
                    {
                        "name": "Jingliang Duan"
                    },
                    {
                        "name": "Shengbo Eben Li"
                    }
                ],
                "author_detail": {
                    "name": "Shengbo Eben Li"
                },
                "author": "Shengbo Eben Li"
            },
            {
                "id": "http://arxiv.org/abs/2602.16467v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16467v1",
                "title": "IndicEval: A Bilingual Indian Educational Evaluation Framework for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IndicEval: A Bilingual Indian Educational Evaluation Framework for Large Language Models"
                },
                "updated": "2026-02-18T13:55:57Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    13,
                    55,
                    57,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16467v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The rapid advancement of large language models (LLMs) necessitates evaluation frameworks that reflect real-world academic rigor and multilingual complexity. This paper introduces IndicEval, a scalable benchmarking platform designed to assess LLM performance using authentic high-stakes examination questions from UPSC, JEE, and NEET across STEM and humanities domains in both English and Hindi. Unlike synthetic benchmarks, IndicEval grounds evaluation in real examination standards, enabling realistic measurement of reasoning, domain knowledge, and bilingual adaptability. The framework automates assessment using Zero-Shot, Few-Shot, and Chain-of-Thought (CoT) prompting strategies and supports modular integration of new models and languages. Experiments conducted on Gemini 2.0 Flash, GPT-4, Claude, and LLaMA 3-70B reveal three major findings. First, CoT prompting consistently improves reasoning accuracy, with substantial gains across subjects and languages. Second, significant cross-model performance disparities persist, particularly in high-complexity examinations. Third, multilingual degradation remains a critical challenge, with marked accuracy drops in Hindi compared to English, especially under Zero-Shot conditions. These results highlight persistent gaps in bilingual reasoning and domain transfer. Overall, IndicEval provides a practice-oriented, extensible foundation for rigorous, equitable evaluation of LLMs in multilingual educational settings and offers actionable insights for improving reasoning robustness and language adaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) necessitates evaluation frameworks that reflect real-world academic rigor and multilingual complexity. This paper introduces IndicEval, a scalable benchmarking platform designed to assess LLM performance using authentic high-stakes examination questions from UPSC, JEE, and NEET across STEM and humanities domains in both English and Hindi. Unlike synthetic benchmarks, IndicEval grounds evaluation in real examination standards, enabling realistic measurement of reasoning, domain knowledge, and bilingual adaptability. The framework automates assessment using Zero-Shot, Few-Shot, and Chain-of-Thought (CoT) prompting strategies and supports modular integration of new models and languages. Experiments conducted on Gemini 2.0 Flash, GPT-4, Claude, and LLaMA 3-70B reveal three major findings. First, CoT prompting consistently improves reasoning accuracy, with substantial gains across subjects and languages. Second, significant cross-model performance disparities persist, particularly in high-complexity examinations. Third, multilingual degradation remains a critical challenge, with marked accuracy drops in Hindi compared to English, especially under Zero-Shot conditions. These results highlight persistent gaps in bilingual reasoning and domain transfer. Overall, IndicEval provides a practice-oriented, extensible foundation for rigorous, equitable evaluation of LLMs in multilingual educational settings and offers actionable insights for improving reasoning robustness and language adaptability."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T13:55:57Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    13,
                    55,
                    57,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Saurabh Bharti"
                    },
                    {
                        "name": "Gaurav Azad"
                    },
                    {
                        "name": "Abhinaw Jagtap"
                    },
                    {
                        "name": "Nachiket Tapas"
                    }
                ],
                "author_detail": {
                    "name": "Nachiket Tapas"
                },
                "author": "Nachiket Tapas"
            },
            {
                "id": "http://arxiv.org/abs/2602.06051v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.06051v3",
                "title": "CAST: Character-and-Scene Episodic Memory for Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAST: Character-and-Scene Episodic Memory for Agents"
                },
                "updated": "2026-02-18T13:48:31Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    13,
                    48,
                    31,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.06051v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.06051v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Episodic memory is a central component of human memory, which refers to the ability to recall coherent events grounded in who, when, and where. However, most agent memory systems only emphasize semantic recall and treat experience as structures such as key-value, vector, or graph, which makes them struggle to represent and retrieve coherent events. To address this challenge, we propose a Character-and-Scene based memory architecture(CAST) inspired by dramatic theory. Specifically, CAST constructs 3D scenes (time/place/topic) and organizes them into character profiles that summarize the events of a character to represent episodic memory. Moreover, CAST complements this episodic memory with a graph-based semantic memory, which yields a robust dual memory design. Experiments demonstrate that CAST has averagely improved 8.11% F1 and 10.21% J(LLM-as-a-Judge) than baselines on various datasets, especially on open and time-sensitive conversational questions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Episodic memory is a central component of human memory, which refers to the ability to recall coherent events grounded in who, when, and where. However, most agent memory systems only emphasize semantic recall and treat experience as structures such as key-value, vector, or graph, which makes them struggle to represent and retrieve coherent events. To address this challenge, we propose a Character-and-Scene based memory architecture(CAST) inspired by dramatic theory. Specifically, CAST constructs 3D scenes (time/place/topic) and organizes them into character profiles that summarize the events of a character to represent episodic memory. Moreover, CAST complements this episodic memory with a graph-based semantic memory, which yields a robust dual memory design. Experiments demonstrate that CAST has averagely improved 8.11% F1 and 10.21% J(LLM-as-a-Judge) than baselines on various datasets, especially on open and time-sensitive conversational questions."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-14T05:06:16Z",
                "published_parsed": [
                    2026,
                    1,
                    14,
                    5,
                    6,
                    16,
                    2,
                    14,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Kexin Ma"
                    },
                    {
                        "name": "Bojun Li"
                    },
                    {
                        "name": "Yuhua Tang"
                    },
                    {
                        "name": "Liting Sun"
                    },
                    {
                        "name": "Ruochun Jin"
                    }
                ],
                "author_detail": {
                    "name": "Ruochun Jin"
                },
                "author": "Ruochun Jin"
            },
            {
                "id": "http://arxiv.org/abs/2508.02515v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.02515v2",
                "title": "PoeTone: A Framework for Constrained Generation of Structured Chinese Songci with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PoeTone: A Framework for Constrained Generation of Structured Chinese Songci with LLMs"
                },
                "updated": "2026-02-18T13:33:12Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    13,
                    33,
                    12,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.02515v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.02515v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This paper presents a systematic investigation into the constrained generation capabilities of large language models (LLMs) in producing Songci, a classical Chinese poetry form characterized by strict structural, tonal, and rhyme constraints defined by Cipai templates. We first develop a comprehensive, multi-faceted evaluation framework that includes: (i) a formal conformity score, (ii) automated quality assessment using LLMs, (iii) human evaluation, and (iv) classification-based probing tasks. Using this framework, we evaluate the generative performance of 18 LLMs, including 3 proprietary models and 15 open-source models across 4 families, under five prompting strategies: zero-shot, one-shot, completion-based, instruction-based, and chain-of-thought. Finally, we propose a Generate-Critic architecture in which the evaluation framework functions as an automated critic. Leveraging the critic's feedback as a scoring function for best-of-N selection, we fine-tune 3 lightweight open-source LLMs via supervised fine-tuning (SFT), resulting in improvements of up to 5.88% in formal conformity. Our findings offer new insights into the generative strengths and limitations of LLMs in producing culturally significant and formally constrained literary texts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a systematic investigation into the constrained generation capabilities of large language models (LLMs) in producing Songci, a classical Chinese poetry form characterized by strict structural, tonal, and rhyme constraints defined by Cipai templates. We first develop a comprehensive, multi-faceted evaluation framework that includes: (i) a formal conformity score, (ii) automated quality assessment using LLMs, (iii) human evaluation, and (iv) classification-based probing tasks. Using this framework, we evaluate the generative performance of 18 LLMs, including 3 proprietary models and 15 open-source models across 4 families, under five prompting strategies: zero-shot, one-shot, completion-based, instruction-based, and chain-of-thought. Finally, we propose a Generate-Critic architecture in which the evaluation framework functions as an automated critic. Leveraging the critic's feedback as a scoring function for best-of-N selection, we fine-tune 3 lightweight open-source LLMs via supervised fine-tuning (SFT), resulting in improvements of up to 5.88% in formal conformity. Our findings offer new insights into the generative strengths and limitations of LLMs in producing culturally significant and formally constrained literary texts."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-04T15:19:22Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    19,
                    22,
                    0,
                    216,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Zhan Qu"
                    },
                    {
                        "name": "Shuzhou Yuan"
                    },
                    {
                        "name": "Michael Färber"
                    }
                ],
                "author_detail": {
                    "name": "Michael Färber"
                },
                "author": "Michael Färber"
            },
            {
                "id": "http://arxiv.org/abs/2602.16438v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16438v1",
                "title": "Intra-Fairness Dynamics: The Bias Spillover Effect in Targeted LLM Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intra-Fairness Dynamics: The Bias Spillover Effect in Targeted LLM Alignment"
                },
                "updated": "2026-02-18T13:19:11Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    13,
                    19,
                    11,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16438v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16438v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Conventional large language model (LLM) fairness alignment largely focuses on mitigating bias along single sensitive attributes, overlooking fairness as an inherently multidimensional and context-specific value. This approach risks creating systems that achieve narrow fairness metrics while exacerbating disparities along untargeted attributes, a phenomenon known as bias spillover. While extensively studied in machine learning, bias spillover remains critically underexplored in LLM alignment. In this work, we investigate how targeted gender alignment affects fairness across nine sensitive attributes in three state-of-the-art LLMs (Mistral 7B, Llama 3.1 8B, Qwen 2.5 7B). Using Direct Preference Optimization and the BBQ benchmark, we evaluate fairness under ambiguous and disambiguous contexts. Our findings reveal noticeable bias spillover: while aggregate results show improvements, context-aware analysis exposes significant degradations in ambiguous contexts, particularly for physical appearance ($p< 0.001$ across all models), sexual orientation, and disability status. We demonstrate that improving fairness along one attribute can inadvertently worsen disparities in others under uncertainty, highlighting the necessity of context-aware, multi-attribute fairness evaluation frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional large language model (LLM) fairness alignment largely focuses on mitigating bias along single sensitive attributes, overlooking fairness as an inherently multidimensional and context-specific value. This approach risks creating systems that achieve narrow fairness metrics while exacerbating disparities along untargeted attributes, a phenomenon known as bias spillover. While extensively studied in machine learning, bias spillover remains critically underexplored in LLM alignment. In this work, we investigate how targeted gender alignment affects fairness across nine sensitive attributes in three state-of-the-art LLMs (Mistral 7B, Llama 3.1 8B, Qwen 2.5 7B). Using Direct Preference Optimization and the BBQ benchmark, we evaluate fairness under ambiguous and disambiguous contexts. Our findings reveal noticeable bias spillover: while aggregate results show improvements, context-aware analysis exposes significant degradations in ambiguous contexts, particularly for physical appearance ($p< 0.001$ across all models), sexual orientation, and disability status. We demonstrate that improving fairness along one attribute can inadvertently worsen disparities in others under uncertainty, highlighting the necessity of context-aware, multi-attribute fairness evaluation frameworks."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T13:19:11Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    13,
                    19,
                    11,
                    2,
                    49,
                    0
                ],
                "arxiv_comment": "Submitted to the BiAlign CHI Workshop 2026",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Eva Paraschou"
                    },
                    {
                        "name": "Line Harder Clemmensen"
                    },
                    {
                        "name": "Sneha Das"
                    }
                ],
                "author_detail": {
                    "name": "Sneha Das"
                },
                "author": "Sneha Das"
            },
            {
                "id": "http://arxiv.org/abs/2602.16430v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16430v1",
                "title": "Designing Production-Scale OCR for India: Multilingual and Domain-Specific Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing Production-Scale OCR for India: Multilingual and Domain-Specific Systems"
                },
                "updated": "2026-02-18T13:03:05Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    13,
                    3,
                    5,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16430v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16430v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Designing Optical Character Recognition (OCR) systems for India requires balancing linguistic diversity, document heterogeneity, and deployment constraints. In this paper, we study two training strategies for building multilingual OCR systems with Vision-Language Models through the Chitrapathak series. We first follow a popular multimodal approach, pairing a generic vision encoder with a strong multilingual language model and training the system end-to-end for OCR. Alternatively, we explore fine-tuning an existing OCR model, despite not being trained for the target languages. Through extensive evaluation on multilingual Indic OCR benchmarks and deployment-oriented metrics, we find that the second strategy consistently achieves better accuracy-latency trade-offs. Chitrapathak-2 achieves 3-6x speedup over its predecessor with being state-of-the-art (SOTA) in Telugu (6.69 char ANLS) and second best in the rest. In addition, we present Parichay, an independent OCR model series designed specifically for 9 Indian government documents to extract structured key fields, achieving 89.8% Exact Match score with a faster inference. Together, these systems achieve SOTA performance and provide practical guidance for building production-scale OCR pipelines in the Indian context.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing Optical Character Recognition (OCR) systems for India requires balancing linguistic diversity, document heterogeneity, and deployment constraints. In this paper, we study two training strategies for building multilingual OCR systems with Vision-Language Models through the Chitrapathak series. We first follow a popular multimodal approach, pairing a generic vision encoder with a strong multilingual language model and training the system end-to-end for OCR. Alternatively, we explore fine-tuning an existing OCR model, despite not being trained for the target languages. Through extensive evaluation on multilingual Indic OCR benchmarks and deployment-oriented metrics, we find that the second strategy consistently achieves better accuracy-latency trade-offs. Chitrapathak-2 achieves 3-6x speedup over its predecessor with being state-of-the-art (SOTA) in Telugu (6.69 char ANLS) and second best in the rest. In addition, we present Parichay, an independent OCR model series designed specifically for 9 Indian government documents to extract structured key fields, achieving 89.8% Exact Match score with a faster inference. Together, these systems achieve SOTA performance and provide practical guidance for building production-scale OCR pipelines in the Indian context."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T13:03:05Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    13,
                    3,
                    5,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Ali Faraz"
                    },
                    {
                        "name": "Raja Kolla"
                    },
                    {
                        "name": "Ashish Kulkarni"
                    },
                    {
                        "name": "Shubham Agarwal"
                    }
                ],
                "author_detail": {
                    "name": "Shubham Agarwal"
                },
                "author": "Shubham Agarwal"
            },
            {
                "id": "http://arxiv.org/abs/2509.13764v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.13764v2",
                "title": "Direct loading of a Sr magneto-optical trap from a thermal atomic beam",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct loading of a Sr magneto-optical trap from a thermal atomic beam"
                },
                "updated": "2026-02-18T13:02:00Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    13,
                    2,
                    0,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.13764v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.13764v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We demonstrate direct loading of a strontium (Sr) magneto-optical trap (MOT) from a thermal atomic beam in a single-chamber vacuum system. The MOT operates without a Zeeman slower, a slowing laser, a two-dimensional MOT, or differential pumping, while the entire system is maintained in the ultra-high-vacuum regime by a single ion pump. At an oven temperature of $395\\,\\mathrm{{}^\\circ C}$, the MOT captures up to $10^{7}$ ${}^{88}\\mathrm{Sr}$ atoms with a loading rate of $10^{7}\\,\\mathrm{atoms\\,s^{-1}}$, while sustaining a background gas pressure of $1 \\times 10^{-9} \\,\\mathrm{Torr}$. At this oven temperature, the MOT lifetime limited by collisions with background gas is $\\sim 5 \\,\\mathrm{s}$, with the atom number primarily constrained by light-assisted two-body collisions. Eliminating differential pumping and precooling stages significantly reduces the system's size, weight, and power requirements, providing a robust and practical platform for field-deployable and spaceborne optical lattice clocks, as well as a variety of other applications requiring compact ultracold atom sources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We demonstrate direct loading of a strontium (Sr) magneto-optical trap (MOT) from a thermal atomic beam in a single-chamber vacuum system. The MOT operates without a Zeeman slower, a slowing laser, a two-dimensional MOT, or differential pumping, while the entire system is maintained in the ultra-high-vacuum regime by a single ion pump. At an oven temperature of $395\\,\\mathrm{{}^\\circ C}$, the MOT captures up to $10^{7}$ ${}^{88}\\mathrm{Sr}$ atoms with a loading rate of $10^{7}\\,\\mathrm{atoms\\,s^{-1}}$, while sustaining a background gas pressure of $1 \\times 10^{-9} \\,\\mathrm{Torr}$. At this oven temperature, the MOT lifetime limited by collisions with background gas is $\\sim 5 \\,\\mathrm{s}$, with the atom number primarily constrained by light-assisted two-body collisions. Eliminating differential pumping and precooling stages significantly reduces the system's size, weight, and power requirements, providing a robust and practical platform for field-deployable and spaceborne optical lattice clocks, as well as a variety of other applications requiring compact ultracold atom sources."
                },
                "tags": [
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-17T07:22:46Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    7,
                    22,
                    46,
                    2,
                    260,
                    0
                ],
                "arxiv_comment": "10 pages, 9 figures",
                "arxiv_primary_category": {
                    "term": "physics.atom-ph"
                },
                "authors": [
                    {
                        "name": "Naohiro Okamoto"
                    },
                    {
                        "name": "Takumi Sato"
                    },
                    {
                        "name": "Takatoshi Aoki"
                    },
                    {
                        "name": "Yoshio Torii"
                    }
                ],
                "author_detail": {
                    "name": "Yoshio Torii"
                },
                "author": "Yoshio Torii"
            },
            {
                "id": "http://arxiv.org/abs/2602.16429v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16429v1",
                "title": "TabAgent: A Framework for Replacing Agentic Generative Components with Tabular-Textual Classifiers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TabAgent: A Framework for Replacing Agentic Generative Components with Tabular-Textual Classifiers"
                },
                "updated": "2026-02-18T13:01:17Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    13,
                    1,
                    17,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16429v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16429v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Agentic systems, AI architectures that autonomously execute multi-step workflows to achieve complex goals, are often built using repeated large language model (LLM) calls for closed-set decision tasks such as routing, shortlisting, gating, and verification. While convenient, this design makes deployments slow and expensive due to cumulative latency and token usage. We propose TabAgent, a framework for replacing generative decision components in closed-set selection tasks with a compact textual-tabular classifier trained on execution traces. TabAgent (i) extracts structured schema, state, and dependency features from trajectories (TabSchema), (ii) augments coverage with schema-aligned synthetic supervision (TabSynth), and (iii) scores candidates with a lightweight classifier (TabHead). On the long-horizon AppWorld benchmark, TabAgent maintains task-level success while eliminating shortlist-time LLM calls, reducing latency by approximately 95% and inference cost by 85-91%. Beyond tool shortlisting, TabAgent generalizes to other agentic decision heads, establishing a paradigm for learned discriminative replacements of generative bottlenecks in production agent architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic systems, AI architectures that autonomously execute multi-step workflows to achieve complex goals, are often built using repeated large language model (LLM) calls for closed-set decision tasks such as routing, shortlisting, gating, and verification. While convenient, this design makes deployments slow and expensive due to cumulative latency and token usage. We propose TabAgent, a framework for replacing generative decision components in closed-set selection tasks with a compact textual-tabular classifier trained on execution traces. TabAgent (i) extracts structured schema, state, and dependency features from trajectories (TabSchema), (ii) augments coverage with schema-aligned synthetic supervision (TabSynth), and (iii) scores candidates with a lightweight classifier (TabHead). On the long-horizon AppWorld benchmark, TabAgent maintains task-level success while eliminating shortlist-time LLM calls, reducing latency by approximately 95% and inference cost by 85-91%. Beyond tool shortlisting, TabAgent generalizes to other agentic decision heads, establishing a paradigm for learned discriminative replacements of generative bottlenecks in production agent architectures."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T13:01:17Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    13,
                    1,
                    17,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Ido Levy"
                    },
                    {
                        "name": "Eilam Shapira"
                    },
                    {
                        "name": "Yinon Goldshtein"
                    },
                    {
                        "name": "Avi Yaeli"
                    },
                    {
                        "name": "Nir Mashkif"
                    },
                    {
                        "name": "Segev Shlomov"
                    }
                ],
                "author_detail": {
                    "name": "Segev Shlomov"
                },
                "author": "Segev Shlomov"
            },
            {
                "id": "http://arxiv.org/abs/2510.24869v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.24869v2",
                "title": "Deep Reinforcement Learning Approach to QoSAware Load Balancing in 5G Cellular Networks under User Mobility and Observation Uncertainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Reinforcement Learning Approach to QoSAware Load Balancing in 5G Cellular Networks under User Mobility and Observation Uncertainty"
                },
                "updated": "2026-02-18T12:46:50Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    12,
                    46,
                    50,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.24869v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.24869v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Efficient mobility management and load balancing are critical to sustaining Quality of Service (QoS) in dense, highly dynamic 5G radio access networks. We present a deep reinforcement learning framework based on Proximal Policy Optimization (PPO) for autonomous, QoS-aware load balancing implemented end-to-end in a lightweight, pure-Python simulation environment. The control problem is formulated as a Markov Decision Process in which the agent periodically adjusts Cell Individual Offset (CIO) values to steer user-cell associations. A multi-objective reward captures key performance indicators (aggregate throughput, latency, jitter, packet loss rate, Jain's fairness index, and handover count), so the learned policy explicitly balances efficiency and stability under user mobility and noisy observations. The PPO agent uses an actor-critic neural network trained from trajectories generated by the Python simulator with configurable mobility (e.g., Gauss-Markov) and stochastic measurement noise. Across 500+ training episodes and stress tests with increasing user density, the PPO policy consistently improves KPI trends (higher throughput and fairness, lower delay, jitter, packet loss, and handovers) and exhibits rapid, stable convergence. Comparative evaluations show that PPO outperforms rule-based ReBuHa and A3 as well as the learning-based CDQL baseline across all KPIs while maintaining smoother learning dynamics and stronger generalization as load increases. These results indicate that PPO's clipped policy updates and advantage-based training yield robust, deployable control for next-generation RAN load balancing using an entirely Python-based toolchain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient mobility management and load balancing are critical to sustaining Quality of Service (QoS) in dense, highly dynamic 5G radio access networks. We present a deep reinforcement learning framework based on Proximal Policy Optimization (PPO) for autonomous, QoS-aware load balancing implemented end-to-end in a lightweight, pure-Python simulation environment. The control problem is formulated as a Markov Decision Process in which the agent periodically adjusts Cell Individual Offset (CIO) values to steer user-cell associations. A multi-objective reward captures key performance indicators (aggregate throughput, latency, jitter, packet loss rate, Jain's fairness index, and handover count), so the learned policy explicitly balances efficiency and stability under user mobility and noisy observations. The PPO agent uses an actor-critic neural network trained from trajectories generated by the Python simulator with configurable mobility (e.g., Gauss-Markov) and stochastic measurement noise. Across 500+ training episodes and stress tests with increasing user density, the PPO policy consistently improves KPI trends (higher throughput and fairness, lower delay, jitter, packet loss, and handovers) and exhibits rapid, stable convergence. Comparative evaluations show that PPO outperforms rule-based ReBuHa and A3 as well as the learning-based CDQL baseline across all KPIs while maintaining smoother learning dynamics and stronger generalization as load increases. These results indicate that PPO's clipped policy updates and advantage-based training yield robust, deployable control for next-generation RAN load balancing using an entirely Python-based toolchain."
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-28T18:20:33Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    18,
                    20,
                    33,
                    1,
                    301,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI"
                },
                "authors": [
                    {
                        "name": "Mehrshad Eskandarpour"
                    },
                    {
                        "name": "Hossein Soleimani"
                    }
                ],
                "author_detail": {
                    "name": "Hossein Soleimani"
                },
                "author": "Hossein Soleimani"
            },
            {
                "id": "http://arxiv.org/abs/2601.18361v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.18361v2",
                "title": "Integrating HAPS, LEO, and Terrestrial Networks: A Cost-Performance Study for IoT Connectivity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating HAPS, LEO, and Terrestrial Networks: A Cost-Performance Study for IoT Connectivity"
                },
                "updated": "2026-02-18T12:37:19Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    12,
                    37,
                    19,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.18361v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.18361v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This work evaluates the potential of High-Altitude Platform Stations (HAPS) and Low Earth Orbit (LEO) satellites as alternative or complementary systems to enhance Internet of Things (IoT) connectivity. We first analyze the transmission erasure probability under different connectivity configurations, including only HAPS or LEO satellites, as well as hybrid architectures that integrate both aerial/spatial and terrestrial infrastructures. To make the analysis more realistic, we considered movement of LEO satellites regarding a fixed region, elevation angle between gateway and devices, and different fading models for terrestrial and non-terrestrial communication. We also analyze LR-FHSS (Long-Range Frequency Hopping Spread Spectrum) random access uplink technology as a potential use case for IoT connectivity, showing the scalability impact of the scenarios. The simulation results demonstrate that HAPS can effectively complement sparse terrestrial networks and improve the performance of satellite-based systems in specific scenarios. Furthermore, considering the deployment and operational costs, respectively, CAPEX and OPEX, the economic analysis reveals that although HAPS exhibits higher costs, these remain within a comparable order of magnitude to LEO and terrestrial deployments. In addition, specific use cases, such as natural disasters, transform HAPS into a competitive technology for conventional infrastructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work evaluates the potential of High-Altitude Platform Stations (HAPS) and Low Earth Orbit (LEO) satellites as alternative or complementary systems to enhance Internet of Things (IoT) connectivity. We first analyze the transmission erasure probability under different connectivity configurations, including only HAPS or LEO satellites, as well as hybrid architectures that integrate both aerial/spatial and terrestrial infrastructures. To make the analysis more realistic, we considered movement of LEO satellites regarding a fixed region, elevation angle between gateway and devices, and different fading models for terrestrial and non-terrestrial communication. We also analyze LR-FHSS (Long-Range Frequency Hopping Spread Spectrum) random access uplink technology as a potential use case for IoT connectivity, showing the scalability impact of the scenarios. The simulation results demonstrate that HAPS can effectively complement sparse terrestrial networks and improve the performance of satellite-based systems in specific scenarios. Furthermore, considering the deployment and operational costs, respectively, CAPEX and OPEX, the economic analysis reveals that although HAPS exhibits higher costs, these remain within a comparable order of magnitude to LEO and terrestrial deployments. In addition, specific use cases, such as natural disasters, transform HAPS into a competitive technology for conventional infrastructures."
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-26T11:10:41Z",
                "published_parsed": [
                    2026,
                    1,
                    26,
                    11,
                    10,
                    41,
                    0,
                    26,
                    0
                ],
                "arxiv_comment": "10 pages, 6 figures. Submitted for per-review",
                "arxiv_primary_category": {
                    "term": "cs.NI"
                },
                "authors": [
                    {
                        "name": "Jean Michel de Souza Sant'Ana"
                    },
                    {
                        "name": "Felipe Augusto Tondo"
                    },
                    {
                        "name": "Nurul Huda Mahmood"
                    },
                    {
                        "name": "Aamir Mahmood"
                    }
                ],
                "author_detail": {
                    "name": "Aamir Mahmood"
                },
                "author": "Aamir Mahmood"
            },
            {
                "id": "http://arxiv.org/abs/2602.16385v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16385v1",
                "title": "Parameter-Free Adaptive Multi-Scale Channel-Spatial Attention Aggregation framework for 3D Indoor Semantic Scene Completion Toward Assisting Visually Impaired",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter-Free Adaptive Multi-Scale Channel-Spatial Attention Aggregation framework for 3D Indoor Semantic Scene Completion Toward Assisting Visually Impaired"
                },
                "updated": "2026-02-18T11:45:01Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    11,
                    45,
                    1,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16385v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16385v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In indoor assistive perception for visually impaired users, 3D Semantic Scene Completion (SSC) is expected to provide structurally coherent and semantically consistent occupancy under strictly monocular vision for safety-critical scene understanding. However, existing monocular SSC approaches often lack explicit modeling of voxel-feature reliability and regulated cross-scale information propagation during 2D-3D projection and multi-scale fusion, making them vulnerable to projection diffusion and feature entanglement and thus limiting structural stability.To address these challenges, this paper presents an Adaptive Multi-scale Attention Aggregation (AMAA) framework built upon the MonoScene pipeline. Rather than introducing a heavier backbone, AMAA focuses on reliability-oriented feature regulation within a monocular SSC framework. Specifically, lifted voxel features are jointly calibrated in semantic and spatial dimensions through parallel channel-spatial attention aggregation, while multi-scale encoder-decoder fusion is stabilized via a hierarchical adaptive feature-gating strategy that regulates information injection across scales.Experiments on the NYUv2 benchmark demonstrate consistent improvements over MonoScene without significantly increasing system complexity: AMAA achieves 27.25% SSC mIoU (+0.31) and 43.10% SC IoU (+0.59). In addition, system-level deployment on an NVIDIA Jetson platform verifies that the complete AMAA framework can be executed stably on embedded hardware. Overall, AMAA improves monocular SSC quality and provides a reliable and deployable perception framework for indoor assistive systems targeting visually impaired users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In indoor assistive perception for visually impaired users, 3D Semantic Scene Completion (SSC) is expected to provide structurally coherent and semantically consistent occupancy under strictly monocular vision for safety-critical scene understanding. However, existing monocular SSC approaches often lack explicit modeling of voxel-feature reliability and regulated cross-scale information propagation during 2D-3D projection and multi-scale fusion, making them vulnerable to projection diffusion and feature entanglement and thus limiting structural stability.To address these challenges, this paper presents an Adaptive Multi-scale Attention Aggregation (AMAA) framework built upon the MonoScene pipeline. Rather than introducing a heavier backbone, AMAA focuses on reliability-oriented feature regulation within a monocular SSC framework. Specifically, lifted voxel features are jointly calibrated in semantic and spatial dimensions through parallel channel-spatial attention aggregation, while multi-scale encoder-decoder fusion is stabilized via a hierarchical adaptive feature-gating strategy that regulates information injection across scales.Experiments on the NYUv2 benchmark demonstrate consistent improvements over MonoScene without significantly increasing system complexity: AMAA achieves 27.25% SSC mIoU (+0.31) and 43.10% SC IoU (+0.59). In addition, system-level deployment on an NVIDIA Jetson platform verifies that the complete AMAA framework can be executed stably on embedded hardware. Overall, AMAA improves monocular SSC quality and provides a reliable and deployable perception framework for indoor assistive systems targeting visually impaired users."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T11:45:01Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    11,
                    45,
                    1,
                    2,
                    49,
                    0
                ],
                "arxiv_comment": "17 pages, 9 figures, 5 tables",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Qi He"
                    },
                    {
                        "name": "XiangXiang Wang"
                    },
                    {
                        "name": "Jingtao Zhang"
                    },
                    {
                        "name": "Yongbin Yu"
                    },
                    {
                        "name": "Hongxiang Chu"
                    },
                    {
                        "name": "Manping Fan"
                    },
                    {
                        "name": "JingYe Cai"
                    },
                    {
                        "name": "Zhenglin Yang"
                    }
                ],
                "author_detail": {
                    "name": "Zhenglin Yang"
                },
                "author": "Zhenglin Yang"
            },
            {
                "id": "http://arxiv.org/abs/2512.07680v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.07680v3",
                "title": "AMBER: A tether-deployable gripping crawler with compliant microspines for canopy manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AMBER: A tether-deployable gripping crawler with compliant microspines for canopy manipulation"
                },
                "updated": "2026-02-18T11:42:55Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    11,
                    42,
                    55,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.07680v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.07680v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This paper presents an aerially deployable crawler designed for adaptive locomotion and manipulation within tree canopies. The system combines compliant microspine-based tracks, a dual-track rotary gripper, and an elastic tail, enabling secure attachment and stable traversal across branches of varying curvature and inclination. Experiments demonstrate reliable gripping up to 90$^\\circ$ body roll and inclination, while effective climbing on branches inclined up to 67.5$^\\circ$, achieving a maximum speed of 0.55 body lengths per second on horizontal branches. The compliant tracks allow yaw steering of up to 10$^\\circ$, enhancing maneuverability on irregular surfaces. Power measurements show efficient operation with a dimensionless cost of transport over an order of magnitude lower than typical hovering power consumption in aerial robots. The crawler provides a robust, low-power platform for environmental sampling and in-canopy sensing. The aerial deployment is demonstrated at a conceptual and feasibility level, while full drone-crawler integration is left as future work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an aerially deployable crawler designed for adaptive locomotion and manipulation within tree canopies. The system combines compliant microspine-based tracks, a dual-track rotary gripper, and an elastic tail, enabling secure attachment and stable traversal across branches of varying curvature and inclination. Experiments demonstrate reliable gripping up to 90$^\\circ$ body roll and inclination, while effective climbing on branches inclined up to 67.5$^\\circ$, achieving a maximum speed of 0.55 body lengths per second on horizontal branches. The compliant tracks allow yaw steering of up to 10$^\\circ$, enhancing maneuverability on irregular surfaces. Power measurements show efficient operation with a dimensionless cost of transport over an order of magnitude lower than typical hovering power consumption in aerial robots. The crawler provides a robust, low-power platform for environmental sampling and in-canopy sensing. The aerial deployment is demonstrated at a conceptual and feasibility level, while full drone-crawler integration is left as future work."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-08T16:17:56Z",
                "published_parsed": [
                    2025,
                    12,
                    8,
                    16,
                    17,
                    56,
                    0,
                    342,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "P. A. Wigner"
                    },
                    {
                        "name": "L. Romanello"
                    },
                    {
                        "name": "A. Hammad"
                    },
                    {
                        "name": "P. H. Nguyen"
                    },
                    {
                        "name": "T. Lan"
                    },
                    {
                        "name": "S. F. Armanini"
                    },
                    {
                        "name": "B. B. Kocer"
                    },
                    {
                        "name": "M. Kovac"
                    }
                ],
                "author_detail": {
                    "name": "M. Kovac"
                },
                "author": "M. Kovac"
            },
            {
                "id": "http://arxiv.org/abs/2602.16379v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16379v1",
                "title": "Label-Consistent Data Generation for Aspect-Based Sentiment Analysis Using LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Label-Consistent Data Generation for Aspect-Based Sentiment Analysis Using LLM Agents"
                },
                "updated": "2026-02-18T11:38:11Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    11,
                    38,
                    11,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16379v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16379v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We propose an agentic data augmentation method for Aspect-Based Sentiment Analysis (ABSA) that uses iterative generation and verification to produce high quality synthetic training examples. To isolate the effect of agentic structure, we also develop a closely matched prompting-based baseline using the same model and instructions. Both methods are evaluated across three ABSA subtasks (Aspect Term Extraction (ATE), Aspect Sentiment Classification (ATSC), and Aspect Sentiment Pair Extraction (ASPE)), four SemEval datasets, and two encoder-decoder models: T5-Base and Tk-Instruct. Our results show that the agentic augmentation outperforms raw prompting in label preservation of the augmented data, especially when the tasks require aspect term generation. In addition, when combined with real data, agentic augmentation provides higher gains, consistently outperforming prompting-based generation. These benefits are most pronounced for T5-Base, while the more heavily pretrained Tk-Instruct exhibits smaller improvements. As a result, augmented data helps T5-Base achieve comparable performance with its counterpart.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose an agentic data augmentation method for Aspect-Based Sentiment Analysis (ABSA) that uses iterative generation and verification to produce high quality synthetic training examples. To isolate the effect of agentic structure, we also develop a closely matched prompting-based baseline using the same model and instructions. Both methods are evaluated across three ABSA subtasks (Aspect Term Extraction (ATE), Aspect Sentiment Classification (ATSC), and Aspect Sentiment Pair Extraction (ASPE)), four SemEval datasets, and two encoder-decoder models: T5-Base and Tk-Instruct. Our results show that the agentic augmentation outperforms raw prompting in label preservation of the augmented data, especially when the tasks require aspect term generation. In addition, when combined with real data, agentic augmentation provides higher gains, consistently outperforming prompting-based generation. These benefits are most pronounced for T5-Base, while the more heavily pretrained Tk-Instruct exhibits smaller improvements. As a result, augmented data helps T5-Base achieve comparable performance with its counterpart."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T11:38:11Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    11,
                    38,
                    11,
                    2,
                    49,
                    0
                ],
                "arxiv_comment": "Accepted to WASSA Workshop at EACL 2026",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Mohammad H. A. Monfared"
                    },
                    {
                        "name": "Lucie Flek"
                    },
                    {
                        "name": "Akbar Karimi"
                    }
                ],
                "author_detail": {
                    "name": "Akbar Karimi"
                },
                "author": "Akbar Karimi"
            },
            {
                "id": "http://arxiv.org/abs/2602.16378v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16378v1",
                "title": "Scalable Base Station Configuration via Bayesian Optimization with Block Coordinate Descent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Base Station Configuration via Bayesian Optimization with Block Coordinate Descent"
                },
                "updated": "2026-02-18T11:37:51Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    11,
                    37,
                    51,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16378v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16378v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This paper proposes a scalable Bayesian optimization (BO) framework for dense base-station (BS) configuration design. BO can find an optimal BS configuration by iterating parameter search, channel simulation, and probabilistic modeling of the objective function. However, its performance is severely affected by the curse of dimensionality, thereby reducing its scalability. To overcome this limitation, the proposed method sequentially optimizes per-BS parameters based on block coordinate descent while fixing the remaining BS configurations, thereby reducing the effective dimensionality of each optimization step. Numerical results demonstrate that the proposed approach significantly outperforms naive optimization in dense deployment scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a scalable Bayesian optimization (BO) framework for dense base-station (BS) configuration design. BO can find an optimal BS configuration by iterating parameter search, channel simulation, and probabilistic modeling of the objective function. However, its performance is severely affected by the curse of dimensionality, thereby reducing its scalability. To overcome this limitation, the proposed method sequentially optimizes per-BS parameters based on block coordinate descent while fixing the remaining BS configurations, thereby reducing the effective dimensionality of each optimization step. Numerical results demonstrate that the proposed approach significantly outperforms naive optimization in dense deployment scenarios."
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T11:37:51Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    11,
                    37,
                    51,
                    2,
                    49,
                    0
                ],
                "arxiv_comment": "2 pages, 3 figures. Accepted for presentation as a poster at IEEE INFOCOM 2026",
                "arxiv_primary_category": {
                    "term": "cs.IT"
                },
                "authors": [
                    {
                        "name": "Kakeru Takamori"
                    },
                    {
                        "name": "Koya Sato"
                    }
                ],
                "author_detail": {
                    "name": "Koya Sato"
                },
                "author": "Koya Sato"
            },
            {
                "id": "http://arxiv.org/abs/2511.17178v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.17178v2",
                "title": "Efficient Robot Design with Multi-Objective Black-Box Optimization and Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Robot Design with Multi-Objective Black-Box Optimization and Large Language Models"
                },
                "updated": "2026-02-18T11:18:33Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    11,
                    18,
                    33,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.17178v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.17178v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1109/ACCESS.2026.3664844",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Various methods for robot design optimization have been developed so far. These methods are diverse, ranging from numerical optimization to black-box optimization. While numerical optimization is fast, it is not suitable for cases involving complex structures or discrete values, leading to frequent use of black-box optimization instead. However, black-box optimization suffers from low sampling efficiency and takes considerable sampling iterations to obtain good solutions. In this study, we propose a method to enhance the efficiency of robot body design based on black-box optimization by utilizing large language models (LLMs). In parallel with the sampling process based on black-box optimization, sampling is performed using LLMs, which are provided with problem settings and extensive feedback. We demonstrate that this method enables more efficient exploration of design solutions and discuss its characteristics and limitations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Various methods for robot design optimization have been developed so far. These methods are diverse, ranging from numerical optimization to black-box optimization. While numerical optimization is fast, it is not suitable for cases involving complex structures or discrete values, leading to frequent use of black-box optimization instead. However, black-box optimization suffers from low sampling efficiency and takes considerable sampling iterations to obtain good solutions. In this study, we propose a method to enhance the efficiency of robot body design based on black-box optimization by utilizing large language models (LLMs). In parallel with the sampling process based on black-box optimization, sampling is performed using LLMs, which are provided with problem settings and extensive feedback. We demonstrate that this method enables more efficient exploration of design solutions and discuss its characteristics and limitations."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-21T11:56:52Z",
                "published_parsed": [
                    2025,
                    11,
                    21,
                    11,
                    56,
                    52,
                    4,
                    325,
                    0
                ],
                "arxiv_comment": "Accepted to IEEE Access, website: https://haraduka.github.io/urdf-llm-opt/ , video: https://www.youtube.com/watch?v=N9iMjx7of1w",
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Kento Kawaharazuka"
                    },
                    {
                        "name": "Yoshiki Obinata"
                    },
                    {
                        "name": "Naoaki Kanazawa"
                    },
                    {
                        "name": "Haoyu Jia"
                    },
                    {
                        "name": "Kei Okada"
                    }
                ],
                "author_detail": {
                    "name": "Kei Okada"
                },
                "author": "Kei Okada",
                "arxiv_doi": "10.1109/ACCESS.2026.3664844"
            },
            {
                "id": "http://arxiv.org/abs/2507.15994v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.15994v2",
                "title": "Scaling Recommender Transformers to One Billion Parameters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Recommender Transformers to One Billion Parameters"
                },
                "updated": "2026-02-18T11:17:15Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    11,
                    17,
                    15,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.15994v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.15994v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1145/3770854.3783916",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "While large transformer models have been successfully used in many real-world applications such as natural language processing, computer vision, and speech processing, scaling transformers for recommender systems remains a challenging problem. Recently, Generative Recommenders framework was proposed to scale beyond typical Deep Learning Recommendation Models (DLRMs). Reformulation of recommendation as sequential transduction task led to improvement of scaling properties in terms of compute. Nevertheless, the largest encoder configuration reported by the HSTU authors amounts only to ~176 million parameters, which is considerably smaller than the hundreds of billions or even trillions of parameters common in modern language models.\n  In this work, we present a recipe for training large transformer recommenders with up to a billion parameters. We show that autoregressive learning on user histories naturally decomposes into two subtasks, feedback prediction and next-item prediction, and demonstrate that such a decomposition scales effectively across a wide range of transformer sizes. Furthermore, we report a successful deployment of our proposed architecture on a large-scale music platform serving millions of users. According to our online A/B tests, this new model increases total listening time by +2.26% and raises the likelihood of user likes by +6.37%, constituting (to our knowledge) the largest improvement in recommendation quality reported for any deep learning-based system in the platform's history.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large transformer models have been successfully used in many real-world applications such as natural language processing, computer vision, and speech processing, scaling transformers for recommender systems remains a challenging problem. Recently, Generative Recommenders framework was proposed to scale beyond typical Deep Learning Recommendation Models (DLRMs). Reformulation of recommendation as sequential transduction task led to improvement of scaling properties in terms of compute. Nevertheless, the largest encoder configuration reported by the HSTU authors amounts only to ~176 million parameters, which is considerably smaller than the hundreds of billions or even trillions of parameters common in modern language models.\n  In this work, we present a recipe for training large transformer recommenders with up to a billion parameters. We show that autoregressive learning on user histories naturally decomposes into two subtasks, feedback prediction and next-item prediction, and demonstrate that such a decomposition scales effectively across a wide range of transformer sizes. Furthermore, we report a successful deployment of our proposed architecture on a large-scale music platform serving millions of users. According to our online A/B tests, this new model increases total listening time by +2.26% and raises the likelihood of user likes by +6.37%, constituting (to our knowledge) the largest improvement in recommendation quality reported for any deep learning-based system in the platform's history."
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-21T18:30:43Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    18,
                    30,
                    43,
                    0,
                    202,
                    0
                ],
                "arxiv_comment": "KDD'2026",
                "arxiv_primary_category": {
                    "term": "cs.IR"
                },
                "authors": [
                    {
                        "name": "Kirill Khrylchenko"
                    },
                    {
                        "name": "Artem Matveev"
                    },
                    {
                        "name": "Sergei Makeev"
                    },
                    {
                        "name": "Vladimir Baikalov"
                    }
                ],
                "author_detail": {
                    "name": "Vladimir Baikalov"
                },
                "author": "Vladimir Baikalov",
                "arxiv_doi": "10.1145/3770854.3783916"
            },
            {
                "id": "http://arxiv.org/abs/2602.16362v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16362v1",
                "title": "How Reliable is Your Service at the Extreme Edge? Analytical Modeling of Computational Reliability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Reliable is Your Service at the Extreme Edge? Analytical Modeling of Computational Reliability"
                },
                "updated": "2026-02-18T11:03:07Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    11,
                    3,
                    7,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16362v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16362v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Extreme Edge Computing (XEC) distributes streaming workloads across consumer-owned devices, exploiting their proximity to users and ubiquitous availability. Many such workloads are AI-driven, requiring continuous neural network inference for tasks like object detection and video analytics. Distributed Inference (DI), which partitions model execution across multiple edge devices, enables these streaming services to meet strict throughput and latency requirements. Yet consumer devices exhibit volatile computational availability due to competing applications and unpredictable usage patterns. This volatility poses a fundamental challenge: how can we quantify the probability that a device, or ensemble of devices, will maintain the processing rate required by a streaming service? This paper presents an analytical framework for computational reliability in XEC, defined as the probability that instantaneous capacity meets demand at a specified Quality of Service (QoS) threshold. We derive closed-form reliability expressions under two information regimes: Minimal Information (MI), requiring only declared operational bounds, and historical data, which refines estimates via Maximum Likelihood Estimation from past observations. The framework extends to multi-device deployments, providing reliability expressions for series, parallel, and partitioned workload configurations. We derive optimal workload allocation rules and analytical bounds for device selection, equipping orchestrators with tractable tools to evaluate deployment feasibility and configure distributed streaming systems. We validate the framework using real-time object detection with YOLO11m model as a representative DI streaming workload; experiments on emulated XED environments demonstrate close agreement between analytical predictions, Monte Carlo sampling, and empirical measurements across diverse capacity and demand configurations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extreme Edge Computing (XEC) distributes streaming workloads across consumer-owned devices, exploiting their proximity to users and ubiquitous availability. Many such workloads are AI-driven, requiring continuous neural network inference for tasks like object detection and video analytics. Distributed Inference (DI), which partitions model execution across multiple edge devices, enables these streaming services to meet strict throughput and latency requirements. Yet consumer devices exhibit volatile computational availability due to competing applications and unpredictable usage patterns. This volatility poses a fundamental challenge: how can we quantify the probability that a device, or ensemble of devices, will maintain the processing rate required by a streaming service? This paper presents an analytical framework for computational reliability in XEC, defined as the probability that instantaneous capacity meets demand at a specified Quality of Service (QoS) threshold. We derive closed-form reliability expressions under two information regimes: Minimal Information (MI), requiring only declared operational bounds, and historical data, which refines estimates via Maximum Likelihood Estimation from past observations. The framework extends to multi-device deployments, providing reliability expressions for series, parallel, and partitioned workload configurations. We derive optimal workload allocation rules and analytical bounds for device selection, equipping orchestrators with tractable tools to evaluate deployment feasibility and configure distributed streaming systems. We validate the framework using real-time object detection with YOLO11m model as a representative DI streaming workload; experiments on emulated XED environments demonstrate close agreement between analytical predictions, Monte Carlo sampling, and empirical measurements across diverse capacity and demand configurations."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T11:03:07Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    11,
                    3,
                    7,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "MHD Saria Allahham"
                    },
                    {
                        "name": "Hossam S. Hassanein"
                    }
                ],
                "author_detail": {
                    "name": "Hossam S. Hassanein"
                },
                "author": "Hossam S. Hassanein"
            },
            {
                "id": "http://arxiv.org/abs/2602.16360v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16360v1",
                "title": "Docking and Persistent Operations for a Resident Underwater Vehicle",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Docking and Persistent Operations for a Resident Underwater Vehicle"
                },
                "updated": "2026-02-18T10:50:04Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    10,
                    50,
                    4,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16360v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16360v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Our understanding of the oceans remains limited by sparse and infrequent observations, primarily because current methods are constrained by the high cost and logistical effort of underwater monitoring, relying either on sporadic surveys across broad areas or on long-term measurements at fixed locations. To overcome these limitations, monitoring systems must enable persistent and autonomous operations without the need for continuous surface support. Despite recent advances, resident underwater vehicles remain uncommon due to persistent challenges in autonomy, robotic resilience, and mechanical robustness, particularly under long-term deployment in harsh and remote environments. This work addresses these problems by presenting the development, deployment, and operation of a resident infrastructure using a docking station with a mini-class Remotely Operated Vehicle (ROV) at 90m depth. The ROVis equipped with enhanced onboard processing and perception, allowing it to autonomously navigate using USBL signals, dock via ArUco marker-based visual localisation fused through an Extended Kalman Filter, and carry out local inspection routines. The system demonstrated a 90% autonomous docking success rate and completed full inspection missions within four minutes, validating the integration of acoustic and visual navigation in real-world conditions. These results show that reliable, untethered operations at depth are feasible, highlighting the potential of resident ROV systems for scalable, cost-effective underwater monitoring.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Our understanding of the oceans remains limited by sparse and infrequent observations, primarily because current methods are constrained by the high cost and logistical effort of underwater monitoring, relying either on sporadic surveys across broad areas or on long-term measurements at fixed locations. To overcome these limitations, monitoring systems must enable persistent and autonomous operations without the need for continuous surface support. Despite recent advances, resident underwater vehicles remain uncommon due to persistent challenges in autonomy, robotic resilience, and mechanical robustness, particularly under long-term deployment in harsh and remote environments. This work addresses these problems by presenting the development, deployment, and operation of a resident infrastructure using a docking station with a mini-class Remotely Operated Vehicle (ROV) at 90m depth. The ROVis equipped with enhanced onboard processing and perception, allowing it to autonomously navigate using USBL signals, dock via ArUco marker-based visual localisation fused through an Extended Kalman Filter, and carry out local inspection routines. The system demonstrated a 90% autonomous docking success rate and completed full inspection missions within four minutes, validating the integration of acoustic and visual navigation in real-world conditions. These results show that reliable, untethered operations at depth are feasible, highlighting the potential of resident ROV systems for scalable, cost-effective underwater monitoring."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T10:50:04Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    10,
                    50,
                    4,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Leonard Günzel"
                    },
                    {
                        "name": "Gabrielė Kasparavičiūtė"
                    },
                    {
                        "name": "Ambjørn Grimsrud Waldum"
                    },
                    {
                        "name": "Bjørn-Magnus Moslått"
                    },
                    {
                        "name": "Abubakar Aliyu Badawi"
                    },
                    {
                        "name": "Celil Yılmaz"
                    },
                    {
                        "name": "Md Shamin Yeasher Yousha"
                    },
                    {
                        "name": "Robert Staven"
                    },
                    {
                        "name": "Martin Ludvigsen"
                    }
                ],
                "author_detail": {
                    "name": "Martin Ludvigsen"
                },
                "author": "Martin Ludvigsen"
            },
            {
                "id": "http://arxiv.org/abs/2602.00663v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.00663v2",
                "title": "SEISMO: Increasing Sample Efficiency in Molecular Optimization with a Trajectory-Aware LLM Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEISMO: Increasing Sample Efficiency in Molecular Optimization with a Trajectory-Aware LLM Agent"
                },
                "updated": "2026-02-18T10:50:04Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    10,
                    50,
                    4,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.00663v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.00663v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Optimizing the structure of molecules to achieve desired properties is a central bottleneck across the chemical sciences, particularly in the pharmaceutical industry where it underlies the discovery of new drugs. Since molecular property evaluation often relies on costly and rate-limited oracles, such as experimental assays, molecular optimization must be highly sample-efficient. To address this, we introduce SEISMO, an LLM agent that performs strictly online, inference-time molecular optimization, updating after every oracle call without the need for population-based or batched learning. SEISMO conditions each proposal on the full optimization trajectory, combining natural-language task descriptions with scalar scores and, when available, structured explanatory feedback. Across the Practical Molecular Optimization benchmark of 23 tasks, SEISMO achieves a 2-3 times higher area under the optimisation curve than prior methods, often reaching near-maximal task scores within 50 oracle calls. Our additional medicinal-chemistry tasks show that providing explanatory feedback further improves efficiency, demonstrating that leveraging domain knowledge and structured information is key to sample-efficient molecular optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing the structure of molecules to achieve desired properties is a central bottleneck across the chemical sciences, particularly in the pharmaceutical industry where it underlies the discovery of new drugs. Since molecular property evaluation often relies on costly and rate-limited oracles, such as experimental assays, molecular optimization must be highly sample-efficient. To address this, we introduce SEISMO, an LLM agent that performs strictly online, inference-time molecular optimization, updating after every oracle call without the need for population-based or batched learning. SEISMO conditions each proposal on the full optimization trajectory, combining natural-language task descriptions with scalar scores and, when available, structured explanatory feedback. Across the Practical Molecular Optimization benchmark of 23 tasks, SEISMO achieves a 2-3 times higher area under the optimisation curve than prior methods, often reaching near-maximal task scores within 50 oracle calls. Our additional medicinal-chemistry tasks show that providing explanatory feedback further improves efficiency, demonstrating that leveraging domain knowledge and structured information is key to sample-efficient molecular optimization."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-31T11:23:48Z",
                "published_parsed": [
                    2026,
                    1,
                    31,
                    11,
                    23,
                    48,
                    5,
                    31,
                    0
                ],
                "arxiv_comment": "Fabian P. Krüger and Andrea Hunklinger contributed equally to this work",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Fabian P. Krüger"
                    },
                    {
                        "name": "Andrea Hunklinger"
                    },
                    {
                        "name": "Adrian Wolny"
                    },
                    {
                        "name": "Tim J. Adler"
                    },
                    {
                        "name": "Igor Tetko"
                    },
                    {
                        "name": "Santiago David Villalba"
                    }
                ],
                "author_detail": {
                    "name": "Santiago David Villalba"
                },
                "author": "Santiago David Villalba"
            },
            {
                "id": "http://arxiv.org/abs/2602.16349v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16349v1",
                "title": "SCAR: Satellite Imagery-Based Calibration for Aerial Recordings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCAR: Satellite Imagery-Based Calibration for Aerial Recordings"
                },
                "updated": "2026-02-18T10:33:24Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    10,
                    33,
                    24,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16349v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16349v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce SCAR, a method for long-term auto-calibration refinement of aerial visual-inertial systems that exploits georeferenced satellite imagery as a persistent global reference. SCAR estimates both intrinsic and extrinsic parameters by aligning aerial images with 2D--3D correspondences derived from publicly available orthophotos and elevation models. In contrast to existing approaches that rely on dedicated calibration maneuvers or manually surveyed ground control points, our method leverages external geospatial data to detect and correct calibration degradation under field deployment conditions. We evaluate our approach on six large-scale aerial campaigns conducted over two years under diverse seasonal and environmental conditions. Across all sequences, SCAR consistently outperforms established baselines (Kalibr, COLMAP, VINS-Mono), reducing median reprojection error by a large margin, and translating these calibration gains into substantially lower visual localization rotation errors and higher pose accuracy. These results demonstrate that SCAR provides accurate, robust, and reproducible calibration over long-term aerial operations without the need for manual intervention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce SCAR, a method for long-term auto-calibration refinement of aerial visual-inertial systems that exploits georeferenced satellite imagery as a persistent global reference. SCAR estimates both intrinsic and extrinsic parameters by aligning aerial images with 2D--3D correspondences derived from publicly available orthophotos and elevation models. In contrast to existing approaches that rely on dedicated calibration maneuvers or manually surveyed ground control points, our method leverages external geospatial data to detect and correct calibration degradation under field deployment conditions. We evaluate our approach on six large-scale aerial campaigns conducted over two years under diverse seasonal and environmental conditions. Across all sequences, SCAR consistently outperforms established baselines (Kalibr, COLMAP, VINS-Mono), reducing median reprojection error by a large margin, and translating these calibration gains into substantially lower visual localization rotation errors and higher pose accuracy. These results demonstrate that SCAR provides accurate, robust, and reproducible calibration over long-term aerial operations without the need for manual intervention."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T10:33:24Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    10,
                    33,
                    24,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Henry Hölzemann"
                    },
                    {
                        "name": "Michael Schleiss"
                    }
                ],
                "author_detail": {
                    "name": "Michael Schleiss"
                },
                "author": "Michael Schleiss"
            },
            {
                "id": "http://arxiv.org/abs/2602.16346v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16346v1",
                "title": "Helpful to a Fault: Measuring Illicit Assistance in Multi-Turn, Multilingual LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Helpful to a Fault: Measuring Illicit Assistance in Multi-Turn, Multilingual LLM Agents"
                },
                "updated": "2026-02-18T10:31:19Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    10,
                    31,
                    19,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16346v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16346v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "LLM-based agents execute real-world workflows via tools and memory. These affordances enable ill-intended adversaries to also use these agents to carry out complex misuse scenarios. Existing agent misuse benchmarks largely test single-prompt instructions, leaving a gap in measuring how agents end up helping with harmful or illegal tasks over multiple turns. We introduce STING (Sequential Testing of Illicit N-step Goal execution), an automated red-teaming framework that constructs a step-by-step illicit plan grounded in a benign persona and iteratively probes a target agent with adaptive follow-ups, using judge agents to track phase completion. We further introduce an analysis framework that models multi-turn red-teaming as a time-to-first-jailbreak random variable, enabling analysis tools like discovery curves, hazard-ratio attribution by attack language, and a new metric: Restricted Mean Jailbreak Discovery. Across AgentHarm scenarios, STING yields substantially higher illicit-task completion than single-turn prompting and chat-oriented multi-turn baselines adapted to tool-using agents. In multilingual evaluations across six non-English settings, we find that attack success and illicit-task completion do not consistently increase in lower-resource languages, diverging from common chatbot findings. Overall, STING provides a practical way to evaluate and stress-test agent misuse in realistic deployment settings, where interactions are inherently multi-turn and often multilingual.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based agents execute real-world workflows via tools and memory. These affordances enable ill-intended adversaries to also use these agents to carry out complex misuse scenarios. Existing agent misuse benchmarks largely test single-prompt instructions, leaving a gap in measuring how agents end up helping with harmful or illegal tasks over multiple turns. We introduce STING (Sequential Testing of Illicit N-step Goal execution), an automated red-teaming framework that constructs a step-by-step illicit plan grounded in a benign persona and iteratively probes a target agent with adaptive follow-ups, using judge agents to track phase completion. We further introduce an analysis framework that models multi-turn red-teaming as a time-to-first-jailbreak random variable, enabling analysis tools like discovery curves, hazard-ratio attribution by attack language, and a new metric: Restricted Mean Jailbreak Discovery. Across AgentHarm scenarios, STING yields substantially higher illicit-task completion than single-turn prompting and chat-oriented multi-turn baselines adapted to tool-using agents. In multilingual evaluations across six non-English settings, we find that attack success and illicit-task completion do not consistently increase in lower-resource languages, diverging from common chatbot findings. Overall, STING provides a practical way to evaluate and stress-test agent misuse in realistic deployment settings, where interactions are inherently multi-turn and often multilingual."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T10:31:19Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    10,
                    31,
                    19,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Nivya Talokar"
                    },
                    {
                        "name": "Ayush K Tarun"
                    },
                    {
                        "name": "Murari Mandal"
                    },
                    {
                        "name": "Maksym Andriushchenko"
                    },
                    {
                        "name": "Antoine Bosselut"
                    }
                ],
                "author_detail": {
                    "name": "Antoine Bosselut"
                },
                "author": "Antoine Bosselut"
            },
            {
                "id": "http://arxiv.org/abs/2602.16345v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16345v1",
                "title": "Multi-Agent Meta-Advisor for UAV Fleet Trajectory Design in Vehicular Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Meta-Advisor for UAV Fleet Trajectory Design in Vehicular Networks"
                },
                "updated": "2026-02-18T10:30:48Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    10,
                    30,
                    48,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16345v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16345v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Future vehicular networks require continuous connectivity to serve highly mobile users in urban environments. To mitigate the coverage limitations of fixed terrestrial macro base stations (MBS) under non line-of-sight (NLoS) conditions, fleets of unmanned aerial base stations (UABSs) can be deployed as aerial base stations, dynamically repositioning to track vehicular users and traffic hotspots in coordination with the terrestrial network. This paper addresses cooperative multi-agent trajectory design under different service areas and takeoff configurations, where rapid and safe adaptation across scenarios is essential. We formulate the problem as a multi-task decentralized partially observable Markov decision process and solve it using centralized training and decentralized execution with double dueling deep Q-network (3DQN), enabling online training for real-world deployments. However, efficient exploration remains a bottleneck, with conventional strategies like $ε$-greedy requiring careful tuning. To overcome this, we propose the multi-agent meta-advisor with advisor override (MAMO). This framework guides agent exploration through a meta-policy learned jointly across tasks. It uses a dynamic override mechanism that allows agents to reject misaligned guidance when the advisor fails to generalize to a specific scenario. Simulation results across three realistic urban scenarios and multiple takeoff configurations show that MAMO achieves faster convergence and higher returns than tuned $ε$-greedy baselines, outperforming both an advisor-only ablation and a single generalized policy. Finally, we demonstrate that the learned UABS fleet significantly improves network performance compared to deployments without aerial support.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Future vehicular networks require continuous connectivity to serve highly mobile users in urban environments. To mitigate the coverage limitations of fixed terrestrial macro base stations (MBS) under non line-of-sight (NLoS) conditions, fleets of unmanned aerial base stations (UABSs) can be deployed as aerial base stations, dynamically repositioning to track vehicular users and traffic hotspots in coordination with the terrestrial network. This paper addresses cooperative multi-agent trajectory design under different service areas and takeoff configurations, where rapid and safe adaptation across scenarios is essential. We formulate the problem as a multi-task decentralized partially observable Markov decision process and solve it using centralized training and decentralized execution with double dueling deep Q-network (3DQN), enabling online training for real-world deployments. However, efficient exploration remains a bottleneck, with conventional strategies like $ε$-greedy requiring careful tuning. To overcome this, we propose the multi-agent meta-advisor with advisor override (MAMO). This framework guides agent exploration through a meta-policy learned jointly across tasks. It uses a dynamic override mechanism that allows agents to reject misaligned guidance when the advisor fails to generalize to a specific scenario. Simulation results across three realistic urban scenarios and multiple takeoff configurations show that MAMO achieves faster convergence and higher returns than tuned $ε$-greedy baselines, outperforming both an advisor-only ablation and a single generalized policy. Finally, we demonstrate that the learned UABS fleet significantly improves network performance compared to deployments without aerial support."
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T10:30:48Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    10,
                    30,
                    48,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI"
                },
                "authors": [
                    {
                        "name": "Leonardo Spampinato"
                    },
                    {
                        "name": "Lorenzo Mario Amorosa"
                    },
                    {
                        "name": "Enrico Testi"
                    },
                    {
                        "name": "Chiara Buratti"
                    },
                    {
                        "name": "Riccardo Marini"
                    }
                ],
                "author_detail": {
                    "name": "Riccardo Marini"
                },
                "author": "Riccardo Marini"
            },
            {
                "id": "http://arxiv.org/abs/2602.16344v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16344v1",
                "title": "Modelling and Analysis of Mechanical and Thermal Response of an Ultrastable, Dual-Axis, Cubic Cavity for Terrestrial and Space Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modelling and Analysis of Mechanical and Thermal Response of an Ultrastable, Dual-Axis, Cubic Cavity for Terrestrial and Space Applications"
                },
                "updated": "2026-02-18T10:30:45Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    10,
                    30,
                    45,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16344v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Transportable all-optical atomic clocks represent the next-generation devices for precision time keeping, ushering a new era in encompassing a wide range of PNT (Positioning, Navigation and Timing) applications in the civil and strategic sectors. Their performance relies on ultra-stable, narrow-linewidth lasers, frequency stabilized to a compact portable optical cavity. Among various designs, the cubic spacer-based ultra-stable cavity is particularly well-suited for transportable applications due to its low sensitivity to vibrations, owing to its symmetric geometry and robust mounting structure. While longer cavities offer a lower fundamental thermal noise floor, one needs to strike a balance between transportability and size. In this aspect, the 7.5 cm dual-axis cubic cavity offers a lower fundamental thermal noise floor in comparison to smaller counterparts, while still retaining a reasonable SWaP (Size, Weight and Power) for terrestrial and aerial PNT applications. Its dual-axis design also enables multi-wavelength laser stabilization, making it a promising candidate for future transportable clock applications. This work presents a detailed study of the 7.5 cm dual-axis cubic cavity using FEM (Finite Element Method) to evaluate its mechanical and thermal stability. We analyze the impact of various geometric factors, mounting forces, and machining imperfections, while also modelling thermal effects such as conduction, radiation, and mirror heating within a vacuum chamber and thermally shielded environment. Our findings provide design insights for developing robust dual-axis optical reference cavities, advancing the deployment of portable atomic clocks for next-generation applications in PNT, geodesy, VLBI (Very Long Baseline Interferometry) and deep space missions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transportable all-optical atomic clocks represent the next-generation devices for precision time keeping, ushering a new era in encompassing a wide range of PNT (Positioning, Navigation and Timing) applications in the civil and strategic sectors. Their performance relies on ultra-stable, narrow-linewidth lasers, frequency stabilized to a compact portable optical cavity. Among various designs, the cubic spacer-based ultra-stable cavity is particularly well-suited for transportable applications due to its low sensitivity to vibrations, owing to its symmetric geometry and robust mounting structure. While longer cavities offer a lower fundamental thermal noise floor, one needs to strike a balance between transportability and size. In this aspect, the 7.5 cm dual-axis cubic cavity offers a lower fundamental thermal noise floor in comparison to smaller counterparts, while still retaining a reasonable SWaP (Size, Weight and Power) for terrestrial and aerial PNT applications. Its dual-axis design also enables multi-wavelength laser stabilization, making it a promising candidate for future transportable clock applications. This work presents a detailed study of the 7.5 cm dual-axis cubic cavity using FEM (Finite Element Method) to evaluate its mechanical and thermal stability. We analyze the impact of various geometric factors, mounting forces, and machining imperfections, while also modelling thermal effects such as conduction, radiation, and mirror heating within a vacuum chamber and thermally shielded environment. Our findings provide design insights for developing robust dual-axis optical reference cavities, advancing the deployment of portable atomic clocks for next-generation applications in PNT, geodesy, VLBI (Very Long Baseline Interferometry) and deep space missions."
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T10:30:45Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    10,
                    30,
                    45,
                    2,
                    49,
                    0
                ],
                "arxiv_comment": "Himanshu Miriyala and Rishabh Pal contributed equally to this work",
                "arxiv_primary_category": {
                    "term": "physics.optics"
                },
                "authors": [
                    {
                        "name": "Himanshu Miriyala"
                    },
                    {
                        "name": "Rishabh Pal"
                    },
                    {
                        "name": "Arijit Sharma"
                    }
                ],
                "author_detail": {
                    "name": "Arijit Sharma"
                },
                "author": "Arijit Sharma"
            },
            {
                "id": "http://arxiv.org/abs/2601.16800v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.16800v2",
                "title": "Large Language Models as Automatic Annotators and Annotation Adjudicators for Fine-Grained Opinion Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models as Automatic Annotators and Annotation Adjudicators for Fine-Grained Opinion Analysis"
                },
                "updated": "2026-02-18T10:27:00Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    10,
                    27,
                    0,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.16800v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.16800v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Fine-grained opinion analysis of text provides a detailed understanding of expressed sentiments, including the addressed entity. Although this level of detail is sound, it requires considerable human effort and substantial cost to annotate opinions in datasets for training models, especially across diverse domains and real-world applications. We explore the feasibility of LLMs as automatic annotators for fine-grained opinion analysis, addressing the shortage of domain-specific labelled datasets. In this work, we use a declarative annotation pipeline. This approach reduces the variability of manual prompt engineering when using LLMs to identify fine-grained opinion spans in text. We also present a novel methodology for an LLM to adjudicate multiple labels and produce final annotations. After trialling the pipeline with models of different sizes for the Aspect Sentiment Triplet Extraction (ASTE) and Aspect-Category-Opinion-Sentiment (ACOS) analysis tasks, we show that LLMs can serve as automatic annotators and adjudicators, achieving high Inter-Annotator Agreement across individual LLM-based annotators. This reduces the cost and human effort needed to create these fine-grained opinion-annotated datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-grained opinion analysis of text provides a detailed understanding of expressed sentiments, including the addressed entity. Although this level of detail is sound, it requires considerable human effort and substantial cost to annotate opinions in datasets for training models, especially across diverse domains and real-world applications. We explore the feasibility of LLMs as automatic annotators for fine-grained opinion analysis, addressing the shortage of domain-specific labelled datasets. In this work, we use a declarative annotation pipeline. This approach reduces the variability of manual prompt engineering when using LLMs to identify fine-grained opinion spans in text. We also present a novel methodology for an LLM to adjudicate multiple labels and produce final annotations. After trialling the pipeline with models of different sizes for the Aspect Sentiment Triplet Extraction (ASTE) and Aspect-Category-Opinion-Sentiment (ACOS) analysis tasks, we show that LLMs can serve as automatic annotators and adjudicators, achieving high Inter-Annotator Agreement across individual LLM-based annotators. This reduces the cost and human effort needed to create these fine-grained opinion-annotated datasets."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-23T14:52:56Z",
                "published_parsed": [
                    2026,
                    1,
                    23,
                    14,
                    52,
                    56,
                    4,
                    23,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Gaurav Negi"
                    },
                    {
                        "name": "MA Waskow"
                    },
                    {
                        "name": "John McCrae"
                    },
                    {
                        "name": "Paul Buitelaar"
                    }
                ],
                "author_detail": {
                    "name": "Paul Buitelaar"
                },
                "author": "Paul Buitelaar"
            },
            {
                "id": "http://arxiv.org/abs/2602.16338v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16338v1",
                "title": "push0: Scalable and Fault-Tolerant Orchestration for Zero-Knowledge Proof Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "push0: Scalable and Fault-Tolerant Orchestration for Zero-Knowledge Proof Generation"
                },
                "updated": "2026-02-18T10:22:33Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    10,
                    22,
                    33,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16338v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16338v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Zero-knowledge proof generation imposes stringent timing and reliability constraints on blockchain systems. For ZK-rollups, delayed proofs cause finality lag and economic loss; for Ethereum's emerging L1 zkEVM, proofs must complete within the 12-second slot window to enable stateless validation. The Ethereum Foundation's Ethproofs initiative coordinates multiple independent zkVMs across proving clusters to achieve real-time block proving, yet no principled orchestration framework addresses the joint challenges of (i) strict head-of-chain ordering, (ii) sub-slot latency bounds, (iii) fault-tolerant task reassignment, and (iv) prover-agnostic workflow composition. We present push0, a cloud-native proof orchestration system that decouples prover binaries from scheduling infrastructure. push0 employs an event-driven dispatcher--collector architecture over persistent priority queues, enforcing block-sequential proving while exploiting intra-block parallelism. We formalize requirements drawn from production ZK-rollup operations and the Ethereum real-time proving specification, then demonstrate via production Kubernetes cluster experiments that push0 achieves 5 ms median orchestration overhead with 99--100% scaling efficiency at 32 dispatchers for realistic workloads--overhead negligible (less than 0.1%) relative to typical proof computation times of 7+ seconds. Controlled Docker experiments validate these results, showing comparable performance (3--10 ms P50) when network variance is eliminated. Production deployment on the Zircuit zkrollup (14+ million mainnet blocks since March 2025) provides ecological validity for these controlled experiments. Our design enables seamless integration of heterogeneous zkVMs, supports automatic task recovery via message persistence, and provides the scheduling primitives necessary for both centralized rollup operators and decentralized multi-prover networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-knowledge proof generation imposes stringent timing and reliability constraints on blockchain systems. For ZK-rollups, delayed proofs cause finality lag and economic loss; for Ethereum's emerging L1 zkEVM, proofs must complete within the 12-second slot window to enable stateless validation. The Ethereum Foundation's Ethproofs initiative coordinates multiple independent zkVMs across proving clusters to achieve real-time block proving, yet no principled orchestration framework addresses the joint challenges of (i) strict head-of-chain ordering, (ii) sub-slot latency bounds, (iii) fault-tolerant task reassignment, and (iv) prover-agnostic workflow composition. We present push0, a cloud-native proof orchestration system that decouples prover binaries from scheduling infrastructure. push0 employs an event-driven dispatcher--collector architecture over persistent priority queues, enforcing block-sequential proving while exploiting intra-block parallelism. We formalize requirements drawn from production ZK-rollup operations and the Ethereum real-time proving specification, then demonstrate via production Kubernetes cluster experiments that push0 achieves 5 ms median orchestration overhead with 99--100% scaling efficiency at 32 dispatchers for realistic workloads--overhead negligible (less than 0.1%) relative to typical proof computation times of 7+ seconds. Controlled Docker experiments validate these results, showing comparable performance (3--10 ms P50) when network variance is eliminated. Production deployment on the Zircuit zkrollup (14+ million mainnet blocks since March 2025) provides ecological validity for these controlled experiments. Our design enables seamless integration of heterogeneous zkVMs, supports automatic task recovery via message persistence, and provides the scheduling primitives necessary for both centralized rollup operators and decentralized multi-prover networks."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T10:22:33Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    10,
                    22,
                    33,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Mohsen Ahmadvand"
                    },
                    {
                        "name": "Rok Pajnič"
                    },
                    {
                        "name": "Ching-Lun Chiu"
                    }
                ],
                "author_detail": {
                    "name": "Ching-Lun Chiu"
                },
                "author": "Ching-Lun Chiu"
            },
            {
                "id": "http://arxiv.org/abs/2602.16320v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16320v1",
                "title": "RefineFormer3D: Efficient 3D Medical Image Segmentation via Adaptive Multi-Scale Transformer with Cross Attention Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RefineFormer3D: Efficient 3D Medical Image Segmentation via Adaptive Multi-Scale Transformer with Cross Attention Fusion"
                },
                "updated": "2026-02-18T09:58:59Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    9,
                    58,
                    59,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16320v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16320v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Accurate and computationally efficient 3D medical image segmentation remains a critical challenge in clinical workflows. Transformer-based architectures often demonstrate superior global contextual modeling but at the expense of excessive parameter counts and memory demands, restricting their clinical deployment. We propose RefineFormer3D, a lightweight hierarchical transformer architecture that balances segmentation accuracy and computational efficiency for volumetric medical imaging. The architecture integrates three key components: (i) GhostConv3D-based patch embedding for efficient feature extraction with minimal redundancy, (ii) MixFFN3D module with low-rank projections and depthwise convolutions for parameter-efficient feature extraction, and (iii) a cross-attention fusion decoder enabling adaptive multi-scale skip connection integration. RefineFormer3D contains only 2.94M parameters, substantially fewer than contemporary transformer-based methods. Extensive experiments on ACDC and BraTS benchmarks demonstrate that RefineFormer3D achieves 93.44\\% and 85.9\\% average Dice scores respectively, outperforming or matching state-of-the-art methods while requiring significantly fewer parameters. Furthermore, the model achieves fast inference (8.35 ms per volume on GPU) with low memory requirements, supporting deployment in resource-constrained clinical environments. These results establish RefineFormer3D as an effective and scalable solution for practical 3D medical image segmentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate and computationally efficient 3D medical image segmentation remains a critical challenge in clinical workflows. Transformer-based architectures often demonstrate superior global contextual modeling but at the expense of excessive parameter counts and memory demands, restricting their clinical deployment. We propose RefineFormer3D, a lightweight hierarchical transformer architecture that balances segmentation accuracy and computational efficiency for volumetric medical imaging. The architecture integrates three key components: (i) GhostConv3D-based patch embedding for efficient feature extraction with minimal redundancy, (ii) MixFFN3D module with low-rank projections and depthwise convolutions for parameter-efficient feature extraction, and (iii) a cross-attention fusion decoder enabling adaptive multi-scale skip connection integration. RefineFormer3D contains only 2.94M parameters, substantially fewer than contemporary transformer-based methods. Extensive experiments on ACDC and BraTS benchmarks demonstrate that RefineFormer3D achieves 93.44\\% and 85.9\\% average Dice scores respectively, outperforming or matching state-of-the-art methods while requiring significantly fewer parameters. Furthermore, the model achieves fast inference (8.35 ms per volume on GPU) with low memory requirements, supporting deployment in resource-constrained clinical environments. These results establish RefineFormer3D as an effective and scalable solution for practical 3D medical image segmentation."
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T09:58:59Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    9,
                    58,
                    59,
                    2,
                    49,
                    0
                ],
                "arxiv_comment": "13 pages, 5 figures, 7 tables",
                "arxiv_primary_category": {
                    "term": "eess.IV"
                },
                "authors": [
                    {
                        "name": "Kavyansh Tyagi"
                    },
                    {
                        "name": "Vishwas Rathi"
                    },
                    {
                        "name": "Puneet Goyal"
                    }
                ],
                "author_detail": {
                    "name": "Puneet Goyal"
                },
                "author": "Puneet Goyal"
            },
            {
                "id": "http://arxiv.org/abs/2601.20568v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.20568v2",
                "title": "Reinforcement Unlearning via Group Relative Policy Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Unlearning via Group Relative Policy Optimization"
                },
                "updated": "2026-02-18T09:58:17Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    9,
                    58,
                    17,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.20568v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.20568v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "During pretraining, LLMs inadvertently memorize sensitive or copyrighted data, posing significant compliance challenges under legal frameworks like the GDPR and the EU AI Act. Fulfilling these mandates demands techniques that can remove information from a deployed model without retraining from scratch. Existing unlearning approaches attempt to address this need, but often leak the very data they aim to erase, sacrifice fluency and robustness, or depend on costly external reward models. We introduce PURGE (Policy Unlearning through Relative Group Erasure), a novel method grounded in the Group Relative Policy Optimization framework that formulates unlearning as a verifiable problem. PURGE uses an intrinsic reward signal that penalizes any mention of forbidden concepts, allowing safe and consistent unlearning. Our approach achieves up to x46 lower token usage per target than state-of-the-art methods, while improving fluency by +5.48% and adversarial robustness by +12.02% over the base model. Extensive evaluation on the Real World Knowledge Unlearning (RWKU) benchmark shows that PURGE reaches 11% unlearning effectiveness while preserving 98% of original utility. PURGE shows that framing LLM unlearning as a verifiable task enables more reliable, efficient, and scalable forgetting, suggesting a promising new direction for unlearning research that combines theoretical guarantees, improved safety, and practical deployment efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "During pretraining, LLMs inadvertently memorize sensitive or copyrighted data, posing significant compliance challenges under legal frameworks like the GDPR and the EU AI Act. Fulfilling these mandates demands techniques that can remove information from a deployed model without retraining from scratch. Existing unlearning approaches attempt to address this need, but often leak the very data they aim to erase, sacrifice fluency and robustness, or depend on costly external reward models. We introduce PURGE (Policy Unlearning through Relative Group Erasure), a novel method grounded in the Group Relative Policy Optimization framework that formulates unlearning as a verifiable problem. PURGE uses an intrinsic reward signal that penalizes any mention of forbidden concepts, allowing safe and consistent unlearning. Our approach achieves up to x46 lower token usage per target than state-of-the-art methods, while improving fluency by +5.48% and adversarial robustness by +12.02% over the base model. Extensive evaluation on the Real World Knowledge Unlearning (RWKU) benchmark shows that PURGE reaches 11% unlearning effectiveness while preserving 98% of original utility. PURGE shows that framing LLM unlearning as a verifiable task enables more reliable, efficient, and scalable forgetting, suggesting a promising new direction for unlearning research that combines theoretical guarantees, improved safety, and practical deployment efficiency."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-28T13:07:58Z",
                "published_parsed": [
                    2026,
                    1,
                    28,
                    13,
                    7,
                    58,
                    2,
                    28,
                    0
                ],
                "arxiv_comment": "Accepted to ICLR 2026",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Efstratios Zaradoukas"
                    },
                    {
                        "name": "Bardh Prenkaj"
                    },
                    {
                        "name": "Gjergji Kasneci"
                    }
                ],
                "author_detail": {
                    "name": "Gjergji Kasneci"
                },
                "author": "Gjergji Kasneci"
            },
            {
                "id": "http://arxiv.org/abs/2510.21190v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.21190v2",
                "title": "The Trojan Example: Jailbreaking LLMs through Template Filling and Unsafety Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Trojan Example: Jailbreaking LLMs through Template Filling and Unsafety Reasoning"
                },
                "updated": "2026-02-18T09:55:23Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    9,
                    55,
                    23,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.21190v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.21190v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As Large Language Models (LLMs) become integral to computing infrastructure, safety alignment serves as the primary security control preventing the generation of harmful payloads. However, this defense remains brittle. Existing jailbreak attacks typically bifurcate into white-box methods, which are inapplicable to commercial APIs due to lack of gradient access, and black-box optimization techniques, which often yield unnatural (e.g., syntactically rigid) or non-transferable (e.g., lacking cross-model generalization) prompts. In this work, we introduce TrojFill, a black-box exploitation framework that bypasses safety filters by targeting a fundamental logic flaw in current alignment paradigms: the decoupling of unsafety reasoning from content generation. TrojFill structurally reframes malicious instructions as a template-filling task required for safety analysis. By embedding obfuscated payloads (e.g., via placeholder substitution) into a \"Trojan\" structure, the attack induces the model to generate prohibited content as a \"demonstrative example\" ostensibly required for a subsequent sentence-by-sentence safety critique. This approach effectively masks the malicious intent from standard intent classifiers. We evaluate TrojFill against representative commercial systems, including GPT-4o, Gemini-2.5, DeepSeek-3.1, and Qwen-Max. Our results demonstrate that TrojFill achieves near-universal bypass rates: reaching 100% Attack Success Rate (ASR) on Gemini-flash-2.5 and DeepSeek-3.1, and 97% on GPT-4o, significantly outperforming existing black-box baselines. Furthermore, unlike optimization-based adversarial prompts, TrojFill generates highly interpretable and transferable attack vectors, exposing a systematic vulnerability inaligned LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) become integral to computing infrastructure, safety alignment serves as the primary security control preventing the generation of harmful payloads. However, this defense remains brittle. Existing jailbreak attacks typically bifurcate into white-box methods, which are inapplicable to commercial APIs due to lack of gradient access, and black-box optimization techniques, which often yield unnatural (e.g., syntactically rigid) or non-transferable (e.g., lacking cross-model generalization) prompts. In this work, we introduce TrojFill, a black-box exploitation framework that bypasses safety filters by targeting a fundamental logic flaw in current alignment paradigms: the decoupling of unsafety reasoning from content generation. TrojFill structurally reframes malicious instructions as a template-filling task required for safety analysis. By embedding obfuscated payloads (e.g., via placeholder substitution) into a \"Trojan\" structure, the attack induces the model to generate prohibited content as a \"demonstrative example\" ostensibly required for a subsequent sentence-by-sentence safety critique. This approach effectively masks the malicious intent from standard intent classifiers. We evaluate TrojFill against representative commercial systems, including GPT-4o, Gemini-2.5, DeepSeek-3.1, and Qwen-Max. Our results demonstrate that TrojFill achieves near-universal bypass rates: reaching 100% Attack Success Rate (ASR) on Gemini-flash-2.5 and DeepSeek-3.1, and 97% on GPT-4o, significantly outperforming existing black-box baselines. Furthermore, unlike optimization-based adversarial prompts, TrojFill generates highly interpretable and transferable attack vectors, exposing a systematic vulnerability inaligned LLMs."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-24T06:43:10Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    6,
                    43,
                    10,
                    4,
                    297,
                    0
                ],
                "arxiv_comment": "under review",
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Mingrui Liu"
                    },
                    {
                        "name": "Sixiao Zhang"
                    },
                    {
                        "name": "Cheng Long"
                    },
                    {
                        "name": "Kwok Yan Lam"
                    }
                ],
                "author_detail": {
                    "name": "Kwok Yan Lam"
                },
                "author": "Kwok Yan Lam"
            },
            {
                "id": "http://arxiv.org/abs/2602.02050v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.02050v2",
                "title": "Rethinking the Role of Entropy in Optimizing Tool-Use Behaviors for Large Language Model Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking the Role of Entropy in Optimizing Tool-Use Behaviors for Large Language Model Agents"
                },
                "updated": "2026-02-18T09:53:43Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    9,
                    53,
                    43,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.02050v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.02050v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Tool-using agents based on Large Language Models (LLMs) excel in tasks such as mathematical reasoning and multi-hop question answering. However, in long trajectories, agents often trigger excessive and low-quality tool calls, increasing latency and degrading inference performance, making managing tool-use behavior challenging. In this work, we conduct entropy-based pilot experiments and observe a strong positive correlation between entropy reduction and high-quality tool calls. Building on this finding, we propose using entropy reduction as a supervisory signal and design two reward strategies to address the differing needs of optimizing tool-use behavior. Sparse outcome rewards provide coarse, trajectory-level guidance to improve efficiency, while dense process rewards offer fine-grained supervision to enhance performance. Experiments across diverse domains show that both reward designs improve tool-use behavior: the former reduces tool calls by 72.07% compared to the average of baselines, while the latter improves performance by 22.27%. These results position entropy reduction as a key mechanism for enhancing tool-use behavior, enabling agents to be more adaptive in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool-using agents based on Large Language Models (LLMs) excel in tasks such as mathematical reasoning and multi-hop question answering. However, in long trajectories, agents often trigger excessive and low-quality tool calls, increasing latency and degrading inference performance, making managing tool-use behavior challenging. In this work, we conduct entropy-based pilot experiments and observe a strong positive correlation between entropy reduction and high-quality tool calls. Building on this finding, we propose using entropy reduction as a supervisory signal and design two reward strategies to address the differing needs of optimizing tool-use behavior. Sparse outcome rewards provide coarse, trajectory-level guidance to improve efficiency, while dense process rewards offer fine-grained supervision to enhance performance. Experiments across diverse domains show that both reward designs improve tool-use behavior: the former reduces tool calls by 72.07% compared to the average of baselines, while the latter improves performance by 22.27%. These results position entropy reduction as a key mechanism for enhancing tool-use behavior, enabling agents to be more adaptive in real-world applications."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-02T12:52:14Z",
                "published_parsed": [
                    2026,
                    2,
                    2,
                    12,
                    52,
                    14,
                    0,
                    33,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Zeping Li"
                    },
                    {
                        "name": "Hongru Wang"
                    },
                    {
                        "name": "Yiwen Zhao"
                    },
                    {
                        "name": "Guanhua Chen"
                    },
                    {
                        "name": "Yixia Li"
                    },
                    {
                        "name": "Keyang Chen"
                    },
                    {
                        "name": "Yixin Cao"
                    },
                    {
                        "name": "Guangnan Ye"
                    },
                    {
                        "name": "Hongfeng Chai"
                    },
                    {
                        "name": "Zhenfei Yin"
                    }
                ],
                "author_detail": {
                    "name": "Zhenfei Yin"
                },
                "author": "Zhenfei Yin"
            },
            {
                "id": "http://arxiv.org/abs/2509.23863v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.23863v3",
                "title": "SPELL: Self-Play Reinforcement Learning for Evolving Long-Context Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPELL: Self-Play Reinforcement Learning for Evolving Long-Context Language Models"
                },
                "updated": "2026-02-18T09:50:21Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    9,
                    50,
                    21,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.23863v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.23863v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Progress in long-context reasoning for large language models (LLMs) has lagged behind other recent advances. This gap arises not only from the intrinsic difficulty of processing long texts, but also from the scarcity of reliable human annotations and programmatically verifiable reward signals. In this paper, we propose SPELL, a multi-role self-play reinforcement learning framework that enables scalable, label-free optimization for long-context reasoning. SPELL integrates three cyclical roles-questioner, responder, and verifier-within a single model to enable continual self-improvement. The questioner generates questions from raw documents paired with reference answers; the responder learns to solve these questions based on the documents; and the verifier evaluates semantic equivalence between the responder's output and the questioner's reference answer, producing reward signals to guide continual training. To stabilize training, we introduce an automated curriculum that gradually increases document length and a reward function that adapts question difficulty to the model's evolving capabilities. Extensive experiments on six long-context benchmarks show that SPELL consistently improves performance across diverse LLMs and outperforms equally sized models fine-tuned on large-scale annotated data. Notably, SPELL achieves an average 7.6-point gain in pass@8 on the strong reasoning model Qwen3-30B-A3B-Thinking, raising its performance ceiling and showing promise for scaling to even more capable models. Our code is available at https://github.com/Tongyi-Zhiwen/Qwen-Doc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Progress in long-context reasoning for large language models (LLMs) has lagged behind other recent advances. This gap arises not only from the intrinsic difficulty of processing long texts, but also from the scarcity of reliable human annotations and programmatically verifiable reward signals. In this paper, we propose SPELL, a multi-role self-play reinforcement learning framework that enables scalable, label-free optimization for long-context reasoning. SPELL integrates three cyclical roles-questioner, responder, and verifier-within a single model to enable continual self-improvement. The questioner generates questions from raw documents paired with reference answers; the responder learns to solve these questions based on the documents; and the verifier evaluates semantic equivalence between the responder's output and the questioner's reference answer, producing reward signals to guide continual training. To stabilize training, we introduce an automated curriculum that gradually increases document length and a reward function that adapts question difficulty to the model's evolving capabilities. Extensive experiments on six long-context benchmarks show that SPELL consistently improves performance across diverse LLMs and outperforms equally sized models fine-tuned on large-scale annotated data. Notably, SPELL achieves an average 7.6-point gain in pass@8 on the strong reasoning model Qwen3-30B-A3B-Thinking, raising its performance ceiling and showing promise for scaling to even more capable models. Our code is available at https://github.com/Tongyi-Zhiwen/Qwen-Doc."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-28T13:08:10Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    13,
                    8,
                    10,
                    6,
                    271,
                    0
                ],
                "arxiv_comment": "Accepted to ICLR 2026",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Ziyi Yang"
                    },
                    {
                        "name": "Weizhou Shen"
                    },
                    {
                        "name": "Chenliang Li"
                    },
                    {
                        "name": "Ruijun Chen"
                    },
                    {
                        "name": "Fanqi Wan"
                    },
                    {
                        "name": "Ming Yan"
                    },
                    {
                        "name": "Xiaojun Quan"
                    },
                    {
                        "name": "Fei Huang"
                    }
                ],
                "author_detail": {
                    "name": "Fei Huang"
                },
                "author": "Fei Huang"
            },
            {
                "id": "http://arxiv.org/abs/2509.00454v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.00454v2",
                "title": "Universal Properties of Activation Sparsity in Modern Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Universal Properties of Activation Sparsity in Modern Large Language Models"
                },
                "updated": "2026-02-18T09:50:19Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    9,
                    50,
                    19,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.00454v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.00454v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Activation sparsity is an intriguing property of deep neural networks that has been extensively studied in ReLU-based models, due to its advantages for efficiency, robustness, and interpretability. However, methods relying on exact zero activations do not directly apply to modern Large Language Models (LLMs), leading to fragmented, model-specific strategies for LLM activation sparsity and a gap in its general understanding. In this work, we introduce a general framework for evaluating sparsity robustness in contemporary LLMs and conduct a systematic investigation of this phenomenon in their feedforward~(FFN) layers. Our results uncover universal properties of activation sparsity across diverse model families and scales. Importantly, we observe that the potential for effective activation sparsity grows with model size, highlighting its increasing relevance as models scale. Furthermore, we present the first study of activation sparsity in diffusion-based LLMs. Overall, our work provides a comprehensive perspective and practical guidance for harnessing activation sparsity in LLM design and acceleration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation sparsity is an intriguing property of deep neural networks that has been extensively studied in ReLU-based models, due to its advantages for efficiency, robustness, and interpretability. However, methods relying on exact zero activations do not directly apply to modern Large Language Models (LLMs), leading to fragmented, model-specific strategies for LLM activation sparsity and a gap in its general understanding. In this work, we introduce a general framework for evaluating sparsity robustness in contemporary LLMs and conduct a systematic investigation of this phenomenon in their feedforward~(FFN) layers. Our results uncover universal properties of activation sparsity across diverse model families and scales. Importantly, we observe that the potential for effective activation sparsity grows with model size, highlighting its increasing relevance as models scale. Furthermore, we present the first study of activation sparsity in diffusion-based LLMs. Overall, our work provides a comprehensive perspective and practical guidance for harnessing activation sparsity in LLM design and acceleration."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-30T10:47:21Z",
                "published_parsed": [
                    2025,
                    8,
                    30,
                    10,
                    47,
                    21,
                    5,
                    242,
                    0
                ],
                "arxiv_comment": "ICLR 2026, main track",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Filip Szatkowski"
                    },
                    {
                        "name": "Patryk Będkowski"
                    },
                    {
                        "name": "Alessio Devoto"
                    },
                    {
                        "name": "Jan Dubiński"
                    },
                    {
                        "name": "Pasquale Minervini"
                    },
                    {
                        "name": "Mikołaj Piórczyński"
                    },
                    {
                        "name": "Simone Scardapane"
                    },
                    {
                        "name": "Bartosz Wójcik"
                    }
                ],
                "author_detail": {
                    "name": "Bartosz Wójcik"
                },
                "author": "Bartosz Wójcik"
            },
            {
                "id": "http://arxiv.org/abs/2602.16304v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16304v1",
                "title": "Mind the Gap: Evaluating LLMs for High-Level Malicious Package Detection vs. Fine-Grained Indicator Identification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mind the Gap: Evaluating LLMs for High-Level Malicious Package Detection vs. Fine-Grained Indicator Identification"
                },
                "updated": "2026-02-18T09:36:46Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    9,
                    36,
                    46,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16304v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The prevalence of malicious packages in open-source repositories, such as PyPI, poses a critical threat to the software supply chain. While Large Language Models (LLMs) have emerged as a promising tool for automated security tasks, their effectiveness in detecting malicious packages and indicators remains underexplored. This paper presents a systematic evaluation of 13 LLMs for detecting malicious software packages. Using a curated dataset of 4,070 packages (3,700 benign and 370 malicious), we evaluate model performance across two tasks: binary classification (package detection) and multi-label classification (identification of specific malicious indicators). We further investigate the impact of prompting strategies, temperature settings, and model specifications on detection accuracy. We find a significant \"granularity gap\" in LLMs' capabilities. While GPT-4.1 achieves near-perfect performance in binary detection (F1 $\\approx$ 0.99), performance degrades by approximately 41\\% when the task shifts to identifying specific malicious indicators. We observe that general models are best for filtering out the majority of threats, while specialized coder models are better at detecting attacks that follow a strict, predictable code structure. Our correlation analysis indicates that parameter size and context width have negligible explanatory power regarding detection accuracy. We conclude that while LLMs are powerful detectors at the package level, they lack the semantic depth required for precise identification at the granular indicator level.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The prevalence of malicious packages in open-source repositories, such as PyPI, poses a critical threat to the software supply chain. While Large Language Models (LLMs) have emerged as a promising tool for automated security tasks, their effectiveness in detecting malicious packages and indicators remains underexplored. This paper presents a systematic evaluation of 13 LLMs for detecting malicious software packages. Using a curated dataset of 4,070 packages (3,700 benign and 370 malicious), we evaluate model performance across two tasks: binary classification (package detection) and multi-label classification (identification of specific malicious indicators). We further investigate the impact of prompting strategies, temperature settings, and model specifications on detection accuracy. We find a significant \"granularity gap\" in LLMs' capabilities. While GPT-4.1 achieves near-perfect performance in binary detection (F1 $\\approx$ 0.99), performance degrades by approximately 41\\% when the task shifts to identifying specific malicious indicators. We observe that general models are best for filtering out the majority of threats, while specialized coder models are better at detecting attacks that follow a strict, predictable code structure. Our correlation analysis indicates that parameter size and context width have negligible explanatory power regarding detection accuracy. We conclude that while LLMs are powerful detectors at the package level, they lack the semantic depth required for precise identification at the granular indicator level."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T09:36:46Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    9,
                    36,
                    46,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Ahmed Ryan"
                    },
                    {
                        "name": "Ibrahim Khalil"
                    },
                    {
                        "name": "Abdullah Al Jahid"
                    },
                    {
                        "name": "Md Erfan"
                    },
                    {
                        "name": "Akond Ashfaque Ur Rahman"
                    },
                    {
                        "name": "Md Rayhanur Rahman"
                    }
                ],
                "author_detail": {
                    "name": "Md Rayhanur Rahman"
                },
                "author": "Md Rayhanur Rahman"
            },
            {
                "id": "http://arxiv.org/abs/2602.15038v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.15038v2",
                "title": "Indic-TunedLens: Interpreting Multilingual Models in Indian Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Indic-TunedLens: Interpreting Multilingual Models in Indian Languages"
                },
                "updated": "2026-02-18T09:34:02Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    9,
                    34,
                    2,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.15038v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.15038v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Multilingual large language models (LLMs) are increasingly deployed in linguistically diverse regions like India, yet most interpretability tools remain tailored to English. Prior work reveals that LLMs often operate in English centric representation spaces, making cross lingual interpretability a pressing concern. We introduce Indic-TunedLens, a novel interpretability framework specifically for Indian languages that learns shared affine transformations. Unlike the standard Logit Lens, which directly decodes intermediate activations, Indic-TunedLens adjusts hidden states for each target language, aligning them with the target output distributions to enable more faithful decoding of model representations. We evaluate our framework on 10 Indian languages using the MMLU benchmark and find that it significantly improves over SOTA interpretability methods, especially for morphologically rich, low resource languages. Our results provide crucial insights into the layer-wise semantic encoding of multilingual transformers. Our model is available at https://huggingface.co/spaces/MihirRajeshPanchal/IndicTunedLens. Our code is available at https://github.com/MihirRajeshPanchal/IndicTunedLens.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual large language models (LLMs) are increasingly deployed in linguistically diverse regions like India, yet most interpretability tools remain tailored to English. Prior work reveals that LLMs often operate in English centric representation spaces, making cross lingual interpretability a pressing concern. We introduce Indic-TunedLens, a novel interpretability framework specifically for Indian languages that learns shared affine transformations. Unlike the standard Logit Lens, which directly decodes intermediate activations, Indic-TunedLens adjusts hidden states for each target language, aligning them with the target output distributions to enable more faithful decoding of model representations. We evaluate our framework on 10 Indian languages using the MMLU benchmark and find that it significantly improves over SOTA interpretability methods, especially for morphologically rich, low resource languages. Our results provide crucial insights into the layer-wise semantic encoding of multilingual transformers. Our model is available at https://huggingface.co/spaces/MihirRajeshPanchal/IndicTunedLens. Our code is available at https://github.com/MihirRajeshPanchal/IndicTunedLens."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-29T15:08:40Z",
                "published_parsed": [
                    2026,
                    1,
                    29,
                    15,
                    8,
                    40,
                    3,
                    29,
                    0
                ],
                "arxiv_comment": "19th Conference of the European Chapter of the Association for Computational Linguistics (EACL) Thirteenth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial) 2026",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Mihir Panchal"
                    },
                    {
                        "name": "Deeksha Varshney"
                    },
                    {
                        "name": "Mamta"
                    },
                    {
                        "name": "Asif Ekbal"
                    }
                ],
                "author_detail": {
                    "name": "Asif Ekbal"
                },
                "author": "Asif Ekbal"
            },
            {
                "id": "http://arxiv.org/abs/2602.16298v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16298v1",
                "title": "MultiCW: A Large-Scale Balanced Benchmark Dataset for Training Robust Check-Worthiness Detection Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MultiCW: A Large-Scale Balanced Benchmark Dataset for Training Robust Check-Worthiness Detection Models"
                },
                "updated": "2026-02-18T09:28:53Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    9,
                    28,
                    53,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16298v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16298v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) are beginning to reshape how media professionals verify information, yet automated support for detecting check-worthy claims a key step in the fact-checking process remains limited. We introduce the Multi-Check-Worthy (MultiCW) dataset, a balanced multilingual benchmark for check-worthy claim detection spanning 16 languages, 7 topical domains, and 2 writing styles. It consists of 123,722 samples, evenly distributed between noisy (informal) and structured (formal) texts, with balanced representation of check-worthy and non-check-worthy classes across all languages. To probe robustness, we also introduce an equally balanced out-of-distribution evaluation set of 27,761 samples in 4 additional languages. To provide baselines, we benchmark 3 common fine-tuned multilingual transformers against a diverse set of 15 commercial and open LLMs under zero-shot settings. Our findings show that fine-tuned models consistently outperform zero-shot LLMs on claim classification and show strong out-of-distribution generalization across languages, domains, and styles. MultiCW provides a rigorous multilingual resource for advancing automated fact-checking and enables systematic comparisons between fine-tuned models and cutting-edge LLMs on the check-worthy claim detection task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are beginning to reshape how media professionals verify information, yet automated support for detecting check-worthy claims a key step in the fact-checking process remains limited. We introduce the Multi-Check-Worthy (MultiCW) dataset, a balanced multilingual benchmark for check-worthy claim detection spanning 16 languages, 7 topical domains, and 2 writing styles. It consists of 123,722 samples, evenly distributed between noisy (informal) and structured (formal) texts, with balanced representation of check-worthy and non-check-worthy classes across all languages. To probe robustness, we also introduce an equally balanced out-of-distribution evaluation set of 27,761 samples in 4 additional languages. To provide baselines, we benchmark 3 common fine-tuned multilingual transformers against a diverse set of 15 commercial and open LLMs under zero-shot settings. Our findings show that fine-tuned models consistently outperform zero-shot LLMs on claim classification and show strong out-of-distribution generalization across languages, domains, and styles. MultiCW provides a rigorous multilingual resource for advancing automated fact-checking and enables systematic comparisons between fine-tuned models and cutting-edge LLMs on the check-worthy claim detection task."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T09:28:53Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    9,
                    28,
                    53,
                    2,
                    49,
                    0
                ],
                "arxiv_comment": "18 pages, 8 figures, 19 tables, EACL-2026",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Martin Hyben"
                    },
                    {
                        "name": "Sebastian Kula"
                    },
                    {
                        "name": "Jan Cegin"
                    },
                    {
                        "name": "Jakub Simko"
                    },
                    {
                        "name": "Ivan Srba"
                    },
                    {
                        "name": "Robert Moro"
                    }
                ],
                "author_detail": {
                    "name": "Robert Moro"
                },
                "author": "Robert Moro"
            },
            {
                "id": "http://arxiv.org/abs/2602.16290v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16290v1",
                "title": "Aladdin-FTI @ AMIYA Three Wishes for Arabic NLP: Fidelity, Diglossia, and Multidialectal Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aladdin-FTI @ AMIYA Three Wishes for Arabic NLP: Fidelity, Diglossia, and Multidialectal Generation"
                },
                "updated": "2026-02-18T09:15:20Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    9,
                    15,
                    20,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16290v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Arabic dialects have long been under-represented in Natural Language Processing (NLP) research due to their non-standardization and high variability, which pose challenges for computational modeling. Recent advances in the field, such as Large Language Models (LLMs), offer promising avenues to address this gap by enabling Arabic to be modeled as a pluricentric language rather than a monolithic system. This paper presents Aladdin-FTI, our submission to the AMIYA shared task. The proposed system is designed to both generate and translate dialectal Arabic (DA). Specifically, the model supports text generation in Moroccan, Egyptian, Palestinian, Syrian, and Saudi dialects, as well as bidirectional translation between these dialects, Modern Standard Arabic (MSA), and English. The code and trained model are publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arabic dialects have long been under-represented in Natural Language Processing (NLP) research due to their non-standardization and high variability, which pose challenges for computational modeling. Recent advances in the field, such as Large Language Models (LLMs), offer promising avenues to address this gap by enabling Arabic to be modeled as a pluricentric language rather than a monolithic system. This paper presents Aladdin-FTI, our submission to the AMIYA shared task. The proposed system is designed to both generate and translate dialectal Arabic (DA). Specifically, the model supports text generation in Moroccan, Egyptian, Palestinian, Syrian, and Saudi dialects, as well as bidirectional translation between these dialects, Modern Standard Arabic (MSA), and English. The code and trained model are publicly available."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T09:15:20Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    9,
                    15,
                    20,
                    2,
                    49,
                    0
                ],
                "arxiv_comment": "13 pages, Paper submitted to the AMIYA shared task at the VarDial workshop, co-located with EACL 2026",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Jonathan Mutal"
                    },
                    {
                        "name": "Perla Al Almaoui"
                    },
                    {
                        "name": "Simon Hengchen"
                    },
                    {
                        "name": "Pierrette Bouillon"
                    }
                ],
                "author_detail": {
                    "name": "Pierrette Bouillon"
                },
                "author": "Pierrette Bouillon"
            },
            {
                "id": "http://arxiv.org/abs/2601.18902v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.18902v2",
                "title": "Flatter Tokens are More Valuable for Speculative Draft Model Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flatter Tokens are More Valuable for Speculative Draft Model Training"
                },
                "updated": "2026-02-18T08:50:51Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    8,
                    50,
                    51,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.18902v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.18902v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Speculative Decoding (SD) is a key technique for accelerating Large Language Model (LLM) inference, but it typically requires training a draft model on a large dataset. We approach this problem from a data-centric perspective, finding that not all training samples contribute equally to the SD acceptance rate. Specifically, our theoretical analysis and empirical validation reveals that tokens inducing flatter predictive distributions from the target model are more valuable than those yielding sharply peaked distributions. Based on this insight, we propose flatness, a new metric to quantify this property, and develop the Sample-level-flatness-based Dataset Distillation (SFDD) approach, which filters the training data to retain only the most valuable samples. Experiments on the EAGLE framework demonstrate that SFDD can achieve over 2$\\times$ training speedup using only 50% of the data, while keeping the final model's inference speedup within 4% of the full-dataset baseline. This work introduces an effective, data-centric approach that substantially improves the training efficiency for Speculative Decoding. Our code is available at https://github.com/fjm9933/Flatness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative Decoding (SD) is a key technique for accelerating Large Language Model (LLM) inference, but it typically requires training a draft model on a large dataset. We approach this problem from a data-centric perspective, finding that not all training samples contribute equally to the SD acceptance rate. Specifically, our theoretical analysis and empirical validation reveals that tokens inducing flatter predictive distributions from the target model are more valuable than those yielding sharply peaked distributions. Based on this insight, we propose flatness, a new metric to quantify this property, and develop the Sample-level-flatness-based Dataset Distillation (SFDD) approach, which filters the training data to retain only the most valuable samples. Experiments on the EAGLE framework demonstrate that SFDD can achieve over 2$\\times$ training speedup using only 50% of the data, while keeping the final model's inference speedup within 4% of the full-dataset baseline. This work introduces an effective, data-centric approach that substantially improves the training efficiency for Speculative Decoding. Our code is available at https://github.com/fjm9933/Flatness."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-26T19:13:22Z",
                "published_parsed": [
                    2026,
                    1,
                    26,
                    19,
                    13,
                    22,
                    0,
                    26,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Jiaming Fan"
                    },
                    {
                        "name": "Daming Cao"
                    },
                    {
                        "name": "Xiangzhong Luo"
                    },
                    {
                        "name": "Jiale Fu"
                    },
                    {
                        "name": "Chonghan Liu"
                    },
                    {
                        "name": "Xu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xu Yang"
                },
                "author": "Xu Yang"
            },
            {
                "id": "http://arxiv.org/abs/2411.04760v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2411.04760v3",
                "title": "Zero-Shot Temporal Resolution Domain Adaptation for Spiking Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot Temporal Resolution Domain Adaptation for Spiking Neural Networks"
                },
                "updated": "2026-02-18T08:38:39Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    8,
                    38,
                    39,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2411.04760v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2411.04760v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1016/j.neunet.2025.108483",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Spiking Neural Networks (SNNs) are biologically-inspired deep neural networks that efficiently extract temporal information while offering promising gains in terms of energy efficiency and latency when deployed on neuromorphic devices. SNN parameters are sensitive to temporal resolution, leading to significant performance drops when the temporal resolution of target data during deployment is not the same as that of the source data used for training, especially when fine-tuning with the target data is not possible during deployment. To address this challenge, we propose three novel domain adaptation methods for adapting neuron parameters to account for the change in time resolution without re-training on target time resolution. The proposed methods are based on a mapping between neuron dynamics in SNNs and State Space Models (SSMs) and are applicable to general neuron models. We evaluate the proposed methods under spatio-temporal data tasks, namely the audio keyword spotting datasets SHD and MSWC, and the neuromorphic image NMINST dataset. Our methods provide an alternative to-and in most cases significantly outperform-the existing reference method that consists of scaling only the time constant. Notably, when the temporal resolution of the target data is double that of the source data, applying one of our proposed methods instead of the benchmark achieves classification accuracy of 89.5% instead of 53.0% on SHD, 93.6% instead of 38.8% on MSWC and 98.5% instead of 97.2% aon NMNIST. Moreover, our results show that high accuracy on high temporal resolution data can be obtained by time-efficient training on lower temporal resolution data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiking Neural Networks (SNNs) are biologically-inspired deep neural networks that efficiently extract temporal information while offering promising gains in terms of energy efficiency and latency when deployed on neuromorphic devices. SNN parameters are sensitive to temporal resolution, leading to significant performance drops when the temporal resolution of target data during deployment is not the same as that of the source data used for training, especially when fine-tuning with the target data is not possible during deployment. To address this challenge, we propose three novel domain adaptation methods for adapting neuron parameters to account for the change in time resolution without re-training on target time resolution. The proposed methods are based on a mapping between neuron dynamics in SNNs and State Space Models (SSMs) and are applicable to general neuron models. We evaluate the proposed methods under spatio-temporal data tasks, namely the audio keyword spotting datasets SHD and MSWC, and the neuromorphic image NMINST dataset. Our methods provide an alternative to-and in most cases significantly outperform-the existing reference method that consists of scaling only the time constant. Notably, when the temporal resolution of the target data is double that of the source data, applying one of our proposed methods instead of the benchmark achieves classification accuracy of 89.5% instead of 53.0% on SHD, 93.6% instead of 38.8% on MSWC and 98.5% instead of 97.2% aon NMNIST. Moreover, our results show that high accuracy on high temporal resolution data can be obtained by time-efficient training on lower temporal resolution data."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-11-07T14:58:51Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    14,
                    58,
                    51,
                    3,
                    312,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "arxiv_journal_ref": "Neural Networks, 2025, 108483, ISSN 0893-6080",
                "authors": [
                    {
                        "name": "Sanja Karilanova"
                    },
                    {
                        "name": "Maxime Fabre"
                    },
                    {
                        "name": "Emre Neftci"
                    },
                    {
                        "name": "Ayça Özçelikkale"
                    }
                ],
                "author_detail": {
                    "name": "Ayça Özçelikkale"
                },
                "author": "Ayça Özçelikkale",
                "arxiv_doi": "10.1016/j.neunet.2025.108483"
            },
            {
                "id": "http://arxiv.org/abs/2512.18454v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.18454v2",
                "title": "Out-of-Distribution Detection in Molecular Complexes via Diffusion Models for Irregular Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Out-of-Distribution Detection in Molecular Complexes via Diffusion Models for Irregular Graphs"
                },
                "updated": "2026-02-18T08:23:03Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    8,
                    23,
                    3,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.18454v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.18454v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Predictive machine learning models generally excel on in-distribution data, but their performance degrades on out-of-distribution (OOD) inputs. Reliable deployment therefore requires robust OOD detection, yet this is particularly challenging for irregular 3D graphs that combine continuous geometry with categorical identities and are unordered by construction. Here, we present a probabilistic OOD detection framework for complex 3D graph data built on a diffusion model that learns a density of the training distribution in a fully unsupervised manner. A key ingredient we introduce is a unified continuous diffusion over both 3D coordinates and discrete features: categorical identities are embedded in a continuous space and trained with cross-entropy, while the corresponding diffusion score is obtained analytically via posterior-mean interpolation from predicted class probabilities. This yields a single self-consistent probability-flow ODE (PF-ODE) that produces per-sample log-likelihoods, providing a principled typicality score for distribution shift. We validate the approach on protein-ligand complexes and construct strict OOD datasets by withholding entire protein families from training. PF-ODE likelihoods identify held-out families as OOD and correlate strongly with prediction errors of an independent binding-affinity model (GEMS), enabling a priori reliability estimates on new complexes. Beyond scalar likelihoods, we show that multi-scale PF-ODE trajectory statistics - including path tortuosity, flow stiffness, and vector-field instability - provide complementary OOD information. Modeling the joint distribution of these trajectory features yields a practical, high-sensitivity detector that improves separation over likelihood-only baselines, offering a label-free OOD quantification workflow for geometric deep learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predictive machine learning models generally excel on in-distribution data, but their performance degrades on out-of-distribution (OOD) inputs. Reliable deployment therefore requires robust OOD detection, yet this is particularly challenging for irregular 3D graphs that combine continuous geometry with categorical identities and are unordered by construction. Here, we present a probabilistic OOD detection framework for complex 3D graph data built on a diffusion model that learns a density of the training distribution in a fully unsupervised manner. A key ingredient we introduce is a unified continuous diffusion over both 3D coordinates and discrete features: categorical identities are embedded in a continuous space and trained with cross-entropy, while the corresponding diffusion score is obtained analytically via posterior-mean interpolation from predicted class probabilities. This yields a single self-consistent probability-flow ODE (PF-ODE) that produces per-sample log-likelihoods, providing a principled typicality score for distribution shift. We validate the approach on protein-ligand complexes and construct strict OOD datasets by withholding entire protein families from training. PF-ODE likelihoods identify held-out families as OOD and correlate strongly with prediction errors of an independent binding-affinity model (GEMS), enabling a priori reliability estimates on new complexes. Beyond scalar likelihoods, we show that multi-scale PF-ODE trajectory statistics - including path tortuosity, flow stiffness, and vector-field instability - provide complementary OOD information. Modeling the joint distribution of these trajectory features yields a practical, high-sensitivity detector that improves separation over likelihood-only baselines, offering a label-free OOD quantification workflow for geometric deep learning."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-20T17:56:15Z",
                "published_parsed": [
                    2025,
                    12,
                    20,
                    17,
                    56,
                    15,
                    5,
                    354,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "David Graber"
                    },
                    {
                        "name": "Victor Armegioiu"
                    },
                    {
                        "name": "Rebecca Buller"
                    },
                    {
                        "name": "Siddhartha Mishra"
                    }
                ],
                "author_detail": {
                    "name": "Siddhartha Mishra"
                },
                "author": "Siddhartha Mishra"
            },
            {
                "id": "http://arxiv.org/abs/2503.16191v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2503.16191v2",
                "title": "Large Language Models for Water Distribution Systems Modeling and Decision-Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Water Distribution Systems Modeling and Decision-Making"
                },
                "updated": "2026-02-18T08:19:08Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    8,
                    19,
                    8,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2503.16191v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2503.16191v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1061/9780784486184.086",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "The integration of Large Language Models (LLMs) into engineering workflows presents new opportunities for making computational tools more accessible. Especially where such tools remain underutilized due to technical or expertise barriers, such as water distribution system (WDS) management. This study introduces LLM-EPANET, an agent-based framework that enables natural language interaction with EPANET, the benchmark WDS simulator. The framework combines retrieval-augmented generation and multi-agent orchestration to automatically translate user queries into executable code, run simulations, and return structured results. A curated set of 69 benchmark queries is introduced to evaluate performance across state-of-the-art LLMs. Results show that LLMs can effectively support a wide range of modeling tasks, achieving 56-81% accuracy overall, and over 90% for simpler queries. These findings highlight the potential of LLM-based modeling to democratize data-driven decision-making in the water sector through transparent, interactive AI interfaces. The framework code and benchmark queries are shared as an open resource: https://github.com/yinon-gold/LLMs-in-WDS-Modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) into engineering workflows presents new opportunities for making computational tools more accessible. Especially where such tools remain underutilized due to technical or expertise barriers, such as water distribution system (WDS) management. This study introduces LLM-EPANET, an agent-based framework that enables natural language interaction with EPANET, the benchmark WDS simulator. The framework combines retrieval-augmented generation and multi-agent orchestration to automatically translate user queries into executable code, run simulations, and return structured results. A curated set of 69 benchmark queries is introduced to evaluate performance across state-of-the-art LLMs. Results show that LLMs can effectively support a wide range of modeling tasks, achieving 56-81% accuracy overall, and over 90% for simpler queries. These findings highlight the potential of LLM-based modeling to democratize data-driven decision-making in the water sector through transparent, interactive AI interfaces. The framework code and benchmark queries are shared as an open resource: https://github.com/yinon-gold/LLMs-in-WDS-Modeling."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-03-20T14:39:11Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    14,
                    39,
                    11,
                    3,
                    79,
                    0
                ],
                "arxiv_comment": "Accepted to EWRI Congress 2025",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Yinon Goldshtein"
                    },
                    {
                        "name": "Gal Perelman"
                    },
                    {
                        "name": "Assaf Schuster"
                    },
                    {
                        "name": "Avi Ostfeld"
                    }
                ],
                "author_detail": {
                    "name": "Avi Ostfeld"
                },
                "author": "Avi Ostfeld",
                "arxiv_doi": "10.1061/9780784486184.086"
            },
            {
                "id": "http://arxiv.org/abs/2602.15485v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.15485v2",
                "title": "SecCodeBench-V2 Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SecCodeBench-V2 Technical Report"
                },
                "updated": "2026-02-18T08:08:18Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    8,
                    8,
                    18,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.15485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.15485v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce SecCodeBench-V2, a publicly released benchmark for evaluating Large Language Model (LLM) copilots' capabilities of generating secure code. SecCodeBench-V2 comprises 98 generation and fix scenarios derived from Alibaba Group's industrial productions, where the underlying security issues span 22 common CWE (Common Weakness Enumeration) categories across five programming languages: Java, C, Python, Go, and JavaScript. SecCodeBench-V2 adopts a function-level task formulation: each scenario provides a complete project scaffold and requires the model to implement or patch a designated target function under fixed interfaces and dependencies. For each scenario, SecCodeBench-V2 provides executable proof-of-concept (PoC) test cases for both functional validation and security verification. All test cases are authored and double-reviewed by security experts, ensuring high fidelity, broad coverage, and reliable ground truth. Beyond the benchmark itself, we build a unified evaluation pipeline that assesses models primarily via dynamic execution. For most scenarios, we compile and run model-generated artifacts in isolated environments and execute PoC test cases to validate both functional correctness and security properties. For scenarios where security issues cannot be adjudicated with deterministic test cases, we additionally employ an LLM-as-a-judge oracle. To summarize performance across heterogeneous scenarios and difficulty levels, we design a Pass@K-based scoring protocol with principled aggregation over scenarios and severity, enabling holistic and comparable evaluation across models. Overall, SecCodeBench-V2 provides a rigorous and reproducible foundation for assessing the security posture of AI coding assistants, with results and artifacts released at https://alibaba.github.io/sec-code-bench. The benchmark is publicly available at https://github.com/alibaba/sec-code-bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce SecCodeBench-V2, a publicly released benchmark for evaluating Large Language Model (LLM) copilots' capabilities of generating secure code. SecCodeBench-V2 comprises 98 generation and fix scenarios derived from Alibaba Group's industrial productions, where the underlying security issues span 22 common CWE (Common Weakness Enumeration) categories across five programming languages: Java, C, Python, Go, and JavaScript. SecCodeBench-V2 adopts a function-level task formulation: each scenario provides a complete project scaffold and requires the model to implement or patch a designated target function under fixed interfaces and dependencies. For each scenario, SecCodeBench-V2 provides executable proof-of-concept (PoC) test cases for both functional validation and security verification. All test cases are authored and double-reviewed by security experts, ensuring high fidelity, broad coverage, and reliable ground truth. Beyond the benchmark itself, we build a unified evaluation pipeline that assesses models primarily via dynamic execution. For most scenarios, we compile and run model-generated artifacts in isolated environments and execute PoC test cases to validate both functional correctness and security properties. For scenarios where security issues cannot be adjudicated with deterministic test cases, we additionally employ an LLM-as-a-judge oracle. To summarize performance across heterogeneous scenarios and difficulty levels, we design a Pass@K-based scoring protocol with principled aggregation over scenarios and severity, enabling holistic and comparable evaluation across models. Overall, SecCodeBench-V2 provides a rigorous and reproducible foundation for assessing the security posture of AI coding assistants, with results and artifacts released at https://alibaba.github.io/sec-code-bench. The benchmark is publicly available at https://github.com/alibaba/sec-code-bench."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-17T10:47:06Z",
                "published_parsed": [
                    2026,
                    2,
                    17,
                    10,
                    47,
                    6,
                    1,
                    48,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Longfei Chen"
                    },
                    {
                        "name": "Ji Zhao"
                    },
                    {
                        "name": "Lanxiao Cui"
                    },
                    {
                        "name": "Tong Su"
                    },
                    {
                        "name": "Xingbo Pan"
                    },
                    {
                        "name": "Ziyang Li"
                    },
                    {
                        "name": "Yongxing Wu"
                    },
                    {
                        "name": "Qijiang Cao"
                    },
                    {
                        "name": "Qiyao Cai"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Yuandong Ni"
                    },
                    {
                        "name": "Junyao He"
                    },
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Chao Ge"
                    },
                    {
                        "name": "Xuhuai Lu"
                    },
                    {
                        "name": "Zeyu Gao"
                    },
                    {
                        "name": "Yuxin Cui"
                    },
                    {
                        "name": "Weisen Chen"
                    },
                    {
                        "name": "Yuxuan Peng"
                    },
                    {
                        "name": "Shengping Wang"
                    },
                    {
                        "name": "Qi Li"
                    },
                    {
                        "name": "Yukai Huang"
                    },
                    {
                        "name": "Yukun Liu"
                    },
                    {
                        "name": "Tuo Zhou"
                    },
                    {
                        "name": "Terry Yue Zhuo"
                    },
                    {
                        "name": "Junyang Lin"
                    },
                    {
                        "name": "Chao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Zhang"
                },
                "author": "Chao Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2602.16253v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16253v1",
                "title": "How Much Does Machine Identity Matter in Anomalous Sound Detection at Test Time?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Much Does Machine Identity Matter in Anomalous Sound Detection at Test Time?"
                },
                "updated": "2026-02-18T08:02:47Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    8,
                    2,
                    47,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16253v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16253v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Anomalous sound detection (ASD) benchmarks typically assume that the identity of the monitored machine is known at test time and that recordings are evaluated in a machine-wise manner. However, in realistic monitoring scenarios with multiple known machines operating concurrently, test recordings may not be reliably attributable to a specific machine, and requiring machine identity imposes deployment constraints such as dedicated sensors per machine. To reveal performance degradations and method-specific differences in robustness that are hidden under standard machine-wise evaluation, we consider a minimal modification of the ASD evaluation protocol in which test recordings from multiple machines are merged and evaluated jointly without access to machine identity at inference time. Training data and evaluation metrics remain unchanged, and machine identity labels are used only for post hoc evaluation. Experiments with representative ASD methods show that relaxing this assumption reveals performance degradations and method-specific differences in robustness that are hidden under standard machine-wise evaluation, and that these degradations are strongly related to implicit machine identification accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anomalous sound detection (ASD) benchmarks typically assume that the identity of the monitored machine is known at test time and that recordings are evaluated in a machine-wise manner. However, in realistic monitoring scenarios with multiple known machines operating concurrently, test recordings may not be reliably attributable to a specific machine, and requiring machine identity imposes deployment constraints such as dedicated sensors per machine. To reveal performance degradations and method-specific differences in robustness that are hidden under standard machine-wise evaluation, we consider a minimal modification of the ASD evaluation protocol in which test recordings from multiple machines are merged and evaluated jointly without access to machine identity at inference time. Training data and evaluation metrics remain unchanged, and machine identity labels are used only for post hoc evaluation. Experiments with representative ASD methods show that relaxing this assumption reveals performance degradations and method-specific differences in robustness that are hidden under standard machine-wise evaluation, and that these degradations are strongly related to implicit machine identification accuracy."
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T08:02:47Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    8,
                    2,
                    47,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS"
                },
                "authors": [
                    {
                        "name": "Kevin Wilkinghoff"
                    },
                    {
                        "name": "Keisuke Imoto"
                    },
                    {
                        "name": "Zheng-Hua Tan"
                    }
                ],
                "author_detail": {
                    "name": "Zheng-Hua Tan"
                },
                "author": "Zheng-Hua Tan"
            },
            {
                "id": "http://arxiv.org/abs/2602.15195v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.15195v2",
                "title": "Weight space Detection of Backdoors in LoRA Adapters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weight space Detection of Backdoors in LoRA Adapters"
                },
                "updated": "2026-02-18T07:52:06Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    7,
                    52,
                    6,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.15195v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.15195v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "LoRA adapters let users fine-tune large language models (LLMs) efficiently. However, LoRA adapters are shared through open repositories like Hugging Face Hub \\citep{huggingface_hub_docs}, making them vulnerable to backdoor attacks. Current detection methods require running the model with test input data -- making them impractical for screening thousands of adapters where the trigger for backdoor behavior is unknown. We detect poisoned adapters by analyzing their weight matrices directly, without running the model -- making our method data-agnostic. Our method extracts simple statistics -- how concentrated the singular values are, their entropy, and the distribution shape -- and flags adapters that deviate from normal patterns. We evaluate the method on 500 LoRA adapters -- 400 clean, and 100 poisoned for Llama-3.2-3B on instruction and reasoning datasets: Alpaca, Dolly, GSM8K, ARC-Challenge, SQuADv2, NaturalQuestions, HumanEval, and GLUE dataset. We achieve 97\\% detection accuracy with less than 2\\% false positives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRA adapters let users fine-tune large language models (LLMs) efficiently. However, LoRA adapters are shared through open repositories like Hugging Face Hub \\citep{huggingface_hub_docs}, making them vulnerable to backdoor attacks. Current detection methods require running the model with test input data -- making them impractical for screening thousands of adapters where the trigger for backdoor behavior is unknown. We detect poisoned adapters by analyzing their weight matrices directly, without running the model -- making our method data-agnostic. Our method extracts simple statistics -- how concentrated the singular values are, their entropy, and the distribution shape -- and flags adapters that deviate from normal patterns. We evaluate the method on 500 LoRA adapters -- 400 clean, and 100 poisoned for Llama-3.2-3B on instruction and reasoning datasets: Alpaca, Dolly, GSM8K, ARC-Challenge, SQuADv2, NaturalQuestions, HumanEval, and GLUE dataset. We achieve 97\\% detection accuracy with less than 2\\% false positives."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-16T21:20:47Z",
                "published_parsed": [
                    2026,
                    2,
                    16,
                    21,
                    20,
                    47,
                    0,
                    47,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "David Puertolas Merenciano"
                    },
                    {
                        "name": "Ekaterina Vasyagina"
                    },
                    {
                        "name": "Raghav Dixit"
                    },
                    {
                        "name": "Kevin Zhu"
                    },
                    {
                        "name": "Ruizhe Li"
                    },
                    {
                        "name": "Javier Ferrando"
                    },
                    {
                        "name": "Maheep Chaudhary"
                    }
                ],
                "author_detail": {
                    "name": "Maheep Chaudhary"
                },
                "author": "Maheep Chaudhary"
            },
            {
                "id": "http://arxiv.org/abs/2602.16246v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16246v1",
                "title": "Toward Scalable Verifiable Reward: Proxy State-Based Evaluation for Multi-turn Tool-Calling LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Scalable Verifiable Reward: Proxy State-Based Evaluation for Multi-turn Tool-Calling LLM Agents"
                },
                "updated": "2026-02-18T07:49:47Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    7,
                    49,
                    47,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16246v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Interactive large language model (LLM) agents operating via multi-turn dialogue and multi-step tool calling are increasingly used in production. Benchmarks for these agents must both reliably compare models and yield on-policy training data. Prior agentic benchmarks (e.g., tau-bench, tau2-bench, AppWorld) rely on fully deterministic backends, which are costly to build and iterate. We propose Proxy State-Based Evaluation, an LLM-driven simulation framework that preserves final state-based evaluation without a deterministic database. Specifically, a scenario specifies the user goal, user/system facts, expected final state, and expected agent behavior, and an LLM state tracker infers a structured proxy state from the full interaction trace. LLM judges then verify goal completion and detect tool/user hallucinations against scenario constraints. Empirically, our benchmark produces stable, model-differentiating rankings across families and inference-time reasoning efforts, and its on-/off-policy rollouts provide supervision that transfers to unseen scenarios. Careful scenario specification yields near-zero simulator hallucination rates as supported by ablation studies. The framework also supports sensitivity analyses over user personas. Human-LLM judge agreement exceeds 90%, indicating reliable automated evaluation. Overall, proxy state-based evaluation offers a practical, scalable alternative to deterministic agentic benchmarks for industrial LLM agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interactive large language model (LLM) agents operating via multi-turn dialogue and multi-step tool calling are increasingly used in production. Benchmarks for these agents must both reliably compare models and yield on-policy training data. Prior agentic benchmarks (e.g., tau-bench, tau2-bench, AppWorld) rely on fully deterministic backends, which are costly to build and iterate. We propose Proxy State-Based Evaluation, an LLM-driven simulation framework that preserves final state-based evaluation without a deterministic database. Specifically, a scenario specifies the user goal, user/system facts, expected final state, and expected agent behavior, and an LLM state tracker infers a structured proxy state from the full interaction trace. LLM judges then verify goal completion and detect tool/user hallucinations against scenario constraints. Empirically, our benchmark produces stable, model-differentiating rankings across families and inference-time reasoning efforts, and its on-/off-policy rollouts provide supervision that transfers to unseen scenarios. Careful scenario specification yields near-zero simulator hallucination rates as supported by ablation studies. The framework also supports sensitivity analyses over user personas. Human-LLM judge agreement exceeds 90%, indicating reliable automated evaluation. Overall, proxy state-based evaluation offers a practical, scalable alternative to deterministic agentic benchmarks for industrial LLM agents."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T07:49:47Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    7,
                    49,
                    47,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Yun-Shiuan Chuang"
                    },
                    {
                        "name": "Chaitanya Kulkarni"
                    },
                    {
                        "name": "Alec Chiu"
                    },
                    {
                        "name": "Avinash Thangali"
                    },
                    {
                        "name": "Zijie Pan"
                    },
                    {
                        "name": "Shivani Shekhar"
                    },
                    {
                        "name": "Yirou Ge"
                    },
                    {
                        "name": "Yixi Li"
                    },
                    {
                        "name": "Uma Kona"
                    },
                    {
                        "name": "Linsey Pang"
                    },
                    {
                        "name": "Prakhar Mehrotra"
                    }
                ],
                "author_detail": {
                    "name": "Prakhar Mehrotra"
                },
                "author": "Prakhar Mehrotra"
            },
            {
                "id": "http://arxiv.org/abs/2509.19680v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.19680v2",
                "title": "PolicyPad: Collaborative Prototyping of LLM Policies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PolicyPad: Collaborative Prototyping of LLM Policies"
                },
                "updated": "2026-02-18T07:48:55Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    7,
                    48,
                    55,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.19680v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.19680v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As LLMs gain adoption in high-stakes domains like mental health, domain experts are increasingly consulted to provide input into policies governing their behavior. From an observation of 19 policymaking workshops with 9 experts over 15 weeks, we identified opportunities to better support rapid experimentation, feedback, and iteration for collaborative policy design processes. We present PolicyPad, an interactive system that facilitates the emerging practice of LLM policy prototyping by drawing from established UX prototyping practices, including heuristic evaluation and storyboarding. Using PolicyPad, policy designers can collaborate on drafting a policy in real time while independently testing policy-informed model behavior with usage scenarios. We evaluate PolicyPad through workshops with 8 groups of 22 domain experts in mental health and law, finding that PolicyPad enhanced collaborative dynamics during policy design, enabled tight feedback loops, and led to novel policy contributions. Overall, our work paves expert-informed paths for advancing AI alignment and safety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As LLMs gain adoption in high-stakes domains like mental health, domain experts are increasingly consulted to provide input into policies governing their behavior. From an observation of 19 policymaking workshops with 9 experts over 15 weeks, we identified opportunities to better support rapid experimentation, feedback, and iteration for collaborative policy design processes. We present PolicyPad, an interactive system that facilitates the emerging practice of LLM policy prototyping by drawing from established UX prototyping practices, including heuristic evaluation and storyboarding. Using PolicyPad, policy designers can collaborate on drafting a policy in real time while independently testing policy-informed model behavior with usage scenarios. We evaluate PolicyPad through workshops with 8 groups of 22 domain experts in mental health and law, finding that PolicyPad enhanced collaborative dynamics during policy design, enabled tight feedback loops, and led to novel policy contributions. Overall, our work paves expert-informed paths for advancing AI alignment and safety."
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-24T01:33:05Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    1,
                    33,
                    5,
                    2,
                    267,
                    0
                ],
                "arxiv_comment": "CHI 2026 paper. Supplementary materials: https://docs.google.com/document/d/1jBmKXusoWmCHfwpmNhSTJtbwZ5fwVWLNppKeqqd_-pY/edit?usp=sharing",
                "arxiv_primary_category": {
                    "term": "cs.HC"
                },
                "authors": [
                    {
                        "name": "K. J. Kevin Feng"
                    },
                    {
                        "name": "Tzu-Sheng Kuo"
                    },
                    {
                        "name": "Quan Ze"
                    },
                    {
                        "name": "Chen"
                    },
                    {
                        "name": "Inyoung Cheong"
                    },
                    {
                        "name": "Kenneth Holstein"
                    },
                    {
                        "name": "Amy X. Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Amy X. Zhang"
                },
                "arxiv_affiliation": "Jim",
                "author": "Amy X. Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2602.15720v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.15720v2",
                "title": "ToaSt: Token Channel Selection and Structured Pruning for Efficient ViT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ToaSt: Token Channel Selection and Structured Pruning for Efficient ViT"
                },
                "updated": "2026-02-18T07:44:58Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    7,
                    44,
                    58,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.15720v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.15720v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Vision Transformers (ViTs) have achieved remarkable success across various vision tasks, yet their deployment is often hindered by prohibitive computational costs. While structured weight pruning and token compression have emerged as promising solutions, they suffer from prolonged retraining times and global propagation that creates optimization challenges, respectively. We propose ToaSt, a decoupled framework applying specialized strategies to distinct ViT components. We apply coupled head-wise structured pruning to Multi-Head Self-Attention modules, leveraging attention operation characteristics to enhance robustness. For Feed-Forward Networks (over 60\\% of FLOPs), we introduce Token Channel Selection (TCS) that enhances compression ratios while avoiding global propagation issues. Our analysis reveals TCS effectively filters redundant noise during selection. Extensive evaluations across nine diverse models, including DeiT, ViT-MAE, and Swin Transformer, demonstrate that ToaSt achieves superior trade-offs between accuracy and efficiency, consistently outperforming existing baselines. On ViT-MAE-Huge, ToaSt achieves 88.52\\% accuracy (+1.64 \\%) with 39.4\\% FLOPs reduction. ToaSt transfers effectively to downstream tasks, achieving 52.2 versus 51.9 mAP on COCO object detection. Code and models will be released upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Transformers (ViTs) have achieved remarkable success across various vision tasks, yet their deployment is often hindered by prohibitive computational costs. While structured weight pruning and token compression have emerged as promising solutions, they suffer from prolonged retraining times and global propagation that creates optimization challenges, respectively. We propose ToaSt, a decoupled framework applying specialized strategies to distinct ViT components. We apply coupled head-wise structured pruning to Multi-Head Self-Attention modules, leveraging attention operation characteristics to enhance robustness. For Feed-Forward Networks (over 60\\% of FLOPs), we introduce Token Channel Selection (TCS) that enhances compression ratios while avoiding global propagation issues. Our analysis reveals TCS effectively filters redundant noise during selection. Extensive evaluations across nine diverse models, including DeiT, ViT-MAE, and Swin Transformer, demonstrate that ToaSt achieves superior trade-offs between accuracy and efficiency, consistently outperforming existing baselines. On ViT-MAE-Huge, ToaSt achieves 88.52\\% accuracy (+1.64 \\%) with 39.4\\% FLOPs reduction. ToaSt transfers effectively to downstream tasks, achieving 52.2 versus 51.9 mAP on COCO object detection. Code and models will be released upon acceptance."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-17T16:52:13Z",
                "published_parsed": [
                    2026,
                    2,
                    17,
                    16,
                    52,
                    13,
                    1,
                    48,
                    0
                ],
                "arxiv_comment": "8 pages, 5 figures",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Hyunchan Moon"
                    },
                    {
                        "name": "Cheonjun Park"
                    },
                    {
                        "name": "Steven L. Waslander"
                    }
                ],
                "author_detail": {
                    "name": "Steven L. Waslander"
                },
                "author": "Steven L. Waslander"
            },
            {
                "id": "http://arxiv.org/abs/2505.03901v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.03901v4",
                "title": "Unveiling the Role of ChatGPT in Software Development: Insights from Developer-ChatGPT Interactions on GitHub",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling the Role of ChatGPT in Software Development: Insights from Developer-ChatGPT Interactions on GitHub"
                },
                "updated": "2026-02-18T07:41:38Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    7,
                    41,
                    38,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.03901v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.03901v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The advent of Large Language Models (LLMs) has introduced a new paradigm in Software Engineering (SE), with generative AI tools like ChatGPT gaining widespread adoption among developers. While ChatGPT's potential has been extensively discussed, empirical evidence about how developers actually use LLMs' assistance in real-world practices remains limited. To bridge this gap, we conducted a large-scale empirical analysis of ChatGPT usage on GitHub, and we presented DevChat, a curated dataset of 2,547 publicly shared ChatGPT conversation links collected from GitHub between May 2023 and June 2024. Through comprehensively analyzing DevChat, we explored the characteristics of developer-ChatGPT interaction patterns and identified five key categories of developers' purposes for sharing developer-ChatGPT conversations during software development. Additionally, we investigated the dominant development-related activities in which ChatGPT is used, and presented a mapping framework that links GitHub data sources, development-related activities, and SE tasks. The findings show that interactions are typically short and task-focused (most are 1-3 turns); developers share conversations mainly to delegate tasks, resolve problems, and acquire knowledge, revealing five purpose categories; ChatGPT is most frequently engaged for Software Implementation and Maintenance & Evolution; we identified 39 fine-grained SE tasks supported by ChatGPT, with Code Generation & Completion as well as Code modification & Optimization being the most prominent. Our study offers a comprehensive mapping of ChatGPT's applications in real-world software development scenarios and provides a foundation for understanding LLMs' practical roles in software development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of Large Language Models (LLMs) has introduced a new paradigm in Software Engineering (SE), with generative AI tools like ChatGPT gaining widespread adoption among developers. While ChatGPT's potential has been extensively discussed, empirical evidence about how developers actually use LLMs' assistance in real-world practices remains limited. To bridge this gap, we conducted a large-scale empirical analysis of ChatGPT usage on GitHub, and we presented DevChat, a curated dataset of 2,547 publicly shared ChatGPT conversation links collected from GitHub between May 2023 and June 2024. Through comprehensively analyzing DevChat, we explored the characteristics of developer-ChatGPT interaction patterns and identified five key categories of developers' purposes for sharing developer-ChatGPT conversations during software development. Additionally, we investigated the dominant development-related activities in which ChatGPT is used, and presented a mapping framework that links GitHub data sources, development-related activities, and SE tasks. The findings show that interactions are typically short and task-focused (most are 1-3 turns); developers share conversations mainly to delegate tasks, resolve problems, and acquire knowledge, revealing five purpose categories; ChatGPT is most frequently engaged for Software Implementation and Maintenance & Evolution; we identified 39 fine-grained SE tasks supported by ChatGPT, with Code Generation & Completion as well as Code modification & Optimization being the most prominent. Our study offers a comprehensive mapping of ChatGPT's applications in real-world software development scenarios and provides a foundation for understanding LLMs' practical roles in software development."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-06T18:16:08Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    18,
                    16,
                    8,
                    1,
                    126,
                    0
                ],
                "arxiv_comment": "Preprint accepted for publication in ACM Transactions on Software Engineering and Methodology (TOSEM), 2026",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Ruiyin Li"
                    },
                    {
                        "name": "Peng Liang"
                    },
                    {
                        "name": "Yifei Wang"
                    },
                    {
                        "name": "Yangxiao Cai"
                    },
                    {
                        "name": "Weisong Sun"
                    },
                    {
                        "name": "Zengyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Zengyang Li"
                },
                "author": "Zengyang Li"
            },
            {
                "id": "http://arxiv.org/abs/2602.16241v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16241v1",
                "title": "Are LLMs Ready to Replace Bangla Annotators?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are LLMs Ready to Replace Bangla Annotators?"
                },
                "updated": "2026-02-18T07:36:41Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    7,
                    36,
                    41,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16241v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16241v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) are increasingly used as automated annotators to scale dataset creation, yet their reliability as unbiased annotators--especially for low-resource and identity-sensitive settings--remains poorly understood. In this work, we study the behavior of LLMs as zero-shot annotators for Bangla hate speech, a task where even human agreement is challenging, and annotator bias can have serious downstream consequences. We conduct a systematic benchmark of 17 LLMs using a unified evaluation framework. Our analysis uncovers annotator bias and substantial instability in model judgments. Surprisingly, increased model scale does not guarantee improved annotation quality--smaller, more task-aligned models frequently exhibit more consistent behavior than their larger counterparts. These results highlight important limitations of current LLMs for sensitive annotation tasks in low-resource languages and underscore the need for careful evaluation before deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used as automated annotators to scale dataset creation, yet their reliability as unbiased annotators--especially for low-resource and identity-sensitive settings--remains poorly understood. In this work, we study the behavior of LLMs as zero-shot annotators for Bangla hate speech, a task where even human agreement is challenging, and annotator bias can have serious downstream consequences. We conduct a systematic benchmark of 17 LLMs using a unified evaluation framework. Our analysis uncovers annotator bias and substantial instability in model judgments. Surprisingly, increased model scale does not guarantee improved annotation quality--smaller, more task-aligned models frequently exhibit more consistent behavior than their larger counterparts. These results highlight important limitations of current LLMs for sensitive annotation tasks in low-resource languages and underscore the need for careful evaluation before deployment."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T07:36:41Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    7,
                    36,
                    41,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Md. Najib Hasan"
                    },
                    {
                        "name": "Touseef Hasan"
                    },
                    {
                        "name": "Souvika Sarkar"
                    }
                ],
                "author_detail": {
                    "name": "Souvika Sarkar"
                },
                "author": "Souvika Sarkar"
            },
            {
                "id": "http://arxiv.org/abs/2602.16240v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16240v1",
                "title": "Submodular Maximization under Supermodular Constraint: Greedy Guarantees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Submodular Maximization under Supermodular Constraint: Greedy Guarantees"
                },
                "updated": "2026-02-18T07:33:51Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    7,
                    33,
                    51,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16240v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16240v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Motivated by a wide range of applications in data mining and machine learning, we consider the problem of maximizing a submodular function subject to supermodular cost constraints. In contrast to the well-understood setting of cardinality and matroid constraints, where greedy algorithms admit strong guarantees, the supermodular constraint regime remains poorly understood -- guarantees for greedy methods and other efficient algorithmic paradigms are largely open. We study this family of fundamental optimization problems under an upper-bound constraint on a supermodular cost function with curvature parameter $γ$. Our notion of supermodular curvature is less restrictive than prior definitions, substantially expanding the class of admissible cost functions. We show that our greedy algorithm that iteratively includes elements maximizing the ratio of the objective and constraint functions, achieves a $\\left(1 - e^{-(1-γ)}\\right)$-approximation before stopping. We prove that this approximation is indeed tight for this algorithm. Further, if the objective function has a submodular curvature $c$, then we show that the bound further improves to $\\left(1 - (1- (1-c)(1-γ))^{1/(1-c)}\\right)$, which can be further improved by continuing to violate the constraint. Finally, we show that the Greedy-Ratio-Marginal in conjunction with binary search leads to a bicriteria approximation for the dual problem -- minimizing a supermodular function under a lower bound constraint on a submodular function. We conduct a number of experiments on a simulation of LLM agents debating over multiple rounds -- the task is to select a subset of agents to maximize correctly answered questions. Our algorithm outperforms all other greedy heuristics, and on smaller problems, it achieves the same performance as the optimal set found by exhaustive search.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated by a wide range of applications in data mining and machine learning, we consider the problem of maximizing a submodular function subject to supermodular cost constraints. In contrast to the well-understood setting of cardinality and matroid constraints, where greedy algorithms admit strong guarantees, the supermodular constraint regime remains poorly understood -- guarantees for greedy methods and other efficient algorithmic paradigms are largely open. We study this family of fundamental optimization problems under an upper-bound constraint on a supermodular cost function with curvature parameter $γ$. Our notion of supermodular curvature is less restrictive than prior definitions, substantially expanding the class of admissible cost functions. We show that our greedy algorithm that iteratively includes elements maximizing the ratio of the objective and constraint functions, achieves a $\\left(1 - e^{-(1-γ)}\\right)$-approximation before stopping. We prove that this approximation is indeed tight for this algorithm. Further, if the objective function has a submodular curvature $c$, then we show that the bound further improves to $\\left(1 - (1- (1-c)(1-γ))^{1/(1-c)}\\right)$, which can be further improved by continuing to violate the constraint. Finally, we show that the Greedy-Ratio-Marginal in conjunction with binary search leads to a bicriteria approximation for the dual problem -- minimizing a supermodular function under a lower bound constraint on a submodular function. We conduct a number of experiments on a simulation of LLM agents debating over multiple rounds -- the task is to select a subset of agents to maximize correctly answered questions. Our algorithm outperforms all other greedy heuristics, and on smaller problems, it achieves the same performance as the optimal set found by exhaustive search."
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T07:33:51Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    7,
                    33,
                    51,
                    2,
                    49,
                    0
                ],
                "arxiv_comment": "16 pages, 9 figures",
                "arxiv_primary_category": {
                    "term": "cs.DS"
                },
                "authors": [
                    {
                        "name": "Ajitesh Srivastava"
                    },
                    {
                        "name": "Shanghua Teng"
                    }
                ],
                "author_detail": {
                    "name": "Shanghua Teng"
                },
                "author": "Shanghua Teng"
            },
            {
                "id": "http://arxiv.org/abs/2602.10577v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.10577v2",
                "title": "Campaign-2-PT-RAG: LLM-Guided Semantic Product Type Attribution for Scalable Campaign Ranking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Campaign-2-PT-RAG: LLM-Guided Semantic Product Type Attribution for Scalable Campaign Ranking"
                },
                "updated": "2026-02-18T07:12:10Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    7,
                    12,
                    10,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.10577v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.10577v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "E-commerce campaign ranking models require large-scale training labels indicating which users purchased due to campaign influence. However, generating these labels is challenging because campaigns use creative, thematic language that does not directly map to product purchases. Without clear product-level attribution, supervised learning for campaign optimization remains limited. We present Campaign-2-PT-RAG, a scalable label generation framework that constructs user-campaign purchase labels by inferring which product types (PTs) each campaign promotes. The framework first interprets campaign content using large language models (LLMs) to capture implicit intent, then retrieves candidate PTs through semantic search over the platform taxonomy. A structured LLM-based classifier evaluates each PT's relevance, producing a campaign-specific product coverage set. User purchases matching these PTs generate positive training labels for downstream ranking models. This approach reframes the ambiguous attribution problem into a tractable semantic alignment task, enabling scalable and consistent supervision for downstream tasks such as campaign ranking optimization in production e-commerce environments. Experiments on internal and synthetic datasets, validated against expert-annotated campaign-PT mappings, show that our LLM-assisted approach generates high-quality labels with 78-90% precision while maintaining over 99% recall.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "E-commerce campaign ranking models require large-scale training labels indicating which users purchased due to campaign influence. However, generating these labels is challenging because campaigns use creative, thematic language that does not directly map to product purchases. Without clear product-level attribution, supervised learning for campaign optimization remains limited. We present Campaign-2-PT-RAG, a scalable label generation framework that constructs user-campaign purchase labels by inferring which product types (PTs) each campaign promotes. The framework first interprets campaign content using large language models (LLMs) to capture implicit intent, then retrieves candidate PTs through semantic search over the platform taxonomy. A structured LLM-based classifier evaluates each PT's relevance, producing a campaign-specific product coverage set. User purchases matching these PTs generate positive training labels for downstream ranking models. This approach reframes the ambiguous attribution problem into a tractable semantic alignment task, enabling scalable and consistent supervision for downstream tasks such as campaign ranking optimization in production e-commerce environments. Experiments on internal and synthetic datasets, validated against expert-annotated campaign-PT mappings, show that our LLM-assisted approach generates high-quality labels with 78-90% precision while maintaining over 99% recall."
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-11T07:03:08Z",
                "published_parsed": [
                    2026,
                    2,
                    11,
                    7,
                    3,
                    8,
                    2,
                    42,
                    0
                ],
                "arxiv_comment": "fix typo and author names",
                "arxiv_primary_category": {
                    "term": "cs.IR"
                },
                "authors": [
                    {
                        "name": "Yiming Che"
                    },
                    {
                        "name": "Mansi Ranjit Mane"
                    },
                    {
                        "name": "Keerthi Gopalakrishnan"
                    },
                    {
                        "name": "Parisa Kaghazgaran"
                    },
                    {
                        "name": "Murali Mohana Krishna Dandu"
                    },
                    {
                        "name": "Archana Venkatachalapathy"
                    },
                    {
                        "name": "Sinduja Subramaniam"
                    },
                    {
                        "name": "Yokila Arora"
                    },
                    {
                        "name": "Evren Korpeoglu"
                    },
                    {
                        "name": "Sushant Kumar"
                    },
                    {
                        "name": "Kannan Achan"
                    }
                ],
                "author_detail": {
                    "name": "Kannan Achan"
                },
                "author": "Kannan Achan"
            },
            {
                "id": "http://arxiv.org/abs/2602.15010v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.15010v2",
                "title": "BPP: Long-Context Robot Imitation Learning by Focusing on Key History Frames",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BPP: Long-Context Robot Imitation Learning by Focusing on Key History Frames"
                },
                "updated": "2026-02-18T07:07:11Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    7,
                    7,
                    11,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.15010v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.15010v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Many robot tasks require attending to the history of past observations. For example, finding an item in a room requires remembering which places have already been searched. However, the best-performing robot policies typically condition only on the current observation, limiting their applicability to such tasks. Naively conditioning on past observations often fails due to spurious correlations: policies latch onto incidental features of training histories that do not generalize to out-of-distribution trajectories upon deployment. We analyze why policies latch onto these spurious correlations and find that this problem stems from limited coverage over the space of possible histories during training, which grows exponentially with horizon. Existing regularization techniques provide inconsistent benefits across tasks, as they do not fundamentally address this coverage problem. Motivated by these findings, we propose Big Picture Policies (BPP), an approach that conditions on a minimal set of meaningful keyframes detected by a vision-language model. By projecting diverse rollouts onto a compact set of task-relevant events, BPP substantially reduces distribution shift between training and deployment, without sacrificing expressivity. We evaluate BPP on four challenging real-world manipulation tasks and three simulation tasks, all requiring history conditioning. BPP achieves 70% higher success rates than the best comparison on real-world evaluations. Videos are available at https://bigpicturepolicies.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many robot tasks require attending to the history of past observations. For example, finding an item in a room requires remembering which places have already been searched. However, the best-performing robot policies typically condition only on the current observation, limiting their applicability to such tasks. Naively conditioning on past observations often fails due to spurious correlations: policies latch onto incidental features of training histories that do not generalize to out-of-distribution trajectories upon deployment. We analyze why policies latch onto these spurious correlations and find that this problem stems from limited coverage over the space of possible histories during training, which grows exponentially with horizon. Existing regularization techniques provide inconsistent benefits across tasks, as they do not fundamentally address this coverage problem. Motivated by these findings, we propose Big Picture Policies (BPP), an approach that conditions on a minimal set of meaningful keyframes detected by a vision-language model. By projecting diverse rollouts onto a compact set of task-relevant events, BPP substantially reduces distribution shift between training and deployment, without sacrificing expressivity. We evaluate BPP on four challenging real-world manipulation tasks and three simulation tasks, all requiring history conditioning. BPP achieves 70% higher success rates than the best comparison on real-world evaluations. Videos are available at https://bigpicturepolicies.github.io/"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-16T18:49:56Z",
                "published_parsed": [
                    2026,
                    2,
                    16,
                    18,
                    49,
                    56,
                    0,
                    47,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Max Sobol Mark"
                    },
                    {
                        "name": "Jacky Liang"
                    },
                    {
                        "name": "Maria Attarian"
                    },
                    {
                        "name": "Chuyuan Fu"
                    },
                    {
                        "name": "Debidatta Dwibedi"
                    },
                    {
                        "name": "Dhruv Shah"
                    },
                    {
                        "name": "Aviral Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Aviral Kumar"
                },
                "author": "Aviral Kumar"
            },
            {
                "id": "http://arxiv.org/abs/2506.04072v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.04072v2",
                "title": "Toward Beginner-Friendly LLMs for Language Learning: Controlling Difficulty in Conversation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Beginner-Friendly LLMs for Language Learning: Controlling Difficulty in Conversation"
                },
                "updated": "2026-02-18T06:39:26Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    6,
                    39,
                    26,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.04072v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.04072v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Practicing conversations with large language models (LLMs) presents a promising alternative to traditional in-person language learning. However, most LLMs generate text at a near-native level of complexity, making them ill-suited for first and second-year beginner learners (CEFR: A1-A2). In this paper, we investigate whether controllable generation techniques can adapt LLM outputs to better support beginners. We evaluate these methods through both automatic metrics and a user study with university-level learners of Japanese. Our findings show that while prompting alone fails, controllable generation techniques can successfully improve output comprehensibility for beginner speakers (from 39.4% to 83.3%). We further introduce a new token-level evaluation metric, Token Miss Rate (TMR), that quantifies the proportion of incomprehensible tokens per utterance and correlates strongly with human judgments. To support future research in AI-assisted language learning, we release our code, models, annotation tools, and dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Practicing conversations with large language models (LLMs) presents a promising alternative to traditional in-person language learning. However, most LLMs generate text at a near-native level of complexity, making them ill-suited for first and second-year beginner learners (CEFR: A1-A2). In this paper, we investigate whether controllable generation techniques can adapt LLM outputs to better support beginners. We evaluate these methods through both automatic metrics and a user study with university-level learners of Japanese. Our findings show that while prompting alone fails, controllable generation techniques can successfully improve output comprehensibility for beginner speakers (from 39.4% to 83.3%). We further introduce a new token-level evaluation metric, Token Miss Rate (TMR), that quantifies the proportion of incomprehensible tokens per utterance and correlates strongly with human judgments. To support future research in AI-assisted language learning, we release our code, models, annotation tools, and dataset."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-04T15:38:21Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    38,
                    21,
                    2,
                    155,
                    0
                ],
                "arxiv_comment": "EACL 2026",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Meiqing Jin"
                    },
                    {
                        "name": "Liam Dugan"
                    },
                    {
                        "name": "Chris Callison-Burch"
                    }
                ],
                "author_detail": {
                    "name": "Chris Callison-Burch"
                },
                "author": "Chris Callison-Burch"
            }
        ]
    }
]